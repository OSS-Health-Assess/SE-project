type,issue_url,comment_url,repo_name,id,issue_num,title,user_login,user_id,user_name,user_email,issue_state,created_at,updated_at,body,reactions
issue,https://api.github.com/repos/apache/horaedb/issues/5,https://api.github.com/repos/apache/horaedb/issues/5,horaedb,1250532113,5,Cache build artifacts to improve CI execution time,waynexia,15380403,Ruihang Xia,,CLOSED,2022-05-27T09:12:39Z,2022-08-15T02:47:08Z,"**Is your feature request related to a problem or challenge? Please describe what you are trying to do.**
CI is slow as it will build all crates twice

**Description**
Only build workspace once. And reuse the artifact to run tests.

**Proposal**
GitHub action provides a plugin [cache](https://github.com/actions/cache) to pass files across jobs that may help.

**Additional context**
Add any other context or screenshots about the feature request here.

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/5,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E5ltK,horaedb,1155947338,5,NA,waynexia,15380403,Ruihang Xia,,NA,2022-06-15T03:37:27Z,2022-06-15T03:37:27Z,"A brilliant post about how to speed up rust builds, CI cache included https://matklad.github.io/2021/09/04/fast-rust-builds.html","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E5ltK/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/11,https://api.github.com/repos/apache/horaedb/issues/11,horaedb,1254785774,11,Submodule is not supported in CI,waynexia,15380403,Ruihang Xia,,CLOSED,2022-06-01T03:08:40Z,2022-09-13T07:25:39Z,"**Describe this problem**
Submodule is not supported in CI

**Steps to reproduce**
Trigger CI, and some cases that need submodule will fail.

**Expected behavior**
CI should be able to pull submodule

**Additional Information**
GitHub Action plugin `checkout` has an option `with_submodule` which should work for us.

One failing test from CI:
```
{ ""type"": ""test"", ""name"": ""serialized_reader::tests::test_file_reader_with_cache"", ""event"": ""failed"", ""stdout"": ""thread 'serialized_reader::tests::test_file_reader_with_cache' panicked at 'failed to get parquet data dir: env `PARQUET_TEST_DATA` is undefined or has empty value, and the pre-defined data dir `/__w/ceresdb/ceresdb/components/parquet/../parquet-testing/data` not found\nHINT: try running `git submodule update --init`', components/parquet/src/tests.rs:55:21\nstack backtrace:\n   0: rust_begin_unwind\n             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/panicking.rs:498:5\n   1: core::panicking::panic_fmt\n             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/panicking.rs:107:14\n   2: parquet::tests::parquet_test_data\n             at ./src/tests.rs:55:21\n   3: parquet::tests::get_test_path\n             at ./src/tests.rs:61:42\n   4: parquet::tests::get_test_file\n             at ./src/tests.rs:68:16\n   5: parquet::serialized_reader::tests::new_filer_reader_with_cache\n             at ./src/serialized_reader.rs:450:25\n   6: parquet::serialized_reader::tests::test_file_reader_with_cache\n             at ./src/serialized_reader.rs:555:22\n   7: parquet::serialized_reader::tests::test_file_reader_with_cache::{{closure}}\n             at ./src/serialized_reader.rs:554:5\n   8: core::ops::function::FnOnce::call_once\n             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/ops/function.rs:227:5\n   9: core::ops::function::FnOnce::call_once\n             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/ops/function.rs:227:5\nnote: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.\n"" }
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/11/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/11,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5EIeoS,horaedb,1143073298,11,NA,waynexia,15380403,Ruihang Xia,,NA,2022-06-01T03:17:19Z,2022-06-01T03:17:19Z,I have ignored some test cases in #12. We should restore them once CI is fixed.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5EIeoS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/13,https://api.github.com/repos/apache/horaedb/issues/13,horaedb,1255140017,13,Tracking issue for OSS integration,waynexia,15380403,Ruihang Xia,,CLOSED,2022-06-01T06:47:53Z,2022-07-18T03:29:07Z,"## Target
- Support Aliyun OSS as storage backend.
- Able to extend OSS to other cloud services.

## Steps
- [x] #15
- [x] #19
- [x] #28
- [x] Local file system as test env #101 
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/13/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/15,https://api.github.com/repos/apache/horaedb/issues/15,horaedb,1257822062,15,Switch `ObjectStore` to upstream crate object_store,waynexia,15380403,Ruihang Xia,,CLOSED,2022-06-02T07:48:37Z,2022-06-07T04:47:36Z,"The object_store component and related interfaces were forked from https://github.com/influxdata/influxdb_iox/tree/main/object_store
Now the upstream has separated it into a standalone crate and evolved more feather-rich and ergonomic interfaces. It would be better to migrate to it.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/15/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/16,https://api.github.com/repos/apache/horaedb/issues/16,horaedb,1257830692,16,`cargo bench` will produce a compile error of titan-sys,waynexia,15380403,Ruihang Xia,,CLOSED,2022-06-02T07:57:07Z,2022-07-08T02:20:53Z,"**Describe this problem**
Run the following command under repo root dir will produce a compile error.
```shell
cargo bench -p benchmarks
```

**Steps to reproduce**
See above.

**Expected behavior**
can run benchmark

**Additional Information**

system: Linux ruihang 5.17.7-arch1-1
cpu: x86_64 Intel(R) Core(TM) i5-9500 CPU @ 3.00GHz

<details>
<summary> Compile Error Log </summary>

```
error: failed to run custom build command for `libtitan_sys v0.0.1 (https://github.com/tikv/rust-rocksdb.git?branch=tikv-5.2#23bd00d5)`

Caused by:
  process didn't exit successfully: `/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-acd593915e6f3c6c/build-script-build` (exit status: 101)
  --- stdout
  CMAKE_TOOLCHAIN_FILE_x86_64-unknown-linux-gnu = None
  CMAKE_TOOLCHAIN_FILE_x86_64_unknown_linux_gnu = None
  HOST_CMAKE_TOOLCHAIN_FILE = None
  CMAKE_TOOLCHAIN_FILE = None
  CMAKE_GENERATOR_x86_64-unknown-linux-gnu = None
  CMAKE_GENERATOR_x86_64_unknown_linux_gnu = None
  HOST_CMAKE_GENERATOR = None
  CMAKE_GENERATOR = None
  CMAKE_PREFIX_PATH_x86_64-unknown-linux-gnu = None
  CMAKE_PREFIX_PATH_x86_64_unknown_linux_gnu = None
  HOST_CMAKE_PREFIX_PATH = None
  CMAKE_PREFIX_PATH = Some(""/home/ruihang/repo/CeresDB/target/release/build/libz-sys-7060968b1392aa90/out/build"")
  CMAKE_x86_64-unknown-linux-gnu = None
  CMAKE_x86_64_unknown_linux_gnu = None
  HOST_CMAKE = None
  CMAKE = None
  running: ""cmake"" ""-Wdev"" ""--debug-output"" ""/home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan"" ""-DPORTABLE=ON"" ""-DROCKSDB_DIR=/home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb"" ""-DWITH_TITAN_TESTS=OFF"" ""-DWITH_TITAN_TOOLS=OFF"" ""-DWITH_ZLIB=ON"" ""-DWITH_BZ2=ON"" ""-DWITH_LZ4=ON"" ""-DWITH_ZSTD=ON"" ""-DWITH_SNAPPY=ON"" ""-DWITH_TITAN_TESTS=OFF"" ""-DWITH_TITAN_TOOLS=OFF"" ""-DCMAKE_INSTALL_PREFIX=/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out"" ""-DCMAKE_C_FLAGS= -ffunction-sections -fdata-sections -fPIC -m64"" ""-DCMAKE_C_COMPILER=/usr/bin/cc"" ""-DCMAKE_CXX_FLAGS= -ffunction-sections -fdata-sections -fPIC -m64"" ""-DCMAKE_CXX_COMPILER=/usr/bin/c++"" ""-DCMAKE_ASM_FLAGS= -ffunction-sections -fdata-sections -fPIC -m64"" ""-DCMAKE_ASM_COMPILER=/usr/bin/cc"" ""-DCMAKE_BUILD_TYPE=RelWithDebInfo"" ""-DCMAKE_VERBOSE_MAKEFILE:BOOL=ON""
  Running with debug output on.
  -- Enabling RTTI in Debug builds only (default)
     Called from: [2]   /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/cmake/rocksdb_flags.cmake
                  [1]   /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/CMakeLists.txt
  -- Configuring done
  -- Generating /home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build
     Called from: [1]   /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/CMakeLists.txt
  -- Generating done
  -- Build files have been written to: /home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build
  running: ""cmake"" ""--build"" ""."" ""--target"" ""titan"" ""--config"" ""RelWithDebInfo"" ""--parallel"" ""6""
  /usr/bin/cmake -S/home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan -B/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build --check-build-system CMakeFiles/Makefile.cmake 0
  /usr/bin/make  -f CMakeFiles/Makefile2 titan
  make[1]: Entering directory '/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build'
  /usr/bin/cmake -S/home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan -B/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build --check-build-system CMakeFiles/Makefile.cmake 0
  /usr/bin/cmake -E cmake_progress_start /home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles 25
  /usr/bin/make  -f CMakeFiles/Makefile2 CMakeFiles/titan.dir/all
  make[2]: Entering directory '/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build'
  /usr/bin/make  -f CMakeFiles/titan_build_version.dir/build.make CMakeFiles/titan_build_version.dir/depend
  make[3]: Entering directory '/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build'
  cd /home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build && /usr/bin/cmake -E cmake_depends ""Unix Makefiles"" /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan /home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build /home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build /home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan_build_version.dir/DependInfo.cmake --color=
  Dependencies file ""CMakeFiles/titan_build_version.dir/titan_build_version.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan_build_version.dir/compiler_depend.internal"".
  Consolidate compiler generated dependencies of target titan_build_version
  make[3]: Leaving directory '/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build'
  /usr/bin/make  -f CMakeFiles/titan_build_version.dir/build.make CMakeFiles/titan_build_version.dir/build
  make[3]: Entering directory '/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build'
  [  4%] Building CXX object CMakeFiles/titan_build_version.dir/titan_build_version.cc.o
  /usr/bin/c++ -DBZIP2 -DHAVE_PCLMUL -DHAVE_SSE42 -DLZ4 -DOS_LINUX -DROCKSDB_FALLOCATE_PRESENT -DROCKSDB_LIB_IO_POSIX -DROCKSDB_MALLOC_USABLE_SIZE -DROCKSDB_PLATFORM_POSIX -DROCKSDB_PTHREAD_ADAPTIVE_MUTEX -DROCKSDB_RANGESYNC_PRESENT -DROCKSDB_SCHED_GETCPU_PRESENT -DROCKSDB_SUPPORT_THREAD_LOCAL -DSNAPPY -DZLIB -DZSTD -I/home/ruihang/repo/CeresDB/target/release/build/snappy-sys-26a487dcbddf5af5/out/build -I/home/ruihang/repo/CeresDB/target/release/build/bzip2-sys-dee24ea472afde1a/out/include -I/home/ruihang/repo/CeresDB/target/release/build/lz4-sys-96e216fd13bc9b20/out/include -I/home/ruihang/repo/CeresDB/target/release/build/libz-sys-7060968b1392aa90/out/include -I/home/ruihang/repo/CeresDB/target/release/build/zstd-sys-b02e0fb93bcc16f8/out/include -I/home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb -I/home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/include -I/home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/include -I/home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/src -I/home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/util -ffunction-sections -fdata-sections -fPIC -m64 -W -Wextra -Wall -Wsign-compare -Wshadow -Wno-unused-parameter -Wno-unused-variable -Woverloaded-virtual -Wnon-virtual-dtor -Wno-missing-field-initializers -Wno-strict-aliasing -std=c++11 -fno-omit-frame-pointer -momit-leaf-frame-pointer -Werror -fno-builtin-memcmp -O2 -g -DNDEBUG -MD -MT CMakeFiles/titan_build_version.dir/titan_build_version.cc.o -MF CMakeFiles/titan_build_version.dir/titan_build_version.cc.o.d -o CMakeFiles/titan_build_version.dir/titan_build_version.cc.o -c /home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/titan_build_version.cc
  make[3]: Leaving directory '/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build'
  [  4%] Built target titan_build_version
  /usr/bin/make  -f CMakeFiles/titan.dir/build.make CMakeFiles/titan.dir/depend
  make[3]: Entering directory '/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build'
  cd /home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build && /usr/bin/cmake -E cmake_depends ""Unix Makefiles"" /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan /home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build /home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build /home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/DependInfo.cmake --color=
  Dependencies file ""CMakeFiles/titan.dir/src/base_db_listener.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/blob_file_builder.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/blob_file_cache.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/blob_file_iterator.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/blob_file_reader.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/blob_file_set.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/blob_file_size_collector.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/blob_format.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/blob_gc.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/blob_gc_job.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/blob_gc_picker.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/blob_storage.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/db.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/db_impl.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/db_impl_files.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/db_impl_gc.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/options.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/table_builder.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/table_factory.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/titan_checkpoint_impl.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/titan_stats.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/util.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Dependencies file ""CMakeFiles/titan.dir/src/version_edit.cc.o.d"" is newer than depends file ""/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build/CMakeFiles/titan.dir/compiler_depend.internal"".
  Consolidate compiler generated dependencies of target titan
  make[3]: Leaving directory '/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build'
  /usr/bin/make  -f CMakeFiles/titan.dir/build.make CMakeFiles/titan.dir/build
  make[3]: Entering directory '/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build'
  [  8%] Building CXX object CMakeFiles/titan.dir/src/db_impl.cc.o
  /usr/bin/c++ -DBZIP2 -DHAVE_PCLMUL -DHAVE_SSE42 -DLZ4 -DOS_LINUX -DROCKSDB_FALLOCATE_PRESENT -DROCKSDB_LIB_IO_POSIX -DROCKSDB_MALLOC_USABLE_SIZE -DROCKSDB_PLATFORM_POSIX -DROCKSDB_PTHREAD_ADAPTIVE_MUTEX -DROCKSDB_RANGESYNC_PRESENT -DROCKSDB_SCHED_GETCPU_PRESENT -DROCKSDB_SUPPORT_THREAD_LOCAL -DSNAPPY -DZLIB -DZSTD -I/home/ruihang/repo/CeresDB/target/release/build/snappy-sys-26a487dcbddf5af5/out/build -I/home/ruihang/repo/CeresDB/target/release/build/bzip2-sys-dee24ea472afde1a/out/include -I/home/ruihang/repo/CeresDB/target/release/build/lz4-sys-96e216fd13bc9b20/out/include -I/home/ruihang/repo/CeresDB/target/release/build/libz-sys-7060968b1392aa90/out/include -I/home/ruihang/repo/CeresDB/target/release/build/zstd-sys-b02e0fb93bcc16f8/out/include -I/home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb -I/home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/include -I/home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/include -I/home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/src -I/home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/util -ffunction-sections -fdata-sections -fPIC -m64 -W -Wextra -Wall -Wsign-compare -Wshadow -Wno-unused-parameter -Wno-unused-variable -Woverloaded-virtual -Wnon-virtual-dtor -Wno-missing-field-initializers -Wno-strict-aliasing -std=c++11 -fno-omit-frame-pointer -momit-leaf-frame-pointer -Werror -fno-builtin-memcmp -O2 -g -DNDEBUG -MD -MT CMakeFiles/titan.dir/src/db_impl.cc.o -MF CMakeFiles/titan.dir/src/db_impl.cc.o.d -o CMakeFiles/titan.dir/src/db_impl.cc.o -c /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/src/db_impl.cc
  make[3]: Leaving directory '/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build'
  make[2]: Leaving directory '/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build'
  make[1]: Leaving directory '/home/ruihang/repo/CeresDB/target/release/build/libtitan_sys-d9fa18b8ce830101/out/build'

  --- stderr
  make: warning: -j6 forced in submake: resetting jobserver mode.
  In file included from /usr/include/c++/12.1.0/atomic:41,
                   from /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/db/db_impl/db_impl.h:11,
                   from /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/src/db_impl.h:3,
                   from /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/src/db_impl.cc:1:
  In member function ‘std::__atomic_base<_IntTp>::__int_type std::__atomic_base<_IntTp>::fetch_add(__int_type, std::memory_order) [with _ITp = long unsigned int]’,
      inlined from ‘void rocksdb::StatisticsImpl<TICKER_MAX, HISTOGRAM_MAX>::recordTick(uint32_t, uint64_t) [with unsigned int TICKER_MAX = 145; unsigned int HISTOGRAM_MAX = 49]’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h:205:59,
      inlined from ‘void rocksdb::RecordTick(Statistics*, uint32_t, uint64_t)’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics.h:33:27,
      inlined from ‘virtual rocksdb::Status rocksdb::titandb::TitanDBImpl::FileManager::BatchFinishFiles(uint32_t, const std::vector<std::pair<std::shared_ptr<rocksdb::titandb::BlobFileMeta>, std::unique_ptr<rocksdb::titandb::BlobFileHandle> > >&)’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/src/db_impl.cc:67:17:
  /usr/include/c++/12.1.0/bits/atomic_base.h:618:35: error: array subscript 153 is above array bounds of ‘std::atomic_uint_fast64_t [145]’ {aka ‘std::atomic<long unsigned int> [145]’} [-Werror=array-bounds]
    618 |       { return __atomic_fetch_add(&_M_i, __i, int(__m)); }
        |                                   ^~~~~
  In file included from /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/src/db_impl.cc:17:
  /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h: In member function ‘virtual rocksdb::Status rocksdb::titandb::TitanDBImpl::FileManager::BatchFinishFiles(uint32_t, const std::vector<std::pair<std::shared_ptr<rocksdb::titandb::BlobFileMeta>, std::unique_ptr<rocksdb::titandb::BlobFileHandle> > >&)’:
  /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h:80:31: note: while referencing ‘rocksdb::StatisticsImpl<145, 49>::StatisticsData::tickers_’
     80 |     std::atomic_uint_fast64_t tickers_[TICKER_MAX];
        |                               ^~~~~~~~
  In member function ‘std::__atomic_base<_IntTp>::__int_type std::__atomic_base<_IntTp>::fetch_add(__int_type, std::memory_order) [with _ITp = long unsigned int]’,
      inlined from ‘void rocksdb::StatisticsImpl<TICKER_MAX, HISTOGRAM_MAX>::recordTick(uint32_t, uint64_t) [with unsigned int TICKER_MAX = 145; unsigned int HISTOGRAM_MAX = 49]’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h:205:59,
      inlined from ‘void rocksdb::RecordTick(Statistics*, uint32_t, uint64_t)’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics.h:33:27,
      inlined from ‘virtual void rocksdb::titandb::TitanDBIterator::SeekToLast()’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/src/db_iter.h:64:17:
  /usr/include/c++/12.1.0/bits/atomic_base.h:618:35: error: array subscript 146 is above array bounds of ‘std::atomic_uint_fast64_t [145]’ {aka ‘std::atomic<long unsigned int> [145]’} [-Werror=array-bounds]
    618 |       { return __atomic_fetch_add(&_M_i, __i, int(__m)); }
        |                                   ^~~~~
  /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h: In member function ‘virtual void rocksdb::titandb::TitanDBIterator::SeekToLast()’:
  /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h:80:31: note: while referencing ‘rocksdb::StatisticsImpl<145, 49>::StatisticsData::tickers_’
     80 |     std::atomic_uint_fast64_t tickers_[TICKER_MAX];
        |                               ^~~~~~~~
  In member function ‘std::__atomic_base<_IntTp>::__int_type std::__atomic_base<_IntTp>::fetch_add(__int_type, std::memory_order) [with _ITp = long unsigned int]’,
      inlined from ‘void rocksdb::StatisticsImpl<TICKER_MAX, HISTOGRAM_MAX>::recordTick(uint32_t, uint64_t) [with unsigned int TICKER_MAX = 145; unsigned int HISTOGRAM_MAX = 49]’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h:205:59,
      inlined from ‘void rocksdb::RecordTick(Statistics*, uint32_t, uint64_t)’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics.h:33:27,
      inlined from ‘virtual void rocksdb::titandb::TitanDBIterator::Prev()’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/src/db_iter.h:102:17:
  /usr/include/c++/12.1.0/bits/atomic_base.h:618:35: error: array subscript 148 is above array bounds of ‘std::atomic_uint_fast64_t [145]’ {aka ‘std::atomic<long unsigned int> [145]’} [-Werror=array-bounds]
    618 |       { return __atomic_fetch_add(&_M_i, __i, int(__m)); }
        |                                   ^~~~~
  /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h: In member function ‘virtual void rocksdb::titandb::TitanDBIterator::Prev()’:
  /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h:80:31: note: while referencing ‘rocksdb::StatisticsImpl<145, 49>::StatisticsData::tickers_’
     80 |     std::atomic_uint_fast64_t tickers_[TICKER_MAX];
        |                               ^~~~~~~~
  In member function ‘std::__atomic_base<_IntTp>::__int_type std::__atomic_base<_IntTp>::fetch_add(__int_type, std::memory_order) [with _ITp = long unsigned int]’,
      inlined from ‘void rocksdb::StatisticsImpl<TICKER_MAX, HISTOGRAM_MAX>::recordTick(uint32_t, uint64_t) [with unsigned int TICKER_MAX = 145; unsigned int HISTOGRAM_MAX = 49]’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h:205:59,
      inlined from ‘void rocksdb::RecordTick(Statistics*, uint32_t, uint64_t)’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics.h:33:27,
      inlined from ‘virtual void rocksdb::titandb::TitanDBIterator::Next()’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/src/db_iter.h:92:17:
  /usr/include/c++/12.1.0/bits/atomic_base.h:618:35: error: array subscript 147 is above array bounds of ‘std::atomic_uint_fast64_t [145]’ {aka ‘std::atomic<long unsigned int> [145]’} [-Werror=array-bounds]
    618 |       { return __atomic_fetch_add(&_M_i, __i, int(__m)); }
        |                                   ^~~~~
  /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h: In member function ‘virtual void rocksdb::titandb::TitanDBIterator::Next()’:
  /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h:80:31: note: while referencing ‘rocksdb::StatisticsImpl<145, 49>::StatisticsData::tickers_’
     80 |     std::atomic_uint_fast64_t tickers_[TICKER_MAX];
        |                               ^~~~~~~~
  In member function ‘std::__atomic_base<_IntTp>::__int_type std::__atomic_base<_IntTp>::fetch_add(__int_type, std::memory_order) [with _ITp = long unsigned int]’,
      inlined from ‘void rocksdb::StatisticsImpl<TICKER_MAX, HISTOGRAM_MAX>::recordTick(uint32_t, uint64_t) [with unsigned int TICKER_MAX = 145; unsigned int HISTOGRAM_MAX = 49]’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h:205:59,
      inlined from ‘void rocksdb::RecordTick(Statistics*, uint32_t, uint64_t)’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics.h:33:27,
      inlined from ‘virtual void rocksdb::titandb::TitanDBIterator::SeekToFirst()’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/src/db_iter.h:55:17:
  /usr/include/c++/12.1.0/bits/atomic_base.h:618:35: error: array subscript 146 is above array bounds of ‘std::atomic_uint_fast64_t [145]’ {aka ‘std::atomic<long unsigned int> [145]’} [-Werror=array-bounds]
    618 |       { return __atomic_fetch_add(&_M_i, __i, int(__m)); }
        |                                   ^~~~~
  /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h: In member function ‘virtual void rocksdb::titandb::TitanDBIterator::SeekToFirst()’:
  /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h:80:31: note: while referencing ‘rocksdb::StatisticsImpl<145, 49>::StatisticsData::tickers_’
     80 |     std::atomic_uint_fast64_t tickers_[TICKER_MAX];
        |                               ^~~~~~~~
  In member function ‘std::__atomic_base<_IntTp>::__int_type std::__atomic_base<_IntTp>::fetch_add(__int_type, std::memory_order) [with _ITp = long unsigned int]’,
      inlined from ‘void rocksdb::StatisticsImpl<TICKER_MAX, HISTOGRAM_MAX>::recordTick(uint32_t, uint64_t) [with unsigned int TICKER_MAX = 145; unsigned int HISTOGRAM_MAX = 49]’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h:205:59,
      inlined from ‘void rocksdb::RecordTick(Statistics*, uint32_t, uint64_t)’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics.h:33:27,
      inlined from ‘virtual void rocksdb::titandb::TitanDBIterator::Seek(const rocksdb::Slice&)’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/src/db_iter.h:73:17:
  /usr/include/c++/12.1.0/bits/atomic_base.h:618:35: error: array subscript 146 is above array bounds of ‘std::atomic_uint_fast64_t [145]’ {aka ‘std::atomic<long unsigned int> [145]’} [-Werror=array-bounds]
    618 |       { return __atomic_fetch_add(&_M_i, __i, int(__m)); }
        |                                   ^~~~~
  /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h: In member function ‘virtual void rocksdb::titandb::TitanDBIterator::Seek(const rocksdb::Slice&)’:
  /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h:80:31: note: while referencing ‘rocksdb::StatisticsImpl<145, 49>::StatisticsData::tickers_’
     80 |     std::atomic_uint_fast64_t tickers_[TICKER_MAX];
        |                               ^~~~~~~~
  In member function ‘std::__atomic_base<_IntTp>::__int_type std::__atomic_base<_IntTp>::fetch_add(__int_type, std::memory_order) [with _ITp = long unsigned int]’,
      inlined from ‘void rocksdb::StatisticsImpl<TICKER_MAX, HISTOGRAM_MAX>::recordTick(uint32_t, uint64_t) [with unsigned int TICKER_MAX = 145; unsigned int HISTOGRAM_MAX = 49]’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h:205:59,
      inlined from ‘void rocksdb::RecordTick(Statistics*, uint32_t, uint64_t)’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics.h:33:27,
      inlined from ‘virtual void rocksdb::titandb::TitanDBIterator::SeekForPrev(const rocksdb::Slice&)’ at /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/src/db_iter.h:82:17:
  /usr/include/c++/12.1.0/bits/atomic_base.h:618:35: error: array subscript 146 is above array bounds of ‘std::atomic_uint_fast64_t [145]’ {aka ‘std::atomic<long unsigned int> [145]’} [-Werror=array-bounds]
    618 |       { return __atomic_fetch_add(&_M_i, __i, int(__m)); }
        |                                   ^~~~~
  /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h: In member function ‘virtual void rocksdb::titandb::TitanDBIterator::SeekForPrev(const rocksdb::Slice&)’:
  /home/ruihang/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h:80:31: note: while referencing ‘rocksdb::StatisticsImpl<145, 49>::StatisticsData::tickers_’
     80 |     std::atomic_uint_fast64_t tickers_[TICKER_MAX];
        |                               ^~~~~~~~
  cc1plus: all warnings being treated as errors
  make[3]: *** [CMakeFiles/titan.dir/build.make:261: CMakeFiles/titan.dir/src/db_impl.cc.o] Error 1
  make[2]: *** [CMakeFiles/Makefile2:88: CMakeFiles/titan.dir/all] Error 2
  make[1]: *** [CMakeFiles/Makefile2:95: CMakeFiles/titan.dir/rule] Error 2
  make: *** [Makefile:172: titan] Error 2
  thread 'main' panicked at '
  command did not execute successfully, got: exit status: 2

  build script failed, must exit now', /home/ruihang/.cargo/registry/src/github.com-1ecc6299db9ec823/cmake-0.1.48/src/lib.rs:975:5
  note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
warning: build failed, waiting for other jobs to finish...
error: build failed
```

</details>
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/16/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/16,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E-ZjI,horaedb,1157208264,16,NA,paomian,2999156,localhost,,NA,2022-06-16T04:06:46Z,2022-06-16T04:06:46Z,"have the same error 
CPU Intel(R) Core(TM) i5-4300U CPU @ 1.90GHz
Fedora Linux 36 (Server Edition)
run `cargo build`

```
gmake: warning: -j4 forced in submake: resetting jobserver mode.
  In file included from /usr/include/c++/12/atomic:41,
                   from /home/paomian/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/7737841/librocksdb_sys/libtitan_sys/../rocksdb/db/db_impl/db_impl.h:11,
                   from /home/paomian/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/7737841/librocksdb_sys/libtitan_sys/titan/src/db_impl.h:3,
                   from /home/paomian/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/7737841/librocksdb_sys/libtitan_sys/titan/src/db_impl.cc:1:
  In member function ‘std::__atomic_base<_IntTp>::__int_type std::__atomic_base<_IntTp>::fetch_add(__int_type, std::memory_order) [with _ITp = long unsigned int]’,
      inlined from ‘void rocksdb::StatisticsImpl<TICKER_MAX, HISTOGRAM_MAX>::recordTick(uint32_t, uint64_t) [with unsigned int TICKER_MAX = 145; unsigned int HISTOGRAM_MAX = 49]’ at /home/paomian/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/7737841/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics_impl.h:205:59,
      inlined from ‘void rocksdb::RecordTick(Statistics*, uint32_t, uint64_t)’ at /home/paomian/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/7737841/librocksdb_sys/libtitan_sys/../rocksdb/monitoring/statistics.h:33:27,
      inlined from ‘virtual rocksdb::Status rocksdb::titandb::TitanDBImpl::FileManager::BatchFinishFiles(uint32_t, const std::vector<std::pair<std::shared_ptr<rocksdb::titandb::BlobFileMeta>, std::unique_ptr<rocksdb::titandb::BlobFileHandle> > >&)’ at /home/paomian/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/7737841/librocksdb_sys/libtitan_sys/titan/src/db_impl.cc:67:17:
  /usr/include/c++/12/bits/atomic_base.h:618:35: error: array subscript 153 is above array bounds of ‘std::atomic_uint_fast64_t [145]’ {aka ‘std::atomic<long unsigned int> [145]’} [-Werror=array-bounds]
    618 |       { return __atomic_fetch_add(&_M_i, __i, int(__m)); }
```
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E-ZjI/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/16,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FJzat,horaedb,1160197805,16,NA,waynexia,15380403,Ruihang Xia,,NA,2022-06-20T09:23:43Z,2022-06-20T09:23:43Z,"Thanks for your reporting @paomian. I think this is caused by GCC 12, is this your toolchain? (by `gcc --version`)

I can compile `rust-rocksdb` in my env after https://github.com/tikv/rust-rocksdb/pull/699. But we cannot bump rocksdb for now until #25 is merged (otherwise `zstd-sys` will conflict). I'll try it once #25 is done.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FJzat/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/16,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FOTBG,horaedb,1161375814,16,NA,paomian,2999156,localhost,,NA,2022-06-21T07:39:40Z,2022-06-21T07:39:40Z,"Yes my gcc version is 12.
But this error about bounds is confused.
```
gcc --version
gcc (GCC) 12.1.1 20220507 (Red Hat 12.1.1-1)
Copyright (C) 2022 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FOTBG/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/18,https://api.github.com/repos/apache/horaedb/issues/18,horaedb,1262771928,18,Rename `udf` sub-crate,waynexia,15380403,Ruihang Xia,,CLOSED,2022-06-07T06:12:02Z,2022-06-15T06:12:55Z,"**Description**
We have a sub-crate `udf` that contains some operator implementation for DataFusion. The name is some kinds of ambiguous, those operators are UDF from DF's perspective, but are the same as other DF build-in from the user. 

**Proposal**
Rename it, like `df-operator` or `df-function`. Or other appropriate names.

**Additional context**
 There is a `query_engine` sub-crate that also extends DF operators. Merge these two crates should also be an option.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/18/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/19,https://api.github.com/repos/apache/horaedb/issues/19,horaedb,1262844730,19,Support Aliyun OSS,waynexia,15380403,Ruihang Xia,,CLOSED,2022-06-07T07:26:15Z,2022-06-10T06:43:03Z,"**Description**

Add support for Aliyun OSS as object storage backend.

Currently we only use `LocalFileSystem`.

**Proposal**

Implement `ObjectStore` using `oss-rust-sdk`.

**Additional context**

Some interface are not supported because:
- `get_range()` is not implemented in sdk
- `list()` and `list_with_delimiter()` are not support in the test env.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/19/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/24,https://api.github.com/repos/apache/horaedb/issues/24,horaedb,1264416635,24,Security: `failure` reports a critical level vulnerability warning,waynexia,15380403,Ruihang Xia,,CLOSED,2022-06-08T08:54:37Z,2023-03-14T06:04:27Z,"**Describe this problem**
https://github.com/CeresDB/ceresdb/security/dependabot/2

**Steps to reproduce**
N/A

**Expected behavior**
Fix this warning

**Additional Information**
>An issue was discovered in the failure crate through 2019-11-13 for Rust. Type confusion can occur when private_get_type_id is overridden.

Dependabot doesn't find a patch version fixes this.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/24/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/24,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QGwwt,horaedb,1343949869,24,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-12-09T07:26:27Z,2022-12-09T07:26:27Z,"Currently, the very old `failure` crate is only used by obkv client, that is to say, we have to upgrade the dependencies of obkv client.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QGwwt/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/24,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Xdwcw,horaedb,1467418416,24,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-14T06:04:27Z,2023-03-14T06:04:27Z,Fixed by #701.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Xdwcw/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/25,https://api.github.com/repos/apache/horaedb/issues/25,horaedb,1265673704,25,"Bump Arrow, Parquet and DataFusion",waynexia,15380403,Ruihang Xia,,CLOSED,2022-06-09T06:48:09Z,2022-06-23T08:31:45Z,"**Description**

Bump versions.
Target:
- Arrow: 13.0.0
- Parquet: 13.0.0
- DataFusion: 8.0.0

Current:
- Arrow: 7.0.0
- Parquet: 7.0.0
- DataFusion: a fork based on 6.0.0


**Proposal**
Maybe you have considered some ideas or solutions about this feature.

**Additional context**

Changelogs
- DataFusion:
  - 7.0.0: https://arrow.apache.org/blog/2022/02/28/datafusion-7.0.0/
  - 8.0.0: https://arrow.apache.org/blog/2022/05/16/datafusion-8.0.0/

Notable Changes:
- DataFusion
  - The `ExecutionPlan` trait is no longer async
  - `RuntimeEnv` is now `TaskContext`, contains more task-relative info like `SessionId` and `TaskId`
  - Bugfix: temp file

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/25/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/27,https://api.github.com/repos/apache/horaedb/issues/27,horaedb,1267245514,27,add method to flush memtable manually,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-06-10T08:42:18Z,2022-06-16T09:43:05Z,"**Is your feature request related to a problem or challenge? Please describe what you are trying to do.**

Add method to flush memtable manually

**Description**
Currently memtable flush is controlled by [should_flush_table](https://github.com/ceresdb/CeresDB/blob/33e95a42277fced611ca50d39e4f3541d1555430/analytic_engine/src/table/data.rs#L381), it would be convenient to provide a method to flush memtable explicitly, to release memory pressure or to persist data to underlying objectstore 

**Proposal**
add a HTTP endpoint to flush memtable
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/27/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/27,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E2RRy,horaedb,1155077234,27,NA,evenyag,7058520,Yingwen,,NA,2022-06-14T11:47:29Z,2022-06-14T11:47:29Z,"Seems `Table` supports [manual flush](https://github.com/CeresDB/ceresdb/blob/8667e4c9859ce3b5fd04e075add05ec154e4f7c6/table_engine/src/table.rs#L457) , maybe the HTTP endpoint can just call `Table::flush()`.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E2RRy/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/27,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E2xgR,horaedb,1155209233,27,NA,waynexia,15380403,Ruihang Xia,,NA,2022-06-14T13:46:29Z,2022-06-14T13:46:29Z,"> Seems `Table` supports [manual flush](https://github.com/CeresDB/ceresdb/blob/8667e4c9859ce3b5fd04e075add05ec154e4f7c6/table_engine/src/table.rs#L457) , maybe the HTTP endpoint can just call `Table::flush()`.

That sounds good! :+1:  Would you like to submit a Pull Request @evenyag ?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E2xgR/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/27,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E5huz,horaedb,1155931059,27,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-06-15T03:05:21Z,2022-06-15T03:05:21Z,"Thanks for your interest, I have started this feature in my forked repo, maybe you can try other issues first. 
- https://github.com/jiacai2050/ceresdb/commits/feat-flush-memtable","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E5huz/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/28,https://api.github.com/repos/apache/horaedb/issues/28,horaedb,1267270181,28,Cache file in local file system,waynexia,15380403,Ruihang Xia,,CLOSED,2022-06-10T09:05:25Z,2022-06-15T11:16:57Z,"
# Target
Provides a local file system cache layer over object store

# Design

Add an `ObjectStore` implementor `CachedStore`, which is made up of:
- Local Store: a local file system cache
- Remote Store: a remote OSS service
- LRU Manager: a cache eviction policy (LRU) to keep the local store size under control.

## Read

On serving a read request, the `CachedStore` will first check if the object is in the local store. If not, it will fetch it from the remote store and store it in the local store. To simplify the implementation, all the read sources returned are based on `LocalStore`. Ascii art below:

Workflow
```
(read)
       Serve I/O requests
              ▲ │
            4 │ │1
              │ │
   ┌──────────┴─▼─────────┐
   │                      │
   │  Object Store Cache  │
   │                      │
   └─┬─▲────────────┬─▲───┘
  2.1│ │3.1      2.2│ │3.2
     │ │            │ │
 ┌───▼─┴──┐      ┌──▼─┴─────┐
 │Local FS│      │Remote OSS│
 └────────┘      └──────────┘
```

## Write

For write requests, we will write the content to both underlying stores.

## Restart

To suit some deploy scenarios that aren't stateless, `ObjectStore` will try to load all existing entries from `LocalStore` to `LRU Manager`.

## Purge

Both read and write operations may trigger purge on `LocalStore`. The purge policy is defined by the `LRU Manager`. It will get a delete list from `LRU Manager` and delete the list from `LocalStore`.

To ensure the total size of `LocalStore` is always less than the threshold, `CachedStore` will first purge enough space for the incoming new objects.

# Other features

## Config
Except for the basic configurations like the underlying provider of `LocalStore` and `RemoteStore`, threshold of `LocalStore` etc., we can also configure other behaviors like whether to cache write request, how to act on restart or cache evict policy.

## Async Cache
It is doable to feed the data from `RemoteStore` directly to the user and write `LocalStore` asynchronously.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/28/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/34,https://api.github.com/repos/apache/horaedb/issues/34,horaedb,1269160327,34,Failed to build on macOS Montery,v0y4g3r,6406592,"Lei, HUANG",,CLOSED,2022-06-13T09:28:21Z,2022-06-14T04:37:30Z,"**Describe this problem**

Build dependency librocksdb failed, compiler complains `unused-but-set-variable`
```
  /Users/lei/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/libtitan_sys/titan/src/db_impl.cc:105:14: error: variable 'file_size' set but not used [-Werror,-Wunused-but-set-variable]
      uint64_t file_size = 0;
               ^
  1 error generated.
  make[3]: *** [CMakeFiles/titan.dir/src/db_impl.cc.o] Error 1
  make[3]: *** Waiting for unfinished jobs....
  make[2]: *** [CMakeFiles/titan.dir/all] Error 2
  make[1]: *** [CMakeFiles/titan.dir/rule] Error 2
  make: *** [titan] Error 2
  thread 'main' panicked at '
  command did not execute successfully, got: exit status: 2

  build script failed, must exit now', /Users/lei/.cargo/registry/src/github.com-1ecc6299db9ec823/cmake-0.1.48/src/lib.rs:975:5
  note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
warning: build failed, waiting for other jobs to finish...
error: build failed
```

C/CXX version
```
$ cc -v
Apple clang version 13.1.6 (clang-1316.0.21.2.5)
Target: arm64-apple-darwin21.5.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin
```

Rust toolchain version
```
$ rustc --version
rustc 1.59.0-nightly (f1ce0e6a0 2022-01-05)
```

System version

```
$ sw_vers
ProductName:	macOS
ProductVersion:	12.4
BuildVersion:	21F7
```

**Steps to reproduce**

1. Clone repo
2. Install required dependencies including ssl/cmake
3. cargo build

**Expected behavior**

Successfully build binaries.


**Additional Information**
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/34/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/34,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ExAvN,horaedb,1153698765,34,NA,v0y4g3r,6406592,"Lei, HUANG",,NA,2022-06-13T09:38:00Z,2022-06-13T09:38:00Z,"As a hacky workaround, one can add warning suppress flags by altering cargo cached CMakeLists.txt.

`-Wno-unused-but-set-variable -Wno-unknown-warning-option -Wno-unused-const-variable`

```
sed -i 's/-W -Wextra -Wall/-W -Wextra -Wall  -Wno-unused-but-set-variable -Wno-unknown-warning-option -Wno-unused-const-variable/g' $HOME/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/23bd00d/librocksdb_sys/rocksdb/CMakeLists.txt
```

And it compiles.

```
$ cargo build
   Compiling grpcio-sys v0.9.1+1.38.0
   Compiling librocksdb_sys v0.1.0 (https://github.com/tikv/rust-rocksdb.git?branch=tikv-5.2#23bd00d5)
   Compiling rocksdb v0.3.0 (https://github.com/tikv/rust-rocksdb.git?branch=tikv-5.2#23bd00d5)
   Compiling grpcio v0.9.1
   Compiling grpcio v0.1.0 (/Users/lei/Workspace/Rust/ceresdb/grpcio)
   Compiling ceresdbproto v0.1.0 (https://github.com/CeresDB/ceresdbproto.git#a7db8811)
   Compiling logger v0.1.0 (/Users/lei/Workspace/Rust/ceresdb/components/logger)
   Compiling common_util v0.1.0 (/Users/lei/Workspace/Rust/ceresdb/common_util)
   Compiling table_engine v0.1.0 (/Users/lei/Workspace/Rust/ceresdb/table_engine)
   Compiling udf v0.1.0 (/Users/lei/Workspace/Rust/ceresdb/udf)
   Compiling wal v0.1.0 (/Users/lei/Workspace/Rust/ceresdb/wal)
   Compiling catalog v0.1.0 (/Users/lei/Workspace/Rust/ceresdb/catalog)
   Compiling analytic_engine v0.1.0 (/Users/lei/Workspace/Rust/ceresdb/analytic_engine)
   Compiling sql v0.1.0 (/Users/lei/Workspace/Rust/ceresdb/sql)
   Compiling system_catalog v0.1.0 (/Users/lei/Workspace/Rust/ceresdb/system_catalog)
   Compiling meta_client v0.1.0 (/Users/lei/Workspace/Rust/ceresdb/meta_client)
   Compiling catalog_impls v0.1.0 (/Users/lei/Workspace/Rust/ceresdb/catalog_impls)
   Compiling query_engine v0.1.0 (/Users/lei/Workspace/Rust/ceresdb/query_engine)
   Compiling interpreters v0.1.0 (/Users/lei/Workspace/Rust/ceresdb/interpreters)
   Compiling server v0.1.0 (/Users/lei/Workspace/Rust/ceresdb/server)
   Compiling ceresdb v0.1.0 (/Users/lei/Workspace/Rust/ceresdb)
    Finished dev [unoptimized + debuginfo] target(s) in 1m 20s
```


But it would be better not to hack cmake file directly.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ExAvN/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/34,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ExFEO,horaedb,1153716494,34,NA,v0y4g3r,6406592,"Lei, HUANG",,NA,2022-06-13T09:56:19Z,2022-06-13T09:56:19Z,"And it won't work if I switch toolchain to GNU/GCC, because when building boringssl required by grpcio, the CMakeLists.txt detects if system is ""APPLE"", and if so, it will pass a clang-specific flag '-stdlib=libc++' which can not be recognized by gcc toolchain.

line 17~19 in `$HOME/.cargo/registry/src/github.com-1ecc6299db9ec823/boringssl-src-0.3.0+688fc5c/boringssl/CMakeLists.txt`
```
if(APPLE)
     set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -stdlib=libc++"")
endif()
```

see also : [C++11 support for GNU/gcc](https://gcc.gnu.org/projects/cxx-status.html#cxx11)","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ExFEO/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/34,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ExGwK,horaedb,1153723402,34,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-06-13T10:03:38Z,2022-06-13T10:03:38Z,"`-Wunused-but-set-variable` seems a new diagnostic flag in version 13 of clang:
https://clang.llvm.org/docs/DiagnosticsReference.html#wunused-but-set-parameter

Such flag does not appear in version 12 of clang:
https://releases.llvm.org/12.0.0/tools/clang/docs/DiagnosticsReference.html","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ExGwK/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/34,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ExHVw,horaedb,1153725808,34,NA,v0y4g3r,6406592,"Lei, HUANG",,NA,2022-06-13T10:06:07Z,2022-06-13T10:06:07Z,"> `-Wunused-but-set-variable` seems a new diagnostic flag in version 13 of clang: https://clang.llvm.org/docs/DiagnosticsReference.html#wunused-but-set-parameter
> 
> Such flag does not appear in version 12 of clang: https://releases.llvm.org/12.0.0/tools/clang/docs/DiagnosticsReference.html

Downgrade to clang-12 doesn't really solve the problem because there will be more warning raised including `unused-const-variable`","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ExHVw/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/34,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ExHhg,horaedb,1153726560,34,NA,waynexia,15380403,Ruihang Xia,,NA,2022-06-13T10:06:47Z,2022-06-13T10:06:47Z,"Thanks for your reporting!

GCC toolchain also has a problem with version 12 (in #16), it might not work either with the correct options. We are tracking this issue.

The first clang-related issue is also reported to upstream https://github.com/tikv/rust-rocksdb/issues/696

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ExHhg/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/34,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ExypV,horaedb,1153903189,34,NA,waynexia,15380403,Ruihang Xia,,NA,2022-06-13T13:17:23Z,2022-06-13T13:17:23Z,"Hi @RayneHwang, could you please try #36 to see if it works?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ExypV/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/34,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5EyHan,horaedb,1153988263,34,NA,v0y4g3r,6406592,"Lei, HUANG",,NA,2022-06-13T14:23:03Z,2022-06-13T14:23:03Z,"> Hi @RayneHwang, could you please try #36 to see if it works?

It now compiles :)","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5EyHan/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/35,https://api.github.com/repos/apache/horaedb/issues/35,horaedb,1269164209,35,CeresDB WeChat QR code,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-06-13T09:31:30Z,2022-06-14T12:25:13Z,![image](https://gitee.com/ceres-db/assets/raw/master/wechat20220613.png),"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/35/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/39,https://api.github.com/repos/apache/horaedb/issues/39,horaedb,1270715405,39,Version Release,li-jin-gou,97824201,kinggo,lilong.21@bytedance.com,CLOSED,2022-06-14T12:16:42Z,2022-06-14T17:30:25Z,"**Which part is this question about**
Version Release
<!---
Is it code base, usage, deployment, documentation or some other part?
-->

**Describe your question**
Will a version be released soon?  https://github.com/CeresDB/ceresdb/releases
<!---
A clear and concise description of what the question is.
-->

**Additional context**

<!---
Add any other context about the problem here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/39/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/43,https://api.github.com/repos/apache/horaedb/issues/43,horaedb,1272939050,43,add mysql server support,dust1,18304424,Yoke,834902408@qq.com,CLOSED,2022-06-16T01:36:09Z,2022-06-27T09:43:58Z,"**Description**

ceresdb doesn't seem to support mysql client yet

**Proposal**

we can use this crate solves that problem by acting as a MySQL server, and delegating operations such as querying and query execution to user-defined logic.

**Additional context**

https://github.com/jonhoo/msql-srv
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/43/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/43,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E-S0l,horaedb,1157180709,43,NA,waynexia,15380403,Ruihang Xia,,NA,2022-06-16T03:07:17Z,2022-06-16T03:07:17Z,"Greeting @dust1 🤗

Your proposal sounds really cool and feasible, do you want to elaborate/implement it? We are interested in this feature though it is not in our current plans. And we are happy to offer any level of assistance.

Please do let us know if this matches your interests as well!","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E-S0l/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/43,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E-UJI,horaedb,1157186120,43,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-06-16T03:18:41Z,2022-06-16T03:18:41Z,"@waynexia Thanks for taking my suggestion. I can try to implement it. I will try to submit a PR this weekend. I don't have much database experience, maybe need your help later. ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E-UJI/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/43,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E-WOa,horaedb,1157194650,43,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-06-16T03:33:54Z,2022-06-16T03:33:54Z,"> @waynexia Thanks for taking my suggestion. I can try to implement it. I will try to submit a PR this weekend. I don't have much database experience, maybe need your help later.

No need to worry about it.🤗

Here are some suggestions I can provide before you starting:
- The mysql service may be set in the `Server` struct https://github.com/CeresDB/ceresdb/blob/dc9b2067330fcb1afb980195b21f7d9300c44306/server/src/server.rs#L64
- The implementation of the delegation may refer to the http service https://github.com/CeresDB/ceresdb/blob/dc9b2067330fcb1afb980195b21f7d9300c44306/server/src/http.rs#L95","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E-WOa/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/43,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E-Wvy,horaedb,1157196786,43,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-06-16T03:38:39Z,2022-06-16T03:38:39Z,"@ShiKaiWi thanks, important suggestions for me! ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E-Wvy/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/43,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E-XZH,horaedb,1157199431,43,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-06-16T03:45:05Z,2022-06-16T03:45:05Z,"FYI, there is another MySQL server wrapper:  https://github.com/datafuselabs/opensrv

It would be better you compare those two crates before you start, thanks.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E-XZH/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/43,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E-Y14,horaedb,1157205368,43,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-06-16T04:00:42Z,2022-06-16T04:00:42Z,"> FYI, there is another MySQL server wrapper: https://github.com/datafuselabs/opensrv
> 
> It would be better you compare those two crates before you start, thanks.

get it!","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5E-Y14/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/45,https://api.github.com/repos/apache/horaedb/issues/45,horaedb,1273128048,45,add inverted index of timeseries,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-06-16T06:46:17Z,2023-03-02T13:50:10Z,"**Description**

Add the inverted index of the timeseries to optimize the time series query for a small number of timelines. Similar to [tsi](https://github.com/influxdata/influxdb/blob/master/tsdb/index/tsi1/doc.go).

**Proposal**

Build:
Option 1: Add index to memtable and build it when data is written.
Option 2: Build when memtable flushes to sst.

Storage:
Option 1: Store in parquet metadata.
Option 2: Store in separate file.

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/45/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/45,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FGypI,horaedb,1159408200,45,NA,aierui,16207137,YIXIAO SHI,aieruishi@gmail.com,NA,2022-06-18T09:32:12Z,2022-06-18T09:32:12Z,"iox 已经去掉了 inverted index, 全靠 datafusion 执行引擎 table scan","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FGypI/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/45,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FIyjQ,horaedb,1159932112,45,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2022-06-20T03:36:27Z,2022-06-20T03:36:27Z,"> iox 已经去掉了 inverted index, 全靠 datafusion 执行引擎 table scan

In some scenes(like point query used in alerts), batch scan may be much more expensive compared to scaning/getting filterd rows with the inverted index.
And now inverted index feature is currently in the poc stage, and we will decide the next step based on specific performance tests.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FIyjQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/45,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FJ_YC,horaedb,1160246786,45,NA,aierui,16207137,YIXIAO SHI,aieruishi@gmail.com,NA,2022-06-20T10:09:42Z,2022-06-20T10:09:42Z,"@chunshao90 Thank your reply,  I'm looking forward to this feature.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FJ_YC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/45,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Wiid-,horaedb,1451894654,45,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-02T13:50:10Z,2023-03-02T13:50:10Z,Duplicated with https://github.com/CeresDB/ceresdb/issues/628,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Wiid-/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/52,https://api.github.com/repos/apache/horaedb/issues/52,horaedb,1274560127,52,CI: check diff on `Cargo.lock`,waynexia,15380403,Ruihang Xia,,CLOSED,2022-06-17T04:40:57Z,2022-08-29T07:17:15Z,"**Description**
Add CI job to check file diff of `Cargo.lock`.

We are tracking `Cargo.lock` with SCM, and should also check it in CI. An incorrect (usually is just not up-to-date) lock file won't fail a build but is annoying.

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

**Proposal**

diff lock file after `cargo build`. Fail if there is any diff.

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/52/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/53,https://api.github.com/repos/apache/horaedb/issues/53,horaedb,1274640587,53,try to replace parquet with parquet2?,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-06-17T06:48:33Z,2022-12-20T01:49:00Z,"**Description**

replace parquet with [parquet2](https://github.com/jorgecarleitao/parquet2)

The five main differentiators in comparison with `parquet` are:
* it uses `#![forbid(unsafe_code)]`
* delegates parallelism downstream
* decouples reading (IO intensive) from computing (CPU intensive)
* it is faster (10-20x when reading to arrow format)
* supports `async` read and write.
* It is integration-tested against pyarrow and (py)spark 3

**Proposal**


**Additional context**

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/53/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/53,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FDiIT,horaedb,1158554131,53,NA,waynexia,15380403,Ruihang Xia,,NA,2022-06-17T06:53:27Z,2022-06-17T06:53:27Z,"One more benefit: switching to `parquet2` can decouple our `parquet` dependence with datafusion, we can update them separately.

But migrating this kind of dep is a big work...","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FDiIT/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/53,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ImN-s,horaedb,1217978284,53,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-08-17T12:59:53Z,2022-08-17T12:59:53Z,"One more reason to migrate to parquet2.

[ArrowWriter](https://docs.rs/parquet/20.0.0/parquet/arrow/arrow_writer/struct.ArrowWriter.html) have no method to retrieve inner writer, but parquet2 does have [into_inner](https://docs.rs/parquet2/0.15.1/parquet2/write/struct.FileWriter.html#method.into_inner)","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ImN-s/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/53,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5IqdZw,horaedb,1219090032,53,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-08-18T06:32:44Z,2022-08-18T06:32:44Z,"Report to upstream:
- https://github.com/apache/arrow-rs/issues/2491","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5IqdZw/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/53,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Iq0Vx,horaedb,1219183985,53,NA,tustvold,1781103,Raphael Taylor-Davies,,NA,2022-08-18T08:25:15Z,2022-08-18T08:25:15Z,"👏 parquet maintainer here, FWIW there is little read performance difference these days that I have been able to reproduce, there is mature support for decoupled IO (async), we integration test against pyarrow, and recent work by myself and others to add page and row-level filter pushdown should dramatically improve the performance of filtered scans.

There are definitely areas to improve, most notably the writer hasn't had the same degree of attention, but by working together we can pull the whole ecosystem along 😀

Anyway enough from me, just thought I'd provide an alternative narrative to the parquet2/arrow2 FUD...","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Iq0Vx/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/53,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5IueWs,horaedb,1220142508,53,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-08-19T01:34:35Z,2022-08-19T01:34:35Z,"@tustvold Awesome work. It's seems we need to re-evaluate the performance of parquet.

> but by working together we can pull the whole ecosystem along 😀

We would love to share what we learn when build CeresDB, and keep communicating with upstream ecosystem to make it better.  🍺","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5IueWs/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/57,https://api.github.com/repos/apache/horaedb/issues/57,horaedb,1276256901,57,Docuement about ceresdb architecuture,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-06-20T01:55:21Z,2022-07-18T09:36:28Z,"**Description**
A document for architecture about ceresdb should be provided because the document is necessary for community deveplopers to start participating in development.
<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

**Proposal**
Provide a document describing the basic architecture of ceresdb.
<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/57/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/58,https://api.github.com/repos/apache/horaedb/issues/58,horaedb,1277045652,58,Document supported hardware architectures,zamazan4ik,7355383,Alexander Zaitsev,zamazan4ik@tut.by,CLOSED,2022-06-20T14:49:25Z,2022-06-29T02:31:00Z,"Hi!

Could you please put the information about supported architectures to the documentation please? E.e. about supported architectures for different operating systems, some specific requirements to the supported instructions, if you have any (e.g. maybe AVX is required - I do not know).

This kind of information is important for the end-users.

Thanks in advance!

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/58/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/58,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FLWHo,horaedb,1160602088,58,NA,waynexia,15380403,Ruihang Xia,,NA,2022-06-20T15:43:44Z,2022-06-20T15:43:44Z,"Hello @zamazan4ik :grinning:

Answer first, to my understanding CeresDB currently doesn't use unreplaceable special instructions. Our dependencies like [arrow](https://github.com/apache/arrow-rs/blob/master/arrow/README.md#features) may make use of SIMD when supported. We currently only test and run CeresDB on x86 Linux (based on CentOS7) so I'm not sure whether ARM is supported. Since we haven't leveraged new kernel features, I guess other recent versions are also supported. We will ensure compilation on macOS, but Windows support is not in our plan.

Hope this is useful to you. I'll complete the document ASAP.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FLWHo/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/58,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FNFKK,horaedb,1161056906,58,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-06-21T01:33:31Z,2022-06-21T01:33:31Z,"@zamazan4ik If you have more questions, welcome to join our slack channel
- https://join.slack.com/t/ceresdbcommunity/shared_invite/zt-1au1ihbdy-5huC9J9s2462yBMIWmerTw","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FNFKK/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/59,https://api.github.com/repos/apache/horaedb/issues/59,horaedb,1279872280,59,`SingleDistinctToGroupBy` panicked at No field named 'SPM_23134806_INFLUENCE_DEFAULT.period',waynexia,15380403,Ruihang Xia,,CLOSED,2022-06-22T09:48:45Z,2023-02-01T10:51:50Z,"**Describe this problem**

<!---
What this problem is and what happened.
-->

<details>
  <summary>Backtrace</summary>
  
```
2022-03-10 11:42:59.471 ERRO [common_util/src/panic.rs:40] thread 'cse-read' panicked 'called `Result::unwrap()` on an `Err` value: Plan(""No field named 'SPM_23134806_INFLUENCE_DEFAULT.period'. Valid fields are 'time_bucket(SPM_23134806_INFLUENCE_DEFAULT.period,Utf8(\""PT1M\""),Utf8(\""yyyy-MM-dd HH:mm:ss\""))', 'alias1'."")' at ""/root/.cargo/git/checkouts/arrow-datafusion-78dc86050a92b8a7/9a77541/datafusion/src/optimizer/single_distinct_to_groupby.rs:106""
   0: common_util::panic::set_panic_hook::{{closure}}
             at /app/ceresdb/ceresdbx/common_util/src/panic.rs:39:18
   1: std::panicking::rust_panic_with_hook
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/panicking.rs:610:17
   2: std::panicking::begin_panic_handler::{{closure}}
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/panicking.rs:502:13
   3: std::sys_common::backtrace::__rust_end_short_backtrace
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/sys_common/backtrace.rs:139:18
   4: rust_begin_unwind
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/panicking.rs:498:5
   5: core::panicking::panic_fmt
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/panicking.rs:107:14
   6: core::result::unwrap_failed
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/result.rs:1690:5
   7: core::result::Result<T,E>::unwrap
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/result.rs:1018:23
      datafusion::optimizer::single_distinct_to_groupby::optimize::{{closure}}
             at /root/.cargo/git/checkouts/arrow-datafusion-78dc86050a92b8a7/9a77541/datafusion/src/optimizer/single_distinct_to_groupby.rs:106:41
      core::iter::adapters::map::map_fold::{{closure}}
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/iter/adapters/map.rs:84:28
      core::ops::function::impls::<impl core::ops::function::FnMut<A> for &mut F>::call_mut
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/ops/function.rs:269:13
      core::iter::traits::iterator::Iterator::fold
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/iter/traits/iterator.rs:2171:21
      <core::iter::adapters::chain::Chain<A,B> as core::iter::traits::iterator::Iterator>::fold
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/iter/adapters/chain.rs:119:19
   8: <core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/iter/adapters/map.rs:124:9
      core::iter::traits::iterator::Iterator::for_each
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/iter/traits/iterator.rs:736:9
      <alloc::vec::Vec<T,A> as alloc::vec::spec_extend::SpecExtend<T,I>>::spec_extend
             at
```


</details>

**Steps to reproduce**

<!---
How to reproduce this problem.
-->

SQL:
```SQL
SELECT   time_bucket(`period`, \'PT1M\', \'yyyy-MM-dd HH:mm:ss\') AS `time`,
         count(DISTINCT(pid))                                     AS `pid`
FROM     spm_23134806_influence_default
WHERE    `period` >= \'2022-03-10 10:41:00\'
AND      `period` < \'2022-03-10 11:42:00\'
GROUP BY time_bucket(`period`, \'PT1M\', \'yyyy-MM-dd HH:mm:ss\')
ORDER BY `time`
```

**Expected behavior**

<!---
What is expected.
-->

Do not panic

**Additional Information**

<!---
If possible, please attach other context you think may relate to this problem. Like *runtime environment*, *modified config*, *error log* etc.
-->
To avoid this we have temporary comment out the optimizer `SingleDistinctToGroupBy`.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/59/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/59,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Go77K,horaedb,1185136330,59,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-15T03:26:53Z,2022-07-15T03:26:53Z,https://github.com/apache/arrow-datafusion/pull/2909 allows skipping failed opt rules. But this case will panic...,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Go77K/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/59,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HlApY,horaedb,1200884312,59,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-08-01T08:27:04Z,2022-08-01T08:27:04Z,"Report to upstream
- https://github.com/apache/arrow-datafusion/issues/2994","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HlApY/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/59,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JeVLH,horaedb,1232687815,59,NA,waynexia,15380403,Ruihang Xia,,NA,2022-08-31T09:19:22Z,2022-08-31T09:19:22Z,"I tried apache/arrow-datafusion#3305 in my local env and it can pass:

```plaintext
~/repo/CeresDB main *5 !3 ?1                                                                                                                                       ruihang@ruihang 17:15:19
> curl --location --request POST 'http://127.0.0.1:5000/sql' \
--header 'Content-Type: application/json' \
--data-raw '{
    ""query"": ""CREATE TABLE `spm_23134806_influence_default` (`pid` double NOT NULL, `period` timestamp NOT NULL, TIMESTAMP KEY(period)) ENGINE=Analytic with (enable_ttl='\''false'\'')""
}'
{""affected_rows"":0}%


~/repo/CeresDB main *5 !3 ?1                                                                                                                                       ruihang@ruihang 17:16:38
> curl --location --request POST 'http://127.0.0.1:5000/sql' \
--header 'Content-Type: application/json' \
--data-raw '{
    ""query"": ""select * from spm_23134806_influence_default""
}'
{""rows"":[]}%


~/repo/CeresDB main *5 !3 ?1                                                                                                                                       ruihang@ruihang 17:16:46
> curl --location --request POST 'http://127.0.0.1:5000/sql' \
--header 'Content-Type: application/json' \
--data-raw '{
    ""query"": ""SELECT   time_bucket(`period`, '\''PT1M'\'', '\''yyyy-MM-dd HH:mm:ss'\'') AS `time`, count(DISTINCT(pid)) AS `pid` FROM     spm_23134806_influence_default WHERE    `period` >= '\''2022-03-10 10:41:00'\'' AND      `period` < '\''2022-03-10 11:42:00'\'' GROUP BY time_bucket(`period`, '\''PT1M'\'', '\''yyyy-MM-dd HH:mm:ss'\'') ORDER BY `time`""
}'
{""rows"":[]}%
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JeVLH/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/59,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5LJQwg,horaedb,1260719136,59,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-09-28T10:40:06Z,2022-09-28T10:40:06Z,"After #269, this should be fixed now 🚀
- https://github.com/apache/arrow-datafusion/blob/master/datafusion/CHANGELOG.md#1200-2022-09-12","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5LJQwg/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/59,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5LJSTD,horaedb,1260725443,59,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-09-28T10:46:27Z,2022-09-28T10:46:27Z,"Ooop, the optimizer rule is still commented out, we should enable it again
https://github.com/CeresDB/ceresdb/blob/e5995be68e318fd66061c7034d6d1633081e4351/query_engine/src/context.rs#L77","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5LJSTD/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/60,https://api.github.com/repos/apache/horaedb/issues/60,horaedb,1281384508,60,Query fields are sorted by creation,dust1,18304424,Yoke,834902408@qq.com,CLOSED,2022-06-23T00:29:52Z,2022-07-18T06:26:20Z,"**Description**
ceresDB doesn't seem to require strict ordering of query result fields.

**Proposal**

Use the order in which fields are created.

**Additional context**

when I create table use: 
```
CREATE TABLE `hello_mysql` (`name` string TAG, `value` double NOT NULL, `t` timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE=Analytic with (enable_ttl='false');
```
and select it. his field order seems to be random? I found that the query result is wrapped in HashMap.
![image](https://user-images.githubusercontent.com/18304424/175182397-bf4b73ce-8cc2-4e44-9dda-d97ba4992267.png)

This doesn't seem to have any effect on normal usage, so I don't know if this situation needs improvement.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/60/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/60,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FXrpQ,horaedb,1163835984,60,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-06-23T01:53:08Z,2022-06-23T01:53:08Z,"AFAIK, most databases don't support strict ordering of query result unless specified by `ORDER BY`. 
- https://stackoverflow.com/questions/8746519/sql-what-is-the-default-order-by-of-queries

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FXrpQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/60,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FXsbk,horaedb,1163839204,60,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-06-23T01:59:40Z,2022-06-23T01:59:40Z,"> AFAIK, most databases don't support strict ordering of query result unless specified by `ORDER BY`.
> 
> * https://stackoverflow.com/questions/8746519/sql-what-is-the-default-order-by-of-queries

ORDER BY should be the result of the query, I mean the order of the fields in the query rather than the order of the data.
The order of fields I specified in `CREATE TABLE` is: `name`, `value`, `t` and `tsid`. do we need to follow this order when returning query results, His current order is: `value`, `tsid`, `name` and `t`","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FXsbk/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/60,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FXuEz,horaedb,1163845939,60,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-06-23T02:13:29Z,2022-06-23T02:13:29Z,"Sorry missing that.

I think query result should match the column ordering when created, I'm a little annoyed with this recently.

```
{
  ""rows"": [
    {
      ""tsid"": 18356650801274184101,
      ""timestamp"": 1655693797000,
      ""value"": 1.0,
      ""ip"": ""1"",
      ""dc"": ""lgab""
    },
    {
      ""value"": 2.0,
      ""ip"": ""1"",
      ""timestamp"": 1655693798000,
      ""dc"": ""lgab"",
      ""tsid"": 18356650801274184101
    }
  ]
}
```
As you can see above, it a little hard to read `value` column as it appears in different locations. ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FXuEz/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/60,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FXvSB,horaedb,1163850881,60,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-06-23T02:22:33Z,2022-06-23T02:22:33Z,"Maybe we can replace HashMap with another data structure https://github.com/datafuselabs/databend/blob/main/common/datablocks/src/data_block.rs#L29
But I haven't understood the storage principle of ceresdb in detail.😂 I don't know what this modification will involve.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FXvSB/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/60,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FXz6v,horaedb,1163869871,60,NA,waynexia,15380403,Ruihang Xia,,NA,2022-06-23T03:01:22Z,2022-06-23T03:01:22Z,"Replace hashmap will not infect the storage layer. Query result is ordered until https://github.com/CeresDB/ceresdb/blob/2fda373f49b17269464afe41b275fcf0ca2cc497/server/src/handlers/sql.rs#L118-L123

`Response`:
https://github.com/CeresDB/ceresdb/blob/2fda373f49b17269464afe41b275fcf0ca2cc497/server/src/handlers/sql.rs#L30-L35

There may have two steps of this change I think:
 - Change `convert_output` and  `Response`, replace that hashmap to something like
```rust
struct Rows {
    column_names: Vec<String>,
    data: Vec<Vec<Datum>>,
}
```
- HTTP interface still returns the map above (but keeps ordering). So we need to change the serialize method of the new `Response`.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FXz6v/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/60,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FX09D,horaedb,1163874115,60,NA,waynexia,15380403,Ruihang Xia,,NA,2022-06-23T03:10:24Z,2022-06-23T03:10:24Z,"BTW we can do this in the subsequent PR, #56 needs not to include this😼","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FX09D/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/60,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FX1Ty,horaedb,1163875570,60,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-06-23T03:13:38Z,2022-06-23T03:13:38Z,"Yes, I stumbled across it last night while testing, so filed an issue. 😂","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FX1Ty/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/60,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GWv4n,horaedb,1180368423,60,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-07-11T12:48:09Z,2022-07-11T12:48:09Z,"hello @jiacai2050 ,Are you dealing with this issue now? if there is not, can I try do this?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GWv4n/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/63,https://api.github.com/repos/apache/horaedb/issues/63,horaedb,1283371339,63,Support `aarch64-unknown-linux-gnu` target,waynexia,15380403,Ruihang Xia,,CLOSED,2022-06-24T07:27:58Z,2023-03-02T13:49:01Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

Add support for `aarch64-unknown-linux-gnu`

**Proposal**

Support build on `aarch64-unknown-linux-gnu ` first. Probably add a new CI job to cover this.

Running on this arch is planned but with a lower priority. 

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/63/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/63,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HTgbM,horaedb,1196295884,63,NA,zwpaper,3764335,Wei Zhang,,NA,2022-07-27T05:55:49Z,2022-07-27T05:55:49Z,"how can we confirm that a target is supported?

I have an aarch64 mac, I can build and run the CeresDB locally, and it seems that most of the functions work as expected besides an SST error log occurred.

I run the CRUD example on README, and get all the correct responses.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HTgbM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/63,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HTjCv,horaedb,1196306607,63,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-07-27T06:12:57Z,2022-07-27T06:12:57Z,"Maybe there are some issues caused by cross compiling to address if we choose to build CeresDB by github actions, e.g. https://github.com/CeresDB/ceresdb-client-py/issues/20.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HTjCv/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/63,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HTmAK,horaedb,1196318730,63,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-27T06:31:42Z,2022-07-27T06:31:42Z,">how can we confirm that a target is supported?

We need to ensure chip, runtime os and dev toolchain are supported. To list a target as ""supported"" I think there are two steps:
- Have CI to prevent untested regression.
- Have the capacity and willingness to solve problems on that target quickly.

But sadly we don't have these two conditions 😣 So the target is ""maybe support"", or in other words, is Tire-2 priority compared to amd64-linux","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HTmAK/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/63,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HUrgc,horaedb,1196603420,63,NA,zwpaper,3764335,Wei Zhang,,NA,2022-07-27T11:24:42Z,2022-07-27T11:24:42Z,"haha, of course, CeresDB should focus on the rapid development currently","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HUrgc/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/63,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WiiF-,horaedb,1451893118,63,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-02T13:49:01Z,2023-03-02T13:49:01Z,"Closing since this issue has been quiet for a while, free free to reopen if others have requirements depending on this.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WiiF-/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/65,https://api.github.com/repos/apache/horaedb/issues/65,horaedb,1283423314,65,Formatter can't recognize newline missing,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-06-24T08:20:23Z,2023-03-02T13:52:49Z,"**Description**
Now the rust formatter can't recognize the necessary newline missing, e.g.:
```rust
use crate::{
    instance::{Instance, InstanceRef},
    mysql::{error::*, worker::MysqlWorker},
};
pub struct MysqlService<C, Q> {
```

Make the formatter can fix this case.
<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

**Proposal**
No proposal yet.
<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/65/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/65,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Fomeg,horaedb,1168271264,65,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-06-28T06:11:49Z,2022-06-28T06:11:49Z,"There is a tracking issue in rustfmt:
- https://github.com/rust-lang/rustfmt/issues/3382
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Fomeg/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/65,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ForGM,horaedb,1168290188,65,NA,waynexia,15380403,Ruihang Xia,,NA,2022-06-28T06:34:48Z,2022-06-28T06:34:48Z,"current version of `blank_lines_lower_bound` is not what we want: https://rust-lang.github.io/rustfmt/?version=v1.5.0&search=#blank_lines_lower_bound

its example behavior is a bit wired
```rust
fn foo() {

    println!(""a"");
}

fn bar() {

    println!(""b"");

    println!(""c"");
}
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ForGM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/65,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FousR,horaedb,1168304913,65,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-06-28T06:52:35Z,2022-06-28T06:52:35Z,"> current version of blank_lines_lower_bound is not what we want

[This comment](https://github.com/rust-lang/rustfmt/issues/3382#issuecomment-864698337) says it's already been fixed in this https://github.com/rust-lang/rustfmt/pull/4295, just not released...","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FousR/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/65,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FozB4,horaedb,1168322680,65,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-06-28T07:11:03Z,2022-06-28T07:11:03Z,"I think [`rust-lang/rustfmt#4295`](https://github.com/rust-lang/rustfmt/pull/4295) is merged to the `rustfmt-v2-rc` branch, which is not going to release.

So the current release(`rustfmt-v1`) does not contain this patch.

`rustfmt-v2` tracking issue:
 rust-lang/rustfmt#3887","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FozB4/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/65,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WijV9,horaedb,1451898237,65,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-02T13:52:36Z,2023-03-02T13:52:36Z,Closing since this won't be fixed with rustfmt-v1,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WijV9/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/66,https://api.github.com/repos/apache/horaedb/issues/66,horaedb,1285387220,66,Support show database show tables,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-06-27T07:43:19Z,2022-07-07T03:38:33Z,"**Description**

support `show database` and `show tables`

**Proposal**

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/66/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/66,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Fn3Wp,horaedb,1168078249,66,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-06-28T00:33:35Z,2022-06-28T00:33:35Z,"does ceresdb support `create database`? Sorry I can't test it right now, but there is no relevant sql example in README.md","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Fn3Wp/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/66,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FoHpG,horaedb,1168144966,66,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-06-28T02:31:54Z,2022-06-28T02:31:54Z,"> does ceresdb support `create database`? Sorry I can't test it right now, but there is no relevant sql example in README.md

Actually, database is called schema in ceresdb and now it can't be created dynamically and can only be configured statically.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FoHpG/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/66,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FoIhK,horaedb,1168148554,66,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-06-28T02:37:46Z,2022-06-28T02:37:46Z,"> Actually, database is called schema in ceresdb and now it can't be created dynamically and can only be configured statically.

Thanks for your answer, maybe I can do this issue, can you assign it to me?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FoIhK/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/67,https://api.github.com/repos/apache/horaedb/issues/67,horaedb,1285457481,67,Does ceresDB support prometheus remote_write data input ?,Buddruggy,32037815,Alan Liang,,CLOSED,2022-06-27T08:44:31Z,2022-06-30T12:04:55Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->
Does ceresDB support prometehus remote_write data input?

https://prometheus.io/docs/prometheus/latest/configuration/configuration/?spm=a2c63.p38356.0.0.1c73743dfdpdeF#remote_write

**Proposal**

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/67/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/67,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FkElM,horaedb,1167083852,67,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2022-06-27T09:04:29Z,2022-06-27T09:04:29Z,We are considering implementing CeresDB as Prometheus remote storage. @waynexia will provide an RFC.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FkElM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/67,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Fk1ZD,horaedb,1167283779,67,NA,waynexia,15380403,Ruihang Xia,,NA,2022-06-27T12:20:31Z,2022-06-27T12:20:31Z,"I'm drafting a proposal but it's mainly about remote read. Writable storage may come later. Though it's not planned in present, it should not be very different from the read path.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Fk1ZD/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/67,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Fuw5S,horaedb,1169886802,67,NA,Buddruggy,32037815,Alan Liang,,NA,2022-06-29T11:51:37Z,2022-06-29T11:51:37Z,"> I'm drafting a proposal but it's mainly about remote read. Writable storage may come later. Though it's not planned in present, it should not be very different from the read path.

OK, thanks for reply! 

And i have another question about the performance of ceresDB, Is there a performance test report comparing influxDB /clickhouse/druid...?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Fuw5S/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/67,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Fx1a2,horaedb,1170691766,67,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-06-30T02:51:31Z,2022-06-30T02:51:31Z,"Hi, the project is still in rapid development, so we haven't done much benchmark work. 

BTW, I would like to know what features do you care most for a Prometheus remote storage?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Fx1a2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/67,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Fy9_M,horaedb,1170989004,67,NA,Buddruggy,32037815,Alan Liang,,NA,2022-06-30T09:32:39Z,2022-06-30T09:32:39Z,"> Hi, the project is still in rapid development, so we haven't done much benchmark work.
> 
> BTW, I would like to know what features do you care most for a Prometheus remote storage?

OK,  i see.

For a prometheus storage, the most thing i care about is the performance, And can be compatible with prometheus query and write protocol. 

From README of ceresDB, i find `ceresDB can handle both time-series and analytics workloads.`, this is very important to me. Because most tsdb cannot handle high cardinality discrete time series data. I have tried many tsdb including prometheus/thanos/victoria metircs..., these tsdb's performance can not meet my requirements. victoria metrics can handle most case, but if data is not typical timeseries data, victoria metrics's performance start to get bad, so i want try ceresDB.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Fy9_M/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/67,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FzH1E,horaedb,1171029316,67,NA,waynexia,15380403,Ruihang Xia,,NA,2022-06-30T10:12:04Z,2022-06-30T10:12:04Z,"Thanks for your interest, this is also the point where we start. Traditional pure time-series system has many limitations in high cardinality datasets. This pains us a lot as well. We want to bring our experience and practice gained in the internal monitor storage system to CeresDB to solve this problem.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5FzH1E/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/67,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Fzh4o,horaedb,1171136040,67,NA,Buddruggy,32037815,Alan Liang,,NA,2022-06-30T12:04:49Z,2022-06-30T12:04:49Z,"OK, I am looking forward!","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Fzh4o/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/68,https://api.github.com/repos/apache/horaedb/issues/68,horaedb,1286914684,68,Implement follower flush,waynexia,15380403,Ruihang Xia,,CLOSED,2022-06-28T07:33:06Z,2022-07-05T06:18:14Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

`flush` dumps memtable to sst file.

CeresDB current doesn't distinguish behaviors between leader and follower. In follower it only needs to purge its memtables and skip dumping.

**Proposal**

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

Implement the follower flush behavior described above. Serval steps I come up with:
- [ ] Receive and track the max sequence number flushed by leader
- [ ] Flush (drop) memtable based on the max seq
- [ ] (considering) Flush behavior after becoming the new leader.

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->
This issue may only cover the implementation part. The trigger will come with a follow up PR, which depends on WAL replication
and/or instance role.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/68/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/70,https://api.github.com/repos/apache/horaedb/issues/70,horaedb,1287215944,70,Add bounds `Send` and `Sync` to some traits,waynexia,15380403,Ruihang Xia,,CLOSED,2022-06-28T11:42:21Z,2022-08-18T03:17:24Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

Some common traits don't bound by `Send` + `Sync` in definition but are required (every place) where they are used, like `WalManager`.

**Proposal**

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

Bounding those traits:
```rust
#[async_trait]
pub trait WalManager: LogWriter + LogReader + Debug + Send + Sync {}
```

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/70/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/70,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5IqHFi,horaedb,1218998626,70,NA,waynexia,15380403,Ruihang Xia,,NA,2022-08-18T03:17:24Z,2022-08-18T03:17:24Z,"This has been done in #163
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5IqHFi/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/71,https://api.github.com/repos/apache/horaedb/issues/71,horaedb,1288095961,71,Define Error as associate type in WalManager,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-06-29T02:31:39Z,2022-07-19T02:58:02Z,"**Description**
define Error as associate type in WalManager

**Proposal**
source code：
https://github.com/CeresDB/ceresdb/blob/1e2e0b362b1d3afb3cef5350adb8dd5118db3e28/wal/src/manager.rs#L260
refer to: https://github.com/CeresDB/ceresdb/blob/839948bd6a551f4af35246eba01ad30d5e773ddd/analytic_engine/src/memtable/key.rs#L118

**Additional context**","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/71/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/71,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GwqTn,horaedb,1187161319,71,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-07-18T11:36:50Z,2022-07-18T11:36:50Z,can this be assigned to me?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GwqTn/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/71,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GxnOO,horaedb,1187410830,71,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-07-18T13:20:52Z,2022-07-18T13:20:52Z,"> can this be assigned to me?

Sure! Just pay attention to that [this issue](https://github.com/CeresDB/ceresdb/issues/72#issuecomment-1181427851) may refactor the `WalManager` at the same time.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GxnOO/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/71,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G162G,horaedb,1188539782,71,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-19T02:58:02Z,2022-07-19T02:58:02Z,Close as discussed in https://github.com/CeresDB/ceresdb/pull/106#issuecomment-1188504653,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G162G/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/72,horaedb,1288105160,72,Refactor EngineBuilder to remove duplicate code,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-06-29T02:49:19Z,2023-02-09T11:21:19Z,"**Description**
Refactor EngineBuilder to remove duplicate code

**Proposal**
code: 
https://github.com/CeresDB/ceresdb/blob/890fecf10848a3d09b485af52ff516deccf771fc/analytic_engine/src/setup.rs#L92
https://github.com/CeresDB/ceresdb/blob/890fecf10848a3d09b485af52ff516deccf771fc/analytic_engine/src/setup.rs#L120
https://github.com/CeresDB/ceresdb/blob/890fecf10848a3d09b485af52ff516deccf771fc/analytic_engine/src/setup.rs#L155

**Additional context**","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/72/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GPpg0,horaedb,1178507316,72,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-07-08T03:36:10Z,2022-07-08T03:36:10Z,"hi, chunshao90.

It seems we can make `Wal` as an `associate type` of EngineBuilder, then the build logic can be reused, like:
```rust
#[async_trait]
pub trait EngineBuilder: Default + Send + Sync {
    type Wal: WalManager + Send + Sync + 'static;

    /// Build the analytic engine from `config` and `engine_runtimes`.
    async fn build(
        &self,
        config: Config,
        engine_runtimes: Arc<EngineRuntimes>,
    ) -> Result<TableEngineRef> {
        assert!(!config.obkv_wal.enable);

        match config.storage {
            crate::storage_options::StorageOptions::Local(ref opts) => {
                let store = open_storage_local(opts.clone()).await?;
                let instance = self
                    .open_instance::<LocalFileSystem>(config.clone(), engine_runtimes, store)
                    .await?;
                Ok(Arc::new(TableEngineImpl::new(instance)))
            }
            crate::storage_options::StorageOptions::Aliyun(ref opts) => {
                let store = open_storage_aliyun(opts.clone()).await?;
                let instance = self
                    .open_instance::<AliyunOSS>(config.clone(), engine_runtimes, store)
                    .await?;
                Ok(Arc::new(TableEngineImpl::new(instance)))
            }
        }
    }

    async fn open_instance<Store>(
        &self,
        config: Config,
        engine_runtimes: Arc<EngineRuntimes>,
        store: Store,
    ) -> Result<InstanceRef<Self::Wal, ManifestImpl<Self::Wal>, Store, FactoryImpl>>
    where
        Store: ObjectStore;
}
```

Is this the expected to this task?
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GPpg0/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GQvhG,horaedb,1178794054,72,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-07-08T09:58:13Z,2022-07-08T09:58:13Z,"@ygf11 This is a good proposal but considering that more implements of these components will be provided in the future, `Wal`, `Store` and other components needed by `Instance` are planned to be made as `Trait Object`.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GQvhG/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GTbcS,horaedb,1179498258,72,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2022-07-09T07:47:12Z,2022-07-09T07:47:12Z,"I agree with @ShiKaiWi 

> @ygf11 This is good proposal but considering that more implements of these components will be provided in the future, `Wal`, `Store` and other components needed by `Instance` are planned to be made as `Trait Object`.

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GTbcS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GTlqW,horaedb,1179540118,72,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-07-09T13:04:36Z,2022-07-09T13:04:36Z,"Thanks for your response. I can think the usage of `Trait Object` here.

`Instance` has four generic parameters now, which are all only used by `space_store`.
```rust
pub struct Instance<Wal, Meta, Store, Fa> {
    /// Space storage
    space_store: Arc<SpaceStore<Wal, Meta, Store, Fa>>,
    ...
}
```

The main issue is that `Wal` and its `associate type` are not `Object Safe`, this needs some refactors. 

I still do not have idea about it, what do you think?

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GTlqW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GayiL,horaedb,1181427851,72,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-12T07:42:45Z,2022-07-12T07:42:45Z,"Vote for removing some type params (I prefer to remove all four params) either. Let's make endeavors to achieve this.

trait `WalManager` presents in `Wal` and `Meta`. It's a bit complicated in my perspective:
```rust
pub trait WalManager: LogWriter + LogReader{
    async fn sequence_num(&self, region_id: RegionId) -> Result<SequenceNumber>;
    async fn mark_delete_entries_up_to(...) -> Result<()>;
    async fn close_gracefully(&self) -> Result<()>;
}

pub trait LogWriter {
    async fn write<P: Payload>(...) -> Result<SequenceNumber>;
}

pub trait LogReader {
    type BatchIter: BatchLogIterator + Send;

    async fn read_batch(...) -> Result<Self::BatchIter>;
}

pub trait BatchLogIterator {
    async fn next_log_entries<D: PayloadDecoder + Send + 'static>(...) -> Result<VecDeque<LogEntry<D::Target>>>;
}
```

Besides other constrains, IMO `LogWriter` and `LogReader` should be parts of `WalManager` rather than bounds. i.e., a `WalManager` itself can read and write.

The main issue is `LogReader`. I propose to remove the `BatchLogIterator` abstraction -- This is bound to WAL implementation and doesn't need to be dynamic. User only needs to specify the decoded data type via `PayloadDecoder` just like the `write()` method. The method now looks like
```rust
pub trait WalManager {
    // with some batching operations
    async fn read_batch<D: PayloadDecoder>(..., mut buffer: VecDeque<D::Target>) -> Result<VecDeque<D::Target>>;
}
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GayiL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Gfp1d,horaedb,1182702941,72,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-07-13T02:46:10Z,2022-07-13T02:46:10Z,"> Thanks for your response. I can think the usage of `Trait Object` here.
> 
> `Instance` has four generic parameters now, which are all only used by `space_store`.
> 
> ```rust
> pub struct Instance<Wal, Meta, Store, Fa> {
>     /// Space storage
>     space_store: Arc<SpaceStore<Wal, Meta, Store, Fa>>,
>     ...
> }
> ```
> 
> The main issue is that `Wal` and its `associate type` are not `Object Safe`, this needs some refactors.
> 
> I still do not have idea about it, what do you think?

Sorry for the late repsonse. The trait of `Wal` indeed needs refactoring and @waynexia has given a feasible proposal. By the way, are you willing to take the refactoring work? :laughing:","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Gfp1d/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GfqES,horaedb,1182703890,72,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-07-13T02:48:28Z,2022-07-13T02:48:28Z,"> pub trait WalManager {
>    async fn read_batch<D: PayloadDecoder>(..., mut buffer: VecDeque<D::Target>) -> Result<VecDeque<D::Target>>;
> }

@waynexia Why the `buffer` is a `VecDeque`? It seems  a `Vec` is more reasonable. How about such a method signature:
```rust
pub trait WalManager {
    async fn read_batch<D: PayloadDecoder>(..., buffer: &mut Vec<D::Target>) -> Result<()>;
}
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GfqES/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Gfq8c,horaedb,1182707484,72,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-13T02:55:53Z,2022-07-13T02:55:53Z,">@waynexia Why the buffer is a VecDeque? It seems a Vec is more reasonable. How about such a method signature:

Wal records need to be consumed from the head (`pop_front`) and replayed to the end (`push_back`). Thus `VecDeque` is used for this queue-like usage. 

Take mutable reference is ok to me :+1:","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Gfq8c/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Gfrax,horaedb,1182709425,72,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-07-13T02:59:51Z,2022-07-13T02:59:51Z,"> Sorry for the late repsonse. The trait of Wal indeed needs refactoring and @waynexia has given a feasible proposal. By the way, are you willing to take the refactoring work? 😆

I'm glad to work on this :D

But maybe I need some time to familiar with codebase.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Gfrax/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Gfr8h,horaedb,1182711585,72,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-13T03:04:35Z,2022-07-13T03:04:35Z,Thanks! Just take this at your pace and feel free to reach out to us if anything is unclear :heart:,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Gfr8h/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Gfy8w,horaedb,1182740272,72,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-07-13T04:03:49Z,2022-07-13T04:03:49Z,"> > @waynexia Why the buffer is a VecDeque? It seems a Vec is more reasonable. How about such a method signature:
> 
> Wal records need to be consumed from the head (`pop_front`) and replayed to the end (`push_back`). Thus `VecDeque` is used for this queue-like usage.
> 
> Take mutable reference is ok to me 👍

I guess no need to do `pop_front` operation because the buffer seems not reusable if `pop_front` is called.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Gfy8w/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Gf5-f,horaedb,1182769055,72,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-13T05:02:14Z,2022-07-13T05:02:14Z,"> I guess no need to do `pop_front` operation because the buffer seems not reusable if `pop_front` is called.

that method won't shrink the container so the buffer is still there. But I'm afraid if we don't pop the element out we may need to copy it to get the ownership for the consumer.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Gf5-f/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GgFYl,horaedb,1182815781,72,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-07-13T06:24:29Z,2022-07-13T06:24:29Z,"> > I guess no need to do `pop_front` operation because the buffer seems not reusable if `pop_front` is called.
> 
> that method won't shrink the container so the buffer is still there. But I'm afraid if we don't pop the element out we may need to copy it to get the ownership for the consumer.

I read the docs about `VecDequeue` and find it out that `pop_front` indeed doesn't shrink the underlying `RawVec` but the poped element is copied from the `RawVec`. Actually it seems that one more copy is necessary if we want to reuse the buffer and take the ownership of elements in it at same time. So I guess `Vec` is enough here.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GgFYl/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GgJKW,horaedb,1182831254,72,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-13T06:46:32Z,2022-07-13T06:46:32Z,I see. So use the element under a reference and copy it if needed is the same with `pop()`. We can replace `pop_front` with iter over a `Vec`.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GgJKW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GvBp9,horaedb,1186732669,72,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-18T03:46:09Z,2022-07-18T03:46:09Z,"Hi @ygf11, I have removed `Store` bounds in #101. It may cause conflict if you have changed it either. Sorry for the inconvenience 🙇

By the way, those traits are widely used. You can change them one by one to avoid a massive change set.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GvBp9/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Gvae2,horaedb,1186834358,72,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-07-18T06:58:47Z,2022-07-18T06:58:47Z,"> Hi @ygf11, I have removed Store bounds in https://github.com/CeresDB/ceresdb/pull/101. It may cause conflict if you have changed it either. Sorry for the inconvenience 🙇

@waynexia Thanks for your kindness remind :D

I am removing `Fa` type param now.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Gvae2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G1woc,horaedb,1188497948,72,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-07-19T01:32:42Z,2022-07-19T01:32:42Z,"I meet some troubles.

To make `WalManager` object safe, `Generic types` are not allowed in `trait assosiate methods`.
```rust
pub trait WalManager {
    // read payload
    async fn read_batch<D: PayloadDecoder>(..., mut buffer: Vec<D::Target>) -> Result<Vec<D::Target>>;

    // write payload 
    async fn write<P: Payload>(&self, ctx: &WriteContext, batch: &LogWriteBatch<P>,) -> Result<SequenceNumber>;
}
```
We can simply use derives to remove `Payload` and `PayloadDecoder`, since we have two derives, `MetaUpdatePayload` and `WritePayload`, so:
```rust
pub trait WalManager {
    async fn read_meta_batch(..., mut buffer: ...)...
    async fn read_write_batch(..., mut buffer: ...)...
    async fn write_payload(&self, ctx: &WriteContext, batch: &LogWriteBatch<WritePayload>)...
    async fn write_meta_payload(&self, ctx: &WriteContext, batch: &LogWriteBatch<MetaUpdatePayload>)...
}
```

It has two drawbacks:
1. `WalManager` only support read and write some payloads, if we want support other payload, we need add more methods.
2. To avoid circle deps, `WritePayload` and `MetaUpdatePayload` need define in `wal` crate.

I think maybe we can define a trait in `analytic_engine`, which is object safe:
```rust
pub trait WalManagerWraper {
    async fn read_meta_batch(..., mut buffer: ...)...
    async fn read_write_batch(..., mut buffer: ...)...
    async fn write_payload(&self, ctx: &WriteContext, batch: &LogWriteBatch<WritePayload>)...
    async fn write_meta_payload(&self, ctx: &WriteContext, batch: &LogWriteBatch<MetaUpdatePayload>)...
}

// rocks version
pub struct RocksWalManagerWrapper {
    inner: RocksImpl,
}

impl WalManagerWraper for RocksWalManagerWrapper {
    ...
}
```




","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G1woc/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G2gmO,horaedb,1188694414,72,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-19T07:23:50Z,2022-07-19T07:23:50Z,">I think maybe we can define a trait in analytic_engine, which is object safe:

This is doable but enumerating all possible implementations is not a good way in my perspective. I've not considered the object safety problem before. I come up with another way to remove those type parameters:
- Fow write, change the parameter to a trait object. like
```rust
pub trait WalManager {
    // write payload 
    async fn write(&self, ctx: &WriteContext, batch: &dyn Payload) -> Result<SequenceNumber>;
}
```
- For read, add an indirect layer to place the type parameter. I write an example in https://play.rust-lang.org/?version=stable&mode=debug&edition=2021&gist=a0d46419f3910cf3c845352babbda71d. The main idea is to make `read` method to return a concrete type (`Decoder` in the playground), and put type parameter `PayloadDecoder` in `Decoder`'s method.  `Decoder` is only used to place type parameters for `WalManager`. Other non-generic logic like codec, batching, version checking, etc. should be placed elsewhere.

Please tell me what you think about this @ygf11 @ShiKaiWi 


-------

A side note, changing `read` and `write` methods in `WalManager` may involve lots of code. For now two different implementations have mixed their own data access logic up with data codec logic. I haven't given an actual try to refactor them so feel free to raise any problem to discuss.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G2gmO/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G6s3k,horaedb,1189793252,72,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-07-20T03:50:31Z,2022-07-20T03:50:31Z,"> I come up with another way to remove those type parameters:

@waynexia Thanks. Your idea is the right direction.

For write, we need also make `payload` as object safe like you suggest.
```rust
pub trait Payload: Send + Sync + Debug {
    type Error: std::error::Error + Send + Sync + 'static;
    /// Compute size of the encoded payload.
    fn encode_size(&self) -> usize;
    /// Append the encoded payload to the `buf`.
    fn encode_to<B: MemBufMut>(&self, buf: &mut B) -> Result<(), Self::Error>;
}
```

Shall we replace `Error` with `Box<dyn std::error::Error>` here? 



 
 ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G6s3k/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/72,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G60iE,horaedb,1189824644,72,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-20T04:46:45Z,2022-07-20T04:46:45Z,">Shall we replace Error with Box<dyn std::error::Error> here?

It's ok to do so 👍 ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G60iE/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/75,https://api.github.com/repos/apache/horaedb/issues/75,horaedb,1289838698,75,Track memory usage in `MemUsageCollector`,waynexia,15380403,Ruihang Xia,,CLOSED,2022-06-30T09:14:41Z,2022-11-10T12:18:42Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

We have implemented a `MemUsageCollector` in https://github.com/CeresDB/ceresdb/blob/6b929ca19a/analytic_engine/src/instance/mem_collector.rs but no one reports memory consumptions to it.

One of the use cases is deciding whether to schedule a flush job based on memory usage:
https://github.com/CeresDB/ceresdb/blob/6b929ca19abf48ba9afd1cb554ec9a25c7783a42/analytic_engine/src/instance/write.rs#L307-L331


**Proposal**

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

Tracking allocation using this collector. For now I think tracks memtable is enough

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/75/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/75,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OGBC2,horaedb,1310199990,75,NA,waynexia,15380403,Ruihang Xia,,NA,2022-11-10T12:18:42Z,2022-11-10T12:18:42Z,Well... This is already implemented when the issue was created.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OGBC2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/77,https://api.github.com/repos/apache/horaedb/issues/77,horaedb,1290357493,77,Hybrid storage format,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-06-30T16:07:14Z,2022-08-30T02:38:49Z,"**Description**

For now, data by default is ordered by `timestamp` column within one SST file(currently in Parquet format), each tag/field being a column. 

| Timestamp | Device ID | Status Code | Tag 1 | Tag 2 |
|--------- |--------- |----------- |----- |----- |
| 12:01     | A         | 0           | v1    | v1    |
| 12:01     | B         | 0           | v2    | v2    |
| 12:02     | A         | 0           | v1    | v1    |
| 12:02     | B         | 1           | v2    | v2    |
| 12:03     | A         | 0           | v1    | v1    |
| 12:03     | B         | 0           | v2    | v2    |
| &#x2026;  |           |             |       |       |

This design is good for OLAP queries, as it will only scan relevant columns, and CeresDB can take advantage of this ordering to filter unnecessary file, reducing IO further.

But for time-series user case like IoT or DevOps, this maybe not the best format. Those queries will typically first group its result by series id(or device-id), then by timestamp. This ordering isn't match with SST, so many random IOs will be incurred.

A general approach is to duplicate data twice: one ordered by timestamp first, and the other ordered by series id first.

Apparently this isn't very cost-effective, and will require some replication algorithm to synchronize data, which is very error-prone. It's best we could solve this `ordering` issue in one format.
<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

**Proposal**

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

This issue propose one potential hybrid format (OLAP and time-series):

| Device ID | Timestamp           | Status Code | Tag 1 | Tag 2 | minTime | maxTime |
|--------- |------------------- |----------- |----- |----- |------- |------- |
| A         | [12:01,12:02,12:03] | [0,0,0]     | v1    | v1    | 12:01   | 12:03   |
| B         | [12:01,12:02,12:03] | [0,1,0]     | v2    | v2    | 12:01   | 12:03   |
| &#x2026;  |                     |             |       |       |         |         |

In the above schema, instead of store timestamp row by row, we put timestamp within a device id in one array, and the corresponding values are also in array type, so we can easily map between them. The table is ordered by device ID.

In this way, we can avoid random IO when query one specific device, since its data are stored together, and this format is also beneficial for OLAP queries since we can use min/maxTime to help reader filter unnecessary chunks.

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->
Some references
- [Building columnar compression in a row-oriented database](https://www.timescale.com/blog/building-columnar-compression-in-a-row-oriented-database/)
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/77/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/77,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GvwdI,horaedb,1186924360,77,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-07-18T08:38:40Z,2022-07-18T08:38:40Z,"I have done a benchmark in my local env, This hybrid format is better than the old one.

Table below summarize read cost in each format(each is read ten times).

## Hybrid

| cost  | row nums |
|----- |-------- |
| 615ms | 10367743 |
| 576ms | 10367743 |
| 585ms | 10367743 |
| 511ms | 10367743 |
| 558ms | 10367743 |
| 569ms | 10367743 |
| 568ms | 10367743 |
| 555ms | 10367743 |
| 557ms | 10367743 |
| 584ms | 10367743 |


## Old

| cost   | row nums |
|------ |-------- |
| 1304ms | 10367743 |
| 1283ms | 10367743 |
| 1276ms | 10367743 |
| 1286ms | 10367743 |
| 1275ms | 10367743 |
| 1272ms | 10367743 |
| 1273ms | 10367743 |
| 1275ms | 10367743 |
| 1275ms | 10367743 |
| 1270ms | 10367743 |


## How it tests

Firstly, my test env is

-   Linux 5.17.7-arch1-1 SMP PREEMPT Thu, 12 May 2022 18:55:54 +0000 x86\_64 GNU/Linux
-   6c16g
-  commit: https://github.com/jiacai2050/ceresdb/tree/d9577d5d417a811d37ff54239b81b44eff1f499c
    - [bench-hybrid.rs](https://github.com/jiacai2050/ceresdb/blob/d9577d5d41/benchmarks/src/bin/bench-hybrid.rs) 

Data is generated using [tsbs](https://github.com/timescale/tsbs/), with config below

```bash
data-source:
  simulator:
    debug: 0
    initial-scale: ""0""
    log-interval: 10s
    max-data-points: ""0""
    max-metric-count: ""1""
    scale: ""50000""
    seed: 100
    timestamp-start: ""2022-07-02T00:00:00Z""
    timestamp-end: ""2022-07-02T01:00:00Z""
    use-case: devops-generic
  type: SIMULATOR
```

This means the generated data source is

-   one metric within one hour, point interval is 10s, 50k series total.

Data sample

```
{
      ""arch"": ""x86"",
      ""region"": ""ap-southeast-1"",
      ""service_environment"": ""test"",
      ""team"": ""SF"",
      ""value"": 473.0,
      ""service_version"": ""0"",
      ""datacenter"": ""ap-southeast-1b"",
      ""timestamp"": 1656720000000,
      ""os"": ""Ubuntu16.04LTS"",
      ""hostname"": ""host_3349"",
      ""rack"": ""80"",
      ""service"": ""6"",
      ""tsid"": 1123006250071095
    }
```

## Next step

Rebase with upstream master, apply this hybrid format with string column(currently only fixed-length column tested).","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GvwdI/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/77,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Ij7LG,horaedb,1217376966,77,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-08-17T02:12:15Z,2022-08-17T02:12:15Z,"# Checklist
- [x] Write #185 
- [x] Read https://github.com/CeresDB/ceresdb/pull/208
- [x] Add table option for storage format https://github.com/CeresDB/ceresdb/pull/218
- [ ] Docs https://github.com/CeresDB/ceresdb/pull/222","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Ij7LG/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/77,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JCPL5,horaedb,1225323257,77,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-08-24T07:43:52Z,2022-08-24T07:43:52Z,"There are some more things need to be done for good performance, leave here to keep a note for myself and hope others interested can get involved.

# Write

-   Support variable-length type for `ListArray`
-   Support table without tsid, only a `row id` is required


# Read

-   Support basic read(without any filter pushdown), WIP
-   Support timestamp column filter, some extra columns may be needed
-   Support variable-length type for `ListArray`
-   Enable a total ordering, to support query with pagination


# Misc

-   Ensure row group size is large enough, in case of list length within same row\_id is to small
-   Use dictionary array type to represent non-collapsible columns to reduce memory usage.
-   Benchmark between two format","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JCPL5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/77,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JCyh2,horaedb,1225468022,77,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2022-08-24T09:29:08Z,2022-08-24T09:29:08Z,"> # Checklist
> * [x]  Write [feat: write hybrid storage format #185](https://github.com/CeresDB/ceresdb/pull/185)
> * [ ]  Read
> * [ ]  Add table option for storage format
> * [ ]  More testcases for write/read

This checklist is outdated.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JCyh2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/82,https://api.github.com/repos/apache/horaedb/issues/82,horaedb,1292986214,82,Move `AlterSchema` and `AlterOption` to data WAL,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-04T10:45:16Z,2022-08-08T04:50:26Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

`AlterSchema` and `AlterOption` are now persisted in the manifest. It would be better to keep manifest simple (only contains SST file information, sequence number, etc.). And schema modification is related to data, putting them in two separate places may bring some problems.

**Proposal**

<!---
Maybe you have considered some ideas or solutions for this feature.
-->

Make manifest only to contain create, drop table, and version edit. The new procedure of altering schema and option is:
- trigger flush  
- persist new schema/option to WAL  
- update OSS metadata (next PR)  
- update manifest to include the DDL record  
- update memory status  

A new file that contains ""present schema"" and ""present option"" will be persisted to OSS as table's metadata.

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/82/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/83,https://api.github.com/repos/apache/horaedb/issues/83,horaedb,1293080336,83,Integration test framework,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-04T12:07:34Z,2022-07-15T02:45:30Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

Provides a test framework to make us easy to write integration tests at SQL level.

**Proposal**

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

I plan to write this in python. It should be able to start server with special (non-default) configs, execute SQL and compare the result. Configuration, input data, SQL and expected output can be put in other text files.

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

Other systems' framework:
- [MariaDB](https://github.com/MariaDB/server/blob/10.10/mysql-test/README)
- [Databend](https://github.com/datafuselabs/databend/blob/main/tests/README.md)
- [ClickHouse](https://github.com/ClickHouse/ClickHouse/blob/master/tests/clickhouse-test)
- [InfluxDB IOx](https://github.com/influxdata/influxdb_iox/blob/main/query_tests/README.md)","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/83/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/83,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5F9lyk,horaedb,1173773476,83,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-07-04T12:39:17Z,2022-07-04T12:39:17Z,I'm OK with python but maybe Rust is a better choice.:rofl:,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5F9lyk/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/83,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GAdJm,horaedb,1174524518,83,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-07-05T02:09:16Z,2022-07-05T02:09:16Z,"IMO, Go/Rust is preferred over others, unless it has obvious advantages.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GAdJm/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/83,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GA89H,horaedb,1174654791,83,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-05T06:08:47Z,2022-07-05T06:08:47Z,👍  I'll check it. I chose python for convenience at first,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GA89H/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/85,https://api.github.com/repos/apache/horaedb/issues/85,horaedb,1293888429,85,"Unit test may fail with ""pure virtual method called"" ",waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-05T07:14:29Z,2022-11-18T08:34:15Z,"**Describe this problem**

<!---
What this problem is and what happened.
-->

UT/CI sometimes is failed with ""pure virtual method called, terminate called without an active exception""

Two cases failed in my env are
```plaintext
        FAIL [   0.353s] analytic_engine tests::drop_test::test_drop_table_once_rocks

--- STDOUT:              analytic_engine tests::drop_test::test_drop_table_once_rocks ---

running 1 test
test tests::drop_test::test_drop_table_once_rocks ... ok

test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 77 filtered out; finished in 0.08s


--- STDERR:              analytic_engine tests::drop_test::test_drop_table_once_rocks ---
pure virtual method called
terminate called without an active exception

        FAIL [   0.314s] analytic_engine tests::open_test::test_open_engine_rocks

--- STDOUT:              analytic_engine tests::open_test::test_open_engine_rocks ---

running 1 test
test tests::open_test::test_open_engine_rocks ... ok

test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 77 filtered out; finished in 0.04s


--- STDERR:              analytic_engine tests::open_test::test_open_engine_rocks ---
pure virtual method called
terminate called without an active exception
```
They are both introduced/modified in #62 

**Steps to reproduce**

Run unit tests

```bash
cargo test --workspace
```

But this won't occur every time (in my local env). 

<!---
How to reproduce this problem.
-->

**Expected behavior**

tests can pass

<!---
What is expected.
-->

**Additional Information**

<!---
If possible, please attach other context you think may relate to this problem. Like *runtime environment*, *modified config*, *error log* etc.
-->

https://github.com/nervosnetwork/ckb/issues/2927 looks like the same problem. 
ref #154 ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/85/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/85,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Js1xD,horaedb,1236491331,85,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-09-05T02:42:22Z,2022-09-05T02:42:22Z,The panic does not happen on my development environment (Linux) ever once by running the tests concerning RocksDB by `rr`. Maybe we should support to reproduce it and capture more information on environment provided by GitHub.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Js1xD/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/85,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Jte64,horaedb,1236659896,85,NA,waynexia,15380403,Ruihang Xia,,NA,2022-09-05T07:54:15Z,2022-09-05T07:54:15Z,"This bug looks like a concurrent bug. `rr` will limit its tracee to use at most one CPU, here is the output I got from `rr record --help`
```
  -u, --cpu-unbound          allow tracees to run on any virtual CPU.
                             Default is to bind to a random CPU.  This option
                             can cause replay divergence: use with
                             caution.
  --bind-to-cpu=<NUM>        Bind to a particular CPU
```
So I guess this is why you cannot reproduce it with `rr`. I'll try `-u` option later","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Jte64/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/85,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Jtl_3,horaedb,1236688887,85,NA,waynexia,15380403,Ruihang Xia,,NA,2022-09-05T08:23:47Z,2022-09-05T08:23:47Z,"Well... things become complicated 😵

TL;DR: I'm giving up `rr` and going to use `gdb` instead. Detailed reason:

`rr record -u` gives this error:
```
> rr record -u /home/ruihang/repo/CeresDB/target/debug/deps/analytic_engine-05df844a1b1791ff
rr: Saving execution to trace directory `/home/ruihang/.local/share/rr/analytic_engine-05df844a1b1791ff-22'.
[FATAL src/record_syscall.cc:4218:rec_prepare_syscall_arch()] 
 (task 426239 (rec:426239) at time 250)
 -> Assertion `t->session().trace_writer().bound_to_cpu() >= 0' failed to hold. rseq not supported with unbound tasks
```
It comes from here https://github.com/rr-debugger/rr/blob/452f652321f87722da64ca363c3deea568ea0b67/src/record_syscall.cc#L4220-L4221
```cpp
      // We can only support rseq when the tracee is bound to a specific CPU. otherwise cpu_id_start
      // and cpu_id fields would need to be managed by rr and would not match reality.
```
And the `rseq` mode is newly added in `glibc 2.35` ([source](https://lwn.net/Articles/883104/)), which is my env's version
```
 /lib/libc.so.6 
GNU C Library (GNU libc) stable release version 2.35.
Copyright (C) 2022 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.
There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A
PARTICULAR PURPOSE.
Compiled by GNU CC version 12.1.0.
libc ABIs: UNIQUE IFUNC ABSOLUTE
For bug reporting instructions, please see:
<https://bugs.archlinux.org/>.
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Jtl_3/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/85,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OqPgo,horaedb,1319696424,85,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-11-18T08:34:15Z,2022-11-18T08:34:15Z,It seems a long time that this doesn't happen. Let's close it now. And reopen it if we encounter it again.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OqPgo/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/87,https://api.github.com/repos/apache/horaedb/issues/87,horaedb,1295605395,87,Affected rows reports wrong number for create/drop statements,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-06T10:14:57Z,2022-07-11T14:28:55Z,"**Describe this problem**

<!---
What this problem is and what happened.
-->

```plaintext
Failed to execute query, err: Server(ServerError { code: 500, msg: ""Failed to convert output, query: DROP TABLE demo;. Caused by: Rpc error, code:500, message:Failed to convert output - affected rows: 1"" })
```
A drop table tells one row is affected

**Steps to reproduce**

```
drop table demo;
```

<!---
How to reproduce this problem.
-->

**Expected behavior**



```
mysql> create table xx like meta_2 ;
Query OK, 0 rows affected (0.11 sec)
```

<!---
What is expected.
-->

**Additional Information**

These SQLs don't actually count rows, 1 is the default value. Change the default value should fix this.


<!---
If possible, please attach other context you think may relate to this problem. Like *runtime environment*, *modified config*, *error log* etc.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/87/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/87,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GKGGu,horaedb,1177051566,87,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-07-07T04:27:40Z,2022-07-07T04:27:40Z,can this be assigned to me? this looks like I didn't set it up before,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GKGGu/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/87,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GKYtL,horaedb,1177127755,87,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-07T06:07:36Z,2022-07-07T06:07:36Z,Thanks for your interest! This is an existing behavior. There are some places that are designed to act like this. E.g: https://github.com/CeresDB/ceresdb/blob/ddaa3e8346d14bcc4432a4b3ac24a28e316d42ca/table_engine/src/table.rs#L445-L454,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GKYtL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/87,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GKZbx,horaedb,1177130737,87,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-07-07T06:10:39Z,2022-07-07T06:10:39Z,It looks like I need to modify the implementation of the `Table` trait so that it defaults to 0?👀,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GKZbx/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/87,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GKaBa,horaedb,1177133146,87,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-07T06:13:16Z,2022-07-07T06:13:16Z,I think so. I haven't inspected it carefully but there might be a number of places that default to 1.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GKaBa/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/87,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GKaaY,horaedb,1177134744,87,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-07-07T06:14:52Z,2022-07-07T06:14:52Z,ok i'll check it.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GKaaY/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/88,https://api.github.com/repos/apache/horaedb/issues/88,horaedb,1296701794,88,Support Arrow format in gRPC service,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-07T02:21:15Z,2023-02-09T11:20:39Z,"**Description**

The query interface now supports avro and json format. https://github.com/CeresDB/ceresdb/blob/f6c9a5b03e23b04e6a06c4f73756f1be9551c0f8/server/src/grpc/query.rs#L42-L45
And avro is the only one actually used. Regardless of json, it's natural to support arrow (in arrow's [ipc format](https://arrow.apache.org/docs/format/Columnar.html#serialization-and-interprocess-communication-ipc)) which is widely used in our server.

**Proposal**

Add support to arrow format. The execution result is `RecordBatch` so it wouldn't take a lot of effort to serialize it on the server side. 
Considering backward compatibility we can keep using avro as the default format, and client can require server to return a specific format it needs. 

The protobuf is defined in https://github.com/CeresDB/ceresdbproto/blob/eba30f7dff736d00be711c40c3f01964655eb10a/protos/storage.proto#L101-L110
```protobuf
message QueryResponse {
  common.ResponseHeader header = 1;
  enum SchemaType {
    AVRO = 0;
    JSON = 1;
  }
  SchemaType schema_type = 2;
  string schema_content = 3;
  repeated bytes rows = 4;
}
```

It also might be necessary to do some renaming. Like `SchemaType` -> `ResponseType`, `rows` -> `chunks` etc.

**Additional context**

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/88/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/88,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GXjRQ,horaedb,1180578896,88,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2022-07-11T15:49:55Z,2022-07-11T15:49:55Z,Maybe I can try it and make corresponding changes to rust client.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GXjRQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/88,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5U4Pfa,horaedb,1424029658,88,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-02-09T11:20:38Z,2023-02-09T11:20:38Z,This has been supported.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5U4Pfa/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/89,https://api.github.com/repos/apache/horaedb/issues/89,horaedb,1296738840,89,"Extend `QueryResponse` to support ""Affected Rows""",waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-07T03:13:00Z,2022-07-19T11:22:59Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

Our `QueryResponse` message doesn't support to pass how many rows are affected to the client.
```protobuf
message QueryResponse {
  common.ResponseHeader header = 1;
  enum SchemaType {
    AVRO = 0;
    JSON = 1;
  }
  SchemaType schema_type = 2;
  string schema_content = 3;
  repeated bytes rows = 4;
}
```

For some statements like INSERT/DELETE this is very useful. MySQL has a document about this https://dev.mysql.com/doc/c-api/8.0/en/mysql-affected-rows.html

>[mysql_affected_rows()](https://dev.mysql.com/doc/c-api/8.0/en/mysql-affected-rows.html) may be called immediately after executing a statement with [mysql_real_query()](https://dev.mysql.com/doc/c-api/8.0/en/mysql-real-query.html) or [mysql_query()](https://dev.mysql.com/doc/c-api/8.0/en/mysql-query.html). It returns the number of rows changed, deleted, or inserted by the last statement if it was an [UPDATE](https://dev.mysql.com/doc/refman/8.0/en/update.html), [DELETE](https://dev.mysql.com/doc/refman/8.0/en/delete.html), or [INSERT](https://dev.mysql.com/doc/refman/8.0/en/insert.html). For [SELECT](https://dev.mysql.com/doc/refman/8.0/en/select.html) statements, [mysql_affected_rows()](https://dev.mysql.com/doc/c-api/8.0/en/mysql-affected-rows.html) works like [mysql_num_rows()](https://dev.mysql.com/doc/c-api/8.0/en/mysql-num-rows.html).

**Proposal**

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

Add a field in `QueryResponse` to tell how many rows are affected. Consider this also used by SELECT which won't ""affect"" rows, I propose this field can be called `row_count`.

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/89/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/89,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GJ6Pj,horaedb,1177002979,89,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-07-07T03:14:45Z,2022-07-07T03:14:45Z,`affected_rows` seems better and it can be set 0 for `SELECT` statement.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GJ6Pj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/89,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GJ69B,horaedb,1177005889,89,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-07T03:19:45Z,2022-07-07T03:19:45Z,"> `affected_rows` seems better and it can be set 0 for `SELECT` statement.

`SELECT` can return the actual count and we don't need special logic for it. like 
>For [SELECT](https://dev.mysql.com/doc/refman/8.0/en/select.html) statements, [mysql_affected_rows()](https://dev.mysql.com/doc/c-api/8.0/en/mysql-affected-rows.html) works like [mysql_num_rows()](https://dev.mysql.com/doc/c-api/8.0/en/mysql-num-rows.html).

But this info can be retrieved by counting the data.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GJ69B/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/89,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GKnau,horaedb,1177188014,89,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-07-07T07:23:34Z,2022-07-07T07:23:34Z,"It seems weird that `affected_rows` is actually`num_rows` for SELECT statement. On the other hand, `row_count` is not semantical enough for `INSERT` or `DELETE`. 

Maybe no need to keep the same meaning of `affected_rows` with mysql?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GKnau/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/89,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GKpMv,horaedb,1177195311,89,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-07T07:32:12Z,2022-07-07T07:32:12Z,"Makes sense. Try to conclude:
- add a field named `affected_rows` to pb
- `affected_rows` is always set to the number of ""affected"" rows. 
  - `CREATE` affects 0 row
  - `INSERT` or `DELETE` affects the rows added or removed
  - `SELECT affects 0 row
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GKpMv/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/89,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GwASa,horaedb,1186989210,89,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-07-18T09:44:51Z,2022-07-18T09:44:51Z,"I intend to add integration test, it's best this issue to be resolved first, so assign this to me.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GwASa/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/90,https://api.github.com/repos/apache/horaedb/issues/90,horaedb,1296765748,90,RFC tracker: Prometheus read extension for CeresDB,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-07T03:48:04Z,2024-10-19T11:32:16Z,"**RFC**
This is proposed in #79. See [rendered](https://github.com/waynexia/ceresdb/blob/rfc-prom-ext/docs/rfcs/20220702-prometheus-read-extension.md)

**Task List**
TBD","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/90/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/90,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WtgY6,horaedb,1454769722,90,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-04T15:07:47Z,2023-03-04T15:07:47Z,"This RFC is kinds of stale, users from community may prefer integrate with Prometheus 'natively', see https://github.com/CeresDB/ceresdb/issues/633.

Since this RFC is not getting any obvious progress, so I think we need to reconsider this.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WtgY6/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/92,https://api.github.com/repos/apache/horaedb/issues/92,horaedb,1297263788,92,Start CeresDB in distributed mode with CeresMeta,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-07-07T11:30:13Z,2022-08-25T09:28:29Z,"**Description**
The target is to run ceresdb in distributed mode and in order for that ceresdb in distributed mode, ceresdb must communicate with ceresmeta by the meta client. 
<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

**Proposal**
CeresMeta's service is built by gRPC and the corresponding protocol has been defined by [the proto file](https://github.com/CeresDB/ceresdbproto/blob/main/protos/meta/service.proto) so first what we need to do is build a grpc client based on that proto file during ceresdb's starting procedure.

And there is no need to keep the old meta client, and the new meta client should provide the capabilities:
- Send keepalive heartbeats to ceresmeta;
- Register handlers for node heartbeats;
- Allocate schema/table id;
- Provide routing information of the cluster from the ceresmeta;

And based on the new meta client (meta_client_v2) a module called cluster will be responsible for managing information from the CeresMeta, including:
- Creating schema, creating/deleting tables;
- Tables to be opened in the ceresdb instance;
- Events such as change of the cluster topology from ceresmeta;
- ...

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**
steps:
- [x] Upgrade [ceresdbproto](https://github.com/CeresDB/ceresdbproto), remove old meta client and refactor meta_client_v2 (#100)
- [x] Enhance cluster module based on meta_client_v2 (#100)
- [x] Refactor volatile catalog implementation (#157)
- [x] Support startup in distributed mode (#190)
- [x] Remove the old meta_client (#205)
- [ ] Provide routing information (#215)
<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/92/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/94,https://api.github.com/repos/apache/horaedb/issues/94,horaedb,1301449307,94,Pushdown `IN` predicate to TableScan,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-07-12T02:24:02Z,2022-07-20T04:43:41Z,"**Description**
Now the predicates whose type are only primitive binary expression are able to be pushed down to table scan. Actually `IN` predicate is also able to be pushed down and used to do filtering duration scanning stage.
<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

**Proposal**
Only the [predicate module](https://github.com/CeresDB/ceresdb/blob/main/table_engine/src/predicate/mod.rs) in table engine needs changes.
- Make `IN` predicate able to be pushed down;
- Support the filtering RowGroups by `IN` expression;

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**
Shall we make more kinds of predicates able to be pushed down.
<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/94/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/95,https://api.github.com/repos/apache/horaedb/issues/95,horaedb,1302836545,95,Build fails on Darwin Kernel  (arch: arm64),archerny,14784412,Arthur Chern,weirong.cwr@gmail.com,CLOSED,2022-07-13T02:55:58Z,2022-07-19T05:09:41Z,"**Describe this problem**

Build CeresDB fails on my MacBook [MacBook Pro (13-inch, M1, 2020)]

**Steps to reproduce**

cargo build and then I will get the compile error, see the additional infos (the error logs)

**Expected behavior**

Compile phase passed, and get the CeresDB binary

**Additional Information**

<details>
  <summary>Detail error log</summary>
  

```
   Compiling fasthash-sys v0.3.2
   Compiling serde v1.0.137
   Compiling thiserror v1.0.31
   Compiling tokio v1.19.2
   Compiling futures-util v0.3.21
   Compiling failure_derive v0.1.8
   Compiling comfy-table v5.0.1
   Compiling multiversion v0.6.1
   Compiling zerocopy-derive v0.2.0
   Compiling tracing v0.1.35
The following warnings were emitted during compilation:

warning: clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
warning: In file included from src/fasthash.cpp:1:
warning: In file included from src/fasthash.hpp:5:
warning: src/smhasher/mum.h:119:15: warning: expression result unused [-Wunused-value]
warning:   lo = v * p, hi;
warning:               ^~
warning: 1 warning generated.
warning: clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
warning: src/smhasher/City.cpp:148:21: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
warning:   for (int i = 0; i < len; i++) {
warning:                   ~ ^ ~~~
warning: 1 warning generated.
warning: clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
warning: In file included from src/smhasher/mum.cc:1:
warning: src/smhasher/mum.h:119:15: warning: expression result unused [-Wunused-value]
warning:   lo = v * p, hi;
warning:               ^~
warning: 1 warning generated.
warning: clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
warning: clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
warning: In file included from src/smhasher/metrohash64crc.cpp:28:
warning: In file included from /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/nmmintrin.h:15:
warning: In file included from /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/smmintrin.h:13:
warning: In file included from /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/tmmintrin.h:13:
warning: In file included from /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/pmmintrin.h:13:
warning: In file included from /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/emmintrin.h:13:
warning: In file included from /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/xmmintrin.h:13:
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:33:5: error: use of undeclared identifier '__builtin_ia32_emms'; did you mean '__builtin_isless'?
warning:     __builtin_ia32_emms();
warning:     ^
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:33:5: note: '__builtin_isless' declared here
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:33:25: error: too few arguments to function call, expected 2, have 0
warning:     __builtin_ia32_emms();
warning:                         ^
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:50:19: error: use of undeclared identifier '__builtin_ia32_vec_init_v2si'
warning:     return (__m64)__builtin_ia32_vec_init_v2si(__i, 0);
warning:                   ^
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:67:12: error: use of undeclared identifier '__builtin_ia32_vec_ext_v2si'
warning:     return __builtin_ia32_vec_ext_v2si((__v2si)__m, 0);
warning:            ^
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:129:19: error: use of undeclared identifier '__builtin_ia32_packsswb'
warning:     return (__m64)__builtin_ia32_packsswb((__v4hi)__m1, (__v4hi)__m2);
warning:                   ^
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:159:19: error: use of undeclared identifier '__builtin_ia32_packssdw'
warning:     return (__m64)__builtin_ia32_packssdw((__v2si)__m1, (__v2si)__m2);
warning:                   ^
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:189:19: error: use of undeclared identifier '__builtin_ia32_packuswb'
warning:     return (__m64)__builtin_ia32_packuswb((__v4hi)__m1, (__v4hi)__m2);
warning:                   ^
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:216:19: error: use of undeclared identifier '__builtin_ia32_punpckhbw'
warning:     return (__m64)__builtin_ia32_punpckhbw((__v8qi)__m1, (__v8qi)__m2);
warning:                   ^
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:239:19: error: use of undeclared identifier '__builtin_ia32_punpckhwd'
warning:     return (__m64)__builtin_ia32_punpckhwd((__v4hi)__m1, (__v4hi)__m2);
warning:                   ^
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:260:19: error: use of undeclared identifier '__builtin_ia32_punpckhdq'
warning:     return (__m64)__builtin_ia32_punpckhdq((__v2si)__m1, (__v2si)__m2);
warning:                   ^
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:287:19: error: use of undeclared identifier '__builtin_ia32_punpcklbw'
warning:     return (__m64)__builtin_ia32_punpcklbw((__v8qi)__m1, (__v8qi)__m2);
warning:                   ^
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:310:19: error: use of undeclared identifier '__builtin_ia32_punpcklwd'
warning:     return (__m64)__builtin_ia32_punpcklwd((__v4hi)__m1, (__v4hi)__m2);
warning:                   ^
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:331:19: error: use of undeclared identifier '__builtin_ia32_punpckldq'
warning:     return (__m64)__builtin_ia32_punpckldq((__v2si)__m1, (__v2si)__m2);
warning:                   ^
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:352:19: error: use of undeclared identifier '__builtin_ia32_paddb'; did you mean '__builtin_arm_addg'?
warning:     return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);
warning:                   ^
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:352:19: note: '__builtin_arm_addg' declared here
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:352:19: error: first argument of MTE builtin function must be a pointer ('__v8qi' (vector of 8 'char' values) invalid)
warning:     return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);
warning:                   ^                    ~~~~~~~~~~~~
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:373:19: error: use of undeclared identifier '__builtin_ia32_paddw'; did you mean '__builtin_arm_addg'?
warning:     return (__m64)__builtin_ia32_paddw((__v4hi)__m1, (__v4hi)__m2);
warning:                   ^
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:352:19: note: '__builtin_arm_addg' declared here
warning:     return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);
warning:                   ^
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:373:19: error: first argument of MTE builtin function must be a pointer ('__v4hi' (vector of 4 'short' values) invalid)
warning:     return (__m64)__builtin_ia32_paddw((__v4hi)__m1, (__v4hi)__m2);
warning:                   ^                    ~~~~~~~~~~~~
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:394:19: error: use of undeclared identifier '__builtin_ia32_paddd'; did you mean '__builtin_arm_addg'?
warning:     return (__m64)__builtin_ia32_paddd((__v2si)__m1, (__v2si)__m2);
warning:                   ^
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:352:19: note: '__builtin_arm_addg' declared here
warning:     return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);
warning:                   ^
warning: /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:394:19: error: first argument of MTE builtin function must be a pointer ('__v2si' (vector of 2 'int' values) invalid)
warning:     return (__m64)__builtin_ia32_paddd((__v2si)__m1, (__v2si)__m2);
warning:                   ^                    ~~~~~~~~~~~~
warning: fatal error: too many errors emitted, stopping now [-ferror-limit=]
warning: 20 errors generated.

error: failed to run custom build command for `fasthash-sys v0.3.2`

Caused by:
  process didn't exit successfully: `/Users/chenwr/workspace/ceresdb/target/debug/build/fasthash-sys-1e6c4b44a7398dc5/build-script-build` (exit status: 101)
  --- stdout
  TARGET = Some(""aarch64-apple-darwin"")
  OPT_LEVEL = Some(""0"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CC_aarch64-apple-darwin = None
  CC_aarch64_apple_darwin = None
  HOST_CC = None
  CC = None
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CFLAGS_aarch64-apple-darwin = None
  CFLAGS_aarch64_apple_darwin = None
  HOST_CFLAGS = None
  CFLAGS = None
  DEBUG = Some(""true"")
  running: ""cc"" ""-O0"" ""-ffunction-sections"" ""-fdata-sections"" ""-fPIC"" ""-g"" ""-Wno-implicit-fallthrough"" ""-Wno-unknown-attributes"" ""-msse4.2"" ""-maes"" ""-mavx"" ""-mavx2"" ""-DT1HA0_RUNTIME_SELECT=1"" ""-DT1HA0_AESNI_AVAILABLE=1"" ""-Wall"" ""-Wextra"" ""-o"" ""/Users/chenwr/workspace/ceresdb/target/debug/build/fasthash-sys-bf79a6c7afd57249/out/src/fasthash.o"" ""-c"" ""src/fasthash.cpp""
  cargo:warning=clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
  cargo:warning=In file included from src/fasthash.cpp:1:
  cargo:warning=In file included from src/fasthash.hpp:5:
  cargo:warning=src/smhasher/mum.h:119:15: warning: expression result unused [-Wunused-value]
  cargo:warning=  lo = v * p, hi;
  cargo:warning=              ^~
  cargo:warning=1 warning generated.
  exit status: 0
  TARGET = Some(""aarch64-apple-darwin"")
  OPT_LEVEL = Some(""0"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CC_aarch64-apple-darwin = None
  CC_aarch64_apple_darwin = None
  HOST_CC = None
  CC = None
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CFLAGS_aarch64-apple-darwin = None
  CFLAGS_aarch64_apple_darwin = None
  HOST_CFLAGS = None
  CFLAGS = None
  DEBUG = Some(""true"")
  running: ""cc"" ""-O0"" ""-ffunction-sections"" ""-fdata-sections"" ""-fPIC"" ""-g"" ""-Wno-implicit-fallthrough"" ""-Wno-unknown-attributes"" ""-msse4.2"" ""-maes"" ""-mavx"" ""-mavx2"" ""-DT1HA0_RUNTIME_SELECT=1"" ""-DT1HA0_AESNI_AVAILABLE=1"" ""-Wall"" ""-Wextra"" ""-o"" ""/Users/chenwr/workspace/ceresdb/target/debug/build/fasthash-sys-bf79a6c7afd57249/out/src/smhasher/City.o"" ""-c"" ""src/smhasher/City.cpp""
  cargo:warning=clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
  cargo:warning=src/smhasher/City.cpp:148:21: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
  cargo:warning=  for (int i = 0; i < len; i++) {
  cargo:warning=                  ~ ^ ~~~
  cargo:warning=1 warning generated.
  exit status: 0
  TARGET = Some(""aarch64-apple-darwin"")
  OPT_LEVEL = Some(""0"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CC_aarch64-apple-darwin = None
  CC_aarch64_apple_darwin = None
  HOST_CC = None
  CC = None
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CFLAGS_aarch64-apple-darwin = None
  CFLAGS_aarch64_apple_darwin = None
  HOST_CFLAGS = None
  CFLAGS = None
  DEBUG = Some(""true"")
  running: ""cc"" ""-O0"" ""-ffunction-sections"" ""-fdata-sections"" ""-fPIC"" ""-g"" ""-Wno-implicit-fallthrough"" ""-Wno-unknown-attributes"" ""-msse4.2"" ""-maes"" ""-mavx"" ""-mavx2"" ""-DT1HA0_RUNTIME_SELECT=1"" ""-DT1HA0_AESNI_AVAILABLE=1"" ""-Wall"" ""-Wextra"" ""-o"" ""/Users/chenwr/workspace/ceresdb/target/debug/build/fasthash-sys-bf79a6c7afd57249/out/src/smhasher/farmhash-c.o"" ""-c"" ""src/smhasher/farmhash-c.c""
  cargo:warning=clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
  exit status: 0
  TARGET = Some(""aarch64-apple-darwin"")
  OPT_LEVEL = Some(""0"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CC_aarch64-apple-darwin = None
  CC_aarch64_apple_darwin = None
  HOST_CC = None
  CC = None
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CFLAGS_aarch64-apple-darwin = None
  CFLAGS_aarch64_apple_darwin = None
  HOST_CFLAGS = None
  CFLAGS = None
  DEBUG = Some(""true"")
  running: ""cc"" ""-O0"" ""-ffunction-sections"" ""-fdata-sections"" ""-fPIC"" ""-g"" ""-Wno-implicit-fallthrough"" ""-Wno-unknown-attributes"" ""-msse4.2"" ""-maes"" ""-mavx"" ""-mavx2"" ""-DT1HA0_RUNTIME_SELECT=1"" ""-DT1HA0_AESNI_AVAILABLE=1"" ""-Wall"" ""-Wextra"" ""-o"" ""/Users/chenwr/workspace/ceresdb/target/debug/build/fasthash-sys-bf79a6c7afd57249/out/src/smhasher/lookup3.o"" ""-c"" ""src/smhasher/lookup3.cpp""
  cargo:warning=clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
  exit status: 0
  TARGET = Some(""aarch64-apple-darwin"")
  OPT_LEVEL = Some(""0"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CC_aarch64-apple-darwin = None
  CC_aarch64_apple_darwin = None
  HOST_CC = None
  CC = None
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CFLAGS_aarch64-apple-darwin = None
  CFLAGS_aarch64_apple_darwin = None
  HOST_CFLAGS = None
  CFLAGS = None
  DEBUG = Some(""true"")
  running: ""cc"" ""-O0"" ""-ffunction-sections"" ""-fdata-sections"" ""-fPIC"" ""-g"" ""-Wno-implicit-fallthrough"" ""-Wno-unknown-attributes"" ""-msse4.2"" ""-maes"" ""-mavx"" ""-mavx2"" ""-DT1HA0_RUNTIME_SELECT=1"" ""-DT1HA0_AESNI_AVAILABLE=1"" ""-Wall"" ""-Wextra"" ""-o"" ""/Users/chenwr/workspace/ceresdb/target/debug/build/fasthash-sys-bf79a6c7afd57249/out/src/smhasher/mum.o"" ""-c"" ""src/smhasher/mum.cc""
  cargo:warning=clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
  cargo:warning=In file included from src/smhasher/mum.cc:1:
  cargo:warning=src/smhasher/mum.h:119:15: warning: expression result unused [-Wunused-value]
  cargo:warning=  lo = v * p, hi;
  cargo:warning=              ^~
  cargo:warning=1 warning generated.
  exit status: 0
  TARGET = Some(""aarch64-apple-darwin"")
  OPT_LEVEL = Some(""0"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CC_aarch64-apple-darwin = None
  CC_aarch64_apple_darwin = None
  HOST_CC = None
  CC = None
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CFLAGS_aarch64-apple-darwin = None
  CFLAGS_aarch64_apple_darwin = None
  HOST_CFLAGS = None
  CFLAGS = None
  DEBUG = Some(""true"")
  running: ""cc"" ""-O0"" ""-ffunction-sections"" ""-fdata-sections"" ""-fPIC"" ""-g"" ""-Wno-implicit-fallthrough"" ""-Wno-unknown-attributes"" ""-msse4.2"" ""-maes"" ""-mavx"" ""-mavx2"" ""-DT1HA0_RUNTIME_SELECT=1"" ""-DT1HA0_AESNI_AVAILABLE=1"" ""-Wall"" ""-Wextra"" ""-o"" ""/Users/chenwr/workspace/ceresdb/target/debug/build/fasthash-sys-bf79a6c7afd57249/out/src/smhasher/metrohash64.o"" ""-c"" ""src/smhasher/metrohash64.cpp""
  cargo:warning=clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
  exit status: 0
  TARGET = Some(""aarch64-apple-darwin"")
  OPT_LEVEL = Some(""0"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CC_aarch64-apple-darwin = None
  CC_aarch64_apple_darwin = None
  HOST_CC = None
  CC = None
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CFLAGS_aarch64-apple-darwin = None
  CFLAGS_aarch64_apple_darwin = None
  HOST_CFLAGS = None
  CFLAGS = None
  DEBUG = Some(""true"")
  running: ""cc"" ""-O0"" ""-ffunction-sections"" ""-fdata-sections"" ""-fPIC"" ""-g"" ""-Wno-implicit-fallthrough"" ""-Wno-unknown-attributes"" ""-msse4.2"" ""-maes"" ""-mavx"" ""-mavx2"" ""-DT1HA0_RUNTIME_SELECT=1"" ""-DT1HA0_AESNI_AVAILABLE=1"" ""-Wall"" ""-Wextra"" ""-o"" ""/Users/chenwr/workspace/ceresdb/target/debug/build/fasthash-sys-bf79a6c7afd57249/out/src/smhasher/metrohash128.o"" ""-c"" ""src/smhasher/metrohash128.cpp""
  cargo:warning=clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
  exit status: 0
  TARGET = Some(""aarch64-apple-darwin"")
  OPT_LEVEL = Some(""0"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CC_aarch64-apple-darwin = None
  CC_aarch64_apple_darwin = None
  HOST_CC = None
  CC = None
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CFLAGS_aarch64-apple-darwin = None
  CFLAGS_aarch64_apple_darwin = None
  HOST_CFLAGS = None
  CFLAGS = None
  DEBUG = Some(""true"")
  running: ""cc"" ""-O0"" ""-ffunction-sections"" ""-fdata-sections"" ""-fPIC"" ""-g"" ""-Wno-implicit-fallthrough"" ""-Wno-unknown-attributes"" ""-msse4.2"" ""-maes"" ""-mavx"" ""-mavx2"" ""-DT1HA0_RUNTIME_SELECT=1"" ""-DT1HA0_AESNI_AVAILABLE=1"" ""-Wall"" ""-Wextra"" ""-o"" ""/Users/chenwr/workspace/ceresdb/target/debug/build/fasthash-sys-bf79a6c7afd57249/out/src/smhasher/MurmurHash1.o"" ""-c"" ""src/smhasher/MurmurHash1.cpp""
  cargo:warning=clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
  exit status: 0
  TARGET = Some(""aarch64-apple-darwin"")
  OPT_LEVEL = Some(""0"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CC_aarch64-apple-darwin = None
  CC_aarch64_apple_darwin = None
  HOST_CC = None
  CC = None
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CFLAGS_aarch64-apple-darwin = None
  CFLAGS_aarch64_apple_darwin = None
  HOST_CFLAGS = None
  CFLAGS = None
  DEBUG = Some(""true"")
  running: ""cc"" ""-O0"" ""-ffunction-sections"" ""-fdata-sections"" ""-fPIC"" ""-g"" ""-Wno-implicit-fallthrough"" ""-Wno-unknown-attributes"" ""-msse4.2"" ""-maes"" ""-mavx"" ""-mavx2"" ""-DT1HA0_RUNTIME_SELECT=1"" ""-DT1HA0_AESNI_AVAILABLE=1"" ""-Wall"" ""-Wextra"" ""-o"" ""/Users/chenwr/workspace/ceresdb/target/debug/build/fasthash-sys-bf79a6c7afd57249/out/src/smhasher/MurmurHash2.o"" ""-c"" ""src/smhasher/MurmurHash2.cpp""
  cargo:warning=clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
  exit status: 0
  TARGET = Some(""aarch64-apple-darwin"")
  OPT_LEVEL = Some(""0"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CC_aarch64-apple-darwin = None
  CC_aarch64_apple_darwin = None
  HOST_CC = None
  CC = None
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CFLAGS_aarch64-apple-darwin = None
  CFLAGS_aarch64_apple_darwin = None
  HOST_CFLAGS = None
  CFLAGS = None
  DEBUG = Some(""true"")
  running: ""cc"" ""-O0"" ""-ffunction-sections"" ""-fdata-sections"" ""-fPIC"" ""-g"" ""-Wno-implicit-fallthrough"" ""-Wno-unknown-attributes"" ""-msse4.2"" ""-maes"" ""-mavx"" ""-mavx2"" ""-DT1HA0_RUNTIME_SELECT=1"" ""-DT1HA0_AESNI_AVAILABLE=1"" ""-Wall"" ""-Wextra"" ""-o"" ""/Users/chenwr/workspace/ceresdb/target/debug/build/fasthash-sys-bf79a6c7afd57249/out/src/smhasher/MurmurHash3.o"" ""-c"" ""src/smhasher/MurmurHash3.cpp""
  cargo:warning=clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
  exit status: 0
  TARGET = Some(""aarch64-apple-darwin"")
  OPT_LEVEL = Some(""0"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CC_aarch64-apple-darwin = None
  CC_aarch64_apple_darwin = None
  HOST_CC = None
  CC = None
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CFLAGS_aarch64-apple-darwin = None
  CFLAGS_aarch64_apple_darwin = None
  HOST_CFLAGS = None
  CFLAGS = None
  DEBUG = Some(""true"")
  running: ""cc"" ""-O0"" ""-ffunction-sections"" ""-fdata-sections"" ""-fPIC"" ""-g"" ""-Wno-implicit-fallthrough"" ""-Wno-unknown-attributes"" ""-msse4.2"" ""-maes"" ""-mavx"" ""-mavx2"" ""-DT1HA0_RUNTIME_SELECT=1"" ""-DT1HA0_AESNI_AVAILABLE=1"" ""-Wall"" ""-Wextra"" ""-o"" ""/Users/chenwr/workspace/ceresdb/target/debug/build/fasthash-sys-bf79a6c7afd57249/out/src/smhasher/Spooky.o"" ""-c"" ""src/smhasher/Spooky.cpp""
  cargo:warning=clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
  exit status: 0
  TARGET = Some(""aarch64-apple-darwin"")
  OPT_LEVEL = Some(""0"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CC_aarch64-apple-darwin = None
  CC_aarch64_apple_darwin = None
  HOST_CC = None
  CC = None
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CFLAGS_aarch64-apple-darwin = None
  CFLAGS_aarch64_apple_darwin = None
  HOST_CFLAGS = None
  CFLAGS = None
  DEBUG = Some(""true"")
  running: ""cc"" ""-O0"" ""-ffunction-sections"" ""-fdata-sections"" ""-fPIC"" ""-g"" ""-Wno-implicit-fallthrough"" ""-Wno-unknown-attributes"" ""-msse4.2"" ""-maes"" ""-mavx"" ""-mavx2"" ""-DT1HA0_RUNTIME_SELECT=1"" ""-DT1HA0_AESNI_AVAILABLE=1"" ""-Wall"" ""-Wextra"" ""-o"" ""/Users/chenwr/workspace/ceresdb/target/debug/build/fasthash-sys-bf79a6c7afd57249/out/src/smhasher/xxhash.o"" ""-c"" ""src/smhasher/xxhash.c""
  cargo:warning=clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
  exit status: 0
  TARGET = Some(""aarch64-apple-darwin"")
  OPT_LEVEL = Some(""0"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CC_aarch64-apple-darwin = None
  CC_aarch64_apple_darwin = None
  HOST_CC = None
  CC = None
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CFLAGS_aarch64-apple-darwin = None
  CFLAGS_aarch64_apple_darwin = None
  HOST_CFLAGS = None
  CFLAGS = None
  DEBUG = Some(""true"")
  running: ""cc"" ""-O0"" ""-ffunction-sections"" ""-fdata-sections"" ""-fPIC"" ""-g"" ""-Wno-implicit-fallthrough"" ""-Wno-unknown-attributes"" ""-msse4.2"" ""-maes"" ""-mavx"" ""-mavx2"" ""-DT1HA0_RUNTIME_SELECT=1"" ""-DT1HA0_AESNI_AVAILABLE=1"" ""-Wall"" ""-Wextra"" ""-o"" ""/Users/chenwr/workspace/ceresdb/target/debug/build/fasthash-sys-bf79a6c7afd57249/out/src/t1ha/src/t1ha0.o"" ""-c"" ""src/t1ha/src/t1ha0.c""
  cargo:warning=clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
  exit status: 0
  TARGET = Some(""aarch64-apple-darwin"")
  OPT_LEVEL = Some(""0"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CC_aarch64-apple-darwin = None
  CC_aarch64_apple_darwin = None
  HOST_CC = None
  CC = None
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CFLAGS_aarch64-apple-darwin = None
  CFLAGS_aarch64_apple_darwin = None
  HOST_CFLAGS = None
  CFLAGS = None
  DEBUG = Some(""true"")
  running: ""cc"" ""-O0"" ""-ffunction-sections"" ""-fdata-sections"" ""-fPIC"" ""-g"" ""-Wno-implicit-fallthrough"" ""-Wno-unknown-attributes"" ""-msse4.2"" ""-maes"" ""-mavx"" ""-mavx2"" ""-DT1HA0_RUNTIME_SELECT=1"" ""-DT1HA0_AESNI_AVAILABLE=1"" ""-Wall"" ""-Wextra"" ""-o"" ""/Users/chenwr/workspace/ceresdb/target/debug/build/fasthash-sys-bf79a6c7afd57249/out/src/t1ha/src/t1ha1.o"" ""-c"" ""src/t1ha/src/t1ha1.c""
  cargo:warning=clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
  exit status: 0
  TARGET = Some(""aarch64-apple-darwin"")
  OPT_LEVEL = Some(""0"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CC_aarch64-apple-darwin = None
  CC_aarch64_apple_darwin = None
  HOST_CC = None
  CC = None
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CFLAGS_aarch64-apple-darwin = None
  CFLAGS_aarch64_apple_darwin = None
  HOST_CFLAGS = None
  CFLAGS = None
  DEBUG = Some(""true"")
  running: ""cc"" ""-O0"" ""-ffunction-sections"" ""-fdata-sections"" ""-fPIC"" ""-g"" ""-Wno-implicit-fallthrough"" ""-Wno-unknown-attributes"" ""-msse4.2"" ""-maes"" ""-mavx"" ""-mavx2"" ""-DT1HA0_RUNTIME_SELECT=1"" ""-DT1HA0_AESNI_AVAILABLE=1"" ""-Wall"" ""-Wextra"" ""-o"" ""/Users/chenwr/workspace/ceresdb/target/debug/build/fasthash-sys-bf79a6c7afd57249/out/src/t1ha/src/t1ha2.o"" ""-c"" ""src/t1ha/src/t1ha2.c""
  cargo:warning=clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
  exit status: 0
  TARGET = Some(""aarch64-apple-darwin"")
  OPT_LEVEL = Some(""0"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CC_aarch64-apple-darwin = None
  CC_aarch64_apple_darwin = None
  HOST_CC = None
  CC = None
  HOST = Some(""aarch64-apple-darwin"")
  TARGET = Some(""aarch64-apple-darwin"")
  HOST = Some(""aarch64-apple-darwin"")
  CFLAGS_aarch64-apple-darwin = None
  CFLAGS_aarch64_apple_darwin = None
  HOST_CFLAGS = None
  CFLAGS = None
  DEBUG = Some(""true"")
  running: ""cc"" ""-O0"" ""-ffunction-sections"" ""-fdata-sections"" ""-fPIC"" ""-g"" ""-Wno-implicit-fallthrough"" ""-Wno-unknown-attributes"" ""-msse4.2"" ""-maes"" ""-mavx"" ""-mavx2"" ""-DT1HA0_RUNTIME_SELECT=1"" ""-DT1HA0_AESNI_AVAILABLE=1"" ""-Wall"" ""-Wextra"" ""-o"" ""/Users/chenwr/workspace/ceresdb/target/debug/build/fasthash-sys-bf79a6c7afd57249/out/src/smhasher/metrohash64crc.o"" ""-c"" ""src/smhasher/metrohash64crc.cpp""
  cargo:warning=clang: warning: argument unused during compilation: '-msse4.2' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-maes' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx' [-Wunused-command-line-argument]
  cargo:warning=clang: warning: argument unused during compilation: '-mavx2' [-Wunused-command-line-argument]
  cargo:warning=In file included from src/smhasher/metrohash64crc.cpp:28:
  cargo:warning=In file included from /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/nmmintrin.h:15:
  cargo:warning=In file included from /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/smmintrin.h:13:
  cargo:warning=In file included from /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/tmmintrin.h:13:
  cargo:warning=In file included from /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/pmmintrin.h:13:
  cargo:warning=In file included from /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/emmintrin.h:13:
  cargo:warning=In file included from /Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/xmmintrin.h:13:
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:33:5: error: use of undeclared identifier '__builtin_ia32_emms'; did you mean '__builtin_isless'?
  cargo:warning=    __builtin_ia32_emms();
  cargo:warning=    ^
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:33:5: note: '__builtin_isless' declared here
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:33:25: error: too few arguments to function call, expected 2, have 0
  cargo:warning=    __builtin_ia32_emms();
  cargo:warning=                        ^
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:50:19: error: use of undeclared identifier '__builtin_ia32_vec_init_v2si'
  cargo:warning=    return (__m64)__builtin_ia32_vec_init_v2si(__i, 0);
  cargo:warning=                  ^
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:67:12: error: use of undeclared identifier '__builtin_ia32_vec_ext_v2si'
  cargo:warning=    return __builtin_ia32_vec_ext_v2si((__v2si)__m, 0);
  cargo:warning=           ^
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:129:19: error: use of undeclared identifier '__builtin_ia32_packsswb'
  cargo:warning=    return (__m64)__builtin_ia32_packsswb((__v4hi)__m1, (__v4hi)__m2);
  cargo:warning=                  ^
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:159:19: error: use of undeclared identifier '__builtin_ia32_packssdw'
  cargo:warning=    return (__m64)__builtin_ia32_packssdw((__v2si)__m1, (__v2si)__m2);
  cargo:warning=                  ^
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:189:19: error: use of undeclared identifier '__builtin_ia32_packuswb'
  cargo:warning=    return (__m64)__builtin_ia32_packuswb((__v4hi)__m1, (__v4hi)__m2);
  cargo:warning=                  ^
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:216:19: error: use of undeclared identifier '__builtin_ia32_punpckhbw'
  cargo:warning=    return (__m64)__builtin_ia32_punpckhbw((__v8qi)__m1, (__v8qi)__m2);
  cargo:warning=                  ^
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:239:19: error: use of undeclared identifier '__builtin_ia32_punpckhwd'
  cargo:warning=    return (__m64)__builtin_ia32_punpckhwd((__v4hi)__m1, (__v4hi)__m2);
  cargo:warning=                  ^
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:260:19: error: use of undeclared identifier '__builtin_ia32_punpckhdq'
  cargo:warning=    return (__m64)__builtin_ia32_punpckhdq((__v2si)__m1, (__v2si)__m2);
  cargo:warning=                  ^
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:287:19: error: use of undeclared identifier '__builtin_ia32_punpcklbw'
  cargo:warning=    return (__m64)__builtin_ia32_punpcklbw((__v8qi)__m1, (__v8qi)__m2);
  cargo:warning=                  ^
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:310:19: error: use of undeclared identifier '__builtin_ia32_punpcklwd'
  cargo:warning=    return (__m64)__builtin_ia32_punpcklwd((__v4hi)__m1, (__v4hi)__m2);
  cargo:warning=                  ^
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:331:19: error: use of undeclared identifier '__builtin_ia32_punpckldq'
  cargo:warning=    return (__m64)__builtin_ia32_punpckldq((__v2si)__m1, (__v2si)__m2);
  cargo:warning=                  ^
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:352:19: error: use of undeclared identifier '__builtin_ia32_paddb'; did you mean '__builtin_arm_addg'?
  cargo:warning=    return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);
  cargo:warning=                  ^
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:352:19: note: '__builtin_arm_addg' declared here
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:352:19: error: first argument of MTE builtin function must be a pointer ('__v8qi' (vector of 8 'char' values) invalid)
  cargo:warning=    return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);
  cargo:warning=                  ^                    ~~~~~~~~~~~~
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:373:19: error: use of undeclared identifier '__builtin_ia32_paddw'; did you mean '__builtin_arm_addg'?
  cargo:warning=    return (__m64)__builtin_ia32_paddw((__v4hi)__m1, (__v4hi)__m2);
  cargo:warning=                  ^
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:352:19: note: '__builtin_arm_addg' declared here
  cargo:warning=    return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);
  cargo:warning=                  ^
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:373:19: error: first argument of MTE builtin function must be a pointer ('__v4hi' (vector of 4 'short' values) invalid)
  cargo:warning=    return (__m64)__builtin_ia32_paddw((__v4hi)__m1, (__v4hi)__m2);
  cargo:warning=                  ^                    ~~~~~~~~~~~~
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:394:19: error: use of undeclared identifier '__builtin_ia32_paddd'; did you mean '__builtin_arm_addg'?
  cargo:warning=    return (__m64)__builtin_ia32_paddd((__v2si)__m1, (__v2si)__m2);
  cargo:warning=                  ^
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:352:19: note: '__builtin_arm_addg' declared here
  cargo:warning=    return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);
  cargo:warning=                  ^
  cargo:warning=/Library/Developer/CommandLineTools/usr/lib/clang/12.0.5/include/mmintrin.h:394:19: error: first argument of MTE builtin function must be a pointer ('__v2si' (vector of 2 'int' values) invalid)
  cargo:warning=    return (__m64)__builtin_ia32_paddd((__v2si)__m1, (__v2si)__m2);
  cargo:warning=                  ^                    ~~~~~~~~~~~~
  cargo:warning=fatal error: too many errors emitted, stopping now [-ferror-limit=]
  cargo:warning=20 errors generated.
  exit status: 1

  --- stderr
  thread 'main' panicked at '

  Internal error occurred: Command ""cc"" ""-O0"" ""-ffunction-sections"" ""-fdata-sections"" ""-fPIC"" ""-g"" ""-Wno-implicit-fallthrough"" ""-Wno-unknown-attributes"" ""-msse4.2"" ""-maes"" ""-mavx"" ""-mavx2"" ""-DT1HA0_RUNTIME_SELECT=1"" ""-DT1HA0_AESNI_AVAILABLE=1"" ""-Wall"" ""-Wextra"" ""-o"" ""/Users/chenwr/workspace/ceresdb/target/debug/build/fasthash-sys-bf79a6c7afd57249/out/src/smhasher/metrohash64crc.o"" ""-c"" ""src/smhasher/metrohash64crc.cpp"" with args ""cc"" did not execute successfully (status code exit status: 1).

  ', /Users/chenwr/.cargo/registry/src/github.com-1ecc6299db9ec823/gcc-0.3.55/src/lib.rs:1672:5
  note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
warning: build failed, waiting for other jobs to finish...
error: build failed
```



</details>","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/95/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/95,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GkqMN,horaedb,1184015117,95,NA,archerny,14784412,Arthur Chern,weirong.cwr@gmail.com,NA,2022-07-14T05:44:35Z,2022-07-14T05:44:35Z,The dependency crate fasthash-sys-0.3.2.crate has some C++ source codes. These codes cannot be compiled on arm platforms.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GkqMN/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/95,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Gkqyp,horaedb,1184017577,95,NA,archerny,14784412,Arthur Chern,weirong.cwr@gmail.com,NA,2022-07-14T05:46:46Z,2022-07-14T05:46:46Z,"Details of the platform dependency: 

![D8305A92-72F8-44D8-B98F-E396689936B1](https://user-images.githubusercontent.com/14784412/178908686-486294e9-9b59-4a06-9e6d-f0fd9b561945.png)
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Gkqyp/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/96,https://api.github.com/repos/apache/horaedb/issues/96,horaedb,1302839770,96,Build guidance for mac users,archerny,14784412,Arthur Chern,weirong.cwr@gmail.com,CLOSED,2022-07-13T03:02:32Z,2022-08-05T06:13:45Z,"**Description**

The doc now have no guidance for different platforms

**Proposal**

The project docs now don't have any build guidance for different platforms other than Ubuntu. We can add these docs to help the users solving problems when they build on their own dev env.

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/96/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/96,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Hz6Jt,horaedb,1204789869,96,NA,tianlinzx,1411754,,,NA,2022-08-04T05:48:31Z,2022-08-04T05:48:31Z,"Yes, we need detailed guidance how to build on Mac.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Hz6Jt/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/96,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Hz75Q,horaedb,1204797008,96,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-08-04T06:00:45Z,2022-08-04T06:00:45Z,"AFAIK, Clang & Rust is all you need to build , are you having any specific issues?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Hz75Q/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/96,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H0Lrr,horaedb,1204861675,96,NA,tianlinzx,1411754,,,NA,2022-08-04T07:16:36Z,2022-08-04T07:16:36Z,"1、xcode-select --install
2、brew install cmake
3、cargo build --release
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H0Lrr/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/96,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H0U58,horaedb,1204899452,96,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-08-04T07:55:53Z,2022-08-04T07:55:53Z,"@tianlinzx Thanks, would you like to submit a PR to add those steps in readme?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H0U58/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/96,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H0vUB,horaedb,1205007617,96,NA,tianlinzx,1411754,,,NA,2022-08-04T09:34:34Z,2022-08-04T09:34:34Z,#169 is submitted.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H0vUB/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/98,https://api.github.com/repos/apache/horaedb/issues/98,horaedb,1305540113,98,Tracking issue for SQL integration tests,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-15T03:52:27Z,2022-07-27T07:46:52Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

We have supported tons of SQLs (on top of DataFusion) but don't have concrete tests for them. This is to add initial and basic coverage.

**Proposal**

SQLs to test:

- insert  
	- insert non null  
	- insert null  
	- insert without required columns  
- query  
	- select from one table  
	- select with operator  
		- datafusion build-in  
		- customized  
			- prom  
			- hll count  
	- select from multiple tables  
		- join  
	- tpc-h queries  
- DDL  
	- add column  
	- remove column  
	- drop table  

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/98/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/98,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G6i1V,horaedb,1189752149,98,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-20T03:00:53Z,2022-07-20T03:00:53Z,"A list from https://github.com/datafuselabs/databend/blob/main/tests/suites/0_stateless/README.md

@jiacai2050:
- [ ] 00_dummy
- [ ] 01_system
- [ ] 02_function
- [ ] 03_dml

@waynexia:
- [ ] 04_explain
- [ ] 05_ddl
- [ ] 06_show
- [ ] 07_optimizer

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G6i1V/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/103,https://api.github.com/repos/apache/horaedb/issues/103,horaedb,1307639848,103,Conditional import deps for `arrow-deps` sub-crate,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-18T09:17:00Z,2022-07-19T10:53:51Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

`arrow-deps` is used to delegate all arrow-related deps (arrow, parquet, datafusion etc.) to facilitate common deps' management. But for other crates that don't need all of them it brings lots of unnecessary deps - a relative discussion in python SDK: https://github.com/CeresDB/ceresdb-client-py/pull/2#issuecomment-1186925905

**Proposal**
- Remove this `arrow-deps` and let each crate import itself.
- Keep `arrow-deps` for sub-crates in this project. Other projects import necessary deps manually.
- Add feature gate to conditional import those deps.


<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**


<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/103/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/103,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G13sw,horaedb,1188526896,103,NA,messense,1556054,messense,,NA,2022-07-19T02:30:17Z,2022-07-19T02:30:17Z,"Since ceresdb-client-py only needs the `Datum` type, what do you think about the following change? Just feature-gate `arrow-deps` for now.

```diff
diff --git a/common_types/Cargo.toml b/common_types/Cargo.toml
index ed2736d..59558f6 100644
--- a/common_types/Cargo.toml
+++ b/common_types/Cargo.toml
@@ -7,11 +7,12 @@ edition = ""2018""
 # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
 
 [features]
+default = [""arrow_deps""]
 test = []
 
 [dependencies]
 # In alphabetical order
-arrow_deps = { path = ""../arrow_deps"" }
+arrow_deps = { path = ""../arrow_deps"", optional = true }
 byteorder = ""1.2""
 bytes = { path = ""../components/bytes"" }
 chrono = ""0.4""
diff --git a/common_types/src/datum.rs b/common_types/src/datum.rs
index 4e9dc03..84ff4e4 100644
--- a/common_types/src/datum.rs
+++ b/common_types/src/datum.rs
@@ -4,6 +4,7 @@
 
 use std::{convert::TryFrom, fmt, str};
 
+#[cfg(feature = ""arrow_deps"")]
 use arrow_deps::{
     arrow::datatypes::{DataType, TimeUnit},
     datafusion::scalar::ScalarValue,
@@ -156,6 +157,7 @@ impl DatumKind {
 
     /// Create DatumKind from [arrow_deps::arrow::datatypes::DataType], if the
     /// type is not supported, returns None
+    #[cfg(feature = ""arrow_deps"")]
     pub fn from_data_type(data_type: &DataType) -> Option<Self> {
         match data_type {
             DataType::Null => Some(Self::Null),
@@ -223,6 +225,7 @@ impl DatumKind {
     }
 }
 
+#[cfg(feature = ""arrow_deps"")]
 impl From<DatumKind> for DataType {
     fn from(kind: DatumKind) -> Self {
         match kind {
@@ -609,6 +612,7 @@ impl Datum {
         }
     }
 
+    #[cfg(feature = ""arrow_deps"")]
     pub fn as_scalar_value(&self) -> Option<ScalarValue> {
         match self {
             Datum::Null => None,
@@ -787,6 +791,7 @@ impl<'a> DatumView<'a> {
         }
     }
 
+    #[cfg(feature = ""arrow_deps"")]
     pub fn from_scalar_value(val: &'a ScalarValue) -> Option<Self> {
         match val {
             ScalarValue::Boolean(v) => v.map(DatumView::Boolean),
diff --git a/common_types/src/lib.rs b/common_types/src/lib.rs
index e247e2f..a07ca1c 100644
--- a/common_types/src/lib.rs
+++ b/common_types/src/lib.rs
@@ -3,14 +3,20 @@
 //! Contains common types
 
 pub mod bytes;
+#[cfg(feature = ""arrow_deps"")]
 pub mod column;
+#[cfg(feature = ""arrow_deps"")]
 pub mod column_schema;
 pub mod datum;
 pub mod hash;
+#[cfg(feature = ""arrow_deps"")]
 pub mod projected_schema;
+#[cfg(feature = ""arrow_deps"")]
 pub mod record_batch;
 pub mod request_id;
+#[cfg(feature = ""arrow_deps"")]
 pub mod row;
+#[cfg(feature = ""arrow_deps"")]
 pub mod schema;
 pub mod string;
 pub mod time;
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G13sw/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/103,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G16p1,horaedb,1188538997,103,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-19T02:56:17Z,2022-07-19T02:56:17Z,">Since ceresdb-client-py only needs the Datum type, what do you think about the following change? Just feature-gate arrow-deps for now.

This looks pretty neat to me 👍 would you like to submit this patch?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G16p1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/103,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G19dV,horaedb,1188550485,103,NA,messense,1556054,messense,,NA,2022-07-19T03:21:20Z,2022-07-19T03:21:20Z,@waynexia #108 ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G19dV/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/104,https://api.github.com/repos/apache/horaedb/issues/104,horaedb,1307722181,104,"Can I change table write_mode  from ""Append"" to ""Overwrite"" dynamically ?",MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2022-07-18T10:14:38Z,2022-07-28T07:20:25Z,"**Which part is this question about**

<!---
Is it code base, usage, deployment, documentation or some other part?
-->

write_mode
**Describe your question**

<!---
A clear and concise description of what the question is.
-->

Can I change table write_mode  from ""Append"" to ""Overwrite"" dynamically ?

**Additional context**

<!---
Add any other context about the problem here.
-->


","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/104/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/104,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GwgQU,horaedb,1187120148,104,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-18T11:19:55Z,2022-07-18T11:19:55Z,"`UpdateMode` is an immutable option that can only be set when `CREATE`ing table: https://github.com/CeresDB/ceresdb/blob/df18e26343da1b29bf4ab8b4f8b8570cd50515a7/analytic_engine/src/table_options.rs#L504-L511

So you cannot change this option dynamically.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5GwgQU/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/104,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HZGco,horaedb,1197762344,104,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2022-07-28T07:20:25Z,2022-07-28T07:20:25Z,tracked by #156 ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HZGco/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/107,https://api.github.com/repos/apache/horaedb/issues/107,horaedb,1308906610,107,Implement `ignore` interceptor in integration test framework ,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-19T02:52:27Z,2022-09-27T03:43:13Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

Some cases are not expected to run due to unstable features, temporary changes or anything else. It should be supported to just ignore them.

**Proposal**

Implement the `ignore` interceptor. Proposed syntax:
```
-- CERESDB ignore: REASON
```
Follows the K-V format. Value for this interceptor is the reason why this case got ignored. It won't take effort and is a comment in fact.

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->
TBD: should we count/record those ignored cases and report it in the end?
ref #154 
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/107/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/107,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5K9sAx,horaedb,1257685041,107,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-09-26T08:32:56Z,2022-09-26T08:32:56Z,"Actually, there is nothing random in the sequential write/read procedure for now. Besides this, it is a good feature for any tests to keep the determined results.

So I think we should fix these random things rather than support the proposed the `ignore interceptor`.

And I find out it that the random table id results from that fact the data generated by the previous run is not reset, so the id for the `created-dropped-created-again` table will be incremented finally, which leads to the test failure.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5K9sAx/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/112,https://api.github.com/repos/apache/horaedb/issues/112,horaedb,1310612861,112,CI: support to run integration tests,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-20T07:59:20Z,2022-10-21T03:47:59Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

SQL integration tests are added in #98, it's useful for testing high-level behaviors. But our CI only run unit tests for now. 

**Proposal**

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

Add job to run those integration tests. We can choose to run it per PR/commit or per day if it takes a long time. But for now it only takes a few mins to complete.

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/112/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/112,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MrTtC,horaedb,1286421314,112,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-10-21T03:47:58Z,2022-10-21T03:47:58Z,#270 #319 already fix this.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MrTtC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/113,https://api.github.com/repos/apache/horaedb/issues/113,horaedb,1310623874,113,table name with back quote has strange behavior in SDK,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-20T08:06:54Z,2022-07-20T12:07:28Z,"**Describe this problem**

<!---
What this problem is and what happened.
-->

run log:
```
DROP TABLE IF EXISTS `t0`;

affected_rows: 0

CREATE TABLE `t0`(a int, t timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE = Analytic;

affected_rows: 0

INSERT INTO TABLE `t0`(a, t) values(1,1);

affected_rows: 1

SELECT * FROM `t0`;

Failed to execute query, err: Server(ServerError { code: 500, msg: ""Failed to create plan, query: SELECT * FROM `t0`;. Caused by: Failed to create plan, err:DataFusion Failed to plan, err:Execution error: MetaProvider not found"" })
```

**Steps to reproduce**

add the following to a integration test
```sql
DROP TABLE IF EXISTS `t0`;

CREATE TABLE `t0`(a int, t timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE = Analytic;

INSERT INTO TABLE `t0`(a, t) values(1,1);

SELECT * FROM `t0`;
```

and run it

<!---
How to reproduce this problem.
-->

**Expected behavior**

the last SQL
```sql
SELECT * FROM `t0`;
```

should return data instead of reporting an error.

<!---
What is expected.
-->

**Additional Information**

<!---
If possible, please attach other context you think may relate to this problem. Like *runtime environment*, *modified config*, *error log* etc.
-->
the client version is 
```
https://github.com/CeresDB/ceresdb-client-rs.git?rev=a935a0ebacd1304d69d25e3e8611afa7d64611d6
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/113/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/114,https://api.github.com/repos/apache/horaedb/issues/114,horaedb,1310635644,114,Missing features of integration test framework,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-20T08:15:39Z,2023-01-07T12:31:55Z,"**List**

- Interceptor
  - [x] support `ignore` interceptor #107 
  - [ ] support `echo` interceptor
  - [ ] able to handle random things https://github.com/CeresDB/ceresdb/issues/114#issuecomment-1218949297
- UI
  - [ ] can only run specified cases
  - [ ] redirect server's log to the specified place
  - [ ] Keep original SQL indents
- Runner
  - [ ] customizable config file in dir level (e.g. `tests/cases/config.toml` and `tests/cases/01_dummy/config.toml`)

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/114/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/114,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HDkQq,horaedb,1192117290,114,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-07-22T02:33:24Z,2022-07-22T02:33:24Z,"List below are features I want to have:

- Able to set config file
- Keep original SQL indents. This is what looks like now:

input
```
CREATE TABLE `03_dml_select_order_table1` (
    `timestamp` timestamp NOT NULL,
    `value` int,
    timestamp KEY (timestamp)) ENGINE=Analytic
WITH(
	 enable_ttl='false'
);
```
output
```
CREATE TABLE `03_dml_select_order_table1` (    `timestamp` timestamp NOT NULL,    `value` int,    timestamp KEY (timestamp)) ENGINE=AnalyticWITH(	 enable_ttl='false');

affected_rows: 0
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HDkQq/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/114,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HDk2K,horaedb,1192119690,114,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-22T02:38:29Z,2022-07-22T02:38:29Z,">List below are features I want to have:

Updated to the description 👍 ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HDk2K/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/114,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Ip7Cx,horaedb,1218949297,114,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-08-18T02:42:52Z,2022-08-18T02:42:52Z,"<img width=""1680"" alt=""image"" src=""https://user-images.githubusercontent.com/3848910/185280982-5cb057b0-4070-4259-84ae-950a5c75b93d.png"">

- /cases/local/07_optimizer/optimizer.sql 
This testcase contains a random result, so we need a `interceptor ` which can replace text before do equality check","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Ip7Cx/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/114,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5R7K9_,horaedb,1374465919,114,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-01-07T12:31:55Z,2023-01-07T12:31:55Z,"This test framework is migrated to https://github.com/CeresDB/sqlness, and this issue is kinds of stale, so close it.

New issues can be created in new repo.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5R7K9_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/115,https://api.github.com/repos/apache/horaedb/issues/115,horaedb,1310749875,115,Simplify SQL syntax that specifies timestamp key column,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-20T09:36:37Z,2022-08-01T03:32:49Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

`TIMESTAMP KEY` is a special column in analytic engine. It need to be set to a column with `Timestamp` type and with `NOT NULL` constrain.
```sql
CREATE TABLE demo (
    name string TAG,
    value double NOT NULL,
    t timestamp NOT NULL,
    TIMESTAMP KEY(t)
) ENGINE=Analytic;
```

It must have one and only one `TIMESTAMP KEY` column in a table.

**Proposal**
there are a few ways to simplify it.

---------

- Specify `TIMESTAMP KEY` without create a column
Considering the constraints, user is actually giving a name to the `TIMESTAMP KEY` column. 
```sql
CREATE TABLE demo (
    name string TAG,
    value double NOT NULL,
    TIMESTAMP KEY(t)
) ENGINE=Analytic;
```

------------

- Move `TIMESTAMP KEY` to column definition.
This makes `TIMESTAMP KEY` act like primary key.
```sql
CREATE TABLE demo (
    name string TAG,
    value double NOT NULL,
    t timestamp NOT NULL TIMESTAMP KEY
) ENGINE=Analytic;
```

-----------
- Do not need to define it and let CeresDB infers the `TIMESTAMP KEY`
Allow user to not specify `TIMESTAMP KEY` in create table SQL. CeresDB will try to find a column with `Timestamp` type and `NOT NULL`. Report error if zero or multiple columns are found.
```sql
CREATE TABLE demo (
    name string TAG,
    value double NOT NULL,
    t timestamp NOT NULL
) ENGINE=Analytic;
```

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/115/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/115,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G8AOV,horaedb,1190134677,115,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-07-20T11:01:43Z,2022-07-20T11:01:43Z,"I think the second one would be better, `t` appears to be a special kind of primary key? maybe we can create it by creating a primary key?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G8AOV/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/115,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G8A6e,horaedb,1190137502,115,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-20T11:04:52Z,2022-07-20T11:04:52Z,"Yes, the second one is derived from PRIMARY KEY's syntax. One key difference between TIMESTAMP KEY and PRIMARY KEY is that we can only define one TIMESTAMP KEY per table.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G8A6e/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/115,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G8B7-,horaedb,1190141694,115,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-07-20T11:09:41Z,2022-07-20T11:09:41Z,Does anyone else have a better opinion? maybe I can try to accomplish this first?😃,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G8B7-/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/115,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G8PnI,horaedb,1190197704,115,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-20T12:11:21Z,2022-07-20T12:11:21Z,"> Does anyone else have a better opinion? maybe I can try to accomplish this first?😃

Assigned, thanks for your help ❤️

But let's discuss this further as this user-facing change needs more attention and is hard to change in the future. Maybe @ShiKaiWi has some thoughts on this.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G8PnI/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/115,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G8zvT,horaedb,1190345683,115,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-07-20T14:17:28Z,2022-07-20T14:17:28Z,"Vote for the second proposal. And to keep consistent with primary key syntax in mysql, the `not null` constraint on timestamp key can be made optional too. As for the `timestamp` type, I prefer to make it required.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G8zvT/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/115,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G_xYJ,horaedb,1191122441,115,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-07-21T07:07:14Z,2022-07-21T07:07:14Z,"Just want to make sure, if we go with the second approach, the old syntax will be kept as it is now?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G_xYJ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/115,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G_4IJ,horaedb,1191150089,115,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-07-21T07:38:00Z,2022-07-21T07:38:00Z,"> Just want to make sure, if we go with the second approach, the old syntax will be kept as it is now?

The old syntax should be still supported.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G_4IJ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/117,https://api.github.com/repos/apache/horaedb/issues/117,horaedb,1312379927,117,Build failed because of these errors! ,theseusyang,863136,theseus yang,,CLOSED,2022-07-21T02:35:23Z,2022-07-21T06:45:53Z,"when executing `docker build -t ceresdb .` , these errors happen:

```
Updating crates.io index
#12 38.89     Updating git repository `https://github.com/waynexia/arrow-datafusion.git`
#12 156.5 error: failed to get `datafusion` as a dependency of package `arrow_deps v0.1.0 (/ceresdb/arrow_deps)`
#12 156.5 
#12 156.5 Caused by:
#12 156.5   failed to load source for dependency `datafusion`
#12 156.5 
#12 156.5 Caused by:
#12 156.5   Unable to update https://github.com/waynexia/arrow-datafusion.git?rev=bb3e002a4d89f9a5104bba34511b4c593a9c0d05#bb3e002a
#12 156.5 
#12 156.5 Caused by:
#12 156.5   failed to clone into: /usr/local/cargo/git/db/arrow-datafusion-8f64698f94ffb3e6
#12 156.5 
#12 156.5 Caused by:
#12 156.5   network failure seems to have happened
#12 156.5   if a proxy or similar is necessary `net.git-fetch-with-cli` may help here
#12 156.5   https://doc.rust-lang.org/cargo/reference/config.html#netgit-fetch-with-cli
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/117/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/117,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G_NJu,horaedb,1190974062,117,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2022-07-21T02:42:24Z,2022-07-21T02:42:24Z,"Check you terminal's network to ensure connecting to github.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G_NJu/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/117,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G_tEJ,horaedb,1191104777,117,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-07-21T06:45:53Z,2022-07-21T06:45:53Z,@theseusyang This seems not a bug and I will close this issue. Feel free to file new issues if you encounter any other problems.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G_tEJ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/118,https://api.github.com/repos/apache/horaedb/issues/118,horaedb,1312438126,118,Redundant back quotes shows in SHOW CREATE TABLE's result,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-21T03:13:17Z,2022-07-21T11:37:52Z,"**Describe this problem**

`SHOW CREATE TABLE` won't use already exist quotes


```
CREATE TABLE `07_optimizer_t` (name string TAG, value double NOT NULL, t timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE=Analytic with (enable_ttl='false');

affected_rows: 0

SHOW CREATE TABLE `07_optimizer_t`;

Table,Create Table,
String(StringBytes(b""`07_optimizer_t`"")),String(StringBytes(b""CREATE TABLE ``07_optimizer_t`` (`t` timestamp NOT NULL, `tsid` uint64 NOT NULL, `name` string TAG, `value` double NOT NULL, PRIMARY KEY(t,tsid), TIMESTAMP KEY(t)) ENGINE=Analytic WITH(arena_block_size='2097152', compaction_strategy='default', compression='ZSTD', enable_ttl='false', num_rows_per_row_group='8192', segment_duration='', ttl='7d', update_mode='OVERWRITE', write_buffer_size='33554432')"")),
```

**Steps to reproduce**


Add the following content to integration test and run it:
```sql
CREATE TABLE `07_optimizer_t` (name string TAG, value double NOT NULL, t timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE=Analytic with (enable_ttl='false');

SHOW CREATE TABLE `07_optimizer_t`;

EXPLAIN SELECT max(value) AS c1, avg(value) AS c2 FROM `07_optimizer_t` GROUP BY name;
```

**Expected behavior**

the result `CREATE TABLE` statement should not contains double back quotes around table name
```sql
CREATE TABLE ``07_optimizer_t`` (`t` timestamp NOT NULL, `tsid` uint64 NOT NULL, `name` string TAG, `value` double NOT NULL, PRIMARY KEY(t,tsid), TIMESTAMP KEY(t)) ENGINE=Analytic WITH(arena_block_size='2097152', compaction_strategy='default', compression='ZSTD', enable_ttl='false', num_rows_per_row_group='8192', segment_duration='', ttl='7d', update_mode='OVERWRITE', write_buffer_size='33554432')
``` 

<!---
What is expected.
-->

**Additional Information**

<!---
If possible, please attach other context you think may relate to this problem. Like *runtime environment*, *modified config*, *error log* etc.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/118/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/118,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G_tR6,horaedb,1191105658,118,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-07-21T06:46:57Z,2022-07-21T06:46:57Z,"Ooops, so many cases to fix ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G_tR6/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/118,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G_1bC,horaedb,1191139010,118,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-07-21T07:25:07Z,2022-07-21T07:25:07Z,"I can't reproduce this issue with this server version:
```
CeresDB Version: 0.1.0
Git branch: main
Git commit: 21bc3e5
Build: 2022-07-21T07:19:54.599454+00:00
```
which version did you test against?


This is what I got:
```
Table,Create Table, String(StringBytes(b""02_function_aggretate_table1"")),String(StringBytes(b""CREATE TABLE `02_function_aggretate_table1` (`timestamp` timestamp NOT NULL, `tsid` uint64 NOT NULL, `arch` string TAG, `datacenter` string TAG, `value` int, PRIMARY KEY(timestamp,tsid), TIMESTAMP KEY(timestamp)) ENGINE=Analytic WITH(arena_block_size='2097152', compaction_strategy='default', compression='UNCOMPRESSED', enable_ttl='false', num_rows_per_row_group='8192', segment_duration='', ttl='7d', update_mode='OVERWRITE', write_buffer_size='3221225472')"")),
```

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5G_1bC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/118,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HAvr-,horaedb,1191377662,118,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-21T11:37:52Z,2022-07-21T11:37:52Z,Ignore this. I forget to re-build the binary 🤧,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HAvr-/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/120,https://api.github.com/repos/apache/horaedb/issues/120,horaedb,1312574138,120,Proposal: make usage of `ScanIter` same as `RocksLogIterator` ,ygf11,3428089,ygf11,yanggf23@gmail.com,CLOSED,2022-07-21T04:41:14Z,2023-02-09T11:21:54Z,"**Description**
The usage of `RocksLogIterator`:
1. seek or next
2. visit key and value

The usage of `ScanIter`:
1. visit key and value
2. next

It is ok in main branch, since it will decode value immediately。
As I introduce #119, the value will be decoded later, and the item will be `&[u8]`, it works in `RocksLogIterator`. But `TableLogIterator` will violate the borrow checker.

https://github.com/CeresDB/ceresdb/blob/21bc3e51bd03890a2998919b2d405ab3adc1b8b4/wal/src/table_kv_impl/region.rs#L686-L708

What do you think? @waynexia @ShiKaiWi @chunshao90 
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/120/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/120,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HAbSS,horaedb,1191294098,120,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-07-21T10:03:52Z,2022-07-21T10:03:52Z,@ygf11 I have fetched your PR changes and compiler don't make any complaint. So could you give more explanation about _TableLogIterator will violate the borrow checker_.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HAbSS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/120,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HAeRj,horaedb,1191306339,120,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-07-21T10:16:25Z,2022-07-21T10:16:25Z,"> I have fetched your PR changes and compiler don't make any complaint. So could you give more explanation about TableLogIterator will violate the borrow checker.

@ShiKaiWi  Sorry, I change a litter to make compiler happy, but it is not correct.

I reset this in my pr, you can run `cargo build` to see the error. ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HAeRj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/120,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HBV5q,horaedb,1191534186,120,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-21T14:10:08Z,2022-07-21T14:10:08Z,"`ScanIter` is constructed by `TableLogIterator::scan_buckets()`. Its semantic is ""the first call to `next()` will drop the first entry and move iter to point to the second entry"". This makes us consume the ""current"" entry before calling `next()`. And this leads to the lifetime problem we are facing.

I haven't inspected how wide this semantic of `ScanIter` is used. If it only presents in this one place we can change it (but at a glance, I can see many references to that trait...). So I'm afraid we need some workarounds:
- Clone the bytes and keep it in `TableLogIterator` to decouple the lifetime.
- Wrap the return type of `ScanIter::value()` with a ref count. This may change lots of places either but logic is simpler.
- Wrap over the `ScanIter` to change its semantics only in this place. 

I plan to dig deeper in the next few days. Maybe we can simply make a clone first to unblock the whole progress.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HBV5q/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/120,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HDgLH,horaedb,1192100551,120,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-07-22T01:56:34Z,2022-07-22T01:56:34Z,"@waynexia  Thanks for your response.

> Its semantic is ""the first call to next() will drop the first entry and move iter to point to the second entry"". This makes us consume the ""current"" entry before calling next(). And this leads to the lifetime problem we are facing.

That's what problem I mean.

> Wrap the return type of ScanIter::value() with a ref count. This may change lots of places either but logic is simpler.

Do you mean `Arc<Vec<u8>>`? 
If so, the value type of `RocksLogIterator` should also be `Arc<Vec<u8>>`, then clone becomes inevitable, which we may care about.

> I plan to dig deeper in the next few days. Maybe we can simply make a clone first to unblock the whole progress

Let's clone it first, and make it better later.

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HDgLH/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/121,https://api.github.com/repos/apache/horaedb/issues/121,horaedb,1312986339,121,Add workflow to push docker image on release,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-07-21T09:50:24Z,2022-07-22T00:52:26Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->
When release, we will provide Docker image for users to use, it would be convenient to automate this process.

**Proposal**

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/121/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/121,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HAtAw,horaedb,1191366704,121,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-21T11:25:15Z,2022-07-21T11:25:15Z,Maybe we need one more action to build docker image (without publishing). It is necessary to do some verification before publishing it.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HAtAw/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/121,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HDYkH,horaedb,1192069383,121,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-07-22T00:52:26Z,2022-07-22T00:52:26Z,"It seem not very useful for me, how could we verify the image without publish? 

Also if we change build stuff, we should test it locally to ensure it works as expected.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HDYkH/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/122,https://api.github.com/repos/apache/horaedb/issues/122,horaedb,1312991898,122,Prepare user guide  for 0.2 release,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-07-21T09:54:27Z,2022-07-22T03:48:22Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->
We will release 0.2 by the end of this month, a user guide is required for user to get started, it should be at least include:
- Data model used by CeresDB
- How to create/insert/query/alter tables


**Proposal**

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/122/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/123,https://api.github.com/repos/apache/horaedb/issues/123,horaedb,1313156207,123,Support dummy select without table for string literal,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-21T12:07:19Z,2022-08-15T09:31:09Z,"**Description**

Support queries like
```sql
select ""string literal"";
```
Current the execution result is 
```
SELECT 'a';

Failed to execute query, err: Client(""Failed to parse schema from JSON"")
```

**Proposal**

Support this.

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->
ref #154 
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/123/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/123,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HKARB,horaedb,1193804865,123,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-07-25T09:26:27Z,2022-07-25T09:26:27Z,"- https://github.com/sqlparser-rs/sqlparser-rs/pull/530

The upstream has already fix this, but not release yet.  Just wait a moment 🤔","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HKARB/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/126,https://api.github.com/repos/apache/horaedb/issues/126,horaedb,1314054277,126,Provide guide about how to use ceresdb docker image,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-07-22T02:23:23Z,2022-07-25T06:24:02Z,"**Description**
The docker image of ceresdb has been released [here](https://hub.docker.com/r/ceresdb/ceresdb-server) but no guide is provided for users to start the container.

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

**Proposal**
We should provide a guide about how to start ceresdb-server container on the [page](https://hub.docker.com/r/ceresdb/ceresdb-server).

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/126/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/127,https://api.github.com/repos/apache/horaedb/issues/127,horaedb,1314405028,127,Select function(scalar) return error,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-07-22T06:26:28Z,2023-03-02T13:41:01Z,"**Describe this problem**

<!---
What this problem is and what happened.
-->
Currently, select function(scalar) will return err like
```
Failed to execute query, err: Server(ServerError { code: 500, msg: ""Failed to execute interpreter, query: SELECT NOT(1);. Caused by: Failed to execute select, err:Failed to execute logical plan, err:Failed to do logical optimization, err:DataFusion Failed to optimize logical plan, err:Internal error: NOT 'Literal { value: Int64(1) }' can't be evaluated because the expression's type is Int64, not boolean or NULL. This was likely caused by a bug in DataFusion's code and we would welcome that you file an bug report in our issue tracker."" })

```
**Steps to reproduce**

Execute this SQL
```
SELECT NOT(1);
```
<!---
How to reproduce this problem.
-->

**Expected behavior**

<!---
What is expected.
-->
It should work without error

**Additional Information**

- https://github.com/CeresDB/ceresdb/blob/334bc6ce13aac2e72a0771419b4aefe2164255e9/tests/cases/00_dummy/select_1.result#L11
- https://github.com/CeresDB/ceresdb/issues/123
<!---
If possible, please attach other context you think may relate to this problem. Like *runtime environment*, *modified config*, *error log* etc.
-->
ref #154 

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/127/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/127,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WifnS,horaedb,1451882962,127,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-02T13:41:01Z,2023-03-02T13:41:01Z,"It seems I have used wrong arg type, sql below run fine.
```
SELECT not(true)
```
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WifnS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/128,https://api.github.com/repos/apache/horaedb/issues/128,horaedb,1314425562,128,Show tables statement support name like syntax,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-07-22T06:37:55Z,2022-11-08T09:10:53Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->
Currently, `show tables` is supported by CeresDB, it would be convenient to support name like syntax like in MySQL.
```
SHOW TABLES LIKE 'pattern'
```

**Proposal**

Support this syntax with reference with MySQL
<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

Currently it will return error below
- https://github.com/CeresDB/ceresdb/blob/334bc6ce13aac2e72a0771419b4aefe2164255e9/tests/cases/01_system/system_tables.result#L15
<!---
Add any other context or screenshots about the feature request here.
-->
ref #154 
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/128/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/128,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MmEd2,horaedb,1285048182,128,NA,QuintinTao,72123724,,,NA,2022-10-20T07:08:22Z,2022-10-20T07:08:22Z,"hi， I wonder that we need to support ""SHOW TABLES LIKE 'pattern'"" or  ""SHOW TABLES `name` LIKE '01_system_table1'""?
Is the `name` esential?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MmEd2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/128,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MmF93,horaedb,1285054327,128,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-10-20T07:14:24Z,2022-10-20T07:14:24Z,"- https://dev.mysql.com/doc/refman/8.0/en/show-tables.html
```
SHOW TABLES LIKE 'pattern'
```
This style is preferred.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MmF93/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/128,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MmGuf,horaedb,1285057439,128,NA,QuintinTao,72123724,,,NA,2022-10-20T07:17:33Z,2022-10-20T07:17:33Z,"Thanks I want to try it.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MmGuf/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/128,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MmIdx,horaedb,1285064561,128,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-10-20T07:24:23Z,2022-10-20T07:24:23Z,"Thanks, go ahead. 

Feel free to ask questions if you have any problems. 
- https://join.slack.com/t/ceresdbcommunity/shared_invite/zt-1dcbv8yq8-Fv8aVUb6ODTL7kxbzs9fnA","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MmIdx/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/128,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MrsFq,horaedb,1286521194,128,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-10-21T06:34:49Z,2022-10-21T06:34:49Z,"There are [two special chars](https://dev.mysql.com/doc/refman/8.0/en/pattern-matching.html) in `pattern`
- `_` to match any single character and 
- `%` to match an arbitrary number of characters (including zero characters).

We can replace `_` to `.` and `%` to `.*` to match regex syntax.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MrsFq/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/128,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MzHeN,horaedb,1288468365,128,NA,QuintinTao,72123724,,,NA,2022-10-24T06:12:10Z,2022-10-24T06:12:10Z,"> There are [two special chars](https://dev.mysql.com/doc/refman/8.0/en/pattern-matching.html) in `pattern`
> 
> * `_` to match any single character and
> * `%` to match an arbitrary number of characters (including zero characters).
> 
> We can replace `_` to `.` and `%` to `.*` to match regex syntax.

OK","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MzHeN/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/129,https://api.github.com/repos/apache/horaedb/issues/129,horaedb,1314432852,129,select system.public.tables will return error,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-07-22T06:41:55Z,2022-08-09T11:52:02Z,"**Describe this problem**
```
SELECT
    `timestamp`,
    `catalog`,
    `schema`,
    `table_name`,
    `engine`
FROM
    system.public.tables
WHERE
    table_name = '01_system_table1';

```
Currently, execute SQL above will cause an error

```

Failed to execute query, err: Server(ServerError { code: 500, msg: ""Failed to execute interpreter, query: SELECT     `timestamp`,     `catalog`,     `schema`,     `table_name`,     `engine` FROM     system.public.tables WHERE     table_name = '01_system_table1';. Caused by: Failed to execute select, err:Failed to execute logical plan, err:Failed to collect record batch stream, err:Stream error, msg:Convert from arrow record batch, err:External error: Execution error: Failed to read table, partition:3, err:Failed to scan table, table:tables, err:Failed to append datum, err:Data type conflict, expect:UInt64, given:String."" })

```

**Steps to reproduce**

Execute SQL above
<!---
How to reproduce this problem.
-->

**Expected behavior**

No error
<!---
What is expected.
-->

**Additional Information**

https://github.com/CeresDB/ceresdb/blob/334bc6ce13aac2e72a0771419b4aefe2164255e9/tests/cases/01_system/system_tables.result#L11

<!---
If possible, please attach other context you think may relate to this problem. Like *runtime environment*, *modified config*, *error log* etc.
-->
ref #154 
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/129/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/129,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HuufE,horaedb,1203431364,129,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-08-03T03:07:11Z,2022-08-03T03:07:11Z,"hello, can assign me?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HuufE/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/131,https://api.github.com/repos/apache/horaedb/issues/131,horaedb,1314575593,131,Add CeresDB data model description in user guide,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-07-22T07:55:36Z,2022-07-27T13:02:57Z,"**Description**
Add CeresDB data model description in user guide, dir in docs/user-guide


**Proposal**

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/131/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/135,https://api.github.com/repos/apache/horaedb/issues/135,horaedb,1314706098,135,Explain SQL interface in the user guide,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-22T08:59:03Z,2022-07-25T03:43:32Z,"**Description**

CeresDB supports a subset of SQL with some custom extension. We should provide a concrete reference for it.

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

**Proposal**

Fill the `SQL syntax` version in the user guide.

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/135/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/138,https://api.github.com/repos/apache/horaedb/issues/138,horaedb,1315858773,138,The `mysql server` error log cannot be located to the exact location,dust1,18304424,Yoke,834902408@qq.com,CLOSED,2022-07-24T09:41:36Z,2023-03-16T08:22:49Z,"**Description**

When I was debugging, I found that the stack information of the exception often could not locate the real exception occurrence point.

**Proposal**

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

when i debugging this sql:
```
CREATE TABLE d222 (
    name string TAG,
    value double NOT NULL,
    t timestamp NOT NULL TIMESTAMP KEY
) ENGINE=Analytic;
```
the error log only goes to the place where the last level exception was thrown.

```
2022-07-24 17:33:22.750 ERRO [server/src/mysql/worker.rs:117] Mysql service Failed to handle sql, err: Failed to create plan, query:CREATE TABLE d222 (
    name string TAG,
    value double NOT NULL,
    t timestamp NOT NULL TIMESTAMP KEY
) ENGINE=Analytic, err:Failed to create plan, err:Table must contain timestamp constraint
2022-07-24 17:33:22.750 ERRO [server/src/mysql/worker.rs:95] MysqlWorker on_query failed. err:Failed to handle sql:CREATE TABLE d222 (
    name string TAG,
    value double NOT NULL,
    t timestamp NOT NULL TIMESTAMP KEY
) ENGINE=Analytic, err:Failed to create plan, query:CREATE TABLE d222 (
    name string TAG,
    value double NOT NULL,
    t timestamp NOT NULL TIMESTAMP KEY
) ENGINE=Analytic, err:Failed to create plan, err:Table must contain timestamp constraint
```
It only goes to `server/src/mysql/worker.rs`, but I know where the exception really happens is not here. 
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/138/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/138,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HJqj2,horaedb,1193715958,138,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-07-25T08:00:36Z,2022-07-25T08:00:36Z,"In CeresDB, we use snafu for error handling, if you want backtrace, just add it in Error enum like this:

https://github.com/CeresDB/ceresdb/blob/a030c06fa2290139938bcd14b16eab3d0a60597c/server/src/context.rs#L14","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HJqj2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/138,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HJq_z,horaedb,1193717747,138,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-07-25T08:02:21Z,2022-07-25T08:02:21Z,got it! I will fix it later. thanks.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HJq_z/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/138,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HJsWV,horaedb,1193723285,138,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-25T08:07:46Z,2022-07-25T08:07:46Z,"The two locations (`server/src/mysql/worker.rs:117` and `server/src/mysql/worker.rs:95`) displayed in log are where we logging those errors (via `error!`). This is not very intuitive indeed.

In this case, I'll first search for `Table must contain timestamp constraint` as I guess it is the error description for the inner error. And I can get `RequireTimestamp` this error variant. Then I can search for it and find the actual place. This method works but it's not very convenient and robust.

The definition of `RequireTimestamp` is 
```rust
    #[snafu(display(""Table must contain timestamp constraint""))]
    RequireTimestamp,
```
which is just kind of constant literal. I.e. we choose not to include a concrete backtrace for this error variant. However according to snafu's [guide](https://docs.rs/snafu/0.7.1/snafu/guide/examples/backtrace/enum.Error.html#variant.UsualCase) it's recommended to include backtrace for leaf error:
>The most common leaf error should always include a backtrace field.

So maybe we need a further discussion about how to define our error type. And I can think of another way that only captures a line number in the leaf error (like `[server/src/mysql/worker.rs:117]` from the log) instead of a ""backtrace"" which should be more lightweight.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HJsWV/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/138,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HOKuF,horaedb,1194896261,138,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-07-26T02:17:40Z,2022-07-26T02:17:40Z,"> So maybe we need a further discussion about how to define our error type.

Actually, the style for definition of error type requires a backtrace for leaf error but this is implicit and maybe we should make it clear in our style guide.



> And I can think of another way that only captures line number in the leaf error (like `[server/src/mysql/worker.rs:117]` from the log) instead of a ""backtrace"" which should be more lightweight.

I guess the information is not enough for troubleshootting if only a line number of leaf error is captured because maybe more than one call path will lead to the same error.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HOKuF/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/139,https://api.github.com/repos/apache/horaedb/issues/139,horaedb,1315873702,139,both WeChat and slack links are dead,zwpaper,3764335,Wei Zhang,,CLOSED,2022-07-24T10:54:53Z,2022-07-25T02:53:07Z,"both WeChat and slack links are dead, maybe needs a more effective way to keep the WeChat invitation image active","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/139/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/139,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HIGM8,horaedb,1193304892,139,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-07-24T12:07:40Z,2022-07-24T12:07:40Z,"I just check slack link, it works as expected...

Make sure your network can connect to slack.com.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HIGM8/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/139,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HIgEU,horaedb,1193410836,139,NA,yeya24,25150124,Ben Ye,benye@amazon.com,NA,2022-07-24T23:00:22Z,2022-07-24T23:00:22Z,I think I encountered the same problem. WeChat QR code expired and cannot join Slack via invitation.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HIgEU/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/139,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HIvWh,horaedb,1193473441,139,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-07-25T02:02:35Z,2022-07-25T02:02:35Z,"<img width=""657"" alt=""image"" src=""https://user-images.githubusercontent.com/3848910/180678691-9f7db56a-5e8b-47dc-95c8-c31fda219aed.png"">

Ooops, I open the slack link in private mode, the link is indeed inactive, no idea why...

Just create a new one, could you guys try this link?
- https://join.slack.com/t/ceresdbcommunity/shared_invite/zt-1dcbv8yq8-Fv8aVUb6ODTL7kxbzs9fnA","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HIvWh/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/139,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HIwdO,horaedb,1193477966,139,NA,yeya24,25150124,Ben Ye,benye@amazon.com,NA,2022-07-25T02:11:55Z,2022-07-25T02:11:55Z,It works thank you!,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HIwdO/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/139,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HIz3y,horaedb,1193491954,139,NA,yeya24,25150124,Ben Ye,benye@amazon.com,NA,2022-07-25T02:38:38Z,2022-07-25T02:38:38Z,The WeChat qrcode still not working though,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HIz3y/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/139,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HI1Fa,horaedb,1193496922,139,NA,zwpaper,3764335,Wei Zhang,,NA,2022-07-25T02:49:58Z,2022-07-25T02:49:58Z,"but at least we have one valid way to communicate, I am also waiting for the reply for the QRCode you asked in slack 🤣","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HI1Fa/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/139,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HI1a7,horaedb,1193498299,139,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-25T02:53:07Z,2022-07-25T02:53:07Z,"Sorry for it. WeChat has been sent in slack, We'll update the link later 😣","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HI1a7/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/143,https://api.github.com/repos/apache/horaedb/issues/143,horaedb,1316445615,143,Upgrade rust edition to 2021,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-25T08:17:11Z,2022-08-09T10:52:03Z,"**Description**

upgrade the rust toolchain to [edition 2021](https://doc.rust-lang.org/edition-guide/rust-2021/index.html). Quote from the edition guide:
>The Rust 2021 Edition contains several changes that bring new capabilities and more consistency to the language, and opens up room for expansion in the future. 

**Proposal**

Upgrade all of our sub-crates.

I'm not sure about the impact of ""default resolver 2"". Other changes should be ok to us.

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/143/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/144,https://api.github.com/repos/apache/horaedb/issues/144,horaedb,1316469485,144,Update `parquet-testing` submodule,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-25T08:36:49Z,2022-09-23T02:57:51Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

the upstream parquet test dataset has two new commits
```bash
~/repo/CeresDB/components/parquet-testing master <2                                                                                                                                                           ruihang@ruihang 16:31:29
> git pull                               
Updating 7175a47..aafd3fc
Fast-forward
 data/README.md                         |  21 ++++++++++++---------
 data/alltypes_tiny_pages.parquet       | Bin 0 -> 454233 bytes
 data/alltypes_tiny_pages_plain.parquet | Bin 0 -> 811756 bytes
 data/delta_length_byte_array.parquet   | Bin 0 -> 3072 bytes
 4 files changed, 12 insertions(+), 9 deletions(-)
 create mode 100644 data/alltypes_tiny_pages.parquet
 create mode 100644 data/alltypes_tiny_pages_plain.parquet
 create mode 100644 data/delta_length_byte_array.parquet
```

**Proposal**

Update the dataset submodule. And maybe adopt some new test cases based on the new data?

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/144/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/145,https://api.github.com/repos/apache/horaedb/issues/145,horaedb,1316597457,145,replace the python2 entrypoint with shell,zwpaper,3764335,Wei Zhang,,CLOSED,2022-07-25T10:23:50Z,2022-07-28T08:19:55Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

1. python2 retired in 2020, we should not stop the process
2. the entrypoint.py is working on modifying files, exec processes, etc., shell script fits the requirement better
3. both supervisor and tinit are used to keep ceresdb alive, but supervisor is not necessary, docker did the trick

**Proposal**

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

1. replace entrypoint.py with entrypoint.sh
2. drop supervisor and run ceresDB under tinit directly

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

if we reach an agreement on this, I would like to take this job
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/145/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/145,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HKZ3B,horaedb,1193909697,145,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-25T11:10:17Z,2022-07-25T11:10:17Z,This sounds good 🚀  Thanks for your help!,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HKZ3B/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/145,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HOgix,horaedb,1194985649,145,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-07-26T04:27:03Z,2022-07-26T04:27:03Z,"I'm fine with first two point.

As for the third, supervisor allows us to better control, manage, and restart the processes inside the container, so I suggest keep it.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HOgix/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/145,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HPHz0,horaedb,1195146484,145,NA,zwpaper,3764335,Wei Zhang,,NA,2022-07-26T08:05:30Z,2022-07-26T08:05:30Z,"IMHO, if we have multi processes to be managed, supervisor would be a choice(maybe even not a good choice), but ceresDB only has one server process.

as for the management part, Docker containers are meant to be short living, process should be restarted with the container, not inside the container.

as for logging, containers should print to stdout/stderr directly so that the logging service like fluentd, filebeat, could collect them easily in CNCF would.

as for os signal etc., tinit did the trick.

so my suggestion would be to drop it, but we can leave it alone if you insisted.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HPHz0/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/145,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HQ7Hh,horaedb,1195618785,145,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-07-26T15:17:33Z,2022-07-26T15:17:33Z,"Thanks for your clarification, your proposal seems more cloud-native, so just go ahead.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HQ7Hh/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/146,https://api.github.com/repos/apache/horaedb/issues/146,horaedb,1316679161,146,Implement `route` interface in HTTP protocol,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-25T11:20:51Z,2023-04-06T02:32:16Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

CeresDB support `route` in gRPC interface. This interface is used to query table route information. Pseudo-code looks like
```rust
fn route(tables: Vec<TableName>) -> Vec<(TableName, EndPoint)>;
```

**Proposal**

Support this interface in HTTP protocol.

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/146/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/147,https://api.github.com/repos/apache/horaedb/issues/147,horaedb,1317612956,147,Auto-redirect traffic to specific instances,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-07-26T02:26:40Z,2023-03-13T09:01:29Z,"**Description**
In some scenarios where CeresDB servers are not accessible for client nodes because of the network isolation, the current end-to-end communications won't work any more.

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

**Proposal**
Make the CeresDB server able to auto-redirect the traffic to the right instances according to the routing inforamtion.

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**
We can only make gRPC service work first.

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/147/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/151,https://api.github.com/repos/apache/horaedb/issues/151,horaedb,1318431312,151,Should optimize the cargo build time,zwpaper,3764335,Wei Zhang,,CLOSED,2022-07-26T15:46:15Z,2023-11-03T06:47:39Z,"I've run `cargo build --release -Ztimings` and here is what I get:

-------

- Top 10

Unit | Total | Codegen | Features
-- | -- | -- | --
 libgit2-sys v0.13.2+1.4.2 build script (run) | 435.1s |   |  
 grpcio-sys v0.9.1+1.38.0 build script (run) | 253.9s |   | bindgen, boringssl-src, openssl, secure, use-bindgen
 librocksdb_sys v0.1.0 build script (run) | 227.5s |   | default, portable
 zstd-sys v2.0.1+zstd.1.5.2 build script (run) | 168.7s |   | default, legacy, std, zdict_builder
 datafusion v9.0.0 | 162.9s | 156.1s (96%) | crypto_expressions, default, regex_expressions, unicode_expressions
 datafusion-physical-expr v9.0.0 | 160.1s | 152.0s (95%) | blake2, blake3, crypto_expressions, default, md-5, regex, regex_expressions, sha2, unicode-segmentation, unicode_expressions
 parquet v15.0.0 | 126.0s | 122.0s (97%) | arrow, base64, brotli, default, flate2, lz4, snap, zstd
 arrow v15.0.0 | 124.4s | 116.9s (94%) | comfy-table, csv, csv_crate, default, flatbuffers, ipc, prettyprint, rand, test_utils
 libz-sys v1.1.8 build script (run) | 92.4s |   | default, libc, static, stock-zlib
 bzip2-sys v0.1.11+1.0.8 build script (run) | 84.6s |   | static

- Summary
Targets:	ceresdb 0.1.0 (lib, bin ""ceresdb-server"")
Profile:	release
Fresh units:	273
Dirty units:	459
Total units:	732
Max concurrency:	10 (jobs=6 ncpu=6)
Build start:	2022-07-26T07:56:01Z
Total time:	607.2s (10m 7.2s)
rustc:	rustc 1.59.0-nightly (f1ce0e6a0 2022-01-05)
Host: x86_64-unknown-linux-gnu
Target: x86_64-unknown-linux-gnu
Max (global) rustc threads concurrency:	0

------

In release building itself I think we can do the following to accelerate it:
- inspect whether we need libgit2. I know a use case is to fetch commit ID as version info. Maybe we can let CI provide this kind of info.
- Condition compiling rocksdb. Rocksdb is used as one of the WAL implementation in CeresDB. In the future it should be a kind of optional debugging dep rather than a requirement.
- Use GitHub Action cache. Theoretically, we can perform incremental compilation for most scenarios. I've configured cache in CI but it seems like it's not working. There is an issue with this topic https://github.com/CeresDB/ceresdb/issues/5. I haven't dug into it.

_Originally posted by @waynexia in https://github.com/CeresDB/ceresdb/issues/148#issuecomment-1195177300_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/151/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/151,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HRZBX,horaedb,1195741271,151,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-26T16:59:19Z,2022-07-26T16:59:19Z,Additional notes: I think the third one (leverage the cache) is the most feasible and can bring a large improvement (in theory) without touching the code.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HRZBX/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/151,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Iq3Qq,horaedb,1219195946,151,NA,waynexia,15380403,Ruihang Xia,,NA,2022-08-18T08:37:14Z,2022-08-18T08:37:14Z,"> Condition compiling rocksdb. Rocksdb is used as one of the WAL implementation in CeresDB. In the future it should be a kind of optional debugging dep rather than a requirement.

I think it's time to achieve this. For test purpose we have memory WAL implementation, RocksDB is not a requirement anymore. I filed https://github.com/CeresDB/ceresdb/issues/206 for this","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Iq3Qq/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/151,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YedKt,horaedb,1484378797,151,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2023-03-27T02:14:26Z,2023-03-27T02:14:26Z,"We have replaced the `grpcio` with `tonic`,`grpcio-sys` is unnecessary to be built now.
We will keep optimzing the build time in later.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YedKt/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/151,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qzxOS,horaedb,1791955858,151,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-11-03T06:47:39Z,2023-11-03T06:47:39Z,I guess most actions that can be taken has been done.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qzxOS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/153,https://api.github.com/repos/apache/horaedb/issues/153,horaedb,1319231738,153,Release v0.3,waynexia,15380403,Ruihang Xia,,CLOSED,2022-07-27T08:52:00Z,2022-08-29T12:13:16Z,"**Description**

We prepare to release `v0.3` at the end of Aug. Here is the feature list:
- Release multi-language client. Include [Java](https://github.com/CeresDB/ceresdb-client-java), [Rust](https://github.com/CeresDB/ceresdb-client-rs) and [Python](https://github.com/CeresDB/ceresdb-client-py).
- Support static cluster mode. And keep pushing toward a full-featured dynamic distributed version (related project: [Distributed CeresDB](https://github.com/orgs/CeresDB/projects/1)).
- Extend supported SQLs (tag: [`A-SQL`](https://github.com/CeresDB/ceresdb/issues?q=is%3Aissue+is%3Aopen+label%3AA-SQL)).
- Implement the [hybrid storage format](https://github.com/CeresDB/ceresdb/issues/77). And support reading from two formats.

Feel free to suggest or discuss other features you would like to add :heart:","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/153/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/153,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HkaEW,horaedb,1200726294,153,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-08-01T05:29:31Z,2022-08-01T05:29:31Z,Will ceresdb support multiple data sources? e.g. read records from mysql's REDO log and structure them into ceresdb's data structure storage,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HkaEW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/153,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Hkhru,horaedb,1200757486,153,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-08-01T06:14:36Z,2022-08-01T06:14:36Z,"> Will ceresdb support multiple data sources? 

This sounds like data ingest, are you meaning bulk load? 

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Hkhru/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/153,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Hkihv,horaedb,1200760943,153,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-08-01T06:18:48Z,2022-08-01T06:18:48Z,"> > Will ceresdb support multiple data sources?
> 
> This sounds like data ingest, are you meaning bulk load?

yes, which means that ceresdb can import data from other existing commercial database files. I don't know much about this, so i not sure the terminology.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Hkihv/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/153,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Hkl-C,horaedb,1200775042,153,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-08-01T06:37:00Z,2022-08-01T06:37:00Z,"I think bulk ingest is an important feature for easy adoption, [prometheus](https://github.com/prometheus/prometheus/pull/8084)/[influxdb](https://docs.influxdata.com/influxdb/cloud/write-data/bulk-ingest-cloud/) all support this, so will we.

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Hkl-C/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/153,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HpL4J,horaedb,1201978889,153,NA,waynexia,15380403,Ruihang Xia,,NA,2022-08-02T03:40:07Z,2022-08-02T03:40:07Z,"This might cover three scenarios. Let's narrow our discussion:
- For offline data migration, our persistent format is relatively straightforward -- only a few metadata and data in the parquet format, all stored in OSS. We can achieve this in a few ways. And for some common formats like CSV or standard parquet generated in other systems, we can also support them directly.
- Online data ingesting, on the other hand, would be a little more complicated. Maybe we need to add support for consuming data from streaming systems like Kafka, Flink, Pulsar or others. They have splendid ecosystems. By supporting them we can easily be integrated into various systems as a downstream warehouse. 
- The last one is querying from other databases. This may be a little off-topic but let me mention it as well. CeresDB is only a query frontend in this situation. In some cases I can imagine there are other projects that can do this. So I'll assign a low priority to this.

Offline migrating implementations are different case by case. We can support needed upstream on demand. Online ingesting also has a few candidating upstream, but I believe there is a common pattern among them. We can choose one to support at first if we decide to work on this. It can take a lot of effort and we need to discuss it further.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HpL4J/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/153,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H9uma,horaedb,1207363994,153,NA,archerny,14784412,Arthur Chern,weirong.cwr@gmail.com,NA,2022-08-07T09:10:04Z,2022-08-07T09:10:04Z,"Thanks for the summary @waynexia . I will give some additional comments on these scenarios. 

1. For data migration or data initialization from external data source, there could be some tools. But as far as I know, demands of this scenario is not so frequent. This feature can be implemented as an independent binary, like tools in mysql ecosystem. We can discuss this feature later.
2. Online data ingestion, this is a much more complex topic. If we start working on this, we should consider latency, consistency, transformation and other aspects in real-time computing. These requirements are commonly implemented using stream-computing framework like Apache Flink. So, in my opinion, the CeresDB project will be more focusing on core features of time-series database its own.
3. For the scenario: querying from other databases, there is a better choice, presto. So, we will not work on this direction.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H9uma/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/153,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JU2fs,horaedb,1230202860,153,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-08-29T12:12:53Z,2022-08-29T12:12:53Z,Released https://github.com/CeresDB/ceresdb/releases/tag/v0.3.0,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JU2fs/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/154,https://api.github.com/repos/apache/horaedb/issues/154,horaedb,1319306862,154,Tracking issue for sql supporting,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-07-27T09:51:51Z,2023-03-02T13:43:19Z,"**Description**

Support for SQL is very important, but it is far from perfect in ceresdb now. Now there are some deficiencies as follow:
+ Tests are not enough.
+ SQL grammar support is not enough.
+ Bugs in supported SQL grammar.
+ The implementation of dialect is too rough(partly due to the insufficient scalability of sqlparser).

**Proposal**
+ We plan to add and test sqls related to system table，ud，tpch first, and continue to improve after. The main works on this part are extending related sqls, adding testcases and fixing found bugs.
+ About refactoring the parser to support dialect, it seems  hard if still using sqlparser crate (see [issue](https://github.com/sqlparser-rs/sqlparser-rs/issues/7)) If you have a good idea, you can put the proposal forward.


**Additional context**","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/154/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/154,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HYMqF,horaedb,1197525637,154,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-07-28T00:46:09Z,2022-07-28T00:46:09Z,Maybe CeresDB can fork sqlparser for secondary development on its basis?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HYMqF/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/154,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HYS0p,horaedb,1197550889,154,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-28T01:35:05Z,2022-07-28T01:35:05Z,"The `sql/parser` submod is a kind of hard-fork.  So fork and/or wrap our dialect on top of it could be an option. https://github.com/CeresDB/ceresdb/blob/21bc3e51bd03890a2998919b2d405ab3adc1b8b4/sql/src/parser.rs#L374

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HYS0p/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/154,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HY0oQ,horaedb,1197689360,154,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2022-07-28T05:42:18Z,2022-07-28T05:42:18Z,"@dust1 @waynexia I intend to refactor as #155 temporarily.
But I think it's still no so good.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HY0oQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/154,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WigXF,horaedb,1451886021,154,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-02T13:43:18Z,2023-03-02T13:43:18Z,"This issue is kinds of stale, closing since no obvious work need to do now.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WigXF/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/155,https://api.github.com/repos/apache/horaedb/issues/155,horaedb,1320377853,155,refactor: move extending part to sqlparser,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-07-28T04:31:23Z,2022-08-10T06:39:33Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->
Now, the method of extending sqlparser in ceresdb is too rough. We copy tons of codes from sqlparser and it's a kind of hard-fork in fact. So as I think, forking sqlparser and moving the extending part to it may be better. 

**Proposal**
+ Moving 'Statement' used in ceresdb to sqlparser.
+ Add related keywords.
+ Add related branch in parse_statement().


<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/155/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/155,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HZMhZ,horaedb,1197787225,155,NA,waynexia,15380403,Ruihang Xia,,NA,2022-07-28T07:46:39Z,2022-07-28T07:46:39Z,">@dust1 @waynexia I intend to refactor as https://github.com/CeresDB/ceresdb/issues/155 temporarily.
But I think it's still no so good.
  -- from https://github.com/CeresDB/ceresdb/issues/154#issuecomment-1197689360

Yep... The burden of maintaining the parser part still exists.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HZMhZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/155,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HwpG8,horaedb,1203933628,155,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2022-08-03T13:13:10Z,2022-08-03T13:13:10Z,"> > @dust1 @waynexia I intend to refactor as #155 temporarily.
> > But I think it's still no so good.
> > -- from [#154 (comment)](https://github.com/CeresDB/ceresdb/issues/154#issuecomment-1197689360)
> 
> Yep... The burden of maintaining the parser part still exists.

The functions(such as parse_columns, parse_optional_column_option, etc) are customized in ceresdb and conflicts with those in sqlparser. It seems not so suitable to move them to sqlparser.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HwpG8/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/156,https://api.github.com/repos/apache/horaedb/issues/156,horaedb,1320502107,156,"Change table write_mode from ""Append"" to ""Overwrite"" dynamically",chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-07-28T07:18:23Z,2024-10-19T11:14:48Z,"**Description**

ref #104

**Proposal**

We may need to support changing table `option:write_mode` from `append` to `overwrite` and `overwrite` to `append` after table created.

**Additional context**
- Attention to the processing of SST reading/writing if change `overwrite` to `append`;
- Is it supported to change `append` to `overwrite`?


","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/156/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/158,https://api.github.com/repos/apache/horaedb/issues/158,horaedb,1320538535,158,Refactor parsing sql comment,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-07-28T07:52:01Z,2022-08-26T02:55:30Z,"**Description**

Replace `ColumnOption::DialectSpecific` with `ColumnOption::Comment` to parse sql comment.

**Proposal**

ref: https://github.com/CeresDB/ceresdb/blob/8d1472a33054ac7f92cc957a30f1a83e6da005f6/sql/src/parser.rs#L476

**Additional context**
ref #154 
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/158/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/162,https://api.github.com/repos/apache/horaedb/issues/162,horaedb,1323762023,162,Refactor write method in WalManager to improve performance ,ygf11,3428089,ygf11,yanggf23@gmail.com,CLOSED,2022-08-01T03:37:44Z,2022-08-18T06:34:28Z,"**Description**

After #159, we pass `dyn Payload` to write method of `WalManager`, this may has performance issue.
It is better to decouple encoder from write method. 

**Proposal**
We can define `WalEncoder`(maybe other name), like:
```rust
pub enum WalEncoder {
    Rocks(RocksLogEncoding),
    Obkv(ObkvLogEncoding)
}

impl WalEncoder {
   pub fn encode_to_write_batch<P:Payload>(&self, log_batch: &LogWriteBatch<P>) -> Result<(WriteBatch, u64)> {
       match self {
         WalEncoder::Rocks(encoder) => encoder.encode(payload),
         WalEncoder::Obkv(encoder) => encoder.encode(payload),
       }
   }
}
```

`Encoder` can get from `walManager`.
```rust
pub trait WalManager {
   fn encoder(&self) -> Encoder;
   fn write(&self, wb: WriteBatch) -> Result<()>,
}
```

So we need encode data before write to `WalManager`.
```rust
   let (wb, max_sequence_num) = self.wal.encoder().encode::<XXXPayload>(log_batch)?;
   self.wal.write(wb);
   ...
```


**Additional context**

cc: @waynexia @ShiKaiWi 
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/162/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/162,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HqiK1,horaedb,1202332341,162,NA,waynexia,15380403,Ruihang Xia,,NA,2022-08-02T10:58:06Z,2022-08-02T10:58:06Z,"Thanks for writing this up. Here are some thoughts:

WAL backend like `RocksDB` or `OBKV` is only responsible for persisting things. I think we need not dispatch the data encoding method on the backend. I.e., WAL's write method should operate on bytes (`Vec<u8>` or `dyn Payload`) rather than a specific data type (what we used to do). However, `Payload::encode_to()` is writing bytes to a mutable buffer, this lazy materialization may reduce some copies compare to pre-encode the data.

So the trade offs are: (a) accept the dynamic dispatch overhead and achieve lazy materialization or (b) remove dynamic dispatch and do encode work in advance.

At a glance both are all not large consumption. But I cannot figure out which is more efficient if I have to rank them. Maybe we need a benchmark to help us to made decision. Personally I prefer the second approach (also the way you proposed). It looks more neat to me 👀","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HqiK1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/162,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Hrw1r,horaedb,1202654571,162,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-08-02T14:08:58Z,2022-08-02T14:08:58Z,"> However, Payload::encode_to() is writing bytes to a mutable buffer, this lazy materialization may reduce some copies compare to pre-encode the data.

Yes, aggree. `pre-encode` has this drawback.

Let me try to add a benchmark first.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Hrw1r/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/162,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Hvx9U,horaedb,1203707732,162,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-08-03T09:27:58Z,2022-08-03T09:27:58Z,"> At a glance both are all not large consumption. But I cannot figure out which is more efficient if I have to rank them. Maybe we need a benchmark to help us to made decision. Personally I prefer the second approach (also the way you proposed). It looks more neat to me 👀

Provided a high throughout reducing some extra copies may make a difference to the performance. However, let the benchmark decide.

Thanks @ygf11 .","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Hvx9U/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/162,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5INRP4,horaedb,1211438072,162,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-08-11T00:33:38Z,2022-08-11T00:33:38Z,"It seems the two `LogEncoding` are almost the same, can we use a common `LogEncoding`?

https://github.com/CeresDB/ceresdb/blob/8cccedd01b2ac33768a6678f133e51a394468bda/wal/src/rocks_impl/encoding.rs#L18-L24

https://github.com/CeresDB/ceresdb/blob/8cccedd01b2ac33768a6678f133e51a394468bda/wal/src/table_kv_impl/encoding.rs#L100-L106","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5INRP4/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/162,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5INaoz,horaedb,1211476531,162,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-08-11T01:57:58Z,2022-08-11T01:57:58Z,"> It seems the two `LogEncoding` are almost the same, can we use a common `LogEncoding`?
> 
> https://github.com/CeresDB/ceresdb/blob/8cccedd01b2ac33768a6678f133e51a394468bda/wal/src/rocks_impl/encoding.rs#L18-L24
> 
> https://github.com/CeresDB/ceresdb/blob/8cccedd01b2ac33768a6678f133e51a394468bda/wal/src/table_kv_impl/encoding.rs#L100-L106

Sure. It seems the `LogEncoding` in the `table_kv_impl/encoding.rs` is a copy from `rocks_impl/encoding.rs`.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5INaoz/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/165,https://api.github.com/repos/apache/horaedb/issues/165,horaedb,1327984180,165,Add timeout parameter to request,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-08-04T02:29:31Z,2023-01-10T10:00:52Z,"**Description**

For now, there is no way for server to control how long a request will be, although a client can cancel a request at any time, the server thread  will still be running, which wastes server resources.


<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

**Proposal**

Add a timeout parameter for every request, then server can simply reject running when exceed this limit, besides it's best that the server can overwrite this value in case of client's misuse or administration.
<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

The proposal above don't deal with client disconnect, it's best the server can detect the disconnection, and cancel request afterwards.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/165/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/165,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Hzl5o,horaedb,1204706920,165,NA,waynexia,15380403,Ruihang Xia,,NA,2022-08-04T03:15:02Z,2022-08-04T03:15:02Z,"This is a good functionality 👍 

But before that, it seems like we don't have a canonical way to config parameters. The only place I know is table options that are set on creating a table (e.g. `TTL`). We should have richer parameters that can be configured on multiple levels. Timeout is one of them, we can set it for tenant, table, or query. Other parameters like max query concurrency, queue up timeout, max querier node, timezone or timestamp precision, batch size, etc. are also configurable (those params above are just some random examples). 

These kinds of parameters are important parts of the user interface. I prefer to start by building an extensible parameter model rather than adding them one by one.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Hzl5o/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/165,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HztDg,horaedb,1204736224,165,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-08-04T04:15:51Z,2022-08-04T04:15:51Z,"> I prefer to start by building an extensible parameter model rather than adding them one by one.

+1, this is important, the interface used by client should be easy to extend, something like below:

```proto
message RequestHeader {
  int32 timeout_ms = 1;
}

message QueryRequest {
  RequestHeader header = 1;
  repeated string metrics = 2;
  string ql = 3;
}

```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5HztDg/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/170,https://api.github.com/repos/apache/horaedb/issues/170,horaedb,1329171845,170,Document upgrade and downgrade processes,zamazan4ik,7355383,Alexander Zaitsev,zamazan4ik@tut.by,CLOSED,2022-08-04T21:02:13Z,2022-08-08T07:20:08Z,"Hi.

After reading the documentation it is not clear for me, how upgrade and downgrade processes should be performed over the database. Could you please describe somewhere in the documentation:

- How to upgrade the database to the newer version (hopefully with no downtime)
- How to downgrade the database to the older version (hopefully with no downtime)
- Maybe some compatibility policies and notes between versions

Thanks in advance!","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/170/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/170,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H46mv,horaedb,1206102447,170,NA,waynexia,15380403,Ruihang Xia,,NA,2022-08-05T06:45:25Z,2022-08-05T06:45:25Z,"Glad you are interested 😃 

Nodes cannot be added or removed ""dynamically"" in the present version -- you need to replace and restart the entire cluster for upgrading or downgrading. Or in other words, we simply do not support changing versions so we didn't document it.

But this is definitely a key feature, we plan to ship it in the next one or two small milestones (v0.4 or v0.5). This is in the design (in mind) stage, @ShiKaiWi may lay out an RFC about how will CeresDB achieve the cluster mode. Graceful upgrade/downgrade should be included.

And for compatibility, we will do our best to keep backward compatibility in the storage format, and will obey the [Semantic Versioning](https://semver.org/) across all interfaces. If any breaking change is made it will be documented in the version note.

Hope that helps ❤️ ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H46mv/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/170,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H5SrB,horaedb,1206201025,170,NA,zamazan4ik,7355383,Alexander Zaitsev,zamazan4ik@tut.by,NA,2022-08-05T08:44:01Z,2022-08-05T08:44:01Z,@waynexia Thank you! Could you please put this information somewhere into the documentation?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H5SrB/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/170,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H-kMZ,horaedb,1207583513,170,NA,waynexia,15380403,Ruihang Xia,,NA,2022-08-08T02:27:16Z,2022-08-08T02:27:16Z,"Sure! Sorry for the late reply, I'll document it this morning","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H-kMZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/171,https://api.github.com/repos/apache/horaedb/issues/171,horaedb,1329397797,171,Bump sqlparser,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-08-05T03:26:51Z,2022-08-05T14:19:06Z,"**Description**

Bump version of sqlparser from 0.18.0 to 0.19.0, some bugs such as #123 can be fixed after doing this. 


**Proposal**
Maybe you have considered some ideas or solutions about this feature.


**Additional context**
Changelogs:
https://github.com/sqlparser-rs/sqlparser-rs/blob/main/CHANGELOG.md

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/171/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/171,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H4-Wi,horaedb,1206117794,171,NA,waynexia,15380403,Ruihang Xia,,NA,2022-08-05T07:06:44Z,2022-08-05T07:06:44Z,I've updated sqlparser in datafusion [d8184e71589e95b27db0e2c0ce0a8c51a6564c7a](https://github.com/waynexia/arrow-datafusion/commit/d8184e71589e95b27db0e2c0ce0a8c51a6564c7a),"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H4-Wi/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/175,https://api.github.com/repos/apache/horaedb/issues/175,horaedb,1331331540,175,Implement table roles,waynexia,15380403,Ruihang Xia,,CLOSED,2022-08-08T05:02:50Z,2023-02-09T11:25:00Z,"**Description**

<!---
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] 
(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)
-->

In distributed mode a table might be one of three roles: `Writer`, `Reader` and `Querier` based on the cluster's status. We need to implement these roles. The server only contains logic when a table is `Writer`, which can write data, append WAL, modify table options or perform compaction. Others two roles are
- `Reader` which only replicates WAL from the corresponding `Writer`, and provides the same query ability as `Writer`
- `Querier` which won't communicate with WAL. It is designed to only provides query ability on top of data in OSS. But we can let `Reader` provide those un-flushed fresh data in the future.

**Proposal**

This has serval parts:
- How __Role__ is represented
  Introduce a trait `RoleTable` and three impls `WriterTable`, `ReaderTable` and `QuerierTable`. The different logics of the same interface are encapsulated inside. This obstruction should only be used to group operation logic. The underlying `TableData` is the same one for different roles (for one table of course).
- Changes to the existing mental level
  The current hierarchy is `Instance` -> `Spaces` -> `Tables`, I propose to insert `RoleTable` between `Space` and `Table`. A `RoleTable` (and its three implementations) holds one `TableData`, and provides top-level interfaces to operate on the data like write or read. We are operating those data directly in `Instance`.
- How to manage the state of table's role
  `RoleTable` provides two mechanisms for syncing roles.
  - A atomic flag to represent current status.
    In most cases we are using `RoleTable` with type like `Arc<dyn RoleTable>`. That `Arc` handler cannot be changed quickly, thus an atomic flag is used to show what the actual status is. This mechanism is (designed) for rejecting calls to incorrect role. E.g., A table changed from `Writer` to `Reader` but the `WriterTable` handler is still kept by some tasks. We can set the flag and fail those tasks.
  - A notifier that sends a message on the last reference is dropped.
    Role changing is a long procedure. We might want to do something after the previous role's handles are fully dropped. This notifier is for that.
- How does WAL replicate (#179)
  WAL is an important and complex part in this ""role system"". `Writer` wants to write to it and `Reader` should keep reading from it. The proposed workflow looks like the following, I plan to use one replicator per `Instance` to replicate all the tables:
```
          register IDs
         need replicate
     ┌─────────────────────┐
     │                     │
     │              ┌──────▼───────┐
┌────┴─────┐        │  background  │
│Role Table│        │WAL Replicator│
└────▲─────┘        └──────┬───────┘
     │                     │
     └─────────────────────┘
          replicate log
            to table
```

**Tasks**
- [x] Implement WAL Replicator (#179)
- [x] `TableImpl` fetch table from `Instance` dynamically (#202)
- [ ] `drop_table` and `close_table` should require a`TableRef`
- [ ] `RoleTable` trait's defination
- [ ] Implement roles 

**Additional context**

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/175/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/175,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5IEaia,horaedb,1209116826,175,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-08-09T09:07:15Z,2022-08-09T09:07:15Z,"> In distributed mode a table might be one of three roles: Writer, Reader and Querier based on the cluster's status.

Those names are modeled after their relation with WAL, not role in a distributed system, in which those are usually called Master/Slave or Leader/Follower.

And `Writer`is kinds of misleading, since it can not only write, but also query. so naming may need reconsidered.

I suggest one possible naming style(with reference to [Kafka](https://kafka.apache.org/documentation/#replication)):

- `Writer` -> `LeaderTable`, like leader replica
- `Reader` -> `InSyncTable`, like in-sync replicas
- `Querier` -> `NoSyncTable`, like out-of-sync replica


","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5IEaia/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/175,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5IIFu8,horaedb,1210080188,175,NA,waynexia,15380403,Ruihang Xia,,NA,2022-08-10T02:32:39Z,2022-08-10T02:32:39Z,">And Writeris kinds of misleading, since it can not only write, but also query. so naming may need reconsidered.

Makes sense. `Leader`/`InSync`/`NoSync` looks more focused on the replicating roles 👍 

I haven't implemented this and we can discuss it with more sparks ✨ ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5IIFu8/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/175,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ISuKV,horaedb,1212867221,175,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2022-08-12T08:34:49Z,2022-08-12T08:34:49Z,Maybe expose table role in `system.public.tables`.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ISuKV/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/175,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ISzZ3,horaedb,1212888695,175,NA,waynexia,15380403,Ruihang Xia,,NA,2022-08-12T09:00:04Z,2022-08-12T09:00:04Z,"> Maybe expose table role in `system.public.tables`.

👍 sounds good","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ISzZ3/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/175,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5U4Qph,horaedb,1424034401,175,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-02-09T11:24:59Z,2023-02-09T11:24:59Z,"The necessity of table role needs more discussion in the future, and maybe follower role won't be introduced for its complexity.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5U4Qph/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/177,https://api.github.com/repos/apache/horaedb/issues/177,horaedb,1331412667,177,Can't build,szguoxz,17734539,,,CLOSED,2022-08-08T06:49:58Z,2022-08-09T06:23:50Z,"error[E0432]: unresolved import `upstream`
 --> grpcio\src\lib.rs:3:9
  |
3 | pub use upstream::*;
  |         ^^^^^^^^ use of undeclared crate or module `upstream`
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/177/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/177,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H_KLR,horaedb,1207739089,177,NA,waynexia,15380403,Ruihang Xia,,NA,2022-08-08T06:55:33Z,2022-08-08T06:55:33Z,"Which OS are you using? We currently don't support compiling on Windows (supported platform: https://github.com/CeresDB/ceresdb/blob/main/README.md#platform-support). I guess this conditional flag causes your problem:
https://github.com/CeresDB/ceresdb/blob/e9f7fa6c34c256eabe474f58e649768ff37e5b69/grpcio/Cargo.toml#L12-L16
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H_KLR/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/177,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H_LSn,horaedb,1207743655,177,NA,waynexia,15380403,Ruihang Xia,,NA,2022-08-08T07:01:34Z,2022-08-08T07:01:34Z,This dep (`grpcio`) says it can be built on windows (https://github.com/tikv/grpc-rs#prerequisites) but we haven't tried it... Sorry for the inconvenience.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5H_LSn/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/177,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5IAMSF,horaedb,1208009861,177,NA,szguoxz,17734539,,,NA,2022-08-08T11:33:30Z,2022-08-08T11:33:30Z,"yes, I am building on windows. Maybe that's the problem.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5IAMSF/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/183,https://api.github.com/repos/apache/horaedb/issues/183,horaedb,1334329068,183,Flush of one table might be triggered multiple times,waynexia,15380403,Ruihang Xia,,CLOSED,2022-08-10T09:20:12Z,2022-09-13T03:11:40Z,"### Describe This Problem

Flush is checked on the write path `preprocess_write()`. That method has three conditions to trigger a flush: `should_flush_instance`, `should_flush_space` and `should_flush_table`. When memory usage reached the threshold in the instance or space level a flush is then triggered. Consider two tables belonging to two different `WorkerLocal`s, they can trigger the flush in the instance or space level simultaneously, while these two flushes are likely to operate on the same table because of the logic of selecting tables.

To my understanding, the procedure described above may lead to two problems:
- A table may be flushed multiple times.
- A table may be operated by another `WorkerLocal` which will break our concurrent invariant ""The worker is single threaded and holding this is equivalent to holding a write lock""

### Proposal

The key points are
- Can we select and operate tables belonging to other workers?
We can select and schedule a flush for any table, but are only allowed to operate tables belonging to the correct worker.
- Should we ignore memory occupied by memtables that are flushing?
`should_flush_table` will distinguish these memories https://github.com/CeresDB/ceresdb/blob/91553aae4d446b4a3f31aa6e211ae1ffb592576f/analytic_engine/src/table/data.rs#L381 Trigger in instance and space levels can have the same logic. 


### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/183/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/183,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5IJpVf,horaedb,1210488159,183,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-08-10T10:36:45Z,2022-08-10T10:36:45Z,assign me!😆,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5IJpVf/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/187,https://api.github.com/repos/apache/horaedb/issues/187,horaedb,1335383840,187,Column order is not what user define when create table,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-08-11T02:03:20Z,2022-11-18T02:49:45Z,"### Describe this problem

Table' columns order is not what user define when create table

### Steps to reproduce

Execute those SQL
```sql
CREATE TABLE test_order (
    host string tag,
    v2 int,
    ts timestamp NOT NULL,
    v1 int,
    timestamp KEY (ts)
)ENGINE = Analytic WITH (
    enable_ttl = 'false'
);

desc test_order;
```
`desc` will output 

```
ts	timestamp	1	0	0
tsid	uint64	1	0	0
host	string	0	1	1
v1	int	0	1	1
v2	int	0	1	0
```

### Expected behavior

The column order should be same with the order when created.

### Additional Information

https://github.com/CeresDB/ceresdb/blob/3e826ffbb98d2c5351e63b28dd9dd4644e7d4c1e/sql/src/planner.rs#L265

The root cause is currently we use a BTreeMap when build create table plan, it should use a vector to keep the ordering

ref #154 ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/187/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/187,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JCTsZ,horaedb,1225341721,187,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-08-24T08:01:53Z,2022-08-24T08:01:53Z,"After some thoughts, I find to fix this issue is a non-trivial task.

Currently CeresDB will put primary key columns at the front, and use a `num_key_columns` in `Schema` to indicate how many columns are in primary key, so if we keep the original column ordering, then we need more info to know which column belongs to primary key.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JCTsZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/187,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KWlNi,horaedb,1247433570,187,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-09-15T00:30:47Z,2022-09-15T00:30:47Z,"Can we reorder fields by subscripting the fields? like    
https://github.com/CeresDB/ceresdb/blob/5c14f0055fb6816793b81bbc50c61b12b3079d5c/server/src/handlers/sql.rs#L201-L207
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KWlNi/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/187,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KXaWS,horaedb,1247651218,187,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-09-15T06:43:37Z,2022-09-15T06:43:37Z,"We can organize columns like what you said, but the more tricky issue is the order is used for primary key identification.

The first `num_key_columns` columns are primary key, if we keep the original order, then we need to keep some info to tell which columns are primary key","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KXaWS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/187,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KXqKN,horaedb,1247715981,187,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-09-15T07:51:02Z,2022-09-15T07:51:02Z,"This looks like the `BTree` needs to be removed, will it infect the storage layer?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KXqKN/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/187,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KZuCk,horaedb,1248256164,187,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-09-15T15:23:26Z,2022-09-15T15:23:26Z,"> will it infect the storage layer?

I think it will.

The rows within one table will be ordered by primary key. For example, the key is composed of `primary key + sequence` in memtable
- https://github.com/CeresDB/ceresdb/blob/a74eb6b5cbb89ef03cf7cf42a464d584bdd0b744/analytic_engine/src/memtable/skiplist/mod.rs#L84
- https://github.com/CeresDB/ceresdb/blob/a74eb6b5cbb89ef03cf7cf42a464d584bdd0b744/analytic_engine/src/memtable/key.rs#L123

If we want to fix this, we can remove `num_key_columns()` method in schema, and replace all its usage with `key_columns()`.

FYI, `num_key_columns` is only used in a few places, so this change may not be very large.
<img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/3848910/190443236-86e3a959-0498-43d2-8743-2e01f726855a.png"">
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KZuCk/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/187,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Khk47,horaedb,1250315835,187,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-09-18T14:04:46Z,2022-09-18T14:04:46Z,"maybe I can try to fix this, can it be assigned to me?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Khk47/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/187,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KiLL7,horaedb,1250472699,187,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-09-19T02:12:12Z,2022-09-19T02:12:12Z,"Sure, go ahead.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KiLL7/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/191,https://api.github.com/repos/apache/horaedb/issues/191,horaedb,1336869145,191,Optimized example configuration to enhance readability,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-08-12T07:44:24Z,2023-03-13T09:04:27Z,"### Describe This Problem

`type` in analytic.storage, analytic.storage.local_store, analytic.storage.remote_store is confusing.
https://github.com/CeresDB/ceresdb/blob/8cccedd01b2ac33768a6678f133e51a394468bda/docs/example.toml#L12
```
[analytic.storage]
type = ""Cache""
cache_opts = { max_cache_size = 10000000 }

[analytic.storage.local_store]
type = ""Local""
data_path = ""/tmp/ceresdb""

[analytic.storage.remote_store]
type = ""Aliyun""
key_id = ""key_id""
key_secret = ""key_secret""
endpoint = ""endpoint""
bucket = ""bucket""
```


### Proposal

Add some notes to explain what the `type` means.

### Additional Context

TODO: Add configuration explanation in user guide.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/191/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/194,https://api.github.com/repos/apache/horaedb/issues/194,horaedb,1338429130,194,Should we support `Date` type?,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-08-15T01:40:33Z,2023-02-24T02:07:05Z,"### Describe This Problem

`Date` is a common type in database, should we support it ?
Every row here must contain timestamp, `Date` type seems a bit redundant, but some information(such as birthday) seems more appropriate to express by `Date`?


### Proposal

+ Use i32/i64 to store date32/date64 like arrow.
+ Add necessary conversions like other defined types.
+ Format it in response.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/194/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/194,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Ig0d8,horaedb,1216563068,194,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-08-16T12:22:32Z,2022-08-16T12:22:32Z,"It is great if `Date` is supported, and we can leave this feature for volunteers in the community.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Ig0d8/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/194,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5J71ed,horaedb,1240422301,194,NA,impactCn,35489877,Sinsy,impactcn@apache.org,NA,2022-09-08T08:51:26Z,2022-09-08T08:51:26Z,I am try. pls assign me.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5J71ed/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/194,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5J74qL,horaedb,1240435339,194,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-09-08T09:02:52Z,2022-09-08T09:02:52Z,"> I am try. pls assign me.

The proposal is a little bit simple, and any discussion is welcome when you have any difficulty in implementing this feature.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5J74qL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/194,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OJh2W,horaedb,1311120790,194,NA,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,NA,2022-11-11T01:35:14Z,2022-11-11T01:35:14Z,"1. A date. The supported range is '1000-01-01' to '9999-12-31'. MySQL displays [DATE](https://dev.mysql.com/doc/refman/8.0/en/datetime.html) values in 'YYYY-MM-DD' format.See:https://dev.mysql.com/doc/refman/8.0/en/date-and-time-type-syntax.html
2. But permits assignment of values to [DATE](https://dev.mysql.com/doc/refman/8.0/en/datetime.html) columns using either strings or numbers. like: STR_TO_DATE,SUB_DATE,See:https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html#function_str-to-date
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OJh2W/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/194,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OJo29,horaedb,1311149501,194,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-11-11T02:26:29Z,2022-11-11T02:26:29Z,"@MachaelLee 
> But permits assignment of values to [DATE](https://dev.mysql.com/doc/refman/8.0/en/datetime.html) columns using either strings or numbers

For such conversion, I guess we already have an example for that -- converting string date literal to timestamp literal.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OJo29/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/194,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OJyd1,horaedb,1311188853,194,NA,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,NA,2022-11-11T03:35:36Z,2022-11-11T03:35:36Z,"> @MachaelLee
> 
> > But permits assignment of values to [DATE](https://dev.mysql.com/doc/refman/8.0/en/datetime.html) columns using either strings or numbers
> 
> For such conversion, I guess we already have an example for that -- converting string date literal to timestamp literal.

It is not common that converting string date literal to timestamp literal. Usually we convert timestamp to date with ` cast( timestamp as date)`;  sometimes we convert date to unixtimetamp with `UNIX_TIMESTAMP()`, it returns the value of the argument as seconds since '1970-01-01 00:00:00' UTC.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OJyd1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/198,https://api.github.com/repos/apache/horaedb/issues/198,horaedb,1340289737,198,RFC for cluster mode of CeresDB,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-08-16T12:36:54Z,2024-10-19T11:15:39Z,"### Describe This Problem

Now many preparations for cluster mode of CeresDB has been done including:
- CeresMeta supports the management of cluster metadata.
- CeresDB supports communications with CeresMeta.
- CeresDB supports to store SST on the Storage.
- ...

However, there is still no document to draw a comprehensive picture for the cluster mode of CeresDB.

### Proposal

Draw an RFC about the CeresDB's cluster mode.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/198/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/206,https://api.github.com/repos/apache/horaedb/issues/206,horaedb,1342746877,206,Make RocksDB a conditional dependency,waynexia,15380403,Ruihang Xia,,CLOSED,2022-08-18T08:36:13Z,2023-10-25T13:52:36Z,"### Describe This Problem

RocksDB is one of the WAL implementations, also the earliest one. We have to depend on it for everything. It's a great project and does help a lot but it also brings lots of problems like flaky tests #85, long compile time #151 or strange compile problem #16.

And things have changed for now. After #62 we have other WAL implementations like one implemented on mem. It's time to make RocksDB an optional dependency and only compile it when needed.


### Proposal

- Make RocksDB and related code (e.g, tests) optional via feature gate
- Migrate existing tests to the memory impl, so they are still available without RocksDB. The memory impl is inside https://github.com/CeresDB/ceresdb/blob/5bfb5772668fa5f09a4467f476e2146725e7ebe5/wal/src/table_kv_impl/wal.rs#L100 with https://github.com/CeresDB/ceresdb/blob/5bfb5772668fa5f09a4467f476e2146725e7ebe5/components/table_kv/src/memory.rs#L193 [This benchmark](https://github.com/CeresDB/ceresdb/blob/main/benchmarks/src/wal_write_bench.rs) already make use of it.
- Make it a default option on standalone mode.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/206/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/206,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Mg-oU,horaedb,1283713556,206,NA,QuintinTao,72123724,,,NA,2022-10-19T09:36:18Z,2022-10-19T09:36:18Z,"hi，I want to participate this issue. but I am not very clear about those proposals.
1.""Make RocksDB and related code (e.g, tests) optional via feature gate""  I can see a conditional in here. 
setup.rs
if config.analytic.obkv_wal.enable {
2.Migrate existing tests to the memory impl, so they are still available without RocksDB. 
which part do you mean? 

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Mg-oU/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/206,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MhBiu,horaedb,1283725486,206,NA,waynexia,15380403,Ruihang Xia,,NA,2022-10-19T09:45:47Z,2022-10-19T09:45:47Z,"Greeting! Thanks for your interesting :heart:

>1.""Make RocksDB and related code (e.g, tests) optional via feature gate"" I can see a conditional in here.
setup.rs
if config.analytic.obkv_wal.enable {

This is one part. But the ""conditional"" I refer to is rust's [conditional compilation](https://doc.rust-lang.org/reference/conditional-compilation.html). That is, telling the compiler not to compile rocksdb-related code under some condition (e.g., via a feature gate). By doing so we only need to pay for rocksdb's compilation when we needed.

>2.Migrate existing tests to the memory impl, so they are still available without RocksDB.
which part do you mean?

This is another aspect. Current we are using RocksDB to backing some local test cases. But in most situation we don't have to rely on it. One of the main scenario is tests using WAL. We have a WAL implementation in memory which can replace RocksDB to accomplish the test.

Hope I answered your question and feel free to point out any uncertain thing 😉","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MhBiu/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/206,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Mksee,horaedb,1284687774,206,NA,QuintinTao,72123724,,,NA,2022-10-19T23:12:43Z,2022-10-19T23:12:43Z,"> Greeting! Thanks for your interesting heart
> 
> > 1.""Make RocksDB and related code (e.g, tests) optional via feature gate"" I can see a conditional in here.
> > setup.rs
> > if config.analytic.obkv_wal.enable {
> 
> This is one part. But the ""conditional"" I refer to is rust's [conditional compilation](https://doc.rust-lang.org/reference/conditional-compilation.html). That is, telling the compiler not to compile rocksdb-related code under some condition (e.g., via a feature gate). By doing so we only need to pay for rocksdb's compilation when we needed.
> 
> > 2.Migrate existing tests to the memory impl, so they are still available without RocksDB.
> > which part do you mean?
> 
> This is another aspect. Current we are using RocksDB to backing some local test cases. But in most situation we don't have to rely on it. One of the main scenario is tests using WAL. We have a WAL implementation in memory which can replace RocksDB to accomplish the test.
> 
> Hope I answered your question and feel free to point out any uncertain thing wink

Thanks for your patient explanation. Let me try.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Mksee/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/206,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5N4Ba0,horaedb,1306531508,206,NA,QuintinTao,72123724,,,NA,2022-11-08T02:25:18Z,2022-11-08T02:25:18Z,"hi， I want to add features in the wal model. default is rocksdb.
```
[features]
default = [""rocksdb""]
rocksdb = []
tablekv = []
```

then I will mark the code with:
```rust
#[cfg(feature = ""rocksdb"")]
#[cfg(feature = ""rocksdb"")]
#[derive(Default)]
pub struct RocksEngineBuilder;
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5N4Ba0/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/206,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5N4Era,horaedb,1306544858,206,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-11-08T02:44:43Z,2022-11-08T02:44:43Z,"> hi， I want to add features in the wal model. default is rocksdb.
> 
> ```
> [features]
> default = [""rocksdb""]
> rocksdb = []
> tablekv = []
> ```
> 
> then I will mark the code with:
> 
> ```rust
> #[cfg(feature = ""rocksdb"")]
> #[cfg(feature = ""rocksdb"")]
> #[derive(Default)]
> pub struct RocksEngineBuilder;
> ```

I guess this is not enough. More things need fixing, including:
- Use the feature gate to control whether to compile `wal/src/rocks_impl` module;
- Use the conditional compilation to control the codes to use memory-wal (you can refer to https://github.com/CeresDB/ceresdb/blob/main/benchmarks/src/wal_write_bench.rs#L68-L74) instead of `wal/src/rocks_impl` if necessary;","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5N4Era/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/206,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5P-tLK,horaedb,1341838026,206,NA,QuintinTao,72123724,,,NA,2022-12-08T01:18:42Z,2022-12-08T01:18:42Z,"> > hi， I want to add features in the wal model. default is rocksdb.
> > ```
> > [features]
> > default = [""rocksdb""]
> > rocksdb = []
> > tablekv = []
> > ```
> > 
> > 
> >     
> >       
> >     
> > 
> >       
> >     
> > 
> >     
> >   
> > then I will mark the code with:
> > ```rust
> > #[cfg(feature = ""rocksdb"")]
> > #[cfg(feature = ""rocksdb"")]
> > #[derive(Default)]
> > pub struct RocksEngineBuilder;
> > ```
> 
> I guess this is not enough. More things need fixing, including:
> 
> * Use the feature gate to control whether to compile `wal/src/rocks_impl` module;
> * Use the conditional compilation to control the codes to use memory-wal (you can refer to https://github.com/CeresDB/ceresdb/blob/main/benchmarks/src/wal_write_bench.rs#L68-L74) instead of `wal/src/rocks_impl` if necessary;

I am sorry. I was busy with other things. May continue now?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5P-tLK/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/206,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QGwGS,horaedb,1343947154,206,NA,QuintinTao,72123724,,,NA,2022-12-09T07:22:25Z,2022-12-09T07:22:25Z,"hi, if I use feature gate to cntrol rocks related code.
I use cfg here
![image](https://user-images.githubusercontent.com/72123724/206647283-2b046d89-d0ad-40d7-a5af-71ad9d59754a.png)

 match config.analytic.wal_storage {
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^ pattern `RocksDB` not covered","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QGwGS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/206,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QHJWG,horaedb,1344050566,206,NA,waynexia,15380403,Ruihang Xia,,NA,2022-12-09T09:16:47Z,2022-12-09T09:16:47Z,Does also add this feature gate to this variant in the enum definition work?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QHJWG/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/206,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QNtfN,horaedb,1345771469,206,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-12-12T02:14:30Z,2022-12-12T02:14:30Z,"> hi, if I use feature gate to cntrol rocks related code. I use cfg here ![image](https://user-images.githubusercontent.com/72123724/206647283-2b046d89-d0ad-40d7-a5af-71ad9d59754a.png)
> 
> match config.analytic.wal_storage { | ^^^^^^^^^^^^^^^^^^^^^^^^^^^ pattern `RocksDB` not covered

Maybe you should add the feature gate to the definition of WalStorageConfig to control the field `WalStorageConfig::RocksDB`.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QNtfN/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/206,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5R7Ks1,horaedb,1374464821,206,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-01-07T12:27:15Z,2023-01-07T12:27:15Z,"@QuintinTao Hi, any progress on this issue?

This issue may not easy to deal with, so if you are having any trouble,  you can ask questions here or choose other [good first issues](https://github.com/CeresDB/ceresdb/labels/good%20first%20issue).

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5R7Ks1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/206,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qDk-V,horaedb,1779322773,206,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2023-10-25T13:50:33Z,2023-10-25T13:50:33Z,finished in #1270 ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qDk-V/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/210,https://api.github.com/repos/apache/horaedb/issues/210,horaedb,1345762690,210,Can Ceresdb support default value for columns?,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2022-08-22T03:40:02Z,2022-11-01T13:09:16Z,"### Describe This Problem

Can Ceresdb support default value for columns something like `timestamp = now()`

### Proposal

* In Oceanbase, I can record data insert time,  like gmt_create = now().

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/210/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/210,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5I012J,horaedb,1221811593,210,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2022-08-22T04:42:29Z,2022-08-22T04:42:29Z,"It is still not supported now , are you interested to make it?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5I012J/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/210,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5I16Gi,horaedb,1222091170,210,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-08-22T09:25:13Z,2022-08-22T09:25:13Z,"It's a very useful feature, and we will support it in the near future.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5I16Gi/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/210,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Jui4D,horaedb,1236938243,210,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-09-05T12:24:05Z,2022-09-05T12:24:05Z,"Does it means we need support default value which is an `expr`?

The grammar may be like:
`CREATE TABLE test(a int DEFAULT b + 1, b int DEFAULT 0, c timestamp DEFAULT now())...;`
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Jui4D/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/210,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Jun_J,horaedb,1236959177,210,NA,waynexia,15380403,Ruihang Xia,,NA,2022-09-05T12:39:40Z,2022-09-05T12:39:40Z,"> Does it means we need support default value which is an `expr`?
> 
> The grammar may be like: `CREATE TABLE test(a int DEFAULT b + 1, b int DEFAULT 0, c timestamp DEFAULT now())...;`

Yes, that gramma looks great! (only we don't support `now()` at present 🤣).","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Jun_J/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/210,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Juo8c,horaedb,1236963100,210,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-09-05T12:42:22Z,2022-09-05T12:42:22Z,"> Yes, that gramma looks great! (only we don't support now() at present 🤣).

Let me have a try :D","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Juo8c/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/210,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KMoXC,horaedb,1244825026,210,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-09-13T02:43:06Z,2022-09-13T02:43:06Z,"> `CREATE TABLE test(a int DEFAULT b + 1, b int DEFAULT 0, c timestamp DEFAULT now())`

In this statement, `a = b + 1` seems a little bit complex to implement, and I suppose the feature that defines default value as any expression) can be supported in another PR. What we can do here is to define the default value as some simple expression, e.g. literal value or some built-in function like `now`.

@ygf11 @waynexia What do you think of it?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KMoXC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/210,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KMp_N,horaedb,1244831693,210,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-09-13T02:55:50Z,2022-09-13T02:55:50Z,"> In this statement, a = b + 1 seems a little bit complex to implement, and I suppose the feature that defines default value as any expression) can be supported in another PR. What we can do here is to define the default value as some simple expression, e.g. literal value or some built-in function like now.

I almost finish most of it, but submiting a simple implement is also ok to me.  I will submit a pr today or tomorrow.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KMp_N/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/210,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KMvPC,horaedb,1244853186,210,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-09-13T03:32:02Z,2022-09-13T03:32:02Z,"> > In this statement, a = b + 1 seems a little bit complex to implement, and I suppose the feature that defines default value as any expression) can be supported in another PR. What we can do here is to define the default value as some simple expression, e.g. literal value or some built-in function like now.
> 
> I almost finish most of it, but submiting a simple implement is also ok to me. I will submit a pr today or tomorrow.

It will be surely great if you have made it.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KMvPC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/210,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KRmaP,horaedb,1246127759,210,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-09-14T01:47:00Z,2022-09-14T01:47:00Z,"
I thought of one corner case when default value defined in expression
> what if two columns's default value reference each other? 

[MySQL](https://dev.mysql.com/doc/refman/8.0/en/data-type-defaults.html) only allow to reference a column defined before it, how will you avoid this?
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KRmaP/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/210,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KRpEf,horaedb,1246138655,210,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-09-14T02:07:35Z,2022-09-14T02:07:35Z,"> I thought of one corner case when default value defined in expression

Yes, it is a problem. So circle reference is not allowed here, we can check and return error when create table.

> [MySQL](https://dev.mysql.com/doc/refman/8.0/en/data-type-defaults.html) only allow to reference a column defined before it, how will you avoid this?

We can reorder the missing columns, the simple columns will run first, and other columns which depends it will run after.

for example:
```sql
CREATE TABLE t(c1 string tag not null, 
               ts timestamp not null, 
               c3 uint32 Default c4, 
               c4 uint32 Default c5, 
               c5 uint32 Default 0, timestamp key(ts),primary key(c1, ts)) \
               ENGINE=Analytic WITH (ttl='70d',update_mode='overwrite',arena_block_size='1KB')""

Insert into t(c1, ts) values(xx, xxx);
``` 

The original order of the missing column is `(c3, c4, c5)`.
After we reorder them, they will be `(c5, c4 c3)`. This running order is ok.

Maybe the behavior of mysql is also an alternative, because the full implement maybe a little complex like @ShiKaiWi says.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KRpEf/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/210,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KRybW,horaedb,1246176982,210,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-09-14T03:12:28Z,2022-09-14T03:12:28Z,"> Maybe the behavior of mysql is also an alternative, because the full implement maybe a little complex like @ShiKaiWi says.

Yep, I think we can follow what MySQL does, leave circle reference detect/reorder for future work. 

A tracking issue can be created to discuss those optimization.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KRybW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/210,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KSDvj,horaedb,1246247907,210,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-09-14T05:21:10Z,2022-09-14T05:21:10Z,"> A tracking issue can be created to discuss those optimization.

I create a tracking issue #252.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KSDvj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/210,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NZVFd,horaedb,1298485597,210,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-11-01T13:09:16Z,2022-11-01T13:09:16Z,"For now, this feature is ready for basic use, so closing.

```sql
CREATE TABLE `test` (
`ts` timestamp TIMESTAMP KEY NOT NULL, 
`t1` string default ""hello"",  
 `ts2` timestamp default now()
) with (
 enable_ttl = 'false'
);

insert into `test`  (ts) 
values (123);

select * from `test`
```
Wii return
```
{
    ""rows"": [
        {
            ""ts"": 123,
            ""tsid"": 0,
            ""t1"": ""hello"",
            ""ts2"": 1667307913166
        }
    ]
}
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NZVFd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/211,https://api.github.com/repos/apache/horaedb/issues/211,horaedb,1345924769,211,Fail to insert negative value,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-08-22T07:08:08Z,2022-08-24T06:44:14Z,"### Describe this problem

Now, CeresDB reports error when inserting a negative value (int or double).

### Steps to reproduce

Execute following sqls:
```sql
CREATE TABLE `test_negative_insert` (`t` timestamp not null, `v` int, TIMESTAMP KEY(t))

INSERT INTO test_negative_insert (t,  v) VALUES (1660233600000, -1);
```

Error message will be thrown:
```plaintext
err:Failed to create plan, query:INSERT INTO test_negative_insert (t,  v) VALUES (1660233600000, -1);, err:Failed to create plan, err:Invalid insert stmt, source expr is not value
```

### Expected behavior

Succeed in inserting negative values.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/211/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/211,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5I23Zp,horaedb,1222342249,211,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-08-22T13:12:22Z,2022-08-22T13:12:22Z,"I find out that the parsed expression of `-1` by sql-parser is `UnaryOp { op: Minus, expr: Value(Number(\“-1\“, false)) }`","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5I23Zp/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/216,https://api.github.com/repos/apache/horaedb/issues/216,horaedb,1350533798,216,Maybe we should use http status code defined in existing lib.,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-08-25T08:31:49Z,2022-09-01T08:37:09Z,"### Describe This Problem

We define http status codes in ceresdb now, but is it better to use existing [lib](https://docs.rs/http/latest/http/status/struct.StatusCode.html)?

### Proposal

Use existing lib of http status codes to replace our manual defined one.

### Additional Context

None.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/216/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/220,https://api.github.com/repos/apache/horaedb/issues/220,horaedb,1351811624,220,"Create/insert with case-sensitive table name success, but not found while select.",Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-08-26T06:27:31Z,2022-11-18T10:43:37Z,"### Describe this problem

Create/insert a table named like ""DeMo"" will success.
But when I select something in this table, error of ""table not found"" will be returned.

### Steps to reproduce

+ Create table:
```
CREATE TABLE DeMo (
    `timestamp` timestamp NOT NULL,
    `arch` string TAG,
    `datacenter` string TAG,
    `value` int,
    timestamp KEY (timestamp)) ENGINE=Analytic
WITH(
	 enable_ttl='false'
);
```
+ Insert something:
```
INSERT INTO DeMo
    (`timestamp`, `arch`, `datacenter`, `value`)
VALUES
    (1658304762, 'x86-64', 'china', 100),
```

+ Select something:
```
SELECT * FROM DeMo;
```


### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/220/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/220,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JNbDH,horaedb,1228255431,220,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-08-26T09:10:38Z,2022-08-26T09:10:38Z,:face_with_spiral_eyes:,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JNbDH/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/220,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JNb6R,horaedb,1228258961,220,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2022-08-26T09:13:51Z,2022-08-26T09:13:51Z,"> 😵‍💫

I think we can only support case-insensitive now?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JNb6R/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/220,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5J6xQS,horaedb,1240142866,220,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-09-08T02:27:57Z,2022-09-08T02:27:57Z,"This [reference](https://dev.mysql.com/doc/refman/8.0/en/identifier-case-sensitivity.html) says whether the identifiers are case-sensitive depends on the OS because of the mysql implementation. And there is no a worry for CeresDB's implementation, so let's make all the identifiers case-sensitive.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5J6xQS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/220,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L2HaL,horaedb,1272477323,220,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-10-09T07:30:16Z,2022-10-09T07:30:16Z,"This 'bug' is introduced by https://github.com/apache/arrow-datafusion/issues/1746.

Maybe this is not a bug, and just quote the ident by **`** if we want to make the table names case sensitive:
> SELECT * FROM `DeMo`;
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L2HaL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/220,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L2Hsf,horaedb,1272478495,220,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-10-09T07:37:36Z,2022-10-09T07:37:36Z,"Yeah, it's a feature, not bug. I think we should:
- Add a testcase
- Update document, https://docs.ceresdb.io/sql/identifier.html","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L2Hsf/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/227,https://api.github.com/repos/apache/horaedb/issues/227,horaedb,1355086546,227,Enhance to hybrid storage format,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-08-30T02:38:31Z,2023-02-10T06:36:30Z,"### Describe This Problem

Hybrid storage format with basic feature is already merged, but there are still things to be done to gain good performance. 

Open an new issue to tracking those.

### Proposal

# Write

-   [x] Support variable-length type for `ListArray`
-   [ ] Support table without tsid, only a `row id` is required
-   [ ] Persist hybrid collapsible info


# Read

-   [ ] Support timestamp column filter, some extra columns may be needed
-   [x] Support variable-length type for `ListArray`
-   [ ] Enable a total ordering, to support query with pagination


# Misc
-   [ ] Compaction support 
-   [ ] Ensure row group size is large enough, in case of list length within same row\_id is to small
-   [ ] Use dictionary array type to represent non-collapsible columns to reduce memory usage.
-   [ ] Benchmark between two format

### Additional Context

https://github.com/CeresDB/ceresdb/issues/77","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/227/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/227,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5U4RCt,horaedb,1424036013,227,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-02-09T11:26:21Z,2023-02-09T11:26:21Z,The poc of hybrid storage has finished.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5U4RCt/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/227,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5U876X,horaedb,1425260183,227,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-02-10T06:36:29Z,2023-02-10T06:36:29Z,"Lessons learnt from this feature.

- Arrow data layout of hybrid storage is same when primary key is `tsid, timestamp`
- The only advantage is less disk usage, which can be gained using dictionary compression.
- Also convert plain arrow data to nested type makes code hard to maintain, and some feature hard to implement.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5U876X/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/228,https://api.github.com/repos/apache/horaedb/issues/228,horaedb,1355092808,228,Release v0.4.0 ,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-08-30T02:49:48Z,2022-10-25T18:40:50Z,"### Describe This Problem

After #153, a new development cycle begins. The planned features are listed in `proposal` section below and release date is September 26, 2022.

Feel free to suggest/discuss other features you would like to have in CeresDB.

### Proposal

- Distributed version support table migration
- Hybrid support variable length type, other improvements, and  benchmark
- Investigate other WAL implementation(besides OBKV)

### Additional Context

https://github.com/CeresDB/ceresdb/milestone/3","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/228/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/229,https://api.github.com/repos/apache/horaedb/issues/229,horaedb,1356675350,229,Sed error while run with docker(use -v to mount config).,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-08-31T02:49:15Z,2022-09-05T06:55:25Z,"### Describe this problem

While run our docker image v0.3.0 with mounted config file, an error from `sed` occurred.
It complains `sed: cannot rename /etc/ceresdb/sedYMPxJV: Device or resource busy`.

### Steps to reproduce

+ Run as follow:
```
docker run -d --name ceresdb-server \
  -p 8831:8831 \
  -p 3307:3307 \
  -p 5440:5440 \
  -v your/path/ceresdb/docs/your-config:/etc/ceresdb/ceresdb.toml \
  ceresdb/ceresdb-server:v0.3.0
```
+ Check its running status:
```
docker logs ceresdb-server
```

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/229/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/229,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JdP6_,horaedb,1232404159,229,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-08-31T03:09:42Z,2022-08-31T03:09:42Z,"So many cases to test, I think we need to add a test here
- https://github.com/CeresDB/ceresdb/blob/4c64860493e4cb7aa6e98435ca1377e1a6421252/.github/workflows/docker-build-image.yml#L24

Now it only test with default config.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JdP6_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/229,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Jd3EK,horaedb,1232564490,229,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2022-08-31T07:27:05Z,2022-08-31T07:27:05Z, I will fix this bug in #230 .,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Jd3EK/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/229,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JtSOA,horaedb,1236607872,229,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-09-05T06:55:25Z,2022-09-05T06:55:25Z,Released in Docker image v0.3.1,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JtSOA/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/231,https://api.github.com/repos/apache/horaedb/issues/231,horaedb,1356957288,231,Update docs config to the latest format,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-08-31T08:01:55Z,2022-10-08T07:18:50Z,"### Describe this problem

The config format is changed in #215, but docs remain the old format. Affected docs:
- https://docs.ceresdb.io/deploy/static_routing.html

### Steps to reproduce

None

### Expected behavior

_No response_

### Additional Information

This is right config for schema shards
```
[[static_route.topology.schema_shards]]
schema = 'public'
auto_create_tables = true

[[static_route.topology.schema_shards.shard_views]]
shard_id = 0

[static_route.topology.schema_shards.shard_views.endpoint]
addr = ""127.0.0.1""
port = 8831

```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/231/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/233,https://api.github.com/repos/apache/horaedb/issues/233,horaedb,1361347073,233,RFC-20220829-distributed-dynamic-routing,ZuLiangWang,11689031,CooooolFrog,coolfrog@apache.org,CLOSED,2022-09-05T02:40:15Z,2022-09-05T03:15:00Z,"### Describe This Problem

Disribeted dynamic routing for CeresDB cluster mode,  provide failover and load balancing for cluster.

### Proposal

In Release v0.3, we initially implemented the cluster mode of CeresDB and provided static routing based on configuration.
However, it is obvious that this simple implementation cannot supported the dynamic scheduling in distributed scenarios.
Through distributed dynamic routing, we hope to achieve：
* Provide disaster recovery and high availability for the distributed mode of CeresDB. 
* Control the scheduling of CeresDB computing nodes and provide good load balancing capability. Automatically schedule the routing relationship between data and computing nodes to solve the hot issues of computing nodes

### Additional Context

- Feature Name: distributed-dynamic-routing
- Tracking Issue: [#198](https://github.com/CeresDB/ceresdb/issues/198)

# Summary
Disribeted dynamic routing for CeresDB cluster mode,  provide failover and load balancing for cluster.

# Motivation
In Release v0.3, we initially implemented the cluster mode of CeresDB and provided static routing based on configuration.
However, it is obvious that this simple implementation cannot supported the dynamic scheduling in distributed scenarios.
Through distributed dynamic routing, we hope to achieve：

- Provide disaster recovery and high availability for the distributed mode of CeresDB. 
- Control the scheduling of CeresDB computing nodes and provide good load balancing capability. Automatically schedule the routing relationship between data and computing nodes to solve the hot issues of computing nodes

# Details
### Cluster Mode Architecture
```
                                               ┌──────────────────┐                                                  
                           ┌───────────────────┤CeresMeta Cluster ├────────────────────┐                             
                           │                   └──────────────────┘                    │                             
                           │     ┌────────┐    ┌────────┐   ┌────────┐   ┌────────┐    │                             
                           │     │        │    │        │   │        │   │        │    │                             
                           │     │  etcd  │    │  etcd  │   │  etcd  │   │  etcd  │    │                             
                           │     │        │    │        │   │        │   │        │    │                             
                           │     └────────┘    └────────┘   └────────┘   └────────┘    │                             
                           └─────────────────────────┬──────▲──────────────────────────┘                             
                                                     │      │                                                        
                                            ┌────────┴──────┴────────┐                                               
                                            │   Routing Infomation   │                                               
                                            └────────┬──────┬────────┘                                               
                                              ┌──────▼──────┴──────┐                                                 
                           ┌──────────────────┤  Compute Cluster   ├────────────────────┐                 ┌─────────┐
                           │                  └────────────────────┘                    │                 │         │
                           │                                                            │  ┌────────────┐ │         │
                           │    ┌───────┐                     ┌───────┐                 │  │Upload/Fetch│ │         │
              ┌──────────┐ │  ┌─┤ Node0 ├───────────────┐   ┌─┤ Node1 ├───────────────┐ │  │    SST     │ │         │
              │Write/Read│ │  │ └───────┘               │   │ └───────┘               │ │  └────────────┘ │         │
┌──────────┐  │ Request  │ │  │ ┌────────┐  ┌────────┐  │   │ ┌────────┐  ┌────────┐  │ ├─────────────────▶         │
│          │  └──────────┘ │  │ │Shard0 L├┐ │Shard1 F├┐ │   │ │Shard0 F├┐ │Shard1 L├┐ │ │                 │ CeresDB │
│  Client  │───────────────▶  │ ├────────┘│ ├────────┘│ │   │ ├────────┘│ ├────────┘│ │ │                 │ Storage │
│          │  ┌─────────┐  │  │ │ Table0  │ │ Table2  │ │   │ │ table0  │ │ table2  │ │ ◀─────────────────┤         │
└──────────┘  │ Routing │  │  │ │ Table1  │ │ Table3  │ │   │ │ table1  │ │ table3  │ │ │                 │         │
              │Infomatio│  │  │ │         │ │         │ │   │ │         │ │         │ │ │                 │         │
              └─────────┘  │  │ └─────────┘ └─────────┘ │   │ └─────────┘ └─────────┘ │ │                 │         │
                           │  │                         │   │                         │ │                 │         │
                           │  └─────────────────────────┘   └─────────────────────────┘ │                 │         │
                           │                                                            │                 │         │
                           └────────────────────────────────────────────────────────────┘                 └─────────┘
                                                                                                                     
                                                                                                                     
                                                                                                                     
                                                                                                                     
                                                                                                                     
                                                                                                                     
                                                                                                                     
                                 ─                                                                                   
```
Explain：

- Shard:  CeresDB Data Shard, One shard corresponds to a batch of tables. Shard's roles are divided into leader and follower, Only the leader is responsible for writing requests.
- Node: CeresDB Compute Node, One node is responsible for processing multiple Shards.
- Table:  Minimum unit of CeresDB scheduling, CeresDB implements distributed scheduling by adjusting the routing of the table.
- Routing: Routing relationship from table to endpoint, The client relies on it to make requests to the correct node.
- CeresMeta: An independently deployed service for managing and scheduling CereDB cluster.

### Metadata Definition
#### ShardRole
```rust
pub enum ShardRole {
    LEADER = 0, 
    FOLLOWER = 1,
    PENDING_LEADER = 2, 
    PENDING_FOLLOWER = 3
}

// LEADER has RW permission
// FOLLOWER has R permission
// PENDING_LEADER is the follower who will become the leader in the process of leader switching，It has R permission
// PENDING_FOLLOWER is the leader who will become the follower in the process of leader switching，It has RW permission
```
```
┌────┐                   ┌────┐          
│ RW ├─────────┐         │ RW ├─────────┐
├────┘         │         ├────┘         │
│    Leader    ◀─────────│PendingLeader │
│              │         │              │
└───────┬──────┘         └───────▲──────┘
        │                        │       
┌────┐  │                ┌────┐  │       
│ R  ├──▼──────┐         │ R  ├──┴──────┐
├────┘         │         ├────┘         │
│   Pending    ├─────────▶   Follower   │
│   Follower   │         │              │
└──────────────┘         └──────────────┘
```
#### ClusterTopologyState
```rust
const (
    ClusterTopology_EMPTY             ClusterTopology_ClusterState = 0 // EMPTY Cluster，unable to provide external services
    ClusterTopology_PREPARE_REBALANCE ClusterTopology_ClusterState = 1 // Rebalance is in progress, and some requests may be affected at this time
    ClusterTopology_AWAITING_CLOSE    ClusterTopology_ClusterState = 2 // The cluster is shutting down and writing is prohibited
    ClusterTopology_AWAITING_OPEN     ClusterTopology_ClusterState = 3 // The cluster is initializing and cannot provide services normally
    ClusterTopology_STABLE            ClusterTopology_ClusterState = 4 // In a stable state, external reading and writing services can be provided normally
)

```
```
┌──────────┐         ┌──────────┐      ┌──────────┐
│          │         │ PREPAPRE │      │          │
│  EMPTY   ◀─────────▶REBALANCE ◀──────▶  STABLE  │
│          │         │          │      │          │
└────▲─────┘         └────▲────▲┘      └─────▲────┘
     │                    │    └────────┐    │     
     │                    │             │    │     
     │               ┌────▼─────┐      ┌┴────┴────┐
     │               │ AWAITING │      │  AWATING │
     └───────────────▶  CLOSE   ├──────▶   OEPN   │
                     │          │      │          │
                     └──────────┘      └──────────┘
```
### Routing scheduling strategy
#### Shard Load Balance
The simple load balancing strategy based on shard,  only ensures that the number of shards distributed on each computing node is balanced, and cannot guarantee that the actual load of the computing node is balanced.

- The implementation is simple, only the routing relationship between shard and computing nodes is processed, and the routing relationship between table and shard is randomly allocated
- Real load balancing cannot be realized. Some computing nodes may become hot spots due to large tables, and need to manually handle the routing from table to shard.
```
                                                ┌───────┐                                                
   ┌──────┐        ┌──────┐         ┌──────┐    │       │    ┌──────┐        ┌──────┐         ┌──────┐   
┌──┤Node0 ├──┐  ┌──┤Node1 ├──┐   ┌──┤Node2 ├──┐ │ Shard │ ┌──┤Node0 ├──┐  ┌──┤Node1 ├──┐   ┌──┤Node2 ├──┐
│  └──────┘  │  │  └──────┘  │   │  └──────┘  │ │ Load  │ │  └──────┘  │  │  └──────┘  │   │  └──────┘  │
│┌─────────┐ │  │┌─────────┐ │   │┌─────────┐ │ │Balance│ │┌─────────┐ │  │┌─────────┐ │   │┌─────────┐ │
││Shard0 L │ │  ││Shard0 F │ │   ││Shard1 L │ │ │       │ ││Shard0 L │ │  ││Shard0 F │ │   ││Shard1 L │ │
│└─────────┘ │  │└─────────┘ │   │└─────────┘ │ └───────┘ │└─────────┘ │  │└─────────┘ │   │└─────────┘ │
│┌─────────┐ │  │┌─────────┐ │   │            │──────────▶│┌─────────┐ │  │┌─────────┐ │   │┌─────────┐ │
││Shard2 L │ │  ││Shard1 F │ │   │            │           ││Shard2 L │ │  ││Shard1 F │ │   ││Shard2 F │ │
│└─────────┘ │  │└─────────┘ │   │            │           │└─────────┘ │  │└─────────┘ │   │└─────────┘ │
│            │  │┌─────────┐ │   │            │           │            │  │            │   │            │
│            │  ││Shard2 F │ │   │            │           │            │  │            │   │            │
│            │  │└─────────┘ │   │            │           │            │  │            │   │            │
└────────────┘  └────────────┘   └────────────┘           └────────────┘  └────────────┘   └────────────┘
```

#### Table Load Balance
The load balancing strategy based on table,  the load metrics of the current compute node are regularly collected from the computing nodes in the form of heartbeat (or a separate channel) to obtain the quantifiable actual load of each computing node, and the table is scheduled according to the actual load to ensure that the load of each node is truly balanced.

- The implementation is relatively complex, and the realtime load needs to be collected and reported by the computing node, with a certain extra cost.
- CeresMeta maintains a map<table, tablecost> for each computing node. When the load of a node reaches the threshold, rebalance is triggered, some tables are selected, migrated to the computing node with the lowest current load, and the routing relationship between shard and table is adjusted.
```
    ┌──────┐        ┌──────┐         ┌──────┐    ┌───────┐    ┌──────┐        ┌──────┐         ┌──────┐    
 ┌──┤Node0 ├──┐  ┌──┤Node1 ├──┐   ┌──┤Node2 ├──┐ │       │ ┌──┤Node0 ├──┐  ┌──┤Node1 ├──┐   ┌──┤Node2 ├──┐ 
 │  └──────┘  │  │  └──────┘  │   │  └──────┘  │ │ Table │ │  └──────┘  │  │  └──────┘  │   │  └──────┘  │ 
 │┌────────┐  │  │            │   │┌────────┐  │ │ Load  │ │┌────────┐  │  │            │   │┌────────┐  │ 
 ││Shard0 L├┐ │  │┌─────────┐ │   ││Shard1 L├┐ │ │Balance│ ││Shard0 L├┐ │  │┌─────────┐ │   ││Shard1 L├┐ │ 
 │├────────┘│ │  ││Shard0 F │ │   │├────────┘│ │ │       │ │├────────┘│ │  ││Shard0 F │ │   │├────────┘│ │ 
 ││ table0  │ │  │└─────────┘ │   ││ table2  │ │ └───────┘ ││ table0  │ │  │└─────────┘ │   ││ table1  │ │ 
 ││ table1  │ │  │┌─────────┐ │   ││ table3  │ │──────────▶││         │ │  │┌─────────┐ │   ││ table2  │ │ 
 ││         │ │  ││Shard1 F │ │   ││         │ │           ││         │ │  ││Shard1 F │ │   ││ table3  │ │ 
 │└─────────┘ │  │└─────────┘ │   │└─────────┘ │           │└─────────┘ │  │└─────────┘ │   │└─────────┘ │ 
 │┌─────────┐ │  │            │   │┌─────────┐ │           │┌─────────┐ │  │            │   │┌─────────┐ │ 
 ││Shard2 L │ │  │            │   ││Shard2 F │ │           ││Shard2 L │ │  │            │   ││Shard2 F │ │ 
 │└─────────┘ │  │            │   │└─────────┘ │           │└─────────┘ │  │            │   │└─────────┘ │ 
┌┴────────────┴┐ └────────────┘  ┌┴────────────┴┐         ┌┴────────────┴┐ └────────────┘  ┌┴────────────┴┐
│  CPUUse=90%  │                 │  CPUUse=30%  │         │  CPUUse=50%  │                 │  CPUUse=50%  │
└──────────────┘                 └──────────────┘         └──────────────┘                 └──────────────┘
```

### Shard Change Process
#### Switch Leader

1. First, set the original leader shard and the follwer to be switched to PENDING_ FOLLOWER and PENDING_ LEADER.
1. Wait for the leader shard to complete the preparatory action before the leader switch, and notify the CeresMeta after the completion. In this process, the new leader cannot process the write request.
1. After receiving this request, meta will update the status of the two shards, and the leader switch is completed.


#### Shard Migrate

- Leader Shard：
   1. First, CeresMeta adjusts the routing relationship and create a new follower shard on the node.
   1. Follower shard notifies CeresMeta after initialization.
   1. CeresMeta started the process of leader switch, switched the leader to the newly created follower shard.
   1. Close the original leader shard after the leader switch is completed.
- Follower Shard：
   1. Adjust shard node mapping to directly assign this shard to another node.

#### Shard Split

1. CeresMeta creates a new shard and distributes it to the node with the lowest load.
1. After the new shard is created, the table routing relationship in the shard is processed, and a part of the table is split to the new shard according to certain rules.

Pending Problem：

1. How to decide which tables need to be split into new shards?

### Processing flow
#### Cluster registration and initialization

1. Meta received the request for cluster registration, create cluster meta info and save in etcd, set the ClusterTopologyState is EMPTY.
1. Waiting for node regisration, When the number of registered nodes has reached minimum number required by the cluster, the cluster initialization process will start.
1. Initialize the shard routing topology and distribute the shard evenly on each node.
1. Set the ClusterTopologyState to STABLE, initialization complete, after that, the cluster can normally provide external services.

#### Create Table 

1. The client initiates a create table request. In RPC mode, it will access the specified endpoint returned by CeresmMeta. In HTTP mode, it will randomly access an available endpoint.
1. After receiving the request, the node will complete the table creation operation and notify CeresMeta. All subsequent reads and writes to this table will be processed by the node. CeresMeta will modify the routing relationship between the table and the shard and associate this table with the shard of the current node.
1. The routing relationship of the new table has been registered. Now the table can be read and written to the outside normally.

#### Write / Read Table

1. Client accesses the endpoint specified by ceresmeta through RPC. Because the client caches the routing relationship, it may be expired.
1. After receiving the request, the node needs to decide whether to process the request normally according to the latest routing table.
   - Shard is in normal status and handles requests normally.
   - The current shard is no longer on this node. The node returns an expiration flag. The client re initiates the route request to refresh the routing relationship.

# Drawbacks
#### Table Split
CeresDB's current minimum scheduling unit is table. It does not support splitting or merging table. It can only migrate tables between nodes. In extreme cases, a single table may be too large for a single node to handle.

# Alternatives
#### Remove Shard
In the current design, the concept of shard adds a lot of complexity, and it seems to have no obvious effect at present. Direct scheduling table can also achieve the same function. Maybe shard can be removed in the future?
#### More complex scheduling
In the current design of scheduling, we only implement load balancing based on table cost. In the future, we can refer to the design of PD scheduler to implement more kinds of schedulers.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/233/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/233,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Js1tJ,horaedb,1236491081,233,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-09-05T02:41:45Z,2022-09-05T02:41:45Z,@ZuLiangWang A PR about this RFC will be better so that it can be reviewed by others.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Js1tJ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/233,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Js11K,horaedb,1236491594,233,NA,waynexia,15380403,Ruihang Xia,,NA,2022-09-05T02:42:59Z,2022-09-05T02:42:59Z,https://github.com/CeresDB/ceresdb/pull/79 is an example,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Js11K/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/233,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Js4st,horaedb,1236503341,233,NA,waynexia,15380403,Ruihang Xia,,NA,2022-09-05T03:15:00Z,2022-09-05T03:15:00Z,moved to https://github.com/CeresDB/ceresdb/pull/234,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Js4st/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/235,https://api.github.com/repos/apache/horaedb/issues/235,horaedb,1363130645,235,Refactor Cluster and Catalog mods,waynexia,15380403,Ruihang Xia,,CLOSED,2022-09-06T11:22:19Z,2022-10-20T08:18:46Z,"### Describe This Problem

A rough dependency graph of related structures:

<!---
source:
```dot
digraph G {
    ""Out Side"" [color = blue]
    CeresMeta [color = blue]

    Cluster [penwidth = 3]
    CatalogManager [penwidth = 3]

    ""Out Side"" -> Cluster
    ""Out Side"" -> CatalogManager

    EventHandler [style = dashed]
    ""SchemaIdAlloc/\nTableIdAlloc"" [style = dashed]

    TableManipulator -> CatalogManager [color = orange, label = ""③""]
    Cluster -> TableManager
    TableManipulator -> TableEngine

    Cluster -> TableManipulator [color = orange, label = ""③""]
    Cluster -> MetaClient

    CatalogManager -> VolatileCatalog [color = green, label = ""1""]

    VolatileCatalog -> ""SchemaIdAlloc/\nTableIdAlloc"" [dir = none, color = green, label = ""2""]

    ""SchemaIdAlloc/\nTableIdAlloc"" -> MetaClient [color = green, label = ""2""]

    MetaClient -> EventHandler [dir = none, color = orange, label = ""②""]
    MetaClient -> EventHandler [dir = none, color = green, label = ""3-2""]
    MetaClient -> CeresMeta [color = green, label = ""3-1""]

    CeresMeta -> MetaClient [color = orange, label = ""①""]

    EventHandler -> Cluster [color = orange, label = ""②""]
    EventHandler -> Cluster [color = green, label = ""3-2""]
}
```
--->

![cluster](https://user-images.githubusercontent.com/15380403/188592338-48dc88dd-a484-4352-94d6-7d139030bdca.svg)

`Cluster` and `CatalogManager` are public, they may be used by `Client` or `CeresMeta` to modify catalog information (create, open or delete tables). This graph shows two paths of creating/droping a table. The **green** one is triggered by server itself and another **orange** one is triggered by `CeresMeta`

- Green path
  `CatalogManager` can create/drop tables when it has write access (the leader role). `3-1` in this path is notifying `CeresMeta` that a new table has been created, and `3-2` is an operation that happens simultaneously that create/drop that table in the leader itself. 
- Orange path
  This path can be treated as counter part of the green one. `CeresMeta` may notify non-leader node that something has changed. It also goes through `EventHandler` to notify `Cluster` and let it take action.


### Proposal

From my first impression, the dependency and call graph is complicated. I'd like to change relative parts in the following ways:
- [x] ~~Combine `VolatileCatalog` and `Cluster` #245
  `Cluster` is responsible to manage `Table`s and `Shard`s while `Catalog` is responsible to manage `Schema`s which is a group of tables in another perspective different from `Shard`. Their functionalities have a big part that overlaps. It's natural to implement both `Cluster` and `Catalog` traits over one struct, so they can use one memory state and maybe one operation logic.~~
- [x] Remove `SchemaIdAlloc` and `TableIdAlloc` #238
  `Id` should be allocated by `CeresMeta`, not the server. And furthermore, the server does need not to allocate ID then create table. The entire procedure is accomplished by `CeresMeta`.
- [x] Split `MetaClient`
  Current `MetaClient` provides two functionalities: poll update from `CeresMeta` and invoke `CeresMeta`'s method. These correspond to the two ways above: green arrows are from server to `CeresMeta` and orange are from `CeresMeta` to server. We can split these two aspects, by making `MetaClient` an actual client that invokes RPC to `CeresMeta` and an `Eventloop` that keeps fetching updates from `CeresMeta`. This can also resolve the cyclic dependency between `Cluster` and `MetaClient`.

Due to https://github.com/CeresDB/ceresdb/issues/235#issuecomment-1275722731, here are the new tracking tasks:
- [x] Remove the current catalog based on cluster, and add table manager to volatile catalog; #295
- [x] Add TableCreator & TableDropper, and refactor the create/drop table procedure; #303 #323 
- [x] Implement the open/close shards of cluster; #307 
### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/235/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/235,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JysvC,horaedb,1238027202,235,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-09-06T11:30:44Z,2022-09-06T11:30:44Z,@waynexia 👍 I can't agree more with this proposal.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5JysvC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/235,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MCfvr,horaedb,1275722731,235,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-10-12T07:36:44Z,2022-10-12T07:36:44Z,"During the development, I find it is hard to combine the `VolatileCatalog` and `Cluster` because create/drop/open/close procedures are too complex to be processed by the combined module in cluster mode, e.g. in the create table procedure, the combined one should be able to handle two different cases: create by user (the request will be forwarded to ceresmeta) and create by ceresmeta (do the real creation), and that is ugly.

Here is the new proposal, which will still keep the `VolatileCatalog` and `Cluster` separate, but the event handler will be removed, and the `TableManager` in the `Cluster` will be exposed and will be held by `VolatileCatalog`:
```
 ┌───────────┐                                             
 │  Cluster  │───────Create/Drop/Open/Close table          
 └───────────┘                │                            
       │                      ▼                            
       │             ┌─────────────────┐                   
       │             │TableManipulator │───────────┐       
       │             └─────────────────┘           │       
       │                                           ▼       
       │                                   ┌──────────────┐
       │                                   │CatalogManager│
       │                                   └──────────────┘
       ▼                                           │       
┌────────────┐                                     │       
│TableManager│◀────────────────────────────────────┘       
└────────────┘     
```

As for the create/drop table procedure, a new proxy called TableCreator will be added to handle different cases:
```plain
         ┌─────────────┐           
      ┌──│TableCreator │────┐      
      │  └─────────────┘    │      
      │                     │      
      ▼                     ▼      
┌───────────┐         ┌───────────┐
│ From user │         │ From meta │
└───────────┘         └───────────┘
      │                     │      
      ▼                     ▼      
┌───────────┐         ┌───────────┐
│Meta client│         │  Cluster  │
└───────────┘         └───────────┘
                            │      
                            ▼      
                      ┌───────────┐
                      │CatalogMana│
                      └───────────┘
```

Here is the tracking task:
- [ ] Remove the current catalog based on cluster, and add table manager to volatile catalog;
- [ ] Add TableCreator & TableDropper, and refactor the create/drop table procedure;
- [ ] Implement the open/close shards of cluster;","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MCfvr/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/237,https://api.github.com/repos/apache/horaedb/issues/237,horaedb,1364021930,237,RFC for dynamic routing of CeresDB cluster mode,ZuLiangWang,11689031,CooooolFrog,coolfrog@apache.org,CLOSED,2022-09-07T03:00:34Z,2023-03-13T09:06:09Z,"### Describe This Problem

In Release v0.3, we initially implemented the cluster mode of CeresDB and provided static routing based on CeresMeta's initialization.
However, it is obvious that this simple implementation cannot support the dynamic scheduling in distributed scenarios.
Through distributed dynamic routing, we hope to achieve：

- High availability.
- Automatic scheduling of CeresDB computing resources for good load balancing.

### Proposal

Draw an RFC for dynamic routing of CeresDB cluster mode.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/237/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/237,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XXa5i,horaedb,1465757282,237,NA,mrrtree,10230091,,,NA,2023-03-13T09:06:04Z,2023-03-13T09:06:04Z,Duplicate with #198,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XXa5i/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/240,https://api.github.com/repos/apache/horaedb/issues/240,horaedb,1364146049,240,Try replace `grpcio` with `tonic`,waynexia,15380403,Ruihang Xia,,CLOSED,2022-09-07T06:15:33Z,2022-09-28T04:36:12Z,"### Describe This Problem

[`grpcio`](https://docs.rs/grpcio/latest/grpcio/) is currently used by CeresDB as gRPC implementation. It's a rust wrapper over the [cpp implementation](https://github.com/grpc/grpc). We have used it for a long time but there are always some issues we are facing:
- long build time https://github.com/CeresDB/ceresdb/issues/151
- memory leak (from C++) https://github.com/grpc/grpc/issues?q=is%3Aissue+leak
- complicated build procedure (from both rust and C++) https://github.com/CeresDB/ceresdb-client-py/issues/20

[`tonic`](https://docs.rs/tonic/latest/tonic/) is a native gRPC implementation with these pros.:
- Native, and safety (maybe? https://github.com/hyperium/tonic/issues?q=is%3Aissue+leak)
- More users (compare to `grpcio`)
- Low memory consumption (see below)

And of cause, its cons.:
- Performance (refer to https://github.com/LesnyRumcajs/grpc_bench/wiki/2022-04-23-bench-results)

  | name                        |   req/s |   avg. latency |        90 % in |        95 % in |        99 % in | avg. cpu |   avg. memory |
  |---|---|---|---|---|---|---|---|
  | rust_tonic_mt               |   64091 |       13.53 ms |       23.42 ms |       28.17 ms |       37.61 ms |  277.41% |     18.16 MiB |
  | rust_grpcio                 |   68377 |       12.11 ms |       18.79 ms |       22.81 ms |       32.61 ms |  259.81% |      25.3 MiB |

  through this table, `grpcio` is faster than `tonic`. QpS on gRPC layer loss around 7%.

### Proposal

With those improvements, I think `tonic` is worth a try

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/240/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/245,https://api.github.com/repos/apache/horaedb/issues/245,horaedb,1371020243,245,Combine `VolatileCatalog` and `Cluster`,waynexia,15380403,Ruihang Xia,,CLOSED,2022-09-13T07:55:30Z,2022-10-12T07:37:37Z,"### Describe This Problem

Reference to #235:
> `Cluster` is responsible to manage `Table`s and `Shard`s while `Catalog` is responsible to manage `Schema`s which is a group of tables in another perspective different from `Shard`. Their functionalities have a big part that overlaps. It's natural to implement both `Cluster` and `Catalog` traits over one struct, so they can use one memory state and maybe one operation logic.



### Proposal

- [ ] Make a triple to act as `TableRef`'s full identifier(token)
  ```rust
  struct TableToken {
      catalog: CatalogId,
      schema: SchemaId,
      table: TableId,
  }
  ```
  We currently use the three layers separately: first get a `Catalog` instance, and get `Schema` instance from it, then get `Table` instance. And the storage needs a nested map struct to put all this infos. The entire procedure can be simplified.

- [ ] Remove `CatalogManager` and `SchemaManager` to reduce the complexity and simplify interfaces. Now only keep a top-level `Manager` and low-level `TableManager` 

  Follows the above description. `CatalogManager` and `SchemaManager`'s interfaces are redundant in some sorts.

- [x] Manage all the `TableRef` in one place, here is `TableManager` insides `Cluster` #260 

  The one who keeps all `TableRef` should be able to serve two requests:
    - get tables by shard
    - get tables by catalog/schema/table
  
  the fields in `TableManager` may look like
  ```rust
  struct TableManager {
      // ...
      tables: BTreeMap<TableToken, TableRef>,
      shards: HashMap<ShardId, HashSet<TableToken>>,
  }
  ```

### Additional Context

- [ ] (optional) Unify the name and id. Like `SchemaName` and `SchemaId`
  ```rust
  enum CatalogToken{
      Id(CatalogId),
      Name(String)
  }
  ```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/245/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/245,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MCgAz,horaedb,1275723827,245,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-10-12T07:37:37Z,2022-10-12T07:37:37Z,"Due to https://github.com/CeresDB/ceresdb/issues/235#issuecomment-1275722731, this issue will be closed.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MCgAz/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/247,https://api.github.com/repos/apache/horaedb/issues/247,horaedb,1372225717,247,Failed to send record batch from the merge iterator when execute explain statement,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-09-14T02:10:44Z,2023-03-14T03:49:19Z,"### Describe this problem

When execute explain on a SQL, the server will output
```
2022-09-14 10:02:17.321 ERRO [analytic_engine/src/instance/read.rs:352] Failed to send record batch from the merge iterator
2022-09-14 10:02:17.321 INFO [analytic_engine/src/row_iter/merge.rs:844] Merge iterator dropped, table_id:TableId(2199023255553), request_id:7802, metrics:Metrics { num_memtables: 1, num_ssts: 1, sst_ids: [12], times_fetch_rows_from_one: 1, total_rows_fetch_from_one: 500, times_fetch_row_from_multiple: 0, duration_since_create: 2.786981ms, init_duration: 2.451551ms, scan_duration: 2.44739ms, scan_count: 2 }, iter_options:IterOptions { batch_size: 500 },
```


### Steps to reproduce

Execute SQL below
```sql
        drop table if exists test;
        create table test(ts timestamp timestamp KEY not null, value double)
        with (
        enable_ttl = 'false'
        );
        insert into test(ts,value) values (123,9);
        explain select value from test;
```

### Expected behavior

No error log

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/247/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/247,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XdVe3,horaedb,1467307959,247,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2023-03-14T03:48:09Z,2023-03-14T03:48:09Z,"At present, when building a plan, there will be asynchronous io to pull data. The explain query does not need data during physical execution, so the data stream will be discarded, which will cause the asynchronous data io thread generated during the build plan to exit unexpectedly.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XdVe3/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/247,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XdVmb,horaedb,1467308443,247,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2023-03-14T03:49:09Z,2023-03-14T03:49:09Z,"Close this issue, tracked at #520.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XdVmb/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/249,https://api.github.com/repos/apache/horaedb/issues/249,horaedb,1372328209,249,Base implemment of default value for columns,ygf11,3428089,ygf11,yanggf23@gmail.com,CLOSED,2022-09-14T04:50:23Z,2022-09-21T02:58:08Z,"### Describe This Problem

 Support default value for columns in Ceresdb.

### Proposal

Make default value work in some base sql statements. sqls:

* Create table.
* Insert table.
* Show create table.

For the default value option, now only support exprs which does not reference other columns, like `10`, ` 1+1`, `now()`..

### Additional Context

#210 ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/249/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/250,https://api.github.com/repos/apache/horaedb/issues/250,horaedb,1372339033,250,Make default value options work like mysql,ygf11,3428089,ygf11,yanggf23@gmail.com,CLOSED,2022-09-14T05:06:00Z,2022-10-13T07:41:39Z,"### Describe This Problem

Support default value for columns in Ceresdb.


### Proposal

In #249, we want to implement the base feature of `Default value option` for columns.

The next work is to make normal expr works, just allow expr to reference other columns.

As @jiacai2050 suggest, [mysql](https://dev.mysql.com/doc/refman/8.0/en/data-type-defaults.html) only allow to reference a column defined before it, so we can do like mysql do, and support other complex feature in the future.



### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/250/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/251,https://api.github.com/repos/apache/horaedb/issues/251,horaedb,1372347079,251,Support default value option regardless their defining orders ,ygf11,3428089,ygf11,yanggf23@gmail.com,CLOSED,2022-09-14T05:17:42Z,2024-10-19T11:31:57Z,"### Describe This Problem

Support default value for columns in Ceresdb.

### Proposal

In #250, we can reference other columns in default value expr.

But it only allow to reference a column defined before it when create/alter table.

To remove this limit, we can:

* Check circle reference when create/alter table.
* Reorder the missing columns, the simple columns will run first, and other columns which depends it will run after.



### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/251/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/252,https://api.github.com/repos/apache/horaedb/issues/252,horaedb,1372348880,252,Tracking issue of default value support for columns,ygf11,3428089,ygf11,yanggf23@gmail.com,CLOSED,2022-09-14T05:20:03Z,2024-10-25T11:46:00Z,"### Describe This Problem

 Support default value for columns in Ceresdb.

### Proposal

Support default value:

* Column default value option can be a normal Expr.
* Column can reference other columns, regardless what defining orders they are.

As @ShiKaiWi and @jiacai2050 suggest, the full feature is complex, I split it into some small tasks.

- [x] https://github.com/CeresDB/ceresdb/issues/249
- [x] https://github.com/CeresDB/ceresdb/issues/250
- [ ] https://github.com/CeresDB/ceresdb/issues/251
- [ ] https://github.com/CeresDB/ceresdb/issues/259

### Additional Context

original issue:
- [ ] https://github.com/CeresDB/ceresdb/issues/210","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/252/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/253,https://api.github.com/repos/apache/horaedb/issues/253,horaedb,1372428623,253,Occurs `attempt to subtract with overflow` panic,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-09-14T06:36:23Z,2022-09-27T11:09:11Z,"### Describe this problem

```
2022-09-13 15:34:34.136 ERRO [common_util/src/panic.rs:40] thread 'cse-read' panicked 'attempt to subtract with overflow' at ""/rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/ops/arith.rs:215""
   0: common_util::panic::set_panic_hook::{{closure}}
             at /app/ceresdb/ceresdbx/common_util/src/panic.rs:39:18
   1: std::panicking::rust_panic_with_hook
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/panicking.rs:610:17
   2: std::panicking::begin_panic_handler::{{closure}}
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/panicking.rs:500:13
   3: std::sys_common::backtrace::__rust_end_short_backtrace
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/sys_common/backtrace.rs:139:18
   4: rust_begin_unwind
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/panicking.rs:498:5
   5: core::panicking::panic_fmt
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/panicking.rs:107:14
   6: core::panicking::panic
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/panicking.rs:48:5
   7: <u64 as core::ops::arith::Sub>::sub
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/ops/arith.rs:208:45
      arrow::compute::kernels::arithmetic::subtract::{{closure}}
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/arrow-7.0.0/src/compute/kernels/arithmetic.rs:1030:40
      arrow::compute::kernels::arithmetic::math_op::{{closure}}
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/arrow-7.0.0/src/compute/kernels/arithmetic.rs:183:23
      core::ops::function::impls::<impl core::ops::function::FnOnce<A> for &mut F>::call_once
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/ops/function.rs:280:13
      core::option::Option<T>::map
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/option.rs:883:29
      <core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::next
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/iter/adapters/map.rs:103:9
      arrow::buffer::mutable::MutableBuffer::from_trusted_len_iter
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/arrow-7.0.0/src/buffer/mutable.rs:440:21
      arrow::buffer::immutable::Buffer::from_trusted_len_iter
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/arrow-7.0.0/src/buffer/immutable.rs:278:9
      arrow::compute::kernels::arithmetic::math_op
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/arrow-7.0.0/src/compute/kernels/arithmetic.rs:189:27
      arrow::compute::kernels::arithmetic::subtract
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/arrow-7.0.0/src/compute/kernels/arithmetic.rs:1030:12
   8: datafusion::physical_plan::expressions::binary::BinaryExpr::evaluate_with_resolved_args
             at /root/.cargo/git/checkouts/arrow-datafusion-78dc86050a92b8a7/3e88f02/datafusion/src/physical_plan/expressions/binary.rs:992:32
   9: <datafusion::physical_plan::expressions::binary::BinaryExpr as datafusion::physical_plan::PhysicalExpr>::evaluate
             at /root/.cargo/git/checkouts/arrow-datafusion-78dc86050a92b8a7/3e88f02/datafusion/src/physical_plan/expressions/binary.rs:876:9
  10: <datafusion::physical_plan::expressions::nth_value::NthValue as datafusion::physical_plan::window_functions::BuiltInWindowFunctionExpr>::create_evaluator::{{closure}}
             at /root/.cargo/git/checkouts/arrow-datafusion-78dc86050a92b8a7/3e88f02/datafusion/src/physical_plan/expressions/nth_value.rs:127:22
      core::iter::adapters::map::map_try_fold::{{closure}}
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/iter/adapters/map.rs:91:28
      core::iter::traits::iterator::Iterator::try_fold
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/iter/traits/iterator.rs:1995:21
      <core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::try_fold
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/iter/adapters/map.rs:117:9
      <core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::try_fold
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/iter/adapters/map.rs:117:9
  11: <core::iter::adapters::ResultShunt<I,E> as core::iter::traits::iterator::Iterator>::try_fold
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/iter/adapters/mod.rs:178:9
      core::iter::traits::iterator::Iterator::find
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/iter/traits/iterator.rs:2463:9
      <core::iter::adapters::ResultShunt<I,E> as core::iter::traits::iterator::Iterator>::next
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/iter/adapters/mod.rs:160:9
      <alloc::vec::Vec<T> as alloc::vec::spec_from_iter_nested::SpecFromIterNested<T,I>>::from_iter
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/alloc/src/vec/spec_from_iter_nested.rs:23:32
      <alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/alloc/src/vec/spec_from_iter.rs:33:9
  12: <alloc::vec::Vec<T> as core::iter::traits::collect::FromIterator<T>>::from_iter
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/alloc/src/vec/mod.rs:2551:9
      core::iter::traits::iterator::Iterator::collect
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/iter/traits/iterator.rs:1745:9
      <core::result::Result<V,E> as core::iter::traits::collect::FromIterator<core::result::Result<A,E>>>::from_iter::{{closure}}
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/result.rs:1972:53
      core::iter::adapters::process_results
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/iter/adapters/mod.rs:149:17
  13: <core::result::Result<V,E> as core::iter::traits::collect::FromIterator<core::result::Result<A,E>>>::from_iter
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/result.rs:1972:9
      core::iter::traits::iterator::Iterator::collect
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/iter/traits/iterator.rs:1745:9
      datafusion::physical_plan::projection::ProjectionStream::batch_project
             at /root/.cargo/git/checkouts/arrow-datafusion-78dc86050a92b8a7/3e88f02/datafusion/src/physical_plan/projection.rs:239:9
  14: <datafusion::physical_plan::projection::ProjectionStream as futures_core::stream::Stream>::poll_next::{{closure}}
             at /root/.cargo/git/checkouts/arrow-datafusion-78dc86050a92b8a7/3e88f02/datafusion/src/physical_plan/projection.rs:267:37
      core::task::poll::Poll<T>::map
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/task/poll.rs:52:43
  15: <datafusion::physical_plan::projection::ProjectionStream as futures_core::stream::Stream>::poll_next
             at /root/.cargo/git/checkouts/arrow-datafusion-78dc86050a92b8a7/3e88f02/datafusion/src/physical_plan/projection.rs:266:20
  16: <table_engine::stream::FromDfStream as futures_core::stream::Stream>::poll_next
             at /app/ceresdb/ceresdbx/table_engine/src/stream.rs:107:15
  17: <core::pin::Pin<P> as futures_core::stream::Stream>::poll_next
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/futures-core-0.3.16/src/stream.rs:120:9
      <S as futures_core::stream::TryStream>::try_poll_next
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/futures-core-0.3.16/src/stream.rs:196:9
      <futures_util::stream::try_stream::try_collect::TryCollect<St,C> as core::future::future::Future>::poll
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/futures-util-0.3.16/src/stream/try_stream/try_collect.rs:46:26
  18: query_engine::executor::collect::{{closure}}
             at /app/ceresdb/ceresdbx/query_engine/src/executor.rs:135:25
      <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/future/mod.rs:84:19
      <query_engine::executor::ExecutorImpl as query_engine::executor::Executor>::execute_logical_plan::{{closure}}
             at /app/ceresdb/ceresdbx/query_engine/src/executor.rs:105:45
      <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/future/mod.rs:84:19
  19: <core::pin::Pin<P> as core::future::future::Future>::poll
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/future/future.rs:123:9
      <interpreters::select::SelectInterpreter<T> as interpreters::interpreter::Interpreter>::execute::{{closure}}
             at /app/ceresdb/ceresdbx/interpreters/src/select.rs:61:52
      <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/future/mod.rs:84:19
  20: <core::pin::Pin<P> as core::future::future::Future>::poll
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/future/future.rs:123:9
      server::grpc::query::fetch_query_output::{{closure}}
             at /app/ceresdb/ceresdbx/server/src/grpc/query.rs:146:19
      <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/future/mod.rs:84:19
  21: server::grpc::query::handle_query::{{closure}}
             at /app/ceresdb/ceresdbx/server/src/grpc/query.rs:44:54
      <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/future/mod.rs:84:19
      <server::grpc::StorageServiceImpl<C,Q> as ceresdbxproto::protos::storage_grpc::StorageService>::query::{{closure}}
             at /app/ceresdb/ceresdbx/server/src/grpc/mod.rs:431:61
      <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/future/mod.rs:84:19
  22: <futures_util::future::future::map::Map<Fut,F> as core::future::future::Future>::poll
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/futures-util-0.3.16/src/future/future/map.rs:55:37
  23: <futures_util::future::future::Map<Fut,F> as core::future::future::Future>::poll
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/futures-util-0.3.16/src/lib.rs:95:13
      <futures_util::future::future::flatten::Flatten<Fut,<Fut as core::future::future::Future>::Output> as core::future::future::Future>::poll
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/futures-util-0.3.16/src/future/future/flatten.rs:50:36
  24: <futures_util::future::future::Then<Fut1,Fut2,F> as core::future::future::Future>::poll
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/futures-util-0.3.16/src/lib.rs:95:13
      tokio::runtime::task::core::CoreStage<T>::poll::{{closure}}
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/task/core.rs:161:17
      tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/loom/std/unsafe_cell.rs:14:9
      tokio::runtime::task::core::CoreStage<T>::poll
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/task/core.rs:151:13
      tokio::runtime::task::harness::poll_future::{{closure}}
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/task/harness.rs:461:19
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/panic/unwind_safe.rs:271:9
      std::panicking::try::do_call
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/panicking.rs:406:40
      std::panicking::try
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/panicking.rs:370:19
  25: std::panic::catch_unwind
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/panic.rs:133:14
      tokio::runtime::task::harness::poll_future
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/task/harness.rs:449:18
  26: tokio::runtime::task::harness::Harness<T,S>::poll_inner
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/task/harness.rs:98:27
      tokio::runtime::task::harness::Harness<T,S>::poll
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/task/harness.rs:53:15
  27: tokio::runtime::task::raw::RawTask::poll
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/task/raw.rs:70:18
      tokio::runtime::task::LocalNotified<S>::run
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/task/mod.rs:343:9
      tokio::runtime::thread_pool::worker::Context::run_task::{{closure}}
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/thread_pool/worker.rs:443:21
      tokio::coop::with_budget::{{closure}}
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/coop.rs:102:9
      std::thread::local::LocalKey<T>::try_with
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/thread/local.rs:412:16
      std::thread::local::LocalKey<T>::with
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/thread/local.rs:388:9
  28: tokio::coop::with_budget
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/coop.rs:95:5
      tokio::coop::budget
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/coop.rs:72:5
      tokio::runtime::thread_pool::worker::Context::run_task
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/thread_pool/worker.rs:419:9
  29: tokio::runtime::thread_pool::worker::Context::run
  30: tokio::runtime::thread_pool::worker::run::{{closure}}
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/thread_pool/worker.rs:371:17
      tokio::macros::scoped_tls::ScopedKey<T>::set
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/macros/scoped_tls.rs:61:9
  31: tokio::runtime::thread_pool::worker::run
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/thread_pool/worker.rs:368:5
  32: tokio::runtime::thread_pool::worker::Launch::launch::{{closure}}
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/thread_pool/worker.rs:347:45
      <tokio::runtime::blocking::task::BlockingTask<T> as core::future::future::Future>::poll
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/blocking/task.rs:42:21
  33: tokio::runtime::task::core::CoreStage<T>::poll
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/task/core.rs:151:13
      tokio::runtime::task::harness::poll_future::{{closure}}
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/task/harness.rs:461:19
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/panic/unwind_safe.rs:271:9
      std::panicking::try::do_call
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/panicking.rs:406:40
      std::panicking::try
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/panicking.rs:370:19
      std::panic::catch_unwind
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/panic.rs:133:14
      tokio::runtime::task::harness::poll_future
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/task/harness.rs:449:18
      tokio::runtime::task::harness::Harness<T,S>::poll_inner
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/task/harness.rs:98:27
      tokio::runtime::task::harness::Harness<T,S>::poll
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/task/harness.rs:53:15
  34: tokio::runtime::task::raw::RawTask::poll
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/task/raw.rs:70:18
      tokio::runtime::task::UnownedTask<S>::run
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/task/mod.rs:379:9
      tokio::runtime::blocking::pool::Inner::run
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/blocking/pool.rs:264:17
  35: tokio::runtime::blocking::pool::Spawner::spawn_thread::{{closure}}
             at /root/.cargo/registry/src/gitlab.alipay-inc.com-bfbf2811318eff5d/tokio-1.15.0/src/runtime/blocking/pool.rs:244:17
      std::sys_common::backtrace::__rust_begin_short_backtrace
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/sys_common/backtrace.rs:123:18
  36: std::thread::Builder::spawn_unchecked::{{closure}}::{{closure}}
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/thread/mod.rs:477:17
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/panic/unwind_safe.rs:271:9
      std::panicking::try::do_call
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/panicking.rs:406:40
      std::panicking::try
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/panicking.rs:370:19
      std::panic::catch_unwind
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/panic.rs:133:14
      std::thread::Builder::spawn_unchecked::{{closure}}
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/thread/mod.rs:476:30
      core::ops::function::FnOnce::call_once{{vtable.shim}}
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/core/src/ops/function.rs:227:5
  37: <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/alloc/src/boxed.rs:1854:9
      <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/alloc/src/boxed.rs:1854:9
      std::sys::unix::thread::Thread::new::thread_start
             at /rustc/f1ce0e6a00593493a12e0e3662119786c761f375/library/std/src/sys/unix/thread.rs:108:17
  38: start_thread
  39: __clone
```

### Steps to reproduce

SQL is :
```
select count(DISTINCT hostname) - count(DISTINCT concat(graphId, hostname)) as 'value' from xx where gmtCreate >= '2022-09-13 00:00:00' and gmtCreate < '2022-09-14 00:00:00'
```

Refer to https://github.com/apache/arrow-rs/pull/2643, this pr has already fixed it, we need to bump to the latest version.

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/253/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/253,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KzFdk,horaedb,1254905700,253,NA,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,NA,2022-09-22T11:42:01Z,2022-09-22T11:42:01Z,"I reproduced this bug;
firstly, I create table 'demo',
```
curl --location --request POST 'http://127.0.0.1:5440/sql' \
--header 'Content-Type: application/json' \
--data-raw '{
    ""query"": ""CREATE TABLE `demo2` (`name` string TAG,`name1` string TAG, `value` uint64 NOT NULL, `t` timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE=Analytic with (enable_ttl='\''false'\'')""
}'

```
Then，insert two record
```
curl --location --request POST 'http://127.0.0.1:5440/sql' \
--header 'Content-Type: application/json' \
--data-raw '{
    ""query"": ""INSERT INTO demo2(t, name,name1, value) VALUES(1651737067000, '\''ceresdb'\'','\''ceresdb0'\'', 100)""
}'
curl --location --request POST 'http://127.0.0.1:5440/sql' \
--header 'Content-Type: application/json' \
--data-raw '{
    ""query"": ""INSERT INTO demo2(t, name,name1, value) VALUES(1651737067000, '\''ceresdb'\'','\''ceresdb1'\'', 99)""
}'
```

Finally, bug repeated with the sql :
```
curl --location --request POST 'http://127.0.0.1:5440/sql' \
--header 'Content-Type: application/json' \
--data-raw '{
    ""query"": ""select min(value)-max(value) from demo2""
}'
```

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KzFdk/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/253,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5K2TpA,horaedb,1255750208,253,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-09-23T02:56:50Z,2022-09-23T02:56:50Z,"

> I reproduced this bug; firstly, I create table 'demo',
> 
> ```
> curl --location --request POST 'http://127.0.0.1:5440/sql' \
> --header 'Content-Type: application/json' \
> --data-raw '{
>     ""query"": ""CREATE TABLE `demo2` (`name` string TAG,`name1` string TAG, `value` uint64 NOT NULL, `t` timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE=Analytic with (enable_ttl='\''false'\'')""
> }'
> ```
> 
> Then，insert two record
> 
> ```
> curl --location --request POST 'http://127.0.0.1:5440/sql' \
> --header 'Content-Type: application/json' \
> --data-raw '{
>     ""query"": ""INSERT INTO demo2(t, name,name1, value) VALUES(1651737067000, '\''ceresdb'\'','\''ceresdb0'\'', 100)""
> }'
> curl --location --request POST 'http://127.0.0.1:5440/sql' \
> --header 'Content-Type: application/json' \
> --data-raw '{
>     ""query"": ""INSERT INTO demo2(t, name,name1, value) VALUES(1651737067000, '\''ceresdb'\'','\''ceresdb1'\'', 99)""
> }'
> ```
> 
> Finally, bug repeated with the sql :
> 
> ```
> curl --location --request POST 'http://127.0.0.1:5440/sql' \
> --header 'Content-Type: application/json' \
> --data-raw '{
>     ""query"": ""select min(value)-max(value) from demo2""
> }'
> ```

Would you like to attach the stack trace of this panic?  (The one in the issue description doesn't belong to the latest version).","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5K2TpA/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/255,https://api.github.com/repos/apache/horaedb/issues/255,horaedb,1373879967,255,assertion failed: end <= self.capacity when insert hybrid storage format table,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-09-15T04:00:15Z,2022-09-22T02:26:36Z,"### Describe this problem

When doing tsbs benchmark, a panic occurred when insert data
```
2022-09-15 11:39:19.404 ERRO [common_util/src/panic.rs:42] thread 'ceres-bg' panicked 'assertion failed: end <= self.capacity' at ""/home/chenxiang/.cargo/registry/src/github.com-1ecc6299db9ec823/arrow-15.0.0/src/buffer/mutable.
rs:105""                                                                                                                                                                                                                            
   0: common_util::panic::set_panic_hook::{{closure}}                                                            
             at /home/chenxiang/ceresdb/common_util/src/panic.rs:41:18                                                                                                                                                             
   1: std::panicking::rust_panic_with_hook                                                                       
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:702:17  
   2: std::panicking::begin_panic_handler::{{closure}}                                                           
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:586:13                                                                                                                                
   3: std::sys_common::backtrace::__rust_end_short_backtrace                            
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/sys_common/backtrace.rs:138:18
   4: rust_begin_unwind                                                                                          
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:584:5                                                                                                                                 
   5: core::panicking::panic_fmt                                                                                 
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/panicking.rs:142:14
   6: core::panicking::panic                                                                                                                                                                                                       
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/panicking.rs:48:5               
   7: arrow::buffer::mutable::MutableBuffer::with_bitset                                                                                                                                                                           
             at /home/chenxiang/.cargo/registry/src/github.com-1ecc6299db9ec823/arrow-15.0.0/src/buffer/mutable.rs:105:9                       
   8: analytic_engine::sst::parquet::hybrid::ListArrayBuilder::build_child_data                                                                                                                                                    
             at /home/chenxiang/ceresdb/analytic_engine/src/sst/parquet/hybrid.rs:159:31         
      analytic_engine::sst::parquet::hybrid::ListArrayBuilder::build                                                                                                                                                               
             at /home/chenxiang/ceresdb/analytic_engine/src/sst/parquet/hybrid.rs:211:26         
   9: analytic_engine::sst::parquet::hybrid::build_hybrid_record::{{closure}}                                                                                                                                                      
             at /home/chenxiang/ceresdb/analytic_engine/src/sst/parquet/hybrid.rs:273:33                         
      core::iter::adapters::map::map_try_fold::{{closure}}                                                                                                                                                                         
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/iter/adapters/map.rs:91:28      
      core::iter::traits::iterator::Iterator::try_fold                                                                                                                                                                             
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/iter/traits/iterator.rs:2238:21
      <core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::try_fold
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/iter/adapters/map.rs:117:9
  10: <core::iter::adapters::GenericShunt<I,R> as core::iter::traits::iterator::Iterator>::try_fold
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/iter/adapters/mod.rs:191:9
      <I as alloc::vec::in_place_collect::SpecInPlaceCollect<T,I>>::collect_in_place
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/alloc/src/vec/in_place_collect.rs:251:13
      alloc::vec::in_place_collect::<impl alloc::vec::spec_from_iter::SpecFromIter<T,I> for alloc::vec::Vec<T>>::from_iter
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/alloc/src/vec/in_place_collect.rs:178:19
      <alloc::vec::Vec<T> as core::iter::traits::collect::FromIterator<T>>::from_iter
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/alloc/src/vec/mod.rs:2648:9
      core::iter::traits::iterator::Iterator::collect
```

### Steps to reproduce

```

./tsbs_generate_data --use-case=""cpu-only"" --seed=123 --initial-scale=50000 --scale=50000 \
    --timestamp-start=""2022-09-05T00:00:00Z"" \
    --timestamp-end=""2022-09-05T12:00:00Z"" \
    --log-interval=""60s"" --format=""ceresdb"" > data.out

./tsbs_load_ceresdb --row-group-size 81920 --storage-format hybrid --file data.out
```


### Expected behavior

No error

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/255/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/256,https://api.github.com/repos/apache/horaedb/issues/256,horaedb,1374180992,256,Support complex filter before merge procedure,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-09-15T08:59:45Z,2022-10-29T15:34:38Z,"### Describe This Problem

A filter procedure according to the query predicates will be applied to the record batch stream from sst before feeding the batches to the merge iterator. However, the filter only supports a very simple form -- `anded` binary expression, so it doesn't work if the query predicate is complex, e.g. `where (hostname = '127.0.0.1' or hostname = '192.168.0.2') and timestamp between 'xxxx' and 'xxxx'`.

### Proposal

The crucial point here is how to make the filter procedure can support complex predicate expressions, and basically there are two approaches to this target:
- Utilize `datafusion`;
- Implement the filter logic manually;

And I vote for the first approach, but we have to figure out how to utilize `datafusion` to implement the filter logic.

### Additional Context
The filter procedure is applied here:
https://github.com/CeresDB/ceresdb/blob/43a84ba3c2ddcee69906e70322060b6dc4e91ddc/analytic_engine/src/row_iter/record_batch_stream.rs#L137
_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/256/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/256,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KxdDN,horaedb,1254478029,256,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-09-22T03:34:54Z,2022-09-22T03:34:54Z,"TSBS is added to CI, we can use it to compare performance before/after fix this issue
- https://github.com/CeresDB/ceresdb/actions/runs/3102504402#summary-8485564354","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KxdDN/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/256,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5LkjR-,horaedb,1267872894,256,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-10-05T03:21:00Z,2022-10-05T03:21:00Z,"To utilize datafusion, we can do:

* Create PhysicalExpr from LogicalExpr via `create_physical_expr`.
* Implement filter logic like `FilterExecStream` do in datafusion.

`create_physical_expr`: https://github.com/apache/arrow-datafusion/blob/45fc415daa7028559ef3477e53a184a114149f9e/datafusion/physical-expr/src/planner.rs#L42

`FilterExecStream`: https://github.com/apache/arrow-datafusion/blob/45fc415daa7028559ef3477e53a184a114149f9e/datafusion/core/src/physical_plan/filter.rs#L180

Maybe I can help do this task :D.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5LkjR-/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/256,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5LpZF4,horaedb,1269141880,256,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-10-06T00:23:53Z,2022-10-06T00:23:53Z,It will be appreciated if you volunteer to help.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5LpZF4/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/256,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L1Qz9,horaedb,1272253693,256,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-10-08T07:30:55Z,2022-10-08T07:30:55Z,"@ygf11 I have updated the code location about the filtering procedure, and I hope it will help:
https://github.com/CeresDB/ceresdb/blob/43a84ba3c2ddcee69906e70322060b6dc4e91ddc/analytic_engine/src/row_iter/record_batch_stream.rs#L137","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L1Qz9/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/256,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L1WXo,horaedb,1272276456,256,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-10-08T09:27:09Z,2022-10-08T09:27:09Z,">  I have updated the code location about the filtering procedure, and I hopes it will help.

Thanks for reminding， it helps a lot.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L1WXo/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/257,https://api.github.com/repos/apache/horaedb/issues/257,horaedb,1374276225,257,Support setting query&write rejection information in config.,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-09-15T10:09:24Z,2022-11-07T11:50:35Z,"### Describe This Problem

Now we use `curl` to set the rejection information, when the server restarts, the rejection information will be lost.
```
curl --location --request POST 'http://localhost:5000/reject' \
--header 'Content-Type: application/json' \
-d '{
    ""operation"":""Add"",
    ""write_reject_list"":[""table_1""],
    ""read_reject_list"":[""table_1""]
}'
```

### Proposal

Support setting query&write rejection information in config.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/257/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/258,https://api.github.com/repos/apache/horaedb/issues/258,horaedb,1375278097,258,Integration tests are not stable,ygf11,3428089,ygf11,yanggf23@gmail.com,CLOSED,2022-09-16T01:43:17Z,2022-10-08T07:32:02Z,"### Describe this problem

For create table test, 

https://github.com/CeresDB/ceresdb/blob/25557c73adaa6815875aaea3ccd5b61bf54e291f/tests/cases/local/05_ddl/create_tables.sql#L64


it will report error like:
```
Failed to execute query, err: Server(ServerError { code: 500, msg: ""Failed to execute interpreter, query: CREATE TABLE `05_create_tables_t8`(c1 int, t1 timestamp NOT NULL TIMESTAMP KEY) ENGINE = Analytic with (storage_format= 'unknown');. Caused by: Failed to execute create table, err:Failed to create table, name:05_create_tables_t8, err:Failed to create table, err:Invalid arguments, err:Invalid options, space_id:2, table:05_create_tables_t8, table_id:2199023255725, err:Unknown storage format. value:\""unknown\""."" })
```

The `table_id` in error message is not stable in my local pc, it will change if I run tests again.

The issue maybe is in test config, we use `Local storage` as the server config for integration tests now.

cc: @waynexia @jiacai2050

### Steps to reproduce

Run tests again.

### Expected behavior

The output should the same as before when we rerun tests.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/258/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/258,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Kb6l3,horaedb,1248831863,258,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-09-16T02:09:42Z,2022-09-16T02:09:42Z,"Yeah, it's already reported in https://github.com/CeresDB/ceresdb/issues/114#issuecomment-1218949297

For your case, you can ignore this error for now, we will support special `interceptor` to replace those random string in near future(maybe one month I guess).

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Kb6l3/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/258,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KcNqk,horaedb,1248909988,258,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-09-16T04:36:54Z,2022-09-16T04:36:54Z,"> For your case, you can ignore this error for now, we will support special interceptor to replace those random string in near future(maybe one month I guess).

Thanks, got it.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KcNqk/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/258,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5K90rW,horaedb,1257720534,258,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-09-26T09:02:30Z,2022-09-26T09:02:30Z,"#270 will fix this. The reason for this bug is that the test data won't be cleaned before any run, while the table id is auto-incremented, so the created, dropped, created-again table may have different table ids every time run the tests.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5K90rW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/258,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5LEJhW,horaedb,1259378774,258,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-09-27T11:42:37Z,2022-09-27T11:42:37Z,"I think of one corner case even when #270 is merged
> if users drop a new table in `00_dummy`, then all table id after `00_dummy` will be affected

So an interceptor may also need to deal with this case.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5LEJhW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/258,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5LENg2,horaedb,1259395126,258,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-09-27T11:54:02Z,2022-09-27T11:54:02Z,"> I think of one corner case even when #270 is merged
> 
> > if users drop a new table in `00_dummy`, then all table id after `00_dummy` will be affected
> 
> So an interceptor may also need to deal with this case.

Actually, just update the result files if the test cases are modified.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5LENg2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/258,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5LJUqm,horaedb,1260735142,258,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-09-28T10:56:13Z,2022-09-28T10:56:13Z,"It may be a little annoying since those other files are not touched.

Fow now I think it's OK to update those files manually since there are not many table ids in our testcases, but I prefer to keep the ability to replace something before diff.  ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5LJUqm/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/258,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L1Q3s,horaedb,1272253932,258,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-10-08T07:32:01Z,2022-10-08T07:32:01Z,"I guess #270 has fixed this, so let's close it.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L1Q3s/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/259,https://api.github.com/repos/apache/horaedb/issues/259,horaedb,1375410839,259,Persist final physical exprs after analyzing default value  options,ygf11,3428089,ygf11,yanggf23@gmail.com,CLOSED,2022-09-16T04:59:50Z,2024-10-25T11:45:32Z,"### Describe This Problem

For default value options, we persist `sqlparser expr` to meta store now.

A better way is to save final physical exprs, because we can avoid to transform from `sqlparser expr` to `physical expr` every time we use it.

### Proposal

Persist final physical exprs after analyzing. The problem is how to serialize datafusion's `PhysicalExpr`:

* Introduce `arrow-ballista`, which has implemented this api, but we need fork and maintain the relationship between `arrow-ballista` and `arrow-datafusion`.
* As suggested by @waynexia, `arrow-datafusion` has a issue about move serde of `PhysicalExpr` to `arrow-datafusion`. we can wait to see it.

Obviously the second way is easier to maintain and implement if the both works. 

### Additional Context

discussion: https://github.com/CeresDB/ceresdb/pull/246#discussion_r971660006","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/259/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/263,https://api.github.com/repos/apache/horaedb/issues/263,horaedb,1379343934,263,parser table name `01_xxx` will error,dust1,18304424,Yoke,834902408@qq.com,CLOSED,2022-09-20T12:31:12Z,2022-09-21T00:37:23Z,"### Describe this problem

When I try to create or delete a table with a table name in the format `01_xxx`, the parsing fails, No need for 01, it seems that as long as it is a number + ""_xx"" format, it will fail to parse

### Steps to reproduce

  ```sql
CREATE TABLE 22_test_order (     
host string tag,  
v2 int, 
ts timestamp NOT NULL,     
v1 int,     
timestamp KEY (ts) 
)
ENGINE = Analytic WITH ( enable_ttl = 'false' )`
```
or
```sql
drop table 01_xxx;
```

error info:
```
ERROR 1105 (HY000): Failed to handle sql:CREATE TABLE 22_test_order (     host string tag,     v2 int,     ts timestamp NOT NULL,     v1 int,     timestamp KEY (ts) )ENGINE = Analytic WITH (     enable_ttl = 'false' ), err:Failed to parse sql, err:Invalid sql, sql:CREATE TABLE 22_test_order (     host string tag,     v2 int,     ts timestamp NOT NULL,     v1 int,     timestamp KEY (ts) )ENGINE = Analytic WITH (     enable_ttl = 'false' ), err:sql parser error: Expected identifier, found: 22
```


### Expected behavior

Can create or drop tables

### Additional Information

error log
```
2022-09-20 20:29:31.889 ERRO [server/src/mysql/worker.rs:113] Mysql service Failed to handle sql, err: Failed to parse sql, err:Invalid sql, sql:CREATE TABLE 22_test_order (     host string tag,     v2 int,     ts timestamp NOT NULL,     v1 int,     timestamp KEY (ts) )ENGINE = Analytic WITH (     enable_ttl = 'false' ), err:sql parser error: Expected identifier, found: 22


2022-09-20 20:29:31.889 ERRO [server/src/mysql/worker.rs:92] MysqlWorker on_query failed. err:Failed to handle sql:CREATE TABLE 22_test_order (     host string tag,     v2 int,     ts timestamp NOT NULL,     v1 int,     timestamp KEY (ts) )ENGINE = Analytic WITH (     enable_ttl = 'false' ), err:Failed to parse sql, err:Invalid sql, sql:CREATE TABLE 22_test_order (     host string tag,     v2 int,     ts timestamp NOT NULL,     v1 int,     timestamp KEY (ts) )ENGINE = Analytic WITH (     enable_ttl = 'false' ), err:sql parser error: Expected identifier, found: 22

```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/263/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/263,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KpgOJ,horaedb,1252393865,263,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-09-20T13:56:48Z,2022-09-20T13:56:48Z,"Have you tried 
```
drop table `01_xxx`
```

In most situation, variable name beginning with number isn't allowed.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KpgOJ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/263,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KsBbY,horaedb,1253054168,263,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-09-21T00:36:48Z,2022-09-21T00:36:48Z,"Oops, I neglected to add `, It was my fault😂, maybe can close this issue","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5KsBbY/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/268,https://api.github.com/repos/apache/horaedb/issues/268,horaedb,1383572958,268,Centralize the logic of choosing worker for table into one place,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-09-23T10:04:12Z,2022-10-19T11:33:27Z,"### Describe This Problem

When carry on operations (write, flush etc.) on one table, a specific worker should be chosen. However, the logic for choosing such a worker distributes across the codebase:

https://github.com/CeresDB/ceresdb/blob/ccf988f5629436927479721b63df53b220967160/analytic_engine/src/instance/write_worker.rs#L307

https://github.com/CeresDB/ceresdb/blob/ccf988f5629436927479721b63df53b220967160/analytic_engine/src/instance/write_worker.rs#L669

https://github.com/CeresDB/ceresdb/blob/ccf988f5629436927479721b63df53b220967160/analytic_engine/src/table/data.rs#L541


### Proposal

We hope the logic of choosing the worker for a specific table should be centralized in one place (one function).

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/268/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/268,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MT9oX,horaedb,1280301591,268,NA,QuintinTao,72123724,,,NA,2022-10-17T05:18:42Z,2022-10-17T05:18:42Z,"could I ?
I want add a function in sst_util.rs
pub fn find_worker(table_id: usize, worker_num: usize) -> usize {
    table_id % worker_num
}

replace the three lines you mentioned.
 if find_worker(table_id, worker_num) != worker_id {","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MT9oX/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/268,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MUDiX,horaedb,1280325783,268,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-10-17T05:55:24Z,2022-10-17T05:55:24Z,"> could I ? I want add a function in sst_util.rs pub fn find_worker(table_id: usize, worker_num: usize) -> usize { table_id % worker_num }
> 
> replace the three lines you mentioned. if find_worker(table_id, worker_num) != worker_id {

@QuintinTao Thanks. Any contributions are welcome. The proposal is nice, and is it better to put the function `find_worker` in `writer_worker.rs`?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MUDiX/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/268,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MUICs,horaedb,1280344236,268,NA,QuintinTao,72123724,,,NA,2022-10-17T06:18:09Z,2022-10-17T06:18:09Z,"> > could I ? I want add a function in sst_util.rs pub fn find_worker(table_id: usize, worker_num: usize) -> usize { table_id % worker_num }
> > replace the three lines you mentioned. if find_worker(table_id, worker_num) != worker_id {
> 
> @QuintinTao Thanks. Any contributions are welcome. The proposal is nice, and is it better to put the function `find_worker` in `writer_worker.rs`?

OK","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MUICs/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/268,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MhjiK,horaedb,1283864714,268,NA,archerny,14784412,Arthur Chern,weirong.cwr@gmail.com,NA,2022-10-19T11:33:27Z,2022-10-19T11:33:27Z,issue was closed by this [PR](https://github.com/CeresDB/ceresdb/pull/311),"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MhjiK/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/271,https://api.github.com/repos/apache/horaedb/issues/271,horaedb,1385642388,271,"When upgrade to parquet 23, some usages are deprecated.",chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-09-26T08:22:23Z,2022-10-28T09:40:57Z,"### Describe This Problem

When upgrade to parquet 23, some usages are deprecated. 
```
warning: use of deprecated struct `arrow_deps::parquet::arrow::ParquetFileArrowReader`: Use ParquetRecordBatchReaderBuilder instead
```

### Proposal

Remove `#[allow(deprecated)]` in `analytic_engine/src/sst/parquet/mod.rs` and `components/parquet/src/lib.rs`.
Use `ParquetRecordBatchReaderBuilder` to refactor `parquet reader`.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/271/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/273,https://api.github.com/repos/apache/horaedb/issues/273,horaedb,1387365554,273,Avoid data copy in parquet reader.,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-09-27T08:43:48Z,2022-11-18T08:32:04Z,"### Describe This Problem

After parquet is upgraded by #269, some api changes and in order to adapt them some performance issues are introduced:
https://github.com/CeresDB/ceresdb/blob/a8fb2c92edb979051fbfbd339d72f65f94628f2c/components/parquet_ext/src/serialized_reader.rs#L195-L202

### Proposal
(TODO)

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/273/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/273,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OqPB6,horaedb,1319694458,273,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-11-18T08:32:03Z,2022-11-18T08:32:03Z,I guess this has been fixed already.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OqPB6/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/277,https://api.github.com/repos/apache/horaedb/issues/277,horaedb,1392010857,277,CI is broken,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-09-30T07:55:53Z,2022-10-11T07:28:10Z,"### Describe this problem

Now, the CI may break because of two reasons:
- UT `test_panic_hook` failed.
- Link fails when build `ceresdb-server` for harness test.

### Steps to reproduce

Run the CI workflow on the main branch.

### Expected behavior

The CI succeeds.

### Additional Information

The CI is broken after #267 is merged. In that PR, some dependencies has been upgraded, and especially the `nix` is upgraded, which may leading to the `test_panic_hook` UT failed.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/277/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/277,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L1NE-,horaedb,1272238398,277,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-10-08T06:05:55Z,2022-10-08T06:05:55Z,"Currently, the `test_panic_hook` will fail only on the github ubuntu runner os, that is to say, the failure can't be reproduced on the local ubuntu 20.04 environment.  #275 Ignore this ut to avoid blocking others development.

As for the link failure, I guess it is caused by the disk quota allocated by github workflow is exceeded, but this has not been proved, so #275 just add a memory report step in the CI workflow now.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L1NE-/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/277,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L8vKG,horaedb,1274212998,277,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-10-11T07:28:10Z,2022-10-11T07:28:10Z,"#292 will reduce the cache size, and after that the ci will not break easily (However, the cache size will become larger and larger after all, and let's clear the cache manually when the disk quota is exceeded).
 ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L8vKG/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/280,https://api.github.com/repos/apache/horaedb/issues/280,horaedb,1401804128,280,RFC tracker for wal on kafka,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-10-08T03:21:36Z,2022-11-19T12:55:30Z,"### Describe This Problem

When using ceresdb of cluster mode, a distributed wal is needed. Although a distributed wal implementation based on oceanbase has existed but the oceanbase cluster is not so easy to build and maintain, I proposal to implement another distributed wal based on kafka.

### Proposal

see [rendered](https://github.com/Rachelint/ceresdb/blob/wal-kafka-rfc/docs/rfcs/20221007-wal-on-kafka.md)

### Additional Context

The development plan as follow:

- [x] Pass the shard info
- [x] Message queue and its Kafka implementation
- [ ] Replayer and log's splitting
    - [ ] Add a `ShardBasedImpl` to `WalManager`(split to read, merge to write)
    - [ ] Add a replayer to intergrate all the table recovering logics
- [ ] `WalManager` 's message queue implementation
    - [x] Async inner iterator in `BatchLogIteratorAdapter` 
    - [ ] Message queue implementation","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/280/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/282,https://api.github.com/repos/apache/horaedb/issues/282,horaedb,1401848239,282,Replay WAL from the flushed sequence number,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-10-08T06:55:56Z,2022-11-27T07:48:25Z,"### Describe This Problem

Currently, all the existing log entries will be read and replayed when replaying wal, and this is correct now for the `mark_delete_entries_up_to` implementation of RocksDB WAL is sync deletion. However, the method `mark_delete_entries_up_to` is not ensured sync operation, especially for Oceanbase / Kafka implementations.

### Proposal

When replaying wal, we should omit the log entries whose sequence number is smaller than the flushed sequence number persisted in the `Manifest`.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/282/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/283,https://api.github.com/repos/apache/horaedb/issues/283,horaedb,1401922720,283,Modify the exist wal interface to adapt the new design,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-10-08T11:54:32Z,2022-10-13T12:51:48Z,"**Description**
Now the wal interface's design is for the mapping `one table to one region (wal unit)`. For adapting to the new wal implementation based on kafka and solving some exist problem (for example, replay is too slow), we plan to modify the wal interface. 


**Proposal**
+ Make `region id` to `WalLocation` (include `region id` and `table id` now).
+ Try best to keep current process in other modules (in fact, I prefer to add new components for avoiding direct interacting between `Instance` and `WalManager` to keep current process in later development).
+ Add a new method `scan` to `WalManager`(implement it in wal on kafka firstly, other implementations will implement and make use of it after).

**Additional context**


<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/283/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/284,https://api.github.com/repos/apache/horaedb/issues/284,horaedb,1401927379,284,Implement wal manager on kafka,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-10-08T12:12:17Z,2023-03-13T09:09:57Z,"**Description**
After modifying the existing wal desigin, we can implement our wal on kafka now. 

**Proposal**
+ Define and implement the log record format.
+ Implement the basic region management, and read, write on region (region's recovery is not considered yet).
+ Define the meta record format and implement region's recovery.
    + The meta in mq implement will includes a shard's all tables' max sequence number.
    + The meta key should be able to be distinguished from normal logs.
    + Before pushing the meta messages, the partition should be frozen firstly.
    + While recovering the region, it will perform the reverse traversal on partition util find one meta log.
<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**
The work is a bit big, I spit it into three stage:

- [x] implement the region meta module.
- [x] define record format .
- [ ] implement record deletion, record reading、record writing method.
- [ ] implement the region manager, region recovery,  and add some config.


","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/284/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/291,https://api.github.com/repos/apache/horaedb/issues/291,horaedb,1403972140,291,replace custom ParquetSstReader to DataFusion's ParquetExec,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-10-11T04:12:10Z,2022-11-18T02:58:18Z,"### Describe This Problem

In current implementation
- `ParquetSstReader` use CacheableSerializedFileReader to read parquet file, and
- `CacheableSerializedFileReader` is modeled after https://github.com/apache/arrow-rs/blob/5.2.0/parquet/src/file/serialized_reader.rs, which contains lots of low-level details about how to parse parquet, and there are already some issues with it, such as #271

### Proposal

By leverage DataFusion's [ParquetExec](https://docs.rs/datafusion/13.0.0/datafusion/physical_plan/file_format/struct.ParquetExec.html) to implement this, we can remove all those low-level details and it's already feature rich, for example:
- Support predicate pushdown
- Support custom [ParquetFileReaderFactory](https://docs.rs/datafusion/13.0.0/datafusion/physical_plan/file_format/trait.ParquetFileReaderFactory.html), which can be used to implement row-group level cache, just as what we do now 

### Additional Context

IOx already does this.
- https://github.com/influxdata/influxdb_iox/pull/5531/files","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/291/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/291,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L9J3Z,horaedb,1274322393,291,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-10-11T08:42:03Z,2022-10-11T08:42:03Z,"Seems this issue is relative to:

* https://github.com/CeresDB/ceresdb/issues/256

In #256, we want to implement filter logics with datafusion's expr.

If we replace `ParquetSstReader` with `ParquetExec`, I think the filter logic is also there.

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L9J3Z/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/291,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L-j_W,horaedb,1274691542,291,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-10-11T13:31:17Z,2022-10-11T13:31:17Z,"@ygf11 yeah, those issue are related, but they work in different levels.

In current implementation, `where` condition is filtered multiple times, 
- The underlying one is what this issue concerns, predicate works on sst file
- The next one is #256, predicate works before pass to `MergeIterator`, which is used to merge multiple source into one deduplicated.
- The last one is `FilterExec`, which utilize DataFusion to ensure all rows match `where` condition","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L-j_W/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/291,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MBmu2,horaedb,1275489206,291,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-10-12T02:04:09Z,2022-10-12T02:04:09Z,"> yeah, those issue are related, but they work in different levels.

Thanks, I almost got it. If I have mistake, please take it out.

First, We will read data from two places, `Memtables` and `Sst files`. And we will read sst with `ParquetExec` after this task.

Predicate works before pass to MergeIterator:
* For `sst files`, we can directly pass `filter expr` to `ParquetExec`.
* For `Memtables`, we still need implement filter logics manually, that's what #256 need do.

Further more, after https://github.com/CeresDB/ceresdb/issues/256 and this task, we can remove `FilterExec`, since we have pushed down all filters.

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MBmu2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/291,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MBrep,horaedb,1275508649,291,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-10-12T02:41:45Z,2022-10-12T02:41:45Z,"There are some mistakes here:
> For sst files, we can directly pass filter expr to ParquetExec.

This is true, but one thing worth mentioning that the data is not ensured to meet the requirements by pushed down predicates (actually the filter is implemented by utilizing the min-max index on the row-group level.)

> For Memtables, we still need implement filter logics manually, that's what https://github.com/CeresDB/ceresdb/issues/256 need do.

So the further filter works for not only the memtables but also the data from sst files. In conclusion, both #256 and #291 are necessary.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MBrep/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/291,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MBtfV,horaedb,1275516885,291,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-10-12T02:56:18Z,2022-10-12T02:56:18Z,"> This is true, but one thing worth mentioning that the data is not ensured to meet the requirements by pushed down predicates (actually the filter is implemented by utilizing the min-max index on the row-group level.)

Thanks for figuring it out. It is clear now :D.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MBtfV/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/291,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Opa6y,horaedb,1319481010,291,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-11-18T02:58:18Z,2022-11-18T02:58:18Z,Already fixed in main.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Opa6y/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/294,https://api.github.com/repos/apache/horaedb/issues/294,horaedb,1404206263,294,Reduce the binary size of ceresdb-server,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-10-11T08:16:46Z,2024-10-19T11:31:37Z,"### Describe this problem

Now the compiled ceresdb-server's size has reached up to 1.3GB, which is unbelievable.

### Steps to reproduce

Just compile the ceresdb-server and check its size.

### Expected behavior

The binary size should not be so large.

### Additional Information

The way to reduce the binary size that I can come up with now is to remove the unused dependencies, including:
- Replace protobuf with prost used by `protos` crate so to remove grpcio dependencies;
- ...","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/294/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/294,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L-pxd,horaedb,1274715229,294,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-10-11T13:48:20Z,2022-10-11T13:48:20Z,"Don't expect it's already exceed 1G. but after `strip` it, it's just 59M, so this issue may not very serious, at least for now.
```
$ ll -h target/release/ceresdb-server ~/bench/ceresdb-server 
-rwxr-xr-x 1 chenxiang chenxiang 1.3G Oct  9 17:11 /home/chenxiang/bench/ceresdb-server
-rwxr-xr-x 2 chenxiang chenxiang  59M Oct 11 21:44 target/release/ceresdb-server
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5L-pxd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/294,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MmK1H,horaedb,1285074247,294,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-10-20T07:32:33Z,2022-10-20T07:32:33Z,"#322 reduce binary size to 500M, but at the cost of long build, we can introduce more profiles to balance size and speed.

- https://github.com/influxdata/influxdb_iox/blob/main/Cargo.toml#L117

IOx have a `quick-release` profile","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MmK1H/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/294,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OGE7u,horaedb,1310215918,294,NA,waynexia,15380403,Ruihang Xia,,NA,2022-11-10T12:33:22Z,2022-11-10T12:33:22Z,"`strip` removes debug info to reduce the binary size. Here is my release binary size without the `release.profile.debug = true`:

```
➜  ceresdb git:(main) ✗ ll -h target/release/
Permissions Size User    Date Modified Name
drwxr-xr-x     - ruihang 10 11 20:19   build
.rwxr-xr-x   61M ruihang 10 11 20:23   ceresdb-server
.rw-r--r--   18k ruihang 10 11 20:23   ceresdb-server.d
drwxr-xr-x     - ruihang 10 11 20:23   deps
drwxr-xr-x     - ruihang 10 11 20:19   examples
drwxr-xr-x     - ruihang 10 11 20:19   incremental
.rw-r--r--   18k ruihang 10 11 20:23   libceresdb.d
.rw-r--r--   17M ruihang 10 11 20:23   libceresdb.rlib
```

So the `strip` is not a feasible way, otherwise we can just eliminate it on building.

>reduce binary size to 500M, but at the cost of long build

link unit and lto are not only the size reducing options. They may also have influence on performance. Maybe open them on every formal release is a better choice. (or `lto = ""full""`, might be even better)","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OGE7u/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/294,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OGL0H,horaedb,1310244103,294,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-11-10T13:00:12Z,2022-11-10T13:00:12Z,"> So the strip is not a feasible way, otherwise we can just eliminate it on building.

It's true. Debug information is necessary for us to troubleshoot in prod env.

In #322, what matters most to reduce binary size is actually the option: `opt-level = ""z""`. However, this may cause performance degradation, and for this reason #322 has been reverted already.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OGL0H/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/294,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OJqTR,horaedb,1311155409,294,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-11-11T02:35:10Z,2022-11-11T02:35:10Z,"> It's true. Debug information is necessary for us to troubleshoot in prod env.

`-Csplit-debuginfo` just get stabilized in 1.65, so we may can rely on this to both reduce binary size and still have to ability to debugging.
- https://github.com/rust-lang/rust/pull/98051/","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OJqTR/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/298,https://api.github.com/repos/apache/horaedb/issues/298,horaedb,1407359238,298,Define a message queue interface and its Kafka implementation,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-10-13T08:19:13Z,2022-10-22T00:48:58Z,"**Description**
In the future, maybe we need to support other message queue, so make an abstract message queue interface may be useful. Otherwise, such an interface is convenient for test, too.


**Proposal**
+ Define a abstract message queue.
+ Implement it for Kafka. 

**Additional context**
The scan method is needed in CeresDB, it will return a iterator and high-level should poll the iterator to get all data in topic. For ensuring scan process's normal running, freezing the partition during scanning is necessary, otherwise error will occur. For reaching the target, following points should be considered:
+ Kafka's message retention. Too old logs may be deleted by Kafka (`default 7 days`, normally won't be reset), but it make almost no difference to us, because CeresDB will flush data periodically (`default 5 hours`), and drop or close tables will trigger flush, too. So it is almost impossible to leave such too old data in wal, and it is abnormal to keep data in wal for such a long time in theory because wal is a write buffer actually.

+ Two leader shard. If there are two leader in a specific shard, it may make the scanning mad, and everything will be unpredictable. To ensure such situation will not happen is the responsibility of `Ceresmeta`.

+ Other threads concurrently operate the partition in a single node. Ensuring no others will operate the partition while it is being scanned due to recovering is necessary. It is easy to control.

<!---
Add any other context or screenshots about the feature request here.
-->

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/298/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/299,https://api.github.com/repos/apache/horaedb/issues/299,horaedb,1407398814,299,CI failed sometimes caused by no space left,ygf11,3428089,ygf11,yanggf23@gmail.com,CLOSED,2022-10-13T08:47:47Z,2022-10-14T09:26:56Z,"### Describe this problem

CI failed sometimes caused by no space left.

see the action: https://github.com/CeresDB/ceresdb/actions/runs/3238830826


### Steps to reproduce

It happens sometimes.

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/299/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/299,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MIiSZ,horaedb,1277306009,299,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-10-13T09:25:46Z,2022-10-13T09:25:46Z,"```
curl https://api.github.com/repos/CeresDB/ceresdb/actions/caches
{
  ""total_count"": 3,
  ""actions_caches"": [
    {
      ""id"": 106,
      ""ref"": ""refs/heads/main"",
      ""key"": ""debug-Linux-e1d3fcbb07f3c8afd0d8cf431daa5eb81080cddc4261b58842339f2067e16276-f2a643d1e3373f16a928bb7f00ef4a052c7dc9b695d87203aab2a3c4aae2adcb"",
      ""version"": ""e74ceba4d6dcd2cb6cfcb03c114afa9a0cd26cc460bb033c0c0e92ca181b2ad3"",
      ""last_accessed_at"": ""2022-10-13T07:42:07.236666700Z"",
      ""created_at"": ""2022-10-11T09:06:11.750000000Z"",
      ""size_in_bytes"": 4391109591
    },
    {
      ""id"": 109,
      ""ref"": ""refs/heads/main"",
      ""key"": ""release-Linux-e1d3fcbb07f3c8afd0d8cf431daa5eb81080cddc4261b58842339f2067e16276-bc493747eb1804ee2211b171e22b765502f7e5a84fd5965defdab98cbfd1d196"",
      ""version"": ""e74ceba4d6dcd2cb6cfcb03c114afa9a0cd26cc460bb033c0c0e92ca181b2ad3"",
      ""last_accessed_at"": ""2022-10-13T03:30:08.936666700Z"",
      ""created_at"": ""2022-10-13T03:30:08.936666700Z"",
      ""size_in_bytes"": 2297060459
    },
    {
      ""id"": 108,
      ""ref"": ""refs/pull/295/merge"",
      ""key"": ""debug-Linux-e1d3fcbb07f3c8afd0d8cf431daa5eb81080cddc4261b58842339f2067e16276-ba120acd5fe6fb02c515b4d01c424f19411fda8394ce7556f453d18ddb550024"",
      ""version"": ""e74ceba4d6dcd2cb6cfcb03c114afa9a0cd26cc460bb033c0c0e92ca181b2ad3"",
      ""last_accessed_at"": ""2022-10-12T11:42:59.270000000Z"",
      ""created_at"": ""2022-10-12T11:42:59.270000000Z"",
      ""size_in_bytes"": 5246334661
    }
  ]
}
```
After #292, targets dir in release mode won't be mixed with it in debug mode, don't expect it's still 4G(id: 106).
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MIiSZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/299,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MIjyi,horaedb,1277312162,299,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-10-13T09:30:44Z,2022-10-13T09:30:44Z,"```
{
  ""total_count"": 1,
  ""actions_caches"": [
    {
      ""id"": 109,
      ""ref"": ""refs/heads/main"",
      ""key"": ""release-Linux-e1d3fcbb07f3c8afd0d8cf431daa5eb81080cddc4261b58842339f2067e16276-bc493747eb1804ee2211b171e22b765502f7e5a84fd5965defdab98cbfd1d196"",
      ""version"": ""e74ceba4d6dcd2cb6cfcb03c114afa9a0cd26cc460bb033c0c0e92ca181b2ad3"",
      ""last_accessed_at"": ""2022-10-13T03:30:08.936666700Z"",
      ""created_at"": ""2022-10-13T03:30:08.936666700Z"",
      ""size_in_bytes"": 2297060459
    }
  ]
}
```
I have deleted 106 and 108, let's wait and see whether this error still exsits","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MIjyi/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/299,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MJILQ,horaedb,1277461200,299,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-10-13T11:30:24Z,2022-10-13T11:30:24Z,"Actually, the target directory will always increase a little after one ci runs because some new artifacts will be generated. What we need may be a mechanism to auto clean the target directory when the decompressed target directory is too large.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MJILQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/299,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MMtzZ,horaedb,1278401753,299,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-10-14T02:50:19Z,2022-10-14T02:50:19Z,"https://github.com/actions/runner-images/issues/2606#issuecomment-772683150

We can remove unused libs to save disk usage.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MMtzZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/299,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MNDIL,horaedb,1278489099,299,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-10-14T05:06:27Z,2022-10-14T05:06:27Z,"Maybe we can remove `target` from cache, If the execution of CI will not slow down too much.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MNDIL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/299,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MNhBW,horaedb,1278611542,299,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-10-14T07:39:36Z,2022-10-14T07:39:36Z,"> Maybe we can remove `target` from cache, If the execution of CI will not slow down too much.

Actually, the target cache matters a lot in speeding up CI. #301 has fixed this issue.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MNhBW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/302,https://api.github.com/repos/apache/horaedb/issues/302,horaedb,1408922699,302,Panic when reading null values of a not-null column defined schema,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-10-14T07:46:19Z,2023-02-09T08:04:42Z,"### Describe this problem

It seems there is no check for insert a null value of a not-null column now, and the datafusion may panic when processing such case. Here is the stacktrace:
```plaintext
2022-10-14 15:12:20.398 ERRO [common_util/src/panic.rs:42] thread 'ceres-read' panicked 'called `Result::unwrap()` on an `Err` value: InvalidArgumentError(""Column 'COUNT(DISTINCT SPM_2055016796_INFLUENCE_DEF
AULT.action)[count distinct]' is declared as non-nullable but contains null values"")' at ""/home/chunshao.rcs/.cargo/git/checkouts/arrow-datafusion-b9eb4f789f8bda1f/d84ea9c/datafusion/core/src/physical_plan/r
epartition.rs:178""
   0: common_util::panic::set_panic_hook::{{closure}}
             at /home/chunshao.rcs/github/CeresDB/ceresdb/common_util/src/panic.rs:41:18
   1: std::panicking::rust_panic_with_hook
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:702:17
   2: std::panicking::begin_panic_handler::{{closure}}
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:588:13
   3: std::sys_common::backtrace::__rust_end_short_backtrace
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/sys_common/backtrace.rs:138:18
   4: rust_begin_unwind
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:584:5
   5: core::panicking::panic_fmt
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/panicking.rs:142:14
   6: core::result::unwrap_failed
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/result.rs:1814:5
   7: core::result::Result<T,E>::unwrap
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/result.rs:1107:23
      datafusion::physical_plan::repartition::BatchPartitioner::partition
             at /home/chunshao.rcs/.cargo/git/checkouts/arrow-datafusion-b9eb4f789f8bda1f/d84ea9c/datafusion/core/src/physical_plan/repartition.rs:178:33
   8: datafusion::physical_plan::repartition::RepartitionExec::pull_from_input::{{closure}}
             at /home/chunshao.rcs/.cargo/git/checkouts/arrow-datafusion-b9eb4f789f8bda1f/d84ea9c/datafusion/core/src/physical_plan/repartition.rs:452:13
      <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/future/mod.rs:91:19
   9: tokio::runtime::task::core::CoreStage<T>::poll::{{closure}}
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/core.rs:165:17
      tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/loom/std/unsafe_cell.rs:14:9
      tokio::runtime::task::core::CoreStage<T>::poll
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/core.rs:155:13
      tokio::runtime::task::harness::poll_future::{{closure}}
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/harness.rs:480:19
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/panic/unwind_safe.rs:271:9
  10: std::panicking::try::do_call
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:492:40
      std::panicking::try
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:456:19
      std::panic::catch_unwind
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panic.rs:137:14
      tokio::runtime::task::harness::poll_future
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/harness.rs:468:18
      tokio::runtime::task::harness::Harness<T,S>::poll_inner
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/harness.rs:104:27
      tokio::runtime::task::harness::Harness<T,S>::poll
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/harness.rs:57:15
        11: tokio::runtime::task::raw::RawTask::poll
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/raw.rs:84:18
      tokio::runtime::task::LocalNotified<S>::run
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/mod.rs:381:9
      tokio::runtime::thread_pool::worker::Context::run_task::{{closure}}
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/thread_pool/worker.rs:458:21
      tokio::coop::with_budget::{{closure}}
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/coop.rs:102:9
      std::thread::local::LocalKey<T>::try_with
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/thread/local.rs:445:16
      std::thread::local::LocalKey<T>::with
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/thread/local.rs:421:9
  12: tokio::coop::with_budget
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/thread_pool/worker.rs:434:9
      tokio::coop::budget
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/coop.rs:72:5
      tokio::runtime::thread_pool::worker::Context::run_task
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/thread_pool/worker.rs:434:9
  13: tokio::runtime::thread_pool::worker::Context::run
  14: tokio::runtime::thread_pool::worker::run::{{closure}}
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/thread_pool/worker.rs:386:17
      tokio::macros::scoped_tls::ScopedKey<T>::set
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/macros/scoped_tls.rs:61:9
  15: tokio::runtime::thread_pool::worker::run
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/thread_pool/worker.rs:383:5
  16: tokio::runtime::thread_pool::worker::Launch::launch::{{closure}}
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/thread_pool/worker.rs:362:45
      <tokio::runtime::blocking::task::BlockingTask<T> as core::future::future::Future>::poll
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/blocking/task.rs:42:21
```

The table schema is:
```sql
{""rows"":[{""Table"":""SPM_2055016796_INFLUENCE_DEFAULT"",""Create Table"":""CREATE TABLE `SPM_2055016796_INFLUENCE_DEFAULT` (`period` timestamp NOT NULL, `tsid` uint64 NOT NULL, `TraceId` string TAG, `_result` string TAG, `groupbyIndex0` string TAG, `idc` string TAG, `ldc` string TAG, `server` string TAG, `action` string TAG, `pid` string TAG, PRIMARY KEY(period,tsid), TIMESTAMP KEY(period)) ENGINE=Analytic WITH(arena_block_size='2097152', compaction_strategy='default', compression='ZSTD', enable_ttl='true', num_rows_per_row_group='8192', segment_duration='2h', storage_format='COLUMNAR', ttl='110d', update_mode='APPEND', write_buffer_size='33554432')""}]}
```

### Steps to reproduce

1. Create a table with tag;
2. Insert a row with a null value as the tag;
3. Query the row;

### Expected behavior

It should not panic.

### Additional Information

At least two things we need to fix:
- Add check to prevent writing null value of a not-null column;
- Make datafusion throw an error rather than panic when processing such a case;
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/302/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/302,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MS0SF,horaedb,1280001157,302,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-10-16T16:16:55Z,2022-10-16T16:16:55Z,"Actually, I can't reproduce this error. After digging into the codebase, I find it the following points:
- The tag column is nullable by default;
- Inserting a null value is disallowed if the tag column is not nullable;

Maybe this problem is caused by altering schema?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MS0SF/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/302,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Mg4WE,horaedb,1283687812,302,NA,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,NA,2022-10-19T09:15:23Z,2022-10-19T09:15:23Z,"@ShiKaiWi  I reproduce this error. 
1. Create table named 'demo' with tag 'name'
```
curl --location --request POST 'http://127.0.0.1:5000/sql' \
--header 'Content-Type: application/json' \
-H 'x-ceresdb-access-tenant: test' \
--data-raw '{
    ""query"": ""CREATE TABLE `demo` (`name` string TAG NULL, `value` double NOT NULL, `t` timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE=Analytic with (enable_ttl='\''false'\'')""
}'
```
The table schema is :

```
CREATE TABLE `demo` (`t` timestamp NOT NULL, `tsid` uint64 NOT NULL, `name` string TAG, `value` double NOT NULL, PRIMARY KEY(t,tsid), TIMESTAMP KEY(t)) ENGINE=Analytic WITH(arena_block_size='2097152', compaction_strategy='default', compression='ZSTD', enable_ttl='false', num_rows_per_row_group='8192', segment_duration='', storage_format='COLUMNAR', ttl='7d', update_mode='OVERWRITE', write_buffer_size='33554432')""
```

2. insert a row,  tag `name` is null

```
curl --location --request POST 'http://127.0.0.1:5000/sql' \
--header 'Content-Type: application/json' \
-H 'x-ceresdb-access-tenant: test' \
--data-raw '{
    ""query"": ""INSERT INTO demo(t,  value) VALUES(1651737067000, 100)""
}'
```
3. Query like following statement， with Group-By and Count(DISTINCT)  operator.

```
curl --location --request POST 'http://127.0.0.1:5000/sql' \
--header 'Content-Type: application/json' \
--header 'x-ceresdb-access-tenant: test' \
--data-raw '{
    ""query"": ""select `t`, count(distinct name) from demo group by `t`""
}'
```
Here is the stacktrace:
```
ERRO [common_util/src/panic.rs:42] thread 'ceres-bg' panicked 'called `Result::unwrap()` on an `Err` value: InvalidArgumentError(""Column 'COUNT(DISTINCT demo.name)[count distinct]' is declared as non-nullable but contains null values"")' at ""/Users/michael/.cargo/git/checkouts/arrow-datafusion-b9eb4f789f8bda1f/d84ea9c/datafusion/core/src/physical_plan/repartition.rs:178""
   0: backtrace::backtrace::libunwind::trace
             at /Users/michael/.cargo/registry/src/github.com-1ecc6299db9ec823/backtrace-0.3.66/src/backtrace/mod.rs:66:5
      backtrace::backtrace::trace_unsynchronized
             at /Users/michael/.cargo/registry/src/github.com-1ecc6299db9ec823/backtrace-0.3.66/src/backtrace/mod.rs:66:5
      backtrace::backtrace::trace
             at /Users/michael/.cargo/registry/src/github.com-1ecc6299db9ec823/backtrace-0.3.66/src/backtrace/mod.rs:53:14
      backtrace::capture::Backtrace::create
             at /Users/michael/.cargo/registry/src/github.com-1ecc6299db9ec823/backtrace-0.3.66/src/capture.rs:176:9
      backtrace::capture::Backtrace::new
```
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Mg4WE/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/302,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NSrHI,horaedb,1296740808,302,NA,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,NA,2022-10-31T08:12:23Z,2022-10-31T08:12:23Z,I submit a bug issue:https://github.com/apache/arrow-datafusion/issues/4040 for datafusion.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NSrHI/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/302,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NTkMz,horaedb,1296974643,302,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-10-31T11:49:55Z,2022-10-31T11:49:55Z,"I try to reproduce this problem with this file: https://github.com/apache/arrow-datafusion/blob/97b3a4b37f54aaa52f8705db3e57b15ee98c24a7/datafusion-examples/examples/memtable.rs#L39

Changes:
```diff
1 file changed, 6 insertions(+), 8 deletions(-)
datafusion-examples/examples/memtable.rs | 14 ++++++--------

modified   datafusion-examples/examples/memtable.rs
@@ -36,14 +36,12 @@ async fn main() -> Result<()> {
     // Register the in-memory table containing the data
     ctx.register_table(""users"", Arc::new(mem_table))?;
 
-    let dataframe = ctx.sql(""SELECT * FROM users;"").await?;
+    let dataframe = ctx
+        .sql(""SELECT id,count(distinct bank_account) From users group by id;"")
+        .await?;
 
     timeout(Duration::from_secs(10), async move {
-        let result = dataframe.collect().await.unwrap();
-        let record_batch = result.get(0).unwrap();
-
-        assert_eq!(1, record_batch.column(0).len());
-        dbg!(record_batch.columns());
+        dataframe.show().await.unwrap();
     })
     .await
     .unwrap();
@@ -56,8 +54,8 @@ fn create_memtable() -> Result<MemTable> {
 }
 
 fn create_record_batch() -> Result<RecordBatch> {
-    let id_array = UInt8Array::from(vec![1]);
-    let account_array = UInt64Array::from(vec![9000]);
+    let id_array = UInt8Array::from(vec![1, 2]);
+    let account_array = UInt64Array::from(vec![None, Some(1)]);
 
     Ok(RecordBatch::try_new(
         get_schema(),


```
Then execute this demo with `cargo run --example memtable`, will output
```
+----+------------------------------------+
| id | COUNT(DISTINCT users.bank_account) |
+----+------------------------------------+
| 2  | 1                                  |
| 1  | 0                                  |
+----+------------------------------------+
```
It works without panic, maybe we need to narrow this problem down, to check if it's our usage issue or upstream issue.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NTkMz/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/304,https://api.github.com/repos/apache/horaedb/issues/304,horaedb,1409122708,304,Can't start ceresdb-server in cluster mode,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-10-14T10:21:05Z,2022-10-17T02:17:09Z,"### Describe this problem

It will panic if start ceresdb-server in cluster mode:
```
ERRO [common_util/src/panic.rs:42] thread 'main' panicked 'Cannot start a runtime from within a runtime. This happens because a function (like `block_on`) attempted to block the current thread while the thread is being used to drive asynchronous tasks.' at ""/Users/wangzuliang/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/thread_pool/mod.rs:89""
```

### Steps to reproduce

Run the ceresdb-server with: `./target/release/ceresdb-server --config ./docs/example-cluster-0.toml`.

### Expected behavior

It won't panic.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/304/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/308,https://api.github.com/repos/apache/horaedb/issues/308,horaedb,1410989215,308,Failed to find memtable to write for schema mismatch,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-10-17T06:32:56Z,2023-06-09T07:49:21Z,"### Describe this problem

Panic with the stacktrace:
```plain
2022-10-17 11:04:48.036 ERRO [analytic_engine/src/instance/write.rs:246] Failed to write to memtable, table:SPM_241044197_INFLUENCE_DEFAULT, table_id:7696581400551, err:Failed to find mutable memtable, table:SPM_241044197_INFLUENCE_DEFAULT, err:Failed to find memtable for write, 
err:Schema mismatch, memtable_version:1, given:2.
Backtrace:
 0 <snafu::backtrace_shim::Backtrace as snafu::GenerateBacktrace>::generate::h3f52f317135a785b
   /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/table/version.rs:41
   analytic_engine::table::version::SchemaMismatch<__T0,__T1>::build::hec315d78c29a32f9
   /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/table/version.rs:41
   analytic_engine::table::version::SchemaMismatch<__T0,__T1>::fail::h55958999369fc666
   /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/table/version.rs:41
   analytic_engine::table::version::TableVersion::memtable_for_write::h489b8f0b6fd929fc
   /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/table/version.rs:636
 1 analytic_engine::table::data::TableData::find_or_create_mutable::h8df5a6e0e2779f2d
   /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/table/data.rs:328
 2 analytic_engine::instance::write::<impl analytic_engine::instance::Instance>::write_to_memtable::h177e7f4f900933fc
   /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/instance/write.rs:469
 3 analytic_engine::instance::write::<impl analytic_engine::instance::Instance>::process_write_table_command::{{closure}}::hb0450cd24573eacb
   /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/instance/write.rs:238
   <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll::h8d6ec5f0357f4395
   /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/future/mod.rs:91
 4 analytic_engine::instance::write_worker::WriteWorker::handle_write_table::{{closure}}::h00ab49ddce0baa9c
   /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/instance/write_worker.rs:810
   <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll::h83fdda7c55aeac42
   /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/future/mod.rs:91
   analytic_engine::instance::write_worker::WriteWorker::run::{{closure}}::h93d0aa56457bd0c4
   /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/instance/write_worker.rs:750
 5 <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll::hb661b1497c326a5a
   /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/future/mod.rs:91
   analytic_engine::instance::write_wo
```

### Steps to reproduce

Have no idea about how to reproduce this panic now.

### Expected behavior

Won't fail.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/308/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/313,https://api.github.com/repos/apache/horaedb/issues/313,horaedb,1412863947,313,support scan parquet file in reverse order,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-10-18T09:17:36Z,2023-11-03T06:50:29Z,"### Describe This Problem

After #312, we only support scan parquet file in one order, `ReversedFileReader` cannot be used with ParquetExec

### Proposal

Implement `ReversedFileReader` logic in datafusion's ParquetExec.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/313/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/313,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qzxqc,horaedb,1791957660,313,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-11-03T06:50:29Z,2023-11-03T06:50:29Z,`ReversedFileReader` has been removed.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qzxqc/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/315,https://api.github.com/repos/apache/horaedb/issues/315,horaedb,1413076770,315,Failed to create catalog manager,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2022-10-18T11:47:49Z,2022-10-26T08:36:42Z,"### Describe this problem

Panic with the stacktrace:
```
2022-10-18 16:01:56.037 ERRO [common_util/src/panic.rs:42] thread 'main' panicked 'Failed to create catalog manager: VisitSysCatalog { source: VisitorOpenTable { source: Unexpected { source: OperateByWriteWorker { space_id: 7, table: ""BIZ_643000197_INFLUENCE_DEFAULT"", table_id: TableId(7696581394717), source: Channel { source: ReadWal { source: Read { source: ""Corruption: block checksum mismatch: expected 3149814890, got 3226198023  in /home/admin/data/ceresdbx/wal/263234.sst offset 30618826 size 1260"", backtrace: Backtrace(   0: <snafu::backtrace_shim::Backtrace as snafu::GenerateBacktrace>::generate
             at /home/chunshao.rcs/github/CeresDB/ceresdb/wal/src/manager.rs:26:21
      <wal::manager::error::Read as snafu::IntoError<wal::manager::error::Error>>::into_error
             at /home/chunshao.rcs/github/CeresDB/ceresdb/wal/src/manager.rs:26:21
      <core::result::Result<T,E> as snafu::ResultExt<T,E>>::context::{{closure}}
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/lib.rs:318:30
      core::result::Result<T,E>::map_err
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/result.rs:855:27
      <core::result::Result<T,E> as snafu::ResultExt<T,E>>::context
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/lib.rs:318:9
      <wal::rocks_impl::manager::RocksLogIterator as wal::manager::BlockingLogIterator>::next_log_entry
             at /home/chunshao.rcs/github/CeresDB/ceresdb/wal/src/rocks_impl/manager.rs:569:25
   1: <wal::manager::BatchLogIteratorAdapter as wal::manager::BatchLogIterator>::next_log_entries::{{closure}}::{{closure}}
             at /home/chunshao.rcs/github/CeresDB/ceresdb/wal/src/manager.rs:320:50
      <tokio::runtime::blocking::task::BlockingT
```

### Steps to reproduce

Have no idea about how to reproduce this panic now.

### Expected behavior

Won't fail.

### Additional Information

maybe this bug similar to #308 ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/315/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/315,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5M_aDd,horaedb,1291690205,315,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-10-26T08:35:45Z,2022-10-26T08:35:45Z,"This is caused by the corruption of the RocksDB files. Usually, such error will happen when writing to the disk which is already full.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5M_aDd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/315,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5M_aXh,horaedb,1291691489,315,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-10-26T08:36:42Z,2022-10-26T08:36:42Z,I guess no need to fix this bug because this is caused by RocksDB. Let's close it.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5M_aXh/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/321,https://api.github.com/repos/apache/horaedb/issues/321,horaedb,1414268772,321,SstMetaData's size is 0 after flush,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-10-19T04:31:31Z,2023-01-04T08:19:05Z,"### Describe this problem

When test #312, I find size of SstMetaData is 0 after flush memtable, this cause following error when restart(reading sst)

```
2022-10-19 11:49:32.342 ERRO [common_util/src/panic.rs:42] thread 'main' panicked 'Failed to create catalog manager: VisitSysCatalog { source: ReadStream { source: ErrWithSource { msg: ""Read record batch"", source: ReadFromSubIter { source: PullRecordBat
ch { source: DecodeRecordBatch { source: ExternalError(ParquetError(General(""CachableParquetFileReader::get_metadata error: Execution error: file size of 0 is less than footer""))) } } } } } }' at ""src/setup.rs:187""
   0: common_util::panic::set_panic_hook::{{closure}}
```

### Steps to reproduce

1. Insert some data
```sql
CREATE TABLE `cpu2` (
`my_ts` timestamp NOT NULL, 
 `t1` string tag,  
 `t2` string tag,  
 `value` double,
  TIMESTAMP KEY(my_ts)) 
  with (
enable_ttl = 'false'
);

insert into cpu2  (my_ts, t1,t2, `value`) 
values
(123, 'web', 'hz', 100),
(124, 'web','hz', 101),
(123, 'ios', 'sh', 100),
(124, 'ios', 'sh', 101);
```
2. flush
```bash
curl --location --request POST 'localhost:5440/flush_memtable'
```

3. restart service

### Expected behavior

No panic

### Additional Information

 memtable is write to sst in
https://github.com/CeresDB/ceresdb/blob/c557007e6c56390d12debe2d69478dcd73ae4796/analytic_engine/src/instance/flush_compaction.rs#L641

but we update size after this, in https://github.com/CeresDB/ceresdb/blob/c557007e6c56390d12debe2d69478dcd73ae4796/analytic_engine/src/instance/flush_compaction.rs#L657
which is too late, since it's already persisted.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/321/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/321,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RscqY,horaedb,1370606232,321,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-01-04T08:19:05Z,2023-01-04T08:19:05Z,Fixed in #513 ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RscqY/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/324,https://api.github.com/repos/apache/horaedb/issues/324,horaedb,1416125987,324,Version in Cargo.toml is inconsistent with the release,archerny,14784412,Arthur Chern,weirong.cwr@gmail.com,CLOSED,2022-10-20T07:38:55Z,2022-10-20T10:17:36Z,"### Describe this problem

We are going to release a new version 0.4.0, however, I found that the version field in Cargo.toml is still 0.1.0. And besides that , version field in previous releases were inconsistent too.

### Steps to reproduce

Cargo.toml
``` toml
[package]
name = ""ceresdb""
version = ""0.1.0""
authors = [""CeresDB Authors <ceresdbservice@gmail.com>""]
edition = ""2021""
```
Current output of build.rs is shown below:
```
cargo:rustc-env=VERGEN_BUILD_TIMESTAMP=2022-10-19T16:55:23.452301+00:00
cargo:rustc-env=VERGEN_BUILD_SEMVER=0.1.0
cargo:rustc-env=VERGEN_GIT_BRANCH=main
cargo:rustc-env=VERGEN_GIT_COMMIT_TIMESTAMP=2022-10-19T07:25:54+00:00
cargo:rustc-env=VERGEN_GIT_SEMVER=0.1.0
cargo:rustc-env=VERGEN_GIT_SHA_SHORT=d7c80c1
cargo:rerun-if-changed=/Users/chenwr/workspace/ceresdata/ceresdb/.git/HEAD
cargo:rerun-if-changed=/Users/chenwr/workspace/ceresdata/ceresdb/.git/refs/heads/main
```


### Expected behavior

Version field should be consistent with current release version.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/324/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/327,https://api.github.com/repos/apache/horaedb/issues/327,horaedb,1417643162,327,Run only the tests in wal lib failed.,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-10-21T03:13:12Z,2022-10-21T04:19:25Z,"### Describe this problem

I want to add some tests to wal lib, and I try to run the tests in wal only, then following error occurred.
I think this error is caused by the workspace dependency....

``` 
*  Executing task: cargo test --package wal --lib -- manager::tests::test_iterator_adapting --exact --nocapture 

    Blocking waiting for file lock on package cache
    Blocking waiting for file lock on package cache
    Blocking waiting for file lock on package cache
    Blocking waiting for file lock on build directory
   Compiling wal v0.1.0 (/Users/kamiu/Desktop/github/ceresdb/wal)
error[E0432]: unresolved import tempfile
```

### Steps to reproduce

cargo test --package wal --lib -- manager::tests::test_iterator_adapting --exact --nocapture 


### Expected behavior

Run it successfully.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/327/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/327,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MrWdG,horaedb,1286432582,327,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-10-21T04:09:14Z,2022-10-21T04:09:14Z,"Try this 
```
cargo test --workspace --package wal ...
```
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MrWdG/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/327,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MrXL6,horaedb,1286435578,327,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2022-10-21T04:14:21Z,2022-10-21T04:14:21Z,"> Try this
> 
> ```
> cargo test --workspace --package wal ...
> ```

I try it at first... but it will run all the tests in workspace while add `--workspace`.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MrXL6/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/327,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MrX7q,horaedb,1286438634,327,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2022-10-21T04:19:24Z,2022-10-21T04:19:24Z,"It will just run the tests in all components corresponding to wal while add --workspace.
It is ok to run like this.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5MrX7q/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/334,https://api.github.com/repos/apache/horaedb/issues/334,horaedb,1419435858,334,"Support two wal designs(one table to one region, one shard to one region)",Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-10-22T18:17:26Z,2023-03-31T08:56:33Z,"**Description**
Now, the wal design in CeresDB is based on 'one table to one region'(region represents a set of wal written to the same storage unit). But its performance is terrible while replaying in rocksdb implementation while wal file became huge, it may be caused by calling `seek` many times. 
And when implementing the wal module based on other component (such as Kafka), I found the mapping 'one table to one region' makes no sense.
So I propose to support two wal designs in the title in CeresDB. 

**Proposal**
Many works are needed for reaching our target, I split works as following:

- [ ] Implement the shard based wal manager demo 
- [ ] Split the wal interface to `WalManager` and `WalInstance` and modify the log format
- [ ] Make rocksdb and obkv implementation supporting shard based mode

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**
#### Implement the shard based wal manager demo
+ Shard based wal manager implement need the `ReadBufferManager`, `Fetcher`, `Splitter` to help to achieve its function. The basic workflow as follow:
```
        ┌─┐                                                                      
        ║""│                                                                      
        └┬┘                                                                      
        ┌┼┐                                                                      
         │          ┌─────────────────┐  ┌────────┐  ┌─────┐  ┌───────┐          
        ┌┴┐         │ReadBufferManager│  │Splitter│  │Table│  │Fetcher│          
  HighLevelCaller   └────────┬────────┘  └───┬────┘  └──┬──┘  └───┬───┘          
  1.Register the shard and its tables        │          │         │              
         │──────────────────>│               │          │         │              
         │                   │               │          │         │              
2.Get a splitter to split the logs on shard  │          │         │              
         │──────────────────>│               │          │         │              
         │                   │               │          │         │              
         │        3.Let splitter run         │          │         │              
         │──────────────────────────────────>│          │         │              
         │                   │               │          │         │              
         │             4.Specific table gets a          │         │              
         │             fetcher to read log through wal manager    │              
         │                   │<─────────────────────────│         │              
         │                   │               │          │         │              
         │                   │            5. fetch logs continuously util the end
         │                   │               │          │────────>│              
         │                   │               │          │         │              
  6.Unregister shard while   │               │          │         │              
  all talbes on it finised to recover        │          │         │              
         │──────────────────>│               │          │         │              
  HighLevelCaller   ┌────────┴────────┐  ┌───┴────┐  ┌──┴──┐  ┌───┴───┐          
        ┌─┐         │ReadBufferManager│  │Splitter│  │Table│  │Fetcher│          
        ║""│         └─────────────────┘  └────────┘  └─────┘  └───────┘          
        └┬┘                                                                      
        ┌┼┐                                                                      
         │                                                                       
        ┌┴┐                                                                      
```
+ The basic protocol between `Splitter` and `Fetcher` is like this:
```
                ┌────────┐                               ┌───────┐                                       
                │Splitter│                               │Fetcher│                                       
                └───┬────┘                               └───┬───┘                                       
                    │               1.Send log               │                                           
                    │ ───────────────────────────────────────>                                           
                    │                                        │                                           
                    │                                        │────┐                                      
                    │                                        │    │ 2.Receive and check the log          
                    │                                        │<───┘                                      
                    │                                        │                                           
                    │                                        │                                           
          ╔══════╤══╪════════════════════════════════════════╪══════════════════════════════════════════╗
          ║ ALT  │  the message is content                   │                                          ║
          ╟──────┘  │                                        │                                          ║
          ║         │                                        │────┐                                     ║
          ║         │                                        │    │ 3.Continue to receive next          ║
          ║         │                                        │<───┘                                     ║
          ╠═════════╪════════════════════════════════════════╪══════════════════════════════════════════╣
          ║ [the message end]                                │                                          ║
          ║         │                                        │────┐                                     ║
          ║         │                                        │    │ 4.Stop and drop the log receiver    ║
          ║         │                                        │<───┘                                     ║
          ║         │                                        │                                          ║
          ║         │            5.Splitter will             │                                          ║
          ║         │            check and know it           │                                          ║
          ║         │ <───────────────────────────────────────                                          ║
          ╚═════════╪════════════════════════════════════════╪══════════════════════════════════════════╝
                    │                                        │                                           
                    │────┐                                                                               
                    │    │ 6.Remove the correspoing log sender                                           
                    │<───┘                                                                               
                    │                                        │                                           
                    │────┐                                                                               
                    │    │ 7.When all senders are remove, stop                                           
                    │<───┘                                                                               
                ┌───┴────┐                               ┌───┴───┐                                       
                │Splitter│                               │Fetcher│                                       
                └────────┘                               └───────┘                                       
```

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/334/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/341,https://api.github.com/repos/apache/horaedb/issues/341,horaedb,1423401079,341,Remove filter plan node in pipeline,ygf11,3428089,ygf11,yanggf23@gmail.com,CLOSED,2022-10-26T04:00:00Z,2023-08-09T11:24:51Z,"### Describe This Problem

For filter pushdown, we only pushdown partial filters to table scan now, that means the filter plan node is still preserved. For example:

`Query`:
```
SELECT  * FROM system.public.tables where table_name='1';
```

`Plan`:
```
| logical_plan  | Projection: #system.public.tables.timestamp, #system.public.tables.catalog, #system.public.tables.schema, #system.public.tables.table_name, #system.public.tables.table_id, #system.public.tables.engine
  Filter: #system.public.tables.table_name = Utf8(""1"")
    TableScan: system.public.tables projection=[timestamp, catalog, schema, table_name, table_id, engine], partial_filters=[#system.public.tables.table_name = Utf8(""1"")] |
| physical_plan | ProjectionExec: expr=[timestamp@0 as timestamp, catalog@1 as catalog, schema@2 as schema, table_name@3 as table_name, table_id@4 as table_id, engine@5 as engine]
  CoalesceBatchesExec: target_batch_size=4096
    FilterExec: table_name@3 = 1
      ScanTable: table=tables, parallelism=8, order=None,                                                                                                                         
```

### Proposal

We have three derives of `Table` trait:
https://github.com/CeresDB/ceresdb/blob/28be2ed754a8dc404499b01797d0e45a412b3f65/analytic_engine/src/table/mod.rs#L84

https://github.com/CeresDB/ceresdb/blob/28be2ed754a8dc404499b01797d0e45a412b3f65/system_catalog/src/lib.rs#L91

https://github.com/CeresDB/ceresdb/blob/28be2ed754a8dc404499b01797d0e45a412b3f65/table_engine/src/memory.rs#L78

In #326, we will support filters pushdown fully in `TableImpl`. If we want to remove filter plan node in pipeline, we need add same filter logic to the other two derives.  


### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/341/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/341,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NAi4q,horaedb,1291988522,341,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-10-26T12:52:48Z,2022-10-26T12:52:48Z,"I remember that the predicate used by the filter plan node can be controlled by the implementation of `TableProvider`:
https://github.com/CeresDB/ceresdb/blob/a8fb2c92edb979051fbfbd339d72f65f94628f2c/table_engine/src/provider.rs#L143

That is to say, for different `Table` implementations, we can use different `TableProviderFilterPushDown`, and there is no need to add the same filter logic for the other two `Table` implementations: `SystemTableAdapter` & `MemoryTable`.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NAi4q/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/341,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NDu64,horaedb,1292824248,341,NA,ygf11,3428089,ygf11,yanggf23@gmail.com,NA,2022-10-27T00:51:43Z,2022-10-27T00:51:43Z,"> That is to say, for different Table implementations, we can use different TableProviderFilterPushDown, and there is no need to add the same filter logic for the other two Table implementations: SystemTableAdapter & MemoryTable.

Yes, it is better to use  different TableProviderFilterPushDown.

As you mentioned in #326, we can move the control of `TableProviderFilterPushDown` to the implementations of tables. like:

```rust
pub trait Table: std::fmt::Debug {
    fn supports_filter_pushdown(&self, _filter: &Expr) -> Result<TableProviderFilterPushDown>;
}

impl TableProvider for TableProviderAdapter {
    fn supports_filter_pushdown(&self, _filter: &Expr) -> Result<TableProviderFilterPushDown> {
        self.table.support_filter_pushdown()
    }
}
```

 ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NDu64/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/341,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ND_gg,horaedb,1292892192,341,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-10-27T02:40:07Z,2022-10-27T02:40:07Z,"> > That is to say, for different Table implementations, we can use different TableProviderFilterPushDown, and there is no need to add the same filter logic for the other two Table implementations: SystemTableAdapter & MemoryTable.
> 
> Yes, it is better to use different TableProviderFilterPushDown.
> 
> As you mentioned in #326, we can move the control of `TableProviderFilterPushDown` to the implementations of tables. like:
> 
> ```rust
> pub trait Table: std::fmt::Debug {
>     fn supports_filter_pushdown(&self, _filter: &Expr) -> Result<TableProviderFilterPushDown>;
> }
> 
> impl TableProvider for TableProviderAdapter {
>     fn supports_filter_pushdown(&self, _filter: &Expr) -> Result<TableProviderFilterPushDown> {
>         self.table.support_filter_pushdown()
>     }
> }
> ```

👍 That sounds great! However, I guess it will be better to avoid using  `TableProviderFilterPushDown ` as the returned type of `supports_filter_pushdown ` of our `Table` trait because `Table` trait has no any dependency on `datafusion`.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ND_gg/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/341,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5cNwVV,horaedb,1547109717,341,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-05-15T02:34:49Z,2023-05-15T02:34:49Z,"From the context, we can just change `supports_filter_pushdown` in `TableProvider` to return `TableProviderFilterPushDown::Exact` to fix this issue?
- https://github.com/CeresDB/ceresdb/blob/dfdf4b615948572d6932f897b5bb94b4450cff1f/table_engine/src/provider.rs#L238
- https://github.com/CeresDB/ceresdb/blob/dfdf4b615948572d6932f897b5bb94b4450cff1f/table_engine/src/provider.rs#L265
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5cNwVV/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/341,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5cP8cX,horaedb,1547683607,341,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2023-05-15T11:30:58Z,2023-05-15T11:30:58Z,assigne me!,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5cP8cX/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/342,https://api.github.com/repos/apache/horaedb/issues/342,horaedb,1423640614,342,Support integration tests with CeresMeta,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-10-26T08:33:00Z,2023-03-31T06:36:17Z,"### Describe This Problem

Currently, basic cluster mode of CeresDB has been supported after V0.4 is released. However, the test for that is all by manual, and we are looking forward to an automatic integration tests for the clustering features.

### Proposal

Just like the TSBS, let's choose a specific version of CeresMeta to build, and run multiple ceresdb-server with cluster configs under the control of CeresMeta. After all the service is ready, run a new harness test suites for the cluster.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/342/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/342,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Y5Mhn,horaedb,1491388519,342,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-31T06:36:17Z,2023-03-31T06:36:17Z,Supported already.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Y5Mhn/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/346,https://api.github.com/repos/apache/horaedb/issues/346,horaedb,1425133128,346,Replace protobuf with prost used by protos crate so to remove grpcio dependencies,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-10-27T06:59:53Z,2022-10-30T04:48:54Z,"### Describe This Problem

Current the `protos` crate uses the `protobuf` dependency to generate codes, but it introduces a huge dependency `grpcio`, which makes the binary size larger and compiling time longer.

### Proposal

Let's replace `protobuf` crate with the `prost` crate.

### Additional Context

This may be helpful for #294","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/346/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/347,https://api.github.com/repos/apache/horaedb/issues/347,horaedb,1425150506,347,Support bulkload data in some popular format,ShiKaiWi,8605990,WEI Xikai,,OPEN,2022-10-27T07:15:47Z,2023-06-01T08:14:03Z,"### Describe This Problem

Currently, it is difficult to import data into ceresdb if the raw data is available in some popular format (CSV, parquet). And the only way is to parse the original data and write it to ceresdb by api.

### Proposal

A bulkload api should be provided to do such things. User just tells ceresdb the data file path and its format, and ceresdb will do the bulkload work by itself.

### Additional Context

Here is the similar feature provided by DuckDB: https://duckdb.org/docs/data/overview","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/347/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/347,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NFzW7,horaedb,1293366715,347,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-10-27T11:11:19Z,2022-10-27T11:11:19Z,Previously discussed at https://github.com/CeresDB/ceresdb/issues/153#issuecomment-1207363994,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NFzW7/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/347,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5drFQr,horaedb,1571574827,347,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-06-01T08:14:03Z,2023-06-01T08:14:03Z,#925 is part of this issue.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5drFQr/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/350,https://api.github.com/repos/apache/horaedb/issues/350,horaedb,1425717099,350,Maybe we should call `mark_delete_entries_up_to` again while table finished to recover,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-10-27T14:17:04Z,2024-10-19T11:31:20Z,"### Describe This Problem

The `flushed` point in table version is updated before the `deleted` point's updating in wal manager.
```
        // Edit table version to remove dumped memtables.
        let mems_to_remove = mems_to_flush.ids();
        let edit = VersionEdit {
            flushed_sequence,
            mems_to_remove,
            files_to_add: files_to_level0,
            files_to_delete: vec![],
        };
        table_data.current_version().apply_edit(edit);

        // Mark sequence <= flushed_sequence to be deleted.
        self.space_store
            .wal_manager
            .mark_delete_entries_up_to(table_data.location(), flushed_sequence)
            .await
            .context(PurgeWal {
                table_location: table_data.location(),
                sequence: flushed_sequence,
            })?;

        Ok(())
```
If the database crashes behind updating in table version but before updating in wal manager. When the database restarts, the `deleted` point in wal manager will fall behind the `flushed` point in table version util new flushing is triggered.
If the table's writing frequency is too low, the duration of falling behind will be too long, and it make a bad difference to wal's delayed deleting.

### Proposal

Call `mark_delete_entries_up_to` again after recovering.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/350/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/354,https://api.github.com/repos/apache/horaedb/issues/354,horaedb,1429348149,354,Disallow defining timestamp key column  as Tag column,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-10-31T06:51:44Z,2022-11-08T05:37:28Z,"### Describe this problem

Now we can define the timestamp key column as tag column, but that is totally meaningless and leads to bad performance.

### Steps to reproduce

Execute the sql to create a table:
```sql
CREATE TABLE IF NOT EXISTS demo (
    name string TAG,
    value double,
    t timestamp NOT NULL TAG,
    TIMESTAMP KEY(t)) ENGINE=Analytic with (enable_ttl=false);
```

### Expected behavior

It should fail rather than succeed.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/354/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/354,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NzPvK,horaedb,1305279434,354,NA,QuintinTao,72123724,,,NA,2022-11-07T08:52:30Z,2022-11-07T08:52:30Z,"hi, I think I can do it.
I will add an user-define error like this.
```rust
  #[snafu(display(""Timestamp column can not be Tag, name:{}"", name))]
    TimestampColumnCanNotTag { name: String },
```
and add check in `create_table_to_plan` like this:
```rust
 if name_column_map.get(&timestamp_name as &str).is_some() {
            ensure!(
                    !name_column_map.get(&timestamp_name as &str).unwrap().is_tag,
                    TimestampColumnCanNotTag {
                        name: &timestamp_name as &str,
                    }
            )
        }
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NzPvK/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/354,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NzRZ8,horaedb,1305286268,354,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-11-07T08:58:18Z,2022-11-07T08:58:18Z,@QuintinTao Your proposal is great. It will be appreciated if you can make such contribution. ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NzRZ8/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/363,https://api.github.com/repos/apache/horaedb/issues/363,horaedb,1434027471,363,Tracking issue for query performance,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-11-03T02:49:52Z,2022-11-15T03:18:04Z,"### Describe This Problem

For now, the main bottleneck within a query is sst scan, there are several improvement can be made to accelerate scan process:
- [x] Cache, as described in #312, the row-group level cache is broken, the async reader fix this but has performance regression, we need to investigate this
- [x] min/max filter is broken in main, #360 fix this
- [x] min/max filter may not enough for columns with high cardinality, in this case bloom filter may be a better choice

### Proposal

As above.

### Additional Context

https://github.com/apache/parquet-format/blob/master/BloomFilter.md","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/363/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/364,https://api.github.com/repos/apache/horaedb/issues/364,horaedb,1434217549,364,Implement system table in cluster mode,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-11-03T07:22:15Z,2023-02-10T09:54:42Z,"### Describe This Problem

In ceresdb cluster mode, an error is reported when the query system table is executed.
```
curl --location --request POST 'http://127.0.0.1:5000/sql' \
> --header 'Content-Type: application/json' \
> -d'{
>     ""query"": ""select * from system.public.tables where table_name='xx'""
> }'
{""code"":500,""message"":""Failed to handle request, err:Failed to create plan, query:select * from system.public.tables where table_name=xx, err:Failed to create plan, err:Failed to generate datafusion plan, err:Execution error: Table is not found, \""catalog:system, schema:public, table:tables\""""}
```

### Proposal

Support systeam table in ceresdb cluster mode.
Refer to https://github.com/CeresDB/ceresdb/blob/b566c57d9c553f9885d7675859f1b4f3e89716fa/catalog_impls/src/lib.rs#L27-L35

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/364/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/365,https://api.github.com/repos/apache/horaedb/issues/365,horaedb,1434222009,365,feat: impl `get_range` func with aliyun oss,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-11-03T07:27:22Z,2022-11-16T09:08:23Z,"### Describe This Problem

I found `get_range` func is not implemented in aliyun oss.
https://github.com/CeresDB/ceresdb/blob/b566c57d9c553f9885d7675859f1b4f3e89716fa/components/object_store/src/aliyun.rs#L115-L120

### Proposal

Refer to [doc](https://help.aliyun.com/document_detail/84825.html) of aliyun oss.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/365/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/365,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NmJAZ,horaedb,1301843993,365,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-11-03T09:36:30Z,2022-11-03T09:36:30Z,"This feature is required if we want to avoid loading all sst data into memory.
It seems oss rust sdk don't support this yet.
- https://github.com/NoXF/oss-rust-sdk/blob/master/src/async_object.rs#L14","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NmJAZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/365,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NzkSw,horaedb,1305363632,365,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-11-07T09:59:19Z,2022-11-07T09:59:19Z,"https://github.com/NoXF/oss-rust-sdk/issues/15

oss sdk already support this.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5NzkSw/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/366,https://api.github.com/repos/apache/horaedb/issues/366,horaedb,1434257793,366,Tracking issue about todo in distributed wal,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-11-03T08:06:02Z,2024-10-19T11:14:49Z,"**Description**
There are some enhancements in wal needing to do in the futures, just mark them now. 

**Proposal**
### On message
- [x] Now manifest based on wal need to scan twice from storage while doing snapshot. Add a cache for this may be better.
- [ ] In wal's message queue implementation, we now sync the snapshot to remote in every flushing. Indeed, we should just need to do the sync work while flushing after dropping table.
- [x] Now, the desigin of wal on message queue is based on the thread model of `one writer for one table`, maybe we should remove this assumption.
- [x] Define some iterators with more explicit semantics.
- [ ] Now we init region(heavy work with network IO) even if it is just created, we should change  `MessageQueue` to know whether it is a new region.
- [x] We should disable the ttl of topic in kafka, but it is not supported in `rskafka` now. It is important so I plan to use another crate to finish it temporarily.
### On OBKV
- [ ] Rename the table name prefix.
- [x] Change the meta table's sharding key to `table id`.
### Manifest(on wal)
- [x] Use the table id as region id.

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/366/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/367,https://api.github.com/repos/apache/horaedb/issues/367,horaedb,1434504323,367,"Implement region's delete, read、write method",Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-11-03T11:27:48Z,2022-11-09T16:39:34Z,"**Description**
After modifying the existing wal desigin, we can implement our wal on kafka now. 

**Proposal**
The implementation will be split into following three phase:
+ read, write, mark deleted.
+ sync region meta snapshot.
+ clean old logs.

##### Read, write, mark deleted
For the region(highest level lock):
+ read and write can be concurrent(read lock).
+ mark deleted is exclusive to them(write lock, using write lock to lock the whole process is not so reasonable, should optimize later).
+ after marking deleted, a region meta snapshot sync will be triggered(it is why we use write lock in marking deleted).

##### Sync region meta snapshot
+ temporarily, we just sync it after mark delete and clean outdated logs.

**Additional context**
Part of #284 



","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/367/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/369,https://api.github.com/repos/apache/horaedb/issues/369,horaedb,1435487182,369,Panicked When create table ,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2022-11-04T03:19:09Z,2023-05-18T08:35:27Z,"### Describe this problem

It seems CeresDB will panic when create a same table
```
2022-11-04 10:28:54.454 ERRO [common_util/src/panic.rs:42] thread 'ceres-write' panicked 'assertion failed: success' at ""analytic_engine/src/space.rs:147""
   0: common_util::panic::set_panic_hook::{{closure}}
             at /home/chunshao.rcs/github/CeresDB/ceresdb/common_util/src/panic.rs:41:18
   1: std::panicking::rust_panic_with_hook
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:702:17
   2: std::panicking::begin_panic_handler::{{closure}}
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:586:13
   3: std::sys_common::backtrace::__rust_end_short_backtrace
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/sys_common/backtrace.rs:138:18
   4: rust_begin_unwind
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:584:5
   5: core::panicking::panic_fmt
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/panicking.rs:142:14
   6: core::panicking::panic
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/panicking.rs:48:5
   7: analytic_engine::space::Space::insert_table
             at /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/space.rs:147:9
   8: analytic_engine::instance::create::<impl analytic_engine::instance::Instance>::process_create_table_command::{{closure}}
             at /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/instance/create.rs:115:9
      <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/future/mod.rs:91:19
      analytic_engine::instance::write_worker::WriteWorker::handle_create_table::{{closure}}
             at /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/instance/write_worker.rs:865:78
      <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/future/mod.rs:91:19
      analytic_engine::instance::write_worker::WriteWorker::run::{{closure}}
             at /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/instance/write_worker.rs:753:50
   9: <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/future/mod.rs:91:19
      analytic_engine::instance::write_worker::WriteGroup::new::{{closure}}
             at /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/instance/write_worker.rs:616:29
      <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/future/mod.rs:91:19
  10: tokio::runtime::task::core::CoreStage<T>::poll::{{closure}}
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/core.rs:165:17
      tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/loom/std/unsafe_cell.rs:14:9
      tokio::runtime::task::core::CoreStage<T>::poll
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/core.rs:155:13
      tokio::runtime::task::harness::poll_future::{{closure}}
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/harness.rs:480:19
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/panic/unwind_safe.rs:271:9
      std::panicking::try::do_call
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:492:40
      std::panicking::try
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:456:19
  11: std::panic::catch_unwind
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panic.rs:137:14
      tokio::runtime::task::harness::poll_future
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/harness.rs:468:18
      tokio::runtime::task::harness::Harness<T,S>::poll_inner
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/harness.rs:104:27
      tokio::runtime::task::harness::Harness<T,S>::poll
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/harness.rs:57:15
  12: tokio::runtime::task::raw::RawTask::poll
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/raw.rs:84:18
      tokio::runtime::task::LocalNotified<S>::run
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/mod.rs:381:9
```

### Steps to reproduce

I cant reproduce this bug with http request.

### Expected behavior

return a error, not panic

### Additional Information

None","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/369/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/369,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5cjLnd,horaedb,1552726493,369,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2023-05-18T08:35:27Z,2023-05-18T08:35:27Z,Refer to #743 .,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5cjLnd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/371,https://api.github.com/repos/apache/horaedb/issues/371,horaedb,1437848498,371,Implement region namespace,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-11-07T06:46:46Z,2022-11-24T09:00:29Z,"**Description**
Implement the region manager for region module which has been implemented(see #367).

**Proposal**
It include two phase:
+ Implement the region's recovery by its meta snapshot.
+ Implement the `WalManager` for `RegionManager`. 

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**
##### When should we create the new region entry?
The new region is possible to be created in following operation:
+ Write.
+ Read(scan region or table).
+ Mark deleted.

It doesn't matter to do this, we just need to consider the behavior while write/read/mark deleted on a empty region and it is not a complex work.

##### When should we create the new table entry in region?
For table entry, it will just be created and inserted in `write` operation. And while not found it in other operation, error will occur.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/371/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/373,https://api.github.com/repos/apache/horaedb/issues/373,horaedb,1438073382,373,Some enhancement works for message queue's kafka implemetation,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-11-07T09:54:41Z,2022-11-08T05:48:06Z,"**Description**
There are some enhancement works we should do now for  wal on message queue, it is helpful for later development.

**Proposal**
It include two phase:
+ Add some useful traits and tests to message queue component.
+ Fix some typos.

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/373/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/378,https://api.github.com/repos/apache/horaedb/issues/378,horaedb,1439531409,378,Return table id in `LogEntry`(wal's reading result),Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-11-08T05:48:19Z,2022-11-08T06:32:59Z,"**Description**
For supporting shard based reading, table id is needed to be returned, and place it in `LogEntry` may be natural.

**Proposal**
+ Add a `table_id` field in `LogEntry`.
+ Return it together while returning `LogEntry`.

<!---
Maybe you have considered some ideas or solutions about this feature.
-->

**Additional context**

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/378/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/384,https://api.github.com/repos/apache/horaedb/issues/384,horaedb,1442415019,384,Implement region's recovery,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-11-09T16:43:07Z,2022-11-11T07:59:41Z,"**Description**
After implementing region's `read`, `write`, `mark  delete` in  #367, we need to implement region's recovery now.

**Proposal**
+ implement region meta's recovery.
+ implement region's recovery based on above step.

**Additional context**
Part of #284 



","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/384/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/386,https://api.github.com/repos/apache/horaedb/issues/386,horaedb,1443144406,386,Unit test in wal module can be more extensible,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-11-10T02:46:30Z,2022-11-10T06:13:39Z,"### Describe This Problem

Now, the tests code in wal module is so hard code ... It doesn't take full advantage of the `WalBuilder` trait.
So it can be more extensible after refactoring. 

### Proposal

Pass the builder to build tests for different implementation of wal.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/386/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/389,https://api.github.com/repos/apache/horaedb/issues/389,horaedb,1444826211,389,let exist wal implementations support scanning logs of whole shard,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-11-11T02:22:01Z,2022-11-18T07:50:37Z,"### Describe This Problem

In exist wal implementations, we can just read logs table by table, it lead to duplicated io(rocksdb based), and a huge number of requests(obkv based).
So, I want to introduce the mapping of `shard to region` to them, and make them support shard based reading. 

### Proposal

+ Logs in exist wal implementations are still managed on table level, the local manager doesn't perceive the concept about `shard`(now mapped to `region`).
+ The information about shard will be passed with every necessary function call. 

Cite a case to express the above design. The `shard` information(always `shard id`), is used a bit like `user name`, `tenant`, it is just a pure logic concept to exist wal(in kafka wal, it is a concept about physical unit).

### Additional Context

##### Obkv logs cleaning
Now, we can't know what obkv tables logs of tables are stored in:
  + now, `table id` decide logs of ceresdb table will be written to which obkv table. But for supporting `shard based read/write`, it's necessary to use  `shard id` to make this decision.
  + howerver, table may be moved between `shard`s in cluster mode(`shard id` will changed), and there aren't ways to track such moving now.

Above problem make trouble to log cleaning. And we can just relax this problem in following ways:
  + traverse all obkv tables to find logs which should be deleted.
  + just run the logs cleaning routine in current shard.

All of them are not good ways... for simplicity, we implement the second way now.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/389/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/391,https://api.github.com/repos/apache/horaedb/issues/391,horaedb,1447974616,391,Enhance cache for OSS storage,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-11-14T12:28:56Z,2022-12-02T10:12:16Z,"### Describe This Problem

Currently, we have `DataCache` & `MetaCache` for underlying storage to speed up the query process, and specifically, for OSS storage, a disk cache with size limit is designed for a larger capacity. However, there are still some issues concerning the current cache design, leading to poor performance of query:
- It costs too much CPU and takes long time to decode the custom metadata (including bloom filters, time range of sst) encoded as extended key value pairs in the parquet footer.
- The memory cache has only a limit on the entry number, and no limit on the size, that is to say, such a cache may need much tuning to make it perform best for the user's data pattern and the memory capacity of the machine on which ceresdb is deployed.

### Proposal

To solve the two issues about the current cache design, I propose:
- Add cache for both custom metadata and parquet metadata, to avoid extra decoding procedure for our custom metadata. #392
- Refactor memory data cache to make it featured with size limit, and integrated into the `object_store` module in order to combine it with disk cache to serve as a two-tier cache.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/391/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/391,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Owrji,horaedb,1321384162,391,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-11-21T02:51:26Z,2022-11-21T02:51:26Z,"- https://github.com/facebook/rocksdb/wiki/Block-Cache

There  are two kinds of cache used in RocksDB, LRU and Clock.

LRU is simple to start with, so I plan to implement one based on https://github.com/jiacai2050/lru-weighted-cache.

Key is `${sst-path}-${range}`, value is bytes, also this cache will support shard to reduce lock contention






","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Owrji/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/391,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OxOwB,horaedb,1321528321,391,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-11-21T06:38:39Z,2022-11-21T06:38:39Z,"There will be two kinds of caches: one is memory, the other is disk. 

Both are limited by memory capacity configured when started, and use LRU to evict inactive ones.

# Memory

    range = [start_offset, end_offset)
    key = sst_id + range
    value = bytes


# Disk

    key = sst_id + align_range(named blocks)
    value = bytes

`align_range` is used for 
1. reduce file number in case of range is too small
2. simplify LRU implementation, we can just use a normal LRU based on entry number.

> Note: a get request's range will be split into multiple blocks.

For example, when `align_range` is 4KB, then a get request `[4000, 5000)` will cause
the both `[0, 4096)` and `[4096, 8192)` to be cached.

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OxOwB/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/391,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OxYkr,horaedb,1321568555,391,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2022-11-21T07:28:37Z,2022-11-21T07:28:37Z,Maybe memory cache should be aligned?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OxYkr/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/391,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Oxa8o,horaedb,1321578280,391,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-11-21T07:39:44Z,2022-11-21T07:39:44Z,"https://github.com/facebook/rocksdb/blob/a8a4ed52a4f4ab57d214ed4984306ea89da32f3f/cache/lru_cache.cc#L380

RocksDB's cache is not aligned, I think advantage of aligned range for memory is not obvious, if it's aligned, then the align unit will much smaller than disk's, which may result in multiple requests for request with large range.

So I guess we can firstly implement memory cache unaligned, then to check how it works for real workload.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Oxa8o/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/394,https://api.github.com/repos/apache/horaedb/issues/394,horaedb,1449097695,394,Empty sst file caused by system crashes,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-11-15T03:25:14Z,2024-10-19T11:32:48Z,"### Describe this problem

The phenomenon is that an empty sst file (stored on the local disk) was recorded in the manifest when the system crashed (actually ceresdb was running in a container and the container crashed).



### Steps to reproduce

It is not easy to reproduce.

### Expected behavior

The empty sst file should not be recorded in the manifest.

### Additional Information

I inspected that the root cause is the missing `fsync` after finishing storing sst to disk and before recording it into manifest.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/394/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/396,https://api.github.com/repos/apache/horaedb/issues/396,horaedb,1449231196,396,Support multipart upload sst file for aliyun oss,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-11-15T06:16:37Z,2023-03-10T06:55:18Z,"### Describe This Problem

According to [Multipart Upload](https://help.aliyun.com/document_detail/31991.htm), it will fail if we try to upload a large file whose size exceeds 100MB directly.

### Proposal

We should support multipart upload file to aliyun oss.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/396/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/397,https://api.github.com/repos/apache/horaedb/issues/397,horaedb,1449301694,397,Migrate current harness tests to sqlness,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-11-15T07:24:47Z,2022-12-16T08:08:21Z,"### Describe This Problem

[sqlness](https://github.com/CeresDB/sqlness) has been created as a separated repo, but ceresdb doesn't use it now.

### Proposal

Let's migrate the harness tests to the sqlness.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/397/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/397,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OZp2T,horaedb,1315347859,397,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-11-15T13:59:22Z,2022-11-15T13:59:22Z,"There is no docs for sqlness, so this may not very friendly for volunteers...

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OZp2T/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/397,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OptFj,horaedb,1319555427,397,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-11-18T05:10:08Z,2022-11-18T05:10:08Z,maybe I can do it. Can you assign it to me?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OptFj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/397,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OpzRi,horaedb,1319580770,397,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-11-18T05:52:41Z,2022-11-18T05:52:41Z,"Yeah, go ahead.

There is a [pending PR](https://github.com/CeresDB/ceresdb/pull/388) to update client, we will merge this soon, you can first figure out how to use sqlness framework since there are no docs for it.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OpzRi/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/397,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Opz7H,horaedb,1319583431,397,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-11-18T05:58:00Z,2022-11-18T05:58:00Z,"> 

ok, I will create a new project to using `sqlness`.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Opz7H/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/397,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OqONV,horaedb,1319691093,397,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-11-18T08:28:08Z,2022-11-18T08:28:08Z,"> > 
> 
> ok, I will create a new project to using `sqlness`.

Actually, `sqlness` is almost same as the codes in the https://github.com/CeresDB/ceresdb/tree/main/tests/harness, maybe there is no need to create a new project?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OqONV/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/397,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OqRhy,horaedb,1319704690,397,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-11-18T08:43:09Z,2022-11-18T08:43:09Z,"> Actually, `sqlness` is almost same as the codes in the https://github.com/CeresDB/ceresdb/tree/main/tests/harness, maybe there is no need to create a new project?

yes, I mean i will try `sqlness` in another new project before this pr merge","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5OqRhy/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/397,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Oqbf1,horaedb,1319745525,397,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-11-18T09:23:49Z,2022-11-18T09:23:49Z,"@dust1  I have bump client to latest version in this PR https://github.com/CeresDB/ceresdb/pull/409, so there is nothing blocking this.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Oqbf1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/397,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Qtgfw,horaedb,1354106864,397,NA,waynexia,15380403,Ruihang Xia,,NA,2022-12-16T02:46:19Z,2022-12-16T02:46:19Z,"> There is no docs for sqlness, so this may not very friendly for volunteers...

Currently, it only contains docs within the doc comment (`///`). Since this crate is not published yet, you need to run `cargo doc --open` to open it 🥲 Not very convenient but it should be better after our first publish.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Qtgfw/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/397,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QuQSj,horaedb,1354302627,397,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-12-16T07:01:11Z,2022-12-16T07:01:11Z,"Got it, I will try to publish to crates.io when #473 is finished(ensure everything works as expected, and CeresDB can be used for demonstration).","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QuQSj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/397,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QuTgf,horaedb,1354315807,397,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-12-16T07:15:26Z,2022-12-16T07:15:26Z,Do you need to add a new environment if you want to demonstrate? Currently only `local`,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QuTgf/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/397,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QucXS,horaedb,1354352082,397,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-12-16T08:01:05Z,2022-12-16T08:01:05Z,"Multiple envs demo is required before publish, we can add it in sqlness repo. 

We can create a demo project here, such as 
```
$tree examples/
examples/
├── distributed
│   └── select.sql
└── local
    └── select.sql

```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QucXS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/402,https://api.github.com/repos/apache/horaedb/issues/402,horaedb,1452846337,402,Potential API/storage format breaking change,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-11-17T07:22:03Z,2023-02-28T03:26:30Z,"### Describe This Problem

When release 1.0, breaking change is not acceptable, so we need to 
1. Remove bad design 
2. Add compatible method to old format 


This issue is mainly used to collect potential breaking changes


### Proposal

As above

### Additional Context

Some examples I can think of
- #321, we need to record sst file size in manifest;
- #340;
- #495 Define the custom format for SST;","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/402/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/402,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QA08g,horaedb,1342394144,402,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-12-08T09:49:08Z,2022-12-08T09:49:08Z,"What I can come up with now includes:
- Define the custom format for SST (now it is totally a parquet file) for the possible brand-new format in the future;
- Refactor the manifest implementation because current one is complicated and may not work well with OBKV;","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QA08g/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/402,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QB4cG,horaedb,1342670598,402,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-12-08T12:40:40Z,2022-12-08T12:40:40Z,"One more thing deserved mentioning here, currently the protocol maps the tenant to the schema, which is actually unreasonable.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QB4cG/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/402,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RscfO,horaedb,1370605518,402,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-01-04T08:18:11Z,2023-01-04T08:18:11Z,- https://github.com/CeresDB/ceresdb/pull/526,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RscfO/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/402,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Rxgut,horaedb,1371933613,402,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-01-05T08:48:13Z,2023-01-05T08:48:13Z,"https://github.com/CeresDB/ceresdb/pull/538 tries to reduce the size of the meta data used in manifest, which may help solve the problem:
> Refactor the manifest implementation because current one is complicated and may not work well with OBKV;","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Rxgut/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/402,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WR1pd,horaedb,1447516765,402,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-02-28T03:26:30Z,2023-02-28T03:26:30Z,All the breaking changes we know are all finished.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WR1pd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/403,https://api.github.com/repos/apache/horaedb/issues/403,horaedb,1452881056,403,write hybrid column data cause index out of bounds,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-11-17T07:54:46Z,2022-12-01T02:46:50Z,"### Describe this problem

In local dev environment, write to hybrid format column may cause errors below:

```
thread 'main' panicked at 'index out of bounds: the len is 193 but the index is 512', analytic_engine/src/sst/parquet/hybrid.rs:355:27                              
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace

```

### Steps to reproduce

This error is related to sst data, couldn't find a reliable way to reproduce this.

### Expected behavior

No panic

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/403/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/408,https://api.github.com/repos/apache/horaedb/issues/408,horaedb,1454684677,408,Support limit the size of sst generated by compaction,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-11-18T08:25:07Z,2023-03-06T02:41:13Z,"### Describe This Problem

Currently, it seems there is no limit on the sst size generated by the compaction procedure, that is to say, some huge sst files may occur. It is ok for the local file system, but it will cause some problems, including higher possibility of failure of downloading or uploading when using the remote object store as the underlying storage layer.

### Proposal

Firstly, we should make the sst size limit configurable. However, this is not enough to handle the case where the size of the compaction output is indeed larger than the limit. So I propose the compaction procedure should support multiple output files. After that, the very large output sst will be split to multiple small ssts.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/408/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/408,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WvtTr,horaedb,1455346923,408,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-06T02:41:13Z,2023-03-06T02:41:13Z,"It seems #483 works well in prod, so closing this.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WvtTr/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/412,https://api.github.com/repos/apache/horaedb/issues/412,horaedb,1455122459,412,Tracking issue for supporting shard level replay,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-11-18T13:27:24Z,2024-03-12T09:00:55Z,"### Describe This Problem

Table level replay is too slow, so we try to support shard level replay to relax this problem.
It is a big work, so I split it into some phases and track its progress here. 

### Proposal

- [x] complete unimplemented `scan` method in all wal implementations.
    https://github.com/CeresDB/ceresdb/pull/400
- [ ] implement wal adapter to keep the reading view of table level while using(and the actual reading work is doing in shard level).
    - [ ] implement the read buffer.
    - [ ] implement the wal adapter.
- [ ] refactor the original recovering process to make use of the wal adapter.
    - [ ] refactor the original recovering process in standalone mode.
    - [ ] add `Replayer` to carry out the  recovering work.
- [ ] bug fixs:
    - [ ] recover by `flushed_sequence` in manifest.
    - [ ] modify `RegionId` to a struct including `ShardId` and its version.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/412/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/412,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM52rYXr,horaedb,1991083499,412,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-03-12T09:00:55Z,2024-03-12T09:00:55Z,Fixed in https://github.com/apache/incubator-horaedb/issues/799,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM52rYXr/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/413,https://api.github.com/repos/apache/horaedb/issues/413,horaedb,1456781732,413,Recovery process in standalone mode may be a bit strange,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-11-20T05:49:56Z,2022-12-23T04:21:39Z,"### Describe This Problem

In standalone mode, we recover table in `Visitor::visit_tables`, as I see, this design is a bit strange. 
I don't think this design will bring any benefits, on the contrary it will make trouble while doing enhancement to recovery, such as recovering table concurrently.
So I propose to refactor it.

### Proposal

+ we just collect the `TableInfo` in `visit_tables`.
+ modify `SchemaImpl::open_table`(table_based) to carry out the actual opening work.
+ we pass the collecting results to above opening method.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/413/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/417,https://api.github.com/repos/apache/horaedb/issues/417,horaedb,1463495698,417,Improve write performance,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-11-24T15:16:43Z,2023-11-03T06:52:25Z,"### Describe This Problem

After a simple test, i found that the current writing performance of a single table on a single machine is 30K rows/s. 
The test machine is 24c, and the cpu usage rate is 1300%.

### Proposal

Optimize the writing process.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/417/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/417,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5PFBBg,horaedb,1326714976,417,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2022-11-24T17:26:58Z,2022-11-24T17:26:58Z,"Maybe I can try to optimize it.

I found some unreasonable designs while reading codes in this part:
## 1. Single table writing
For single table writing, the write way is like:
```
                                                                      
                      |                                              -
                      |                                               
    +-------------+   |                                               
    |  Write wal  |   |                                               
    +-------------+   |                                               
---------------------------------------------                         
    +--------------+  |                                               
    |Write memtable|  |                                               
    +--------------+  |                                               
----------------------|----------------------                   -     
                      | +-------------+                               
                      | |  Write wal  |                               
                      | +-------------+                       -       
---------------------------------------------                         
                      | +--------------+                              
                      | |Write memtable|                              
                      | +--------------+                              
                      |                   
```
In this write way, we assume the `write memtable` operation is a very lightweight work, but it may be not so light as we think. As I consider, following ways cay be tried to ease the problem:
+ Refactor the write way to pipeline model:
```
                      |                                              -
                      |                                               
    +-------------+   |                                               
    |  Write wal  |   |                                               
    +-------------+   |                                               
---------------------------------------------                         
    +--------------+  | +-------------+                               
    |Write memtable|  | |  Write wal  |                               
    +--------------+  | +-------------+                               
----------------------|----------------------                   -     
                      | +--------------+                              
                      | |Write memtable|                              
                      | +--------------+                      -       
---------------------------------------------                         
                      |                                               
                      |                                               
```
+ Parallel the cpu bound task, I found some operations can in `write memtable` can executed parallelly actually.
+ Some strategy like `group commit` in rocksdb can be tried, too.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5PFBBg/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/417,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5PFJkV,horaedb,1326749973,417,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2022-11-24T18:10:06Z,2022-11-24T18:10:06Z,"## 2. Multiple tables writing
We limit the write worker's amount, as I see the target for this design is for `balancing the read and write performance`, because it is a well-known problem about how to balance the performance among `read`, `write` and `compact` in LSM database. 
It is a good design in theory, but our implementation now is too rough, assuming table 1 and table 2 are assigned to a task, the write way will be like:
```
                     |                                      
                     |                                      
     +-------------+ |                                      
     | Table 1 IO  | |                                      
     +-------------+ |                                      
 ------------------------------------------                 
     +-------------+ |                                      
     |   Table 1   | |                                      
     |   compute   | |                                      
     +-------------+ |                                      
-------------------------------------------                 
                     | +-------------+                      
                     | | Table 2 IO  |                      
                     | +-------------+                      
---------------------|-----------------------               
                     | +-------------+                      
                     | |   Table 2   |                      
                     | |   compute   |                      
                     | +-------------+                      
                     |                                      
                                                            
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5PFJkV/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/417,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5PF9Va,horaedb,1326962010,417,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-11-25T02:36:52Z,2022-11-25T02:36:52Z,"> Some strategy like group commit in rocksdb can be tried, too.

This sounds more practical, and we can also support different write option, such as not flush/wait wal write.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5PF9Va/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/417,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5PGCv5,horaedb,1326984185,417,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2022-11-25T03:19:46Z,2022-11-25T03:19:46Z,"> > Some strategy like group commit in rocksdb can be tried, too.
> 
> This sounds more practical, and we can also support different write option, such as not flush/wait wal write.

+ I think, maybe we can split current `write_group` to `write_compute_group` and `write_io_group`, too. As I see, `io` and `computation` may be unreasonable to be managed together. 
+ For parallel computation, maybe we can use some tools like rayon?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5PGCv5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/419,https://api.github.com/repos/apache/horaedb/issues/419,horaedb,1463932558,419,whether to need to adjust the `ArrowSchemaMetaKey: : NumKeyColumns`,dust1,18304424,Yoke,834902408@qq.com,CLOSED,2022-11-25T01:21:36Z,2023-01-11T03:36:53Z,"### Describe This Problem

after the modification of #340 . 
The concept of `NumKeyColumns` is no longer important, so does it need to be changed when building arrow schema meta data here?
https://github.com/CeresDB/ceresdb/blob/c0f498bca3cf1efc36936ff3b591b7f22621ddbd/common_types/src/schema.rs#L1129-L1134

### Proposal

Maybe save the `primary_key_indexes`?

### Additional Context

This may run into serialization problems","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/419/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/419,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5PF8en,horaedb,1326958503,419,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-11-25T02:28:24Z,2022-11-25T02:28:24Z,"Yes, `NumKeyColumns` is no longer useful anymore, and should be removed.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5PF8en/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/419,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5PF_IE,horaedb,1326969348,419,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2022-11-25T02:54:46Z,2022-11-25T02:54:46Z,maybe I can do this after another pr finish,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5PF_IE/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/419,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5PK4Im,horaedb,1328251430,419,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-11-27T13:50:57Z,2022-11-27T13:50:57Z,"Sorry for my delay, assigned!","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5PK4Im/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/419,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5SJaxR,horaedb,1378200657,419,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-01-11T03:36:52Z,2023-01-11T03:36:52Z,I guess this issue has been solved by #526.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5SJaxR/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/425,https://api.github.com/repos/apache/horaedb/issues/425,horaedb,1465362152,425,Add Codecov in our repo,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-11-27T07:57:27Z,2023-01-16T11:07:59Z,"### Describe This Problem

Test coverage statistics are currently missing.

### Proposal

Add [Codecov](https://github.com/apps/codecov) in our repo.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/425/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/427,https://api.github.com/repos/apache/horaedb/issues/427,horaedb,1465704278,427,test_table_compact_current panic sometimes in CI,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-11-28T02:26:59Z,2023-03-08T05:42:07Z,"### Describe this problem

After switch sst reader to async, `test_table_compact_current_segment_rocks` testcase will panic
```
---- tests::compaction_test::test_table_compact_current_segment_rocks stdout ----
thread 'tests::compaction_test::test_table_compact_current_segment_rocks' panicked at 'called `Result::unwrap()` on an `Err` value: ErrWithSource { msg: ""read record batch"", source: ReadFromSubIter { source: PullRecordBatch { source: DecodeRecordBatch { source: ParquetError { source: General(""ObjectStoreReader::get_bytes error: Object at location /tmp/.tmpjcYM19/store/100/2199023255554/18.sst not found: No such file or directory (os error 2)""), backtrace: Backtrace(   0: <snafu::backtrace_shim::Backtrace as snafu::GenerateBacktrace>::generate
```



### Steps to reproduce

```
cargo test --workspace test_table_compact_current_segment_rocks
```

### Expected behavior

No panic

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/427/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/428,https://api.github.com/repos/apache/horaedb/issues/428,horaedb,1465710816,428,Bad name in ReadableSize,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-11-28T02:39:17Z,2023-03-26T04:00:46Z,"### Describe this problem

https://github.com/CeresDB/ceresdb/blob/659b55985a14f835cff5bbb2df4eea1b0a540366/common_util/src/config.rs#L149

In `ReadableSize`, there is a method called `as_bytes()`, which is very confusing, since the original meaning of `as_bytes` is to return `&[u8]`, not `u64`

### Steps to reproduce

No required.

### Expected behavior

_No response_

### Additional Information

I suggest we name it `as_byte` to follow other methods in ReadableSize
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/428/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/429,https://api.github.com/repos/apache/horaedb/issues/429,horaedb,1466006177,429,Conventional commit guide link is broken in Chinese version of README-CN.md,Huachao,2796177,Huachao Mao,huachaomao@gmail.com,CLOSED,2022-11-28T08:24:44Z,2022-11-28T08:54:13Z,"### Describe this problem

```md
## 如何贡献
[如何参与 CeresDB 代码贡献](CONTRIBUTING.md)

[约定式提交](docs/dev/conventional-commit.md)
```
The document link in 「约定式提交」is broken since [612dae6](https://github.com/CeresDB/ceresdb/commit/612dae61222bd8431f44b3998b3b4c394f42dc87).

### Steps to reproduce

None

### Expected behavior

The link works.

### Additional Information

None","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/429/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/435,https://api.github.com/repos/apache/horaedb/issues/435,horaedb,1467628317,435,Support bloom filter in hybrid storage format,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-11-29T08:08:34Z,2023-08-24T13:00:34Z,"### Describe This Problem

Hybrid storage compresses multi-record batches into one record batch, so bloom filter is wrong in hybrid storage.

### Proposal

Support bloom filter in hybrid storage format.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/435/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/435,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5PrXQD,horaedb,1336767491,435,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2022-12-05T05:20:22Z,2022-12-05T05:20:22Z,"The write procedure has been refactored, reopen this issue.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5PrXQD/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/435,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5k1D4-,horaedb,1691631166,435,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-08-24T13:00:33Z,2023-08-24T13:00:33Z,Hybrid format is not supported anymore.  See https://github.com/CeresDB/ceresdb/pull/1172,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5k1D4-/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/436,https://api.github.com/repos/apache/horaedb/issues/436,horaedb,1467915337,436,Support dynamic setting of log level to avoid server restart,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-11-29T11:41:25Z,2022-12-05T07:49:51Z,"### Describe This Problem

Support dynamic setting of log level to avoid server restart.

### Proposal

TODO

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/436/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/441,https://api.github.com/repos/apache/horaedb/issues/441,horaedb,1471014696,441,Serious meta information overwritten bug of distributed wal in cluster mode,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-12-01T09:57:58Z,2022-12-01T10:24:34Z,"### Describe this problem

If a shard (which is mapped to region in wal) is move between nodes like this:
```
A --> B --> ... --> A
```
Then wal module in mode A can't distinguish if the shard has been moved.
This may cause a serious bug: 
+ the shard's wal meta information may be modify in other nodes.
+ when it is moved to A(original node), A doesn't know it and think it is same as before it was moved.
+ A persists the old meta information to storage and overwritten the new one which are persisted in other nodes. 

### Steps to reproduce

+ see `test_move_from_nodes` of `wal/src/tests/read_write.rs` in [pr](https://github.com/CeresDB/ceresdb/pull/422).
+ set `region_version_3` equal to `region_version_1`.


### Expected behavior

Can run normally in cluster mode.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/441/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/442,https://api.github.com/repos/apache/horaedb/issues/442,horaedb,1471415969,442,Identifiers should be case-sensitive for all protocols by default,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-12-01T14:29:02Z,2023-02-15T08:19:51Z,"### Describe This Problem

In current implementation, we delegate SQL execution to DataFusion, it will automatically convert identifiers to lower case if not quoted, this behavior is what other DBMS does.

But for SQL besides SELECT, identifiers are unchanged no matter whether they are quoted, this cause conflict when 
- CREATE/INSERT table with uppercase letter, such as `Test`
- Then SELECT like `select * from Test` will cause error since no table named `test` exists


### Proposal

<del>Convert identifiers without quoted to lower case for all SQL </del>

After some discussion, proposal above doesn't make much sense in practical. The reason is that there are other protocols support(gRPC) in CeresDB, and they are case-sensitive by default, if we want to keep them consistent with SQL layer, then we need to convert ALL identifiers without quote to lower-case, which is hard to maintain, and may introduce bug when new interface is added without normalized identifiers, so I suggest

> All protocols in CeresDB are case-sensitive by default(with or without quote)

In this way, we only need to modify statements passed to DataFusion, all other interface stay the same with before.

- This may not consistent with other DBMS, but I think it will cause less confusion for CeresDB users.
### Additional Context

- https://github.com/apache/arrow-datafusion/issues/1746
- #220 
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/442/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/448,https://api.github.com/repos/apache/horaedb/issues/448,horaedb,1475901864,448,Centralize the table name converting logics in one place,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-12-05T07:37:17Z,2022-12-07T11:57:37Z,"### Describe This Problem

Now, we will convert all table name to lowercase, but the logics are placed in files.
Due to this, while changing related codes, there is a high probability of omission.

### Proposal

+ Use `TableName` to represent all metric names.
+ Centralize the logic in `TableName`.

### Additional Context

When we changed it, we need to change logics in such files:
+ server/src/grpc/storage_service/route.rs
+ server/src/grpc/storage_service/write.rs
+ sql/src/ast.rs","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/448/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/452,https://api.github.com/repos/apache/horaedb/issues/452,horaedb,1478517271,452,Should we add `TimeWindowSized` compaction strategy?,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-12-06T08:15:51Z,2023-05-31T03:45:40Z,"### Describe This Problem

Currently, the compaction strategy adopts the TimeWindow strategy by default.
SST size can be up to GB. Performance may be better on local disk,
But in oss, will a certain size of sst achieve better performance, such as 200M?

### Proposal

Should we add `TimeWindowSized` compaction strategy?

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/452/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/452,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5PztlQ,horaedb,1338956112,452,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-12-06T08:28:39Z,2022-12-06T08:28:39Z,"Firstly, I guess `TimeWindow` strategy with a new configuration for sst size threshold is enough. As for performance, I'm not sure about it and I guess we need real cases to tell us about it.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5PztlQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/452,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5di-7o,horaedb,1569451752,452,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-05-31T03:45:40Z,2023-05-31T03:45:40Z,This issue has been resolved in main.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5di-7o/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/467,https://api.github.com/repos/apache/horaedb/issues/467,horaedb,1484624253,467,Possible enhancements in first query,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2022-12-08T13:17:37Z,2023-03-31T08:57:49Z,"### Describe This Problem

The speed of first query in ceresdb with oss is acceptable now, but more enhancements can still be done.

### Proposal

- [x] Make mergeIterator initializing concurrently
  #466 
- [x] Make pulling sst from sst concurrently
- [x] Get data after pulling from remote storage immediately rather than waiting until it is written to disk cache 

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/467/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/467,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QGKz_,horaedb,1343794431,467,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-12-09T03:18:54Z,2022-12-09T03:18:54Z,"There are several TODO in disk_cache.rs now, fix them would address your second proposal.
- https://github.com/CeresDB/ceresdb/blob/505b9aca71ccc1ae0f120ef655509b85bfa89fce/components/object_store/src/disk_cache.rs#L132
- https://github.com/CeresDB/ceresdb/blob/505b9aca71ccc1ae0f120ef655509b85bfa89fce/components/object_store/src/disk_cache.rs#L175","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QGKz_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/467,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Wvrz_,horaedb,1455340799,467,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-06T02:32:19Z,2023-03-06T02:32:19Z,Closing since no obvious work need to be done now.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Wvrz_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/468,https://api.github.com/repos/apache/horaedb/issues/468,horaedb,1486218281,468,More metrics for observability of important modules,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-12-09T07:28:13Z,2023-11-03T06:36:24Z,"### Describe This Problem

Currently, the existing metrics are not enough for troubleshooting performance issues. We need more metrics for better observability on important modules.

### Proposal

Currently, these modules need metrics:
- [x] Object store: query range distribution, read/write latency, read/write throughput, counters for rpc call. by #478 
- [x] Write: time cost for critical path
- [ ] Query: time cost for critical path

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/468/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/469,https://api.github.com/repos/apache/horaedb/issues/469,horaedb,1486246080,469,Control over the memory consumption of query,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-12-09T07:45:59Z,2023-11-03T06:43:48Z,"### Describe This Problem

Currently, there is no limit on the memory consumption of one query, leading to frequent OOMs on the prod environment.


### Proposal

No specific proposal yet.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/469/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/469,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qzwpd,horaedb,1791953501,469,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-11-03T06:43:47Z,2023-11-03T06:43:47Z,Some mechanism has been adopted.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qzwpd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/470,https://api.github.com/repos/apache/horaedb/issues/470,horaedb,1486264638,470,Restriction on bad SQL for server stability,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-12-09T07:57:50Z,2022-12-21T02:56:20Z,"### Describe This Problem

Currently, one SQL with no predicates querying on a very big table may cause large consumption of CPU and memory, leading to instability of server.

### Proposal

We may need a module to do such things:
- Based provided rules, prevent the execution of any SQL which may lead to instability of server;
- Such rules are configurable, e.g. a rule for resolving the problem above may be called `disable_query_without_predicate`;

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/470/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/471,https://api.github.com/repos/apache/horaedb/issues/471,horaedb,1486435482,471,heap profiling is broken,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-12-09T09:43:34Z,2022-12-15T12:03:10Z,"### Describe this problem

When do heap profiling, there will be following errors:

```
{""code"":500,""message"":""Fail to do heap profiling, err:Profile Error: Jemalloc(An interface with side effects failed in some way not directly related to `mallctl*()` read/write processing.).""}

```

### Steps to reproduce

```
curl 0:5440/debug/heap_profile/10
```

### Expected behavior

No error

### Additional Information

- https://github.com/tikv/tikv/issues/9538
- Following patch is required to fix `file permission error`
```diff
diff --git a/components/profile/src/lib.rs b/components/profile/src/lib.rs
index 45c997d..4942567 100644
--- a/components/profile/src/lib.rs
+++ b/components/profile/src/lib.rs
@@ -4,7 +4,7 @@
 
 use std::{
     fmt::Formatter,
-    fs::File,
+    fs::{File, OpenOptions},
     io,
     io::Read,
     sync::{Mutex, MutexGuard},
@@ -105,14 +105,15 @@ impl Profiler {
 
         // clearing the profile output file before dumping profile results.
         {
-            let f = File::open(PROFILE_OUTPUT_FILE_PATH).map_err(|e| {
-                error!(""Failed to open prof data file, err:{}"", e);
-                Error::IO(e)
-            })?;
-            f.set_len(0).map_err(|e| {
-                error!(""Failed to truncate profile output file, err:{}"", e);
-                Error::IO(e)
-            })?;
+            let f = OpenOptions::new()
+                .create(true)
+                .write(true)
+                .truncate(true)
+                .open(PROFILE_OUTPUT_FILE_PATH)
+                .map_err(|e| {
+                    error!(""Failed to open prof data file, err:{}"", e);
+                    Error::IO(e)
+                })?;
         }
 

```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/471/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/471,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QpIzm,horaedb,1352961254,471,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-12-15T12:03:10Z,2022-12-15T12:03:10Z,"> When do heap profiling, there will be following errors:
> {""code"":500,""message"":""Fail to do heap profiling, err:Profile Error: Jemalloc(An interface with side effects failed in some way not directly related to `mallctl*()` read/write processing.).""}

Actually, this error is caused by missing environment variable: `export MALLOC_CONF=prof:true`.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5QpIzm/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/486,https://api.github.com/repos/apache/horaedb/issues/486,horaedb,1500347285,486,Build sst file in more resource friendly way,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-12-16T14:44:17Z,2023-03-22T02:47:42Z,"### Describe This Problem

In current implementation, sst write involves two loop:
1. First loop to calculate bloom filters of each row group
2. Second loop to write to parquet

Two loops mean more cpu usage, what's worse it that this may eat too many memory.

### Proposal

It's best we can reduce build procedure to one loop, this depends on https://github.com/apache/arrow-rs/issues/3356

If this is not possible, then we may need to spit RecordBatch to disk in order to reduce memory consumption.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/486/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/486,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RGZy1,horaedb,1360633013,486,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2022-12-21T01:12:49Z,2022-12-21T01:12:49Z,"Glad parquet has accepted my proposal, we can just wait version release for both data fusion and parquet.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RGZy1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/486,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XlXyl,horaedb,1469414565,486,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-15T06:25:49Z,2023-03-15T06:25:49Z,"## Proposal

### Current procedure:
```
1. Fetch all record batches from the input stream to organize them in row groups;
2. Build the metadata based on the row groups;
3. Encode all the row groups and obtain the encoded bytes;
4. Upload the encoded bytes into OSS;
```

### New procedure:
```
while true {
 1. Fetch enough rows from the input stream to form a row group, break if the input stream is exhausted;
 2. Collect the necesary information from the row group for building final metadata;
 3. Encode the row group and upload the encoded bytes;
}
4. Encode and upload the final metadata to OSS.
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XlXyl/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/486,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Xljoj,horaedb,1469463075,486,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-15T07:10:49Z,2023-03-15T07:10:49Z,"However, after reviewing the api of parquet writer, async writing has been supported yet, and the relating issue is https://github.com/apache/arrow-rs/issues/1269.

To solve this problem, I guess there are two ways:
- Implement this feature in the upstream which may takes more time to move on.
- Write the local file first and then upload to OSS.

Personally, I vote for the first way.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Xljoj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/489,https://api.github.com/repos/apache/horaedb/issues/489,horaedb,1502173132,489,Reduce the memory consumption required by Compaction,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-12-19T02:06:57Z,2023-05-31T03:46:29Z,"### Describe This Problem

Current compaction procedure consumes much more memory than expected. The theoretical usage of memory should be a little more than the total size of all the input ssts, but now the actual consumption is 4-10 times theoretical value.

### Proposal

The reasons for the high memory consumption, I can come up with these:
- #486
- The input data which will be invalidated later should be put back into the cache;
- Some indexes (e.g. bloom filter) are not necessary to be loaded because predicates won't be used by compaction.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/489/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/489,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5di_Bv,horaedb,1569452143,489,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-05-31T03:46:29Z,2023-05-31T03:46:29Z,Fixed in main.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5di_Bv/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/492,https://api.github.com/repos/apache/horaedb/issues/492,horaedb,1502607028,492,Support table partition,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-12-19T09:30:54Z,2023-01-03T03:30:22Z,"### Describe This Problem

For very large tables, such as a single table of 10 billion+ rows, the single node cannot support such a large amount query and write workload. We need to support table partition.

### Proposal

Some jobs are needed:
1. Support create partition table.
     * [x] ceresmeta support create partition table and open sub table.  https://github.com/CeresDB/ceresmeta/pull/123
     * [x] ceresdb-server support parse sql which contains partition info. #487 
     * [x] ceresdb-server store table partition info, and create partition table. #485 
2. [x] Support query partition table. #512 
3. [x] Support write partition table. #512 
4. [x] Impl a ceresdb inner sdk, to communicate with each other ceresdb-server, transfer insert and query task to specific ceresdb-server. #509 
5. [x] Define new insert and query grpc service for inner sdk. #508 

### Additional Context

TODO:
+ Support `alter` on partition table.
+ Support `drop` partition table.
+ More complete partition rule implementation
  + support column type checking
  + support linear hash
  + support  `in`, `and`, `or` expressions","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/492/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/495,https://api.github.com/repos/apache/horaedb/issues/495,horaedb,1504258984,495,Impose a custom format of SST,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2022-12-20T09:36:22Z,2023-01-17T08:28:09Z,"### Describe This Problem

Currently, the format of sst is just parquet, which may lead to some possible issues in the future upgrading, e.g. introducing some new format like TSM to organize the data.

### Proposal

Impose the new custom format of SST, which may look like:
```
┌───────────────┐
│               │
│               │
│    Payload    │
│               │
│               │
├───────────────┤
│    Footer     │
└───────────────┘
```

And the footer should contain these fields:
```
/// The version number of the format for upgrading compatibility.
version: u16
/// Format of payload, e.g. Parquet, TSM
payload_format: u16
/// A number denotes that this is our custom format.
magical number: u32
```

So that the footer will be a fixed size 64B.

### Additional Context

Although this custom format brings us some great benefits, the sst can't be used by the third-party applications which can process the parquet files.

### Related work
Currently, the sst type is used to decide which type of sst reader/builder to create, but this option is configured for table level, that is to say, one table can only use one sst type. I guess this is not reasonable, and it is more flexible and elegant to allow the data of one table being encoded in different sst formats. So I guess we make this option only works when deciding the type of sst builder, and add a new sst type called `Auto` for the flexibility of choosing a better sst format.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/495/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/495,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RAgOS,horaedb,1359086482,495,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-12-20T09:44:02Z,2022-12-20T09:44:02Z,"Here is the [TSM format](https://docs.influxdata.com/influxdb/v1.8/concepts/storage_engine/#tsm-files):
```
+--------+------------------------------------+-------------+--------------+
| Header |               Blocks               |    Index    |    Footer    |
|5 bytes |              N bytes               |   N bytes   |   4 bytes    |
+--------+------------------------------------+-------------+--------------+
+-------------------+
|      Header       |
+-------------------+
|  Magic  │ Version |
| 4 bytes │ 1 byte  |
+-------------------+
+---------+
│ Footer  │
+---------+
│Index Ofs│
│ 8 bytes │
+---------+
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RAgOS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/495,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RAgs2,horaedb,1359088438,495,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-12-20T09:45:43Z,2022-12-20T09:45:43Z,"Here is rocksdb's [BlockBasedTable Format](https://github.com/facebook/rocksdb/wiki/Rocksdb-BlockBasedTable-Format):
```
<beginning_of_file>
[data block 1]
[data block 2]
...
[data block N]
[meta block 1: filter block]                  (see section: ""filter"" Meta Block)
[meta block 2: index block]
[meta block 3: compression dictionary block]  (see section: ""compression dictionary"" Meta Block)
[meta block 4: range deletion block]          (see section: ""range deletion"" Meta Block)
[meta block 5: stats block]                   (see section: ""properties"" Meta Block)
...
[meta block K: future extended block]  (we may add more meta blocks in the future)
[metaindex block]
[Footer]                               (fixed size; starts at file_size - sizeof(Footer))
<end_of_file>
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RAgs2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/495,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RAhEI,horaedb,1359089928,495,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-12-20T09:47:01Z,2022-12-20T09:47:01Z,"Here is the [Parquet File Format](https://parquet.apache.org/docs/file-format/):
```
4-byte magic number ""PAR1""
<Column 1 Chunk 1 + Column Metadata>
<Column 2 Chunk 1 + Column Metadata>
...
<Column N Chunk 1 + Column Metadata>
<Column 1 Chunk 2 + Column Metadata>
<Column 2 Chunk 2 + Column Metadata>
...
<Column N Chunk 2 + Column Metadata>
...
<Column 1 Chunk M + Column Metadata>
<Column 2 Chunk M + Column Metadata>
...
<Column N Chunk M + Column Metadata>
File Metadata
4-byte length in bytes of file metadata
4-byte magic number ""PAR1""
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RAhEI/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/495,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Rxj3-,horaedb,1371946494,495,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-01-05T09:01:11Z,2023-01-05T09:01:11Z,"After discussions with @jiacai2050, we decide to store the file storage format in the manifest rather than imposing a custom storage format.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Rxj3-/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/495,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5SJTIo,horaedb,1378169384,495,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-01-11T02:38:46Z,2023-01-11T02:38:46Z,"Finally, we find that self-describing SST will bring us many benefits compared with storing the file format in the manifest which leads to the dilemma that the separate SSTs can't be decoded.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5SJTIo/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/495,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Sjamj,horaedb,1385015715,495,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-01-17T08:28:09Z,2023-01-17T08:28:09Z,Fixed by #570,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Sjamj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/497,https://api.github.com/repos/apache/horaedb/issues/497,horaedb,1505813635,497,Is `analytic_engine` a proper name?,archerny,14784412,Arthur Chern,weirong.cwr@gmail.com,CLOSED,2022-12-21T07:08:33Z,2023-11-03T06:44:28Z,"### Describe This Problem

For both traditional time-series workloads and analytic workloads, we use the component named `analytic_engine`. It seems not reasonable.

### Proposal

Rename it to `engine`

### Additional Context

no","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/497/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/497,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RHlTW,horaedb,1360942294,497,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-12-21T07:17:49Z,2022-12-21T07:17:49Z,"It does make senses.

Actually, this engine is initially designed for analytical workload. However, now it is expected to handle both time-series and analytical workload.

And maybe we can call it `MetricEngine`, and the multi-engine architecture may include `Log`, `Trace` engine in the future if possible.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RHlTW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/497,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RH0jf,horaedb,1361004767,497,NA,archerny,14784412,Arthur Chern,weirong.cwr@gmail.com,NA,2022-12-21T08:31:02Z,2022-12-21T08:31:02Z,"As far as I can see, `Metric` is a domain specific concept. It fits well for scenarios in monitoring and IoT. However, there are plenty of other use cases, such as time-series analysis for stocks, bonds etc.

So, I'm considering changing it to a more general name.

BTW, there is another component named `query_engine`. Lots of engines 🐶","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RH0jf/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/497,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RM7rH,horaedb,1362344647,497,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2022-12-22T03:03:23Z,2022-12-22T03:03:23Z," > As far as I can see, Metric is a domain specific concept. It fits well for scenarios in monitoring and IoT. However, there are plenty of other use cases, such as time-series analysis for stocks, bonds etc.
>
> So, I'm considering changing it to a more general name.
Agreed.

> BTW, there is another component named query_engine. Lots of engines 🐶

Actually, their relationship has been explained in the architecture doc:
```
┌──────────────────────────────────────────┐
│       RPC Layer (HTTP/gRPC/MySQL)        │
└──────────────────────────────────────────┘
┌──────────────────────────────────────────┐
│                 SQL Layer                │
│ ┌─────────────────┐  ┌─────────────────┐ │
│ │     Parser      │  │     Planner     │ │
│ └─────────────────┘  └─────────────────┘ │
└──────────────────────────────────────────┘
┌───────────────────┐  ┌───────────────────┐
│    Interpreter    │  │      Catalog      │
└───────────────────┘  └───────────────────┘
┌──────────────────────────────────────────┐
│               Query Engine               │
│ ┌─────────────────┐  ┌─────────────────┐ │
│ │    Optimizer    │  │    Executor     │ │
│ └─────────────────┘  └─────────────────┘ │
└──────────────────────────────────────────┘
┌──────────────────────────────────────────┐
│         Pluggable Table Engine           │
│  ┌────────────────────────────────────┐  │
│  │              Analytic              │  │
│  │┌────────────────┐┌────────────────┐│  │
│  ││      Wal       ││    Memtable    ││  │
│  │└────────────────┘└────────────────┘│  │
│  │┌────────────────┐┌────────────────┐│  │
│  ││     Flush      ││   Compaction   ││  │
│  │└────────────────┘└────────────────┘│  │
│  │┌────────────────┐┌────────────────┐│  │
│  ││    Manifest    ││  Object Store  ││  │
│  │└────────────────┘└────────────────┘│  │
│  └────────────────────────────────────┘  │
│  ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │
│           Another Table Engine        │  │
│  └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │
└──────────────────────────────────────────┘
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RM7rH/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/510,https://api.github.com/repos/apache/horaedb/issues/510,horaedb,1510982066,510,Xor Filters: Faster and Smaller Than Bloom Filters,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-12-26T14:02:56Z,2023-02-13T09:11:50Z,"### Describe This Problem

In currently implementation, bloom filters are used to reduce unnecessary data involved in one Query. There is one annoying issues:
- large memory consumption, every bloom filter is 256 bytes.

### Proposal

There is an interesting algorithm Xor filters to address bloom filters' issue. We can adopt this structure as alternatives of bloom filters.

![image](https://user-images.githubusercontent.com/3848910/209556567-4bad3901-3b25-42d1-bf8e-f347a3dbcd92.png)


### Additional Context

- https://lemire.me/blog/2019/12/19/xor-filters-faster-and-smaller-than-bloom-filters/
- https://bohutang.me/2022/11/21/databend-xor-filter/","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/510/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/510,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5UyVIL,horaedb,1422479883,510,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-02-08T11:58:14Z,2023-02-08T11:58:14Z,"Here are the details about some simple benchmark:
## Input
- The record looks like `host_{number}`;
- The total number of the records is 27890000;
- Reorganize the records into chunks which contains 8192 records, that is to say, the number of chunks is 3405;
- Every chunk has only nearly 300 unique records;
- For every chunk, a bloom filter or xor filter will be built for testing;

## Data
- Bloom filter with 2048 bits and 3 hash functions: 149 chunks are filtered out and 148 is false positive, false positive rate is 0.04;
- Bloom filter with 3200 bits and 7 hash functions: false positive rate is 0.0059
- Xorfilter with nearly 3200 bits: 17 chunks are filtered out and 16 is false positive, false positive rate is 0.004;

## Conclusion
- **50% more storage leads to 10X speed.**
- Xorfilter performs better than bloom filter.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5UyVIL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/516,https://api.github.com/repos/apache/horaedb/issues/516,horaedb,1511809043,516,Add unit tests in avro.rs,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2022-12-27T13:33:41Z,2023-03-02T12:30:39Z,"### Describe This Problem

Missing unit tests in avro.rs.

### Proposal

Add some unit test in avro.rs.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/516/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/516,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5R64g0,horaedb,1374390324,516,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-01-07T06:18:25Z,2023-01-07T06:18:25Z,"Have you considered to encode/decode RecordBatch directly using https://docs.rs/arrow-ipc/30.0.0/arrow_ipc/index.html?

I think avro is not suitable for encode/decode RecordBatch, it may incur more cpu overhead than arrow-ipc.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5R64g0/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/516,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WiJj1,horaedb,1451792629,516,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-02T12:30:39Z,2023-03-02T12:30:39Z,Avro is removed from our codebase ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WiJj1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/520,https://api.github.com/repos/apache/horaedb/issues/520,horaedb,1513547780,520,Avoid IO in query plan stage,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-12-29T09:23:27Z,2024-11-22T07:08:36Z,"### Describe this problem

In current implementation, when we create a ExecutionPlan for ScanTable, it will begin to scan table, this is problematic since in build plan stage, NO IO should happen, all operations should be light-weighted.

Some issue I can think of:
1. Incorrect metrics stats of ExecutionPlan
2. `explain` statement will take longer time to execute. 

### Steps to reproduce

No need

### Expected behavior

_No response_

### Additional Information

https://github.com/CeresDB/ceresdb/blob/c54a4cbf1db5f9f18435f5d419fa638d57749af3/table_engine/src/provider.rs#L214

We need to move `maybe_init_stream` inside `ScanTable::execute`","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/520/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/525,https://api.github.com/repos/apache/horaedb/issues/525,horaedb,1514352424,525,Bad case in insert_mode,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2022-12-30T08:51:21Z,2022-12-30T12:30:43Z,"### Describe this problem

There is a bad case in integration tests
```
Environment local run finished, cost:1677ms
Different cases:
[
    ""~/CeresDB/integration_tests/cases/local/03_dml/insert_mode"",
]
Environment local run failed, error:RunFailed { count: 1 }.
Error: Run failed. 1 cases can't pass
make: *** [run] Error 1

198c198,202
< Failed to execute query, err: Server(ServerError { code: 500, msg: ""Failed to convert output, query: SELECT \n     * \n FROM \n     `03_dml_insert_mode_table4` \n ORDER BY \n     `c1` ASC;. Caused by: Rpc error, code:500, message:failed to convert record batch, cause:Failed to write avro record, err:Value does not match schema."" })
---
> tsid,timestamp,c1,c2,c3,c4,c5,
> Int64(0),Timestamp(Timestamp(1)),Int64(10),String(StringBytes(b""123"")),Int64(11),Int64(12),Int64(3),
> Int64(0),Timestamp(Timestamp(2)),Int64(20),String(StringBytes(b""123"")),Int64(21),Int64(22),Int64(4),
> Int64(0),Timestamp(Timestamp(3)),Int64(30),String(StringBytes(b""123"")),Int64(31),Int64(32),Int64(5),
> 

```

### Steps to reproduce


```
cd integration_tests && make run
```

### Expected behavior

No error

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/525/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/529,https://api.github.com/repos/apache/horaedb/issues/529,horaedb,1516953626,529,add new a target `make dev-setup` to install dependencies,dust1,18304424,Yoke,834902408@qq.com,CLOSED,2023-01-03T05:44:20Z,2023-04-11T09:59:40Z,"### Describe This Problem

it just echo branch

### Proposal

Initialize the environment or install the necessary tools, e.g. `pkg-config`, `libssl-dev`.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/529/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/529,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RoRJd,horaedb,1369510493,529,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-01-03T08:47:02Z,2023-01-03T08:47:02Z,"I think current usage is a convention, `init` is a common task(such as setting env vars) which others can depend on.

It should be light weighted, install dependencies is kinds of a heavy task, so it may not suit here.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RoRJd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/529,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RoSHx,horaedb,1369514481,529,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2023-01-03T08:52:36Z,2023-01-03T08:52:36Z,"Whether `ceresdb` will have commands to install dependencies in the future? e.g. `make setup`.
I recently used `ceresdb` in a new environment and would it be much easier to have such a command","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RoSHx/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/529,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RocS3,horaedb,1369556151,529,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-01-03T09:35:23Z,2023-01-03T09:35:23Z,"We can add new a target to install those dependencies. Example:

```
dev-setup:
  if macOS
    brew install protobuf
  if Ubuntu 
    sudo apt install --yes protobuf-compiler

```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RocS3/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/531,https://api.github.com/repos/apache/horaedb/issues/531,horaedb,1517316790,531,Table enginle close table  after  the table was successfully created,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2023-01-03T12:20:58Z,2024-10-19T11:30:00Z,"### Describe this problem

logs like this:
```
2023-01-03 19:51:49.824 INFO [server/src/handlers/sql.rs:115] sql handler try to process request, request_id:3619, request:Request { query: ""CREATE TABLE SPM_637057036_INFLUENCE_DEFAULT(period timestamp NOT NULL ,TraceId string TAG NULL ,_result string TAG NULL ,server string TAG NULL ,idc string TAG NULL ,ldc string TAG NULL ,TIMESTAMP KEY(period))  ENGINE=Analytic WITH( ttl='3d', update_mode='APPEND')"" }
2023-01-03 19:51:49.824 INFO [meta_client/src/meta_impl.rs:123] Meta client try to create table, req:CreateTableRequest { header: Some(RequestHeader { node: ""11.39.17.21:8831"", cluster_name: ""defaultCluster"" }), schema_name: ""public"", name: ""SPM_637057036_INFLUENCE_DEFAULT"", encoded_schema: [0, 10, 10, 10, 4, 116, 115, 105, 100, 16, 5, 32, 1, 10, 12, 10, 6, 112, 101, 114, 105, 111, 100, 16, 1, 32, 2, 10, 17, 10, 7, 84, 114, 97, 99, 101, 73, 100, 16, 4, 24, 1, 32, 3, 40, 1, 10, 17, 10, 7, 95, 114, 101, 115, 117, 108, 116, 16, 4, 24, 1, 32, 4, 40, 1, 10, 16, 10, 6, 115, 101, 114, 118, 101, 114, 16, 4, 24, 1, 32, 5, 40, 1, 10, 13, 10, 3, 105, 100, 99, 16, 4, 24, 1, 32, 6, 40, 1, 10, 13, 10, 3, 108, 100, 99, 16, 4, 24, 1, 32, 7, 40, 1, 16, 1, 24, 1, 34, 2, 0, 1], engine: ""Analytic"", create_if_not_exist: false, options: {""ttl"": ""3d"", ""update_mode"": ""APPEND""}, encoded_partition_info: [], partition_table_info: None }
2023-01-03 19:51:49.831 INFO [server/src/grpc/meta_event_service/mod.rs:98] Receive request from meta, req:CreateTableOnShardRequest { update_shard_info: Some(UpdateShardInfo { curr_shard_info: Some(ShardInfo { id: 4, role: Leader, version: 9 }), prev_version: 8 }), table_info: Some(TableInfo { id: 64, name: ""SPM_637057036_INFLUENCE_DEFAULT"", schema_id: 0, schema_name: ""public"" }), encoded_schema: [0, 10, 10, 10, 4, 116, 115, 105, 100, 16, 5, 32, 1, 10, 12, 10, 6, 112, 101, 114, 105, 111, 100, 16, 1, 32, 2, 10, 17, 10, 7, 84, 114, 97, 99, 101, 73, 100, 16, 4, 24, 1, 32, 3, 40, 1, 10, 17, 10, 7, 95, 114, 101, 115, 117, 108, 116, 16, 4, 24, 1, 32, 4, 40, 1, 10, 16, 10, 6, 115, 101, 114, 118, 101, 114, 16, 4, 24, 1, 32, 5, 40, 1, 10, 13, 10, 3, 105, 100, 99, 16, 4, 24, 1, 32, 6, 40, 1, 10, 13, 10, 3, 108, 100, 99, 16, 4, 24, 1, 32, 7, 40, 1, 16, 1, 24, 1, 34, 2, 0, 1], engine: ""Analytic"", create_if_not_exist: false, options: {""update_mode"": ""APPEND"", ""ttl"": ""3d""}, encoded_partition_info: [] }
2023-01-03 19:51:49.831 INFO [analytic_engine/src/engine.rs:75] Table engine impl create table, space_id:0, request:CreateTableRequest { catalog_name: ""ceresdb"", schema_name: ""public"", schema_id: SchemaId(0), table_id: TableId(64), table_name: ""SPM_637057036_INFLUENCE_DEFAULT"", table_schema: Schema { timestamp_index: 1, tsid_index: Some(0), column_schemas: ColumnSchemas { columns: [ColumnSchema { id: 1, name: ""tsid"", data_type: UInt64, is_nullable: false, is_tag: false, comment: """", escaped_name: ""tsid"", default_value: None }, ColumnSchema { id: 2, name: ""period"", data_type: Timestamp, is_nullable: false, is_tag: false, comment: """", escaped_name: ""period"", default_value: None }, ColumnSchema { id: 3, name: ""TraceId"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""TraceId"", default_value: None }, ColumnSchema { id: 4, name: ""_result"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""_result"", default_value: None }, ColumnSchema { id: 5, name: ""server"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""server"", default_value: None }, ColumnSchema { id: 6, name: ""idc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""idc"", default_value: None }, ColumnSchema { id: 7, name: ""ldc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""ldc"", default_value: None }] }, version: 1, primary_key_indexes: [0, 1] }, engine: ""Analytic"", options: {""update_mode"": ""APPEND"", ""ttl"": ""3d""}, state: Stable, shard_id: 4, cluster_version: 1, partition_info: None }
2023-01-03 19:51:49.831 INFO [analytic_engine/src/instance/create.rs:31] Instance create table, request:CreateTableRequest { catalog_name: ""ceresdb"", schema_name: ""public"", schema_id: SchemaId(0), table_id: TableId(64), table_name: ""SPM_637057036_INFLUENCE_DEFAULT"", table_schema: Schema { timestamp_index: 1, tsid_index: Some(0), column_schemas: ColumnSchemas { columns: [ColumnSchema { id: 1, name: ""tsid"", data_type: UInt64, is_nullable: false, is_tag: false, comment: """", escaped_name: ""tsid"", default_value: None }, ColumnSchema { id: 2, name: ""period"", data_type: Timestamp, is_nullable: false, is_tag: false, comment: """", escaped_name: ""period"", default_value: None }, ColumnSchema { id: 3, name: ""TraceId"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""TraceId"", default_value: None }, ColumnSchema { id: 4, name: ""_result"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""_result"", default_value: None }, ColumnSchema { id: 5, name: ""server"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""server"", default_value: None }, ColumnSchema { id: 6, name: ""idc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""idc"", default_value: None }, ColumnSchema { id: 7, name: ""ldc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""ldc"", default_value: None }] }, version: 1, primary_key_indexes: [0, 1] }, engine: ""Analytic"", options: {""update_mode"": ""APPEND"", ""ttl"": ""3d""}, state: Stable, shard_id: 4, cluster_version: 1, partition_info: None }
2023-01-03 19:51:49.831 INFO [analytic_engine/src/meta/details.rs:228] Manifest store update, request:MetaUpdateRequest { location: WalLocation { versioned_region_id: VersionedRegionId { version: 1, id: 4 }, table_id: 64 }, meta_update: AddTable(AddTableMeta { space_id: 0, table_id: TableId(64), table_name: ""SPM_637057036_INFLUENCE_DEFAULT"", schema: Schema { timestamp_index: 1, tsid_index: Some(0), column_schemas: ColumnSchemas { columns: [ColumnSchema { id: 1, name: ""tsid"", data_type: UInt64, is_nullable: false, is_tag: false, comment: """", escaped_name: ""tsid"", default_value: None }, ColumnSchema { id: 2, name: ""period"", data_type: Timestamp, is_nullable: false, is_tag: false, comment: """", escaped_name: ""period"", default_value: None }, ColumnSchema { id: 3, name: ""TraceId"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""TraceId"", default_value: None }, ColumnSchema { id: 4, name: ""_result"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""_result"", default_value: None }, ColumnSchema { id: 5, name: ""server"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""server"", default_value: None }, ColumnSchema { id: 6, name: ""idc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""idc"", default_value: None }, ColumnSchema { id: 7, name: ""ldc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""ldc"", default_value: None }] }, version: 1, primary_key_indexes: [0, 1] }, opts: TableOptions { segment_duration: None, update_mode: Append, storage_format: Columnar, enable_ttl: true, ttl: ReadableDuration(259200s), arena_block_size: 2097152, write_buffer_size: 33554432, compaction_strategy: Default, num_rows_per_row_group: 8192, compression: Zstd }, partition_info: None }) }
2023-01-03 19:51:49.851 INFO [meta_client/src/meta_impl.rs:133] Meta client finish creating table, resp:CreateTableResponse { header: Some(ResponseHeader { code: 0, error: """" }), created_table: Some(TableInfo { id: 64, name: ""SPM_637057036_INFLUENCE_DEFAULT"", schema_id: 0, schema_name: ""public"" }), shard_info: Some(ShardInfo { id: 4, role: Leader, version: 9 }) }
2023-01-03 19:51:49.851 INFO [interpreters/src/table_manipulator/meta_based.rs:94] Create table by meta successfully, req:CreateTableRequest { schema_name: ""public"", name: ""SPM_637057036_INFLUENCE_DEFAULT"", encoded_schema: [0, 10, 10, 10, 4, 116, 115, 105, 100, 16, 5, 32, 1, 10, 12, 10, 6, 112, 101, 114, 105, 111, 100, 16, 1, 32, 2, 10, 17, 10, 7, 84, 114, 97, 99, 101, 73, 100, 16, 4, 24, 1, 32, 3, 40, 1, 10, 17, 10, 7, 95, 114, 101, 115, 117, 108, 116, 16, 4, 24, 1, 32, 4, 40, 1, 10, 16, 10, 6, 115, 101, 114, 118, 101, 114, 16, 4, 24, 1, 32, 5, 40, 1, 10, 13, 10, 3, 105, 100, 99, 16, 4, 24, 1, 32, 6, 40, 1, 10, 13, 10, 3, 108, 100, 99, 16, 4, 24, 1, 32, 7, 40, 1, 16, 1, 24, 1, 34, 2, 0, 1], engine: ""Analytic"", create_if_not_exist: false, options: {""ttl"": ""3d"", ""update_mode"": ""APPEND""}, partition_table_info: None, encoded_partition_info: [] }, resp:CreateTableResponse { created_table: TableInfo { id: 64, name: ""SPM_637057036_INFLUENCE_DEFAULT"", schema_id: 0, schema_name: ""public"" }, shard_info: ShardInfo { id: 4, role: Leader, version: 9 } }
2023-01-03 19:51:49.851 INFO [server/src/handlers/sql.rs:186] sql handler finished, request_id:3619, cost:27ms, request:Request { query: ""CREATE TABLE SPM_637057036_INFLUENCE_DEFAULT(period timestamp NOT NULL ,TraceId string TAG NULL ,_result string TAG NULL ,server string TAG NULL ,idc string TAG NULL ,ldc string TAG NULL ,TIMESTAMP KEY(period))  ENGINE=Analytic WITH( ttl='3d', update_mode='APPEND')"" }
2023-01-03 19:52:04.485 INFO [analytic_engine/src/engine.rs:165] Table engine impl close table, space_id:0, request:CloseTableRequest { catalog_name: ""ceresdb"", schema_name: ""public"", schema_id: SchemaId(0), table_name: ""SPM_637057036_INFLUENCE_DEFAULT"", table_id: TableId(64), engine: ""Analytic"" }
2023-01-03 19:52:04.485 INFO [analytic_engine/src/instance/close.rs:25] Instance close table, request:CloseTableRequest { catalog_name: ""ceresdb"", schema_name: ""public"", schema_id: SchemaId(0), table_name: ""SPM_637057036_INFLUENCE_DEFAULT"", table_id: TableId(64), engine: ""Analytic"" }
```

### Steps to reproduce

Create different tables multiple times

### Expected behavior

Table engine do not close table

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/531/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/533,https://api.github.com/repos/apache/horaedb/issues/533,horaedb,1518201379,533,Limit the number of columns in a table,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-01-04T02:36:39Z,2023-11-03T06:52:46Z,"### Describe This Problem

The number of columns in the table will affect the query write performance, we need to limit the number of columns.

### Proposal

Add a new parameter in config to limit the number of columns.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/533/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/533,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RsPtU,horaedb,1370553172,533,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-01-04T07:05:10Z,2023-01-04T07:05:10Z,"It's awkward to add this limit to a column-based database, it should be optimized for table with wide columns.

I think a high level task is to investigate which part cause bottleneck, and try to fix them. ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RsPtU/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/534,https://api.github.com/repos/apache/horaedb/issues/534,horaedb,1518208920,534,Can not find sst file when executing query,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2023-01-04T02:49:53Z,2023-03-08T07:10:02Z,"### Describe this problem

Error message is:
```
2023-01-03 20:23:31.891 ERRO [server/src/grpc/storage_service/mod.rs:258] Failed to handle request, mod:query, handler:handle_query, err:Rpc error, code:500, message:Failed to execute interpreter, query:SELECT data_source,day_of_growth,gmt_create,gmt_modified,id,net_value,net_value_date,profit_seven_days,profit_ten_thousand,restored_net_value,symbol,total_net_value FROM `quotresearch_fund_quotation_v1` WHERE (is_dfd_del=0 OR is_dfd_del IS NULL) AND net_value_date>=1664972608726 AND net_value_date<=1672748608726, cause:Failed to execute select, err:Failed to execute logical plan, err:Failed to collect record batch stream, err:Stream error, msg:convert from arrow record batch, err:External error: Stream error, msg:read record batch, err:Failed to read data from the sub iterator, err:PullRecordBatch { source: DecodeRecordBatch { source: ParquetError { source: General(""Failed to fetch ranges from object store, err:Generic Aliyun error: Failed to get range of object at path:ceresdbfin/0/172/1969.sst, err:error sending request for url (http://antsys-ceresdbfin.cn-shanghai-alipay-internal.oss-alipay.aliyuncs.com/ceresdbfin/0/172/1969.sst?): error trying to connect: dns error: failed to lookup address information: Name or service not known""), backtrace: Backtrace(   0: <snafu::backtrace_shim::Backtrace as snafu::GenerateBacktrace>::generate
```

And there is no file 'ceresdbfin/0/172/1969.sst' in oss:
![Uploading image.png…]()


### Steps to reproduce

None

### Expected behavior

Query result

### Additional Information

Maybe it is related to file merging：
```
2023-01-03 20:23:31.992 INFO [analytic_engine/src/row_iter/merge.rs:850] Merge iterator dropped, table_id:TableId(172), request_id:598719, metrics:Metrics { num_memtables: 1, num_ssts: 9, sst_ids: [820, 1969, 2002, 2034, 2098, 2179, 2184, 2191, 2199], times_fetch_rows_from_one: 0, total_rows_fetch_from_one: 0, times_fetch_row_from_multiple: 368838, duration_since_create: 1.022840061s, init_duration: 49.685179ms, scan_duration: 167.588723ms, scan_count: 422 }, iter_options:IterOptions { batch_size: 500, sst_background_read_parallelism: 8 },
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/534/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/534,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RrzVP,horaedb,1370436943,534,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-01-04T03:04:41Z,2023-01-04T03:04:41Z,"I think this issue is similar with https://github.com/CeresDB/ceresdb/issues/427.

The problem is that we have no snapshot guarantee for a query. 

When a query begin, we should make a snapshot of files it required, and KEEP them duration query execution, only after query finish, we can delete files used in this query.

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5RrzVP/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/534,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XAGKV,horaedb,1459643029,534,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-08T07:09:48Z,2023-03-08T07:09:48Z,Fixed in #699 ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XAGKV/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/554,https://api.github.com/repos/apache/horaedb/issues/554,horaedb,1527037310,554,Optimize local WAL size,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-01-10T09:19:27Z,2024-10-19T11:14:49Z,"### Describe This Problem

In my local env, I find WAL(rocksdb based) is very large compared with SST, even after manually flush.

```
1.5G    data/wal
76M     data/manifest
371M    data/store
1.9G    data
```

### Proposal

Do a compaction after `delete_range` to try remove dead entry from SST.

https://github.com/CeresDB/ceresdb/blob/9ab659a99417b41f3a4007e59bbe9b8f3ff65b8b/wal/src/rocks_impl/manager.rs#L120
### Additional Context

https://github.com/facebook/rocksdb/wiki/RocksDB-Tuning-Guide#trigger-compaction-on-deletes","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/554/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/554,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5q7lEL,horaedb,1794003211,554,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-11-06T02:49:07Z,2023-11-06T02:49:07Z,"@ShiKaiWi I don't think this issue get fixed, wal encode is a general way to reduce wal size, but the issue here is to optimize for rocksdb-based, they have no direct relation.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5q7lEL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/557,https://api.github.com/repos/apache/horaedb/issues/557,horaedb,1527594151,557,Support INSERT INTO SELECT,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-01-10T15:30:22Z,2024-07-15T09:15:40Z,"### Describe This Problem

Enhance SQL support.

`INSERT INTO SELECT` is useful to migrate table, benchmark(to quickly generate lots of data).



### Proposal

Implement this SQL syntax.
```sql
INSERT INTO table2 (column1, column2, column3, ...)
SELECT column1, column2, column3, ...
FROM table1
WHERE condition;
```
### Additional Context
- https://dev.mysql.com/doc/refman/8.0/en/insert-select.html
- https://www.w3schools.com/mysql/mysql_insert_into_select.asp","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/557/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/557,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tsDve,horaedb,1840266206,557,NA,caicancai,77189278,Cancai Cai,,NA,2023-12-05T08:35:33Z,2023-12-05T08:35:33Z,"Hi, I'm interested in this let me try, but this one may take a little time.
This requirement seems a little difficult for me, but I want to try it","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tsDve/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/557,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tsZUi,horaedb,1840354594,557,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-12-05T09:21:00Z,2023-12-05T09:21:00Z,"Thanks, assigned.

You need to know how horaedb parse sql, how a plan is created and executed. This is indeed a non-trivial task, feel free to ask any questions when you are in trouble.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tsZUi/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/557,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vkZYj,horaedb,1871812131,557,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-12-29T07:37:05Z,2023-12-29T07:37:05Z,"@caicancai Hi, a few weeks passed, how's it going? Any problems?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vkZYj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/557,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vka1O,horaedb,1871818062,557,NA,caicancai,77189278,Cancai Cai,,NA,2023-12-29T07:51:08Z,2023-12-29T07:51:08Z,"> @caicancai Hi, a few weeks passed, how's it going? Any problems?

There doesn't seem to be any progress. I was busy with work in December. I'm sorry for that. I may start this part of the work on New Year's Day, if this feature is not urgent.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vka1O/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/557,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vkbIr,horaedb,1871819307,557,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-12-29T07:54:06Z,2023-12-29T07:54:06Z,"Hi, take your time, just want to know if there is any problems.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vkbIr/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/557,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vkbek,horaedb,1871820708,557,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-12-29T07:57:17Z,2023-12-29T07:57:17Z,"You can join our slack channel to discuss with us.
- https://github.com/apache/incubator-horaedb?tab=readme-ov-file#contributing","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vkbek/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/557,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vwrMG,horaedb,1875030790,557,NA,caicancai,77189278,Cancai Cai,,NA,2024-01-03T08:57:17Z,2024-01-03T08:57:17Z,"@jiacai2050 Hello, I tried it during the holiday. I expected that this feature would take up a lot of my energy. Since I have been busy recently, I may not have so much energy. I chose to give up this feature. I am very sorry for taking up so much time.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vwrMG/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/557,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5v-I-g,horaedb,1878560672,557,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-01-05T12:03:21Z,2024-01-05T12:03:21Z,"Thanks for trying out, This task is a little complex, you need to understand how query and write works at the same time.

> PS: I remove the `good first issue` tag.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5v-I-g/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/557,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5-AYCW,horaedb,2114027670,557,NA,dracoooooo,55609330,Draco,dracode01@gmail.com,NA,2024-05-16T04:56:37Z,2024-05-16T04:56:37Z,"Hi @jiacai2050, I'd like to take this task.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5-AYCW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/557,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5-AwHZ,horaedb,2114126297,557,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-05-16T06:26:26Z,2024-05-16T06:26:26Z,👍 Much appreciated.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5-AwHZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/558,https://api.github.com/repos/apache/horaedb/issues/558,horaedb,1528323082,558,Deprecate custom UDF time_bucket in favor of date_bin,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-01-11T02:44:22Z,2024-10-19T11:14:49Z,"### Describe This Problem

In early version of CeresDB, there is a custom UDF `time_bucket` to align time of values, but now datafusion support it out of box, so I suggest deprecate our `time_bucket`, and use `data_bin` instead. Ex:

```sql
SELECT
      DATE_BIN(INTERVAL '15' minute, time, TIMESTAMP '2001-01-01T00:00:00Z') AS time,
      val
    FROM (
      VALUES
        (TIMESTAMP '2021-06-10 17:05:00Z', 0.5),
        (TIMESTAMP '2021-06-10 17:19:10Z', 0.3)
      ) as t (time, val)


        ""+---------------------+-----+"",
        ""| time                | val |"",
        ""+---------------------+-----+"",
        ""| 2021-06-10 17:00:00 | 0.5 |"",
        ""| 2021-06-10 17:15:00 | 0.3 |"",
        ""+---------------------+-----+"",
```

### Proposal

1. Completely remove it before 1.0 is out.
2. If cannot remove it, add a deprecation warning.

### Additional Context

https://github.com/apache/arrow-datafusion/issues/3015","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/558/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/558,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5SJUrL,horaedb,1378175691,558,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-01-11T02:51:13Z,2023-01-11T02:51:13Z,"I wonder whether `time_bucket` can be implemented by `data_bin`. And I prefer to keep the `time_bucket` UDF if it can, because `time_bucket` has been provided for use in some scenarios.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5SJUrL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/558,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5SJVte,horaedb,1378179934,558,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-01-11T02:58:59Z,2023-01-11T02:58:59Z,"I think yes, those two functions are almost the same, only with difference in how to set `time duration`
- date_bin: `INTERVAL '15' minute`
- time_bucket: `PT15M`","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5SJVte/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/558,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5SJe4D,horaedb,1378217475,558,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-01-11T04:10:27Z,2023-01-11T04:10:27Z,Anyone interested in this issue can try reimplement time_bucket based on `date_bin`,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5SJe4D/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/558,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5SP5Hf,horaedb,1379897823,558,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-01-12T07:06:43Z,2023-01-12T07:06:43Z,Found one bug related with time_bucket: https://github.com/CeresDB/ceresdb/issues/563,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5SP5Hf/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/558,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XgUQQ,horaedb,1468089360,558,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2023-03-14T13:18:23Z,2023-03-14T13:18:23Z,assigne me. ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XgUQQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/558,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5X_VHs,horaedb,1476219372,558,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2023-03-20T13:17:57Z,2023-03-20T13:17:57Z,"`date_bin` seems to have less functionality than `time_bucket`, and there is a big difference between the two. Do I just need to keep date_bin's `date_bin(stride, source, origin)` syntax?
```rust
                ScalarFunction::DateBin => Ok(date_bin(
                    parse_expr(&args[0], registry)?,
                    parse_expr(&args[1], registry)?,
                    parse_expr(&args[2], registry)?,
                )),
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5X_VHs/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/558,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5X_ut8,horaedb,1476324220,558,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-20T14:20:56Z,2023-03-20T14:20:56Z,"Yes, in fact only first two args is used in time_bucket.
```rs
fn new_udf() -> ScalarUdf {
    // args:
    // - timestamp column.
    // - period.
    // - input timestamp format in PARTITION BY (unsed now).
    // - input timezone (ignored now).
    // - timestamp output format (ignored now).
    let func = |args: &[ColumnarValue]| {
        let bucket = TimeBucket::parse_args(args)
            .box_err()
            .context(InvalidArguments)?;

        let result_column = bucket.call().box_err().context(CallFunction)?;

        Ok(ColumnarValue::Array(result_column))
    };
```
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5X_ut8/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/559,https://api.github.com/repos/apache/horaedb/issues/559,horaedb,1528366878,559,Panicked when create table,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2023-01-11T03:32:36Z,2023-03-16T08:07:55Z,"### Describe this problem

Here is the stack
```
2023-01-11 11:01:11.797 ERRO [server/src/http.rs:129] Http service Failed to handle sql, err:Failed to create plan, query:DESCRIBE MMM_2197135594_INFLUENCE_GRAY_SANDBOX_OUTPUT_TABLE, err:Failed to create plan, err:Table not found, table:MMM_2197135594_INFLUENCE_GRAY_SANDBOX_OUTPU
T_TABLE
2023-01-11 11:01:11.798 ERRO [common_util/src/panic.rs:42] thread 'ceres-write' panicked 'assertion failed: success' at ""analytic_engine/src/space.rs:147""
   0: common_util::panic::set_panic_hook::{{closure}}
             at /home/chunshao.rcs/github/CeresDB/ceresdb/common_util/src/panic.rs:41:18
   1: std::panicking::rust_panic_with_hook
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:702:17
   2: std::panicking::begin_panic_handler::{{closure}}
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:586:13
   3: std::sys_common::backtrace::__rust_end_short_backtrace
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/sys_common/backtrace.rs:138:18
   4: rust_begin_unwind
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:584:5
   5: core::panicking::panic_fmt
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/panicking.rs:142:14
   6: core::panicking::panic
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/panicking.rs:48:5
   7: analytic_engine::space::Space::insert_table
             at /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/space.rs:147:9
   8: analytic_engine::instance::create::<impl analytic_engine::instance::Instance>::process_create_table_command::{{closure}}
             at /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/instance/create.rs:115:9
      <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/future/mod.rs:91:19
      analytic_engine::instance::write_worker::WriteWorker::handle_create_table::{{closure}}
             at /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/instance/write_worker.rs:865:78
      <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/future/mod.rs:91:19
      analytic_engine::instance::write_worker::WriteWorker::run::{{closure}}
             at /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/instance/write_worker.rs:753:50
   9: <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/future/mod.rs:91:19
      analytic_engine::instance::write_worker::WriteGroup::new::{{closure}}
             at /home/chunshao.rcs/github/CeresDB/ceresdb/analytic_engine/src/instance/write_worker.rs:616:29
      <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/future/mod.rs:91:19
  10: tokio::runtime::task::core::CoreStage<T>::poll::{{closure}}
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/core.rs:165:17
      tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/loom/std/unsafe_cell.rs:14:9
      tokio::runtime::task::core::CoreStage<T>::poll
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/core.rs:155:13
      tokio::runtime::task::harness::poll_future::{{closure}}
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/harness.rs:480:19
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/panic/unwind_safe.rs:271:9
      std::panicking::try::do_call
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:492:40
      std::panicking::try
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:456:19
  11: std::panic::catch_unwind
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panic.rs:137:14
      tokio::runtime::task::harness::poll_future
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/harness.rs:468:18
      tokio::runtime::task::harness::Harness<T,S>::poll_inner
             at /home/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.20.1/src/runtime/task/harness.rs:104:27
      tokio::runtime::task::harness::Harness<T,S>::poll
```

### Steps to reproduce

I guess the table had been created and dropped for many times, then panicked when creating the table

### Expected behavior

Create table successfully

### Additional Information

None","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/559/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/559,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5SJbR-,horaedb,1378202750,559,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-01-11T03:41:37Z,2023-01-11T03:41:37Z,@MachaelLee Maybe the commit id of the binary helps troubleshoot because the code location is wrong in the latest main branch.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5SJbR-/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/559,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XdM34,horaedb,1467272696,559,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-14T02:52:11Z,2023-03-14T02:52:11Z,"```sh
2023-03-14T02:31:10.846583855Z 2023-03-14 02:31:10.846 ERRO [common_util/src/panic.rs:42] thread 'ceres-write' panicked 'assertion failed: success' at ""analytic_engine/src/space.rs:159""
2023-03-14T02:31:10.846590798Z    0: common_util::panic::set_panic_hook::{{closure}}
2023-03-14T02:31:10.846598388Z              at ceresdb/common_util/src/panic.rs:41:18
2023-03-14T02:31:10.846603760Z    1: <alloc::boxed::Box<F,A> as core::ops::function::Fn<Args>>::call
2023-03-14T02:31:10.846610344Z              at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:2002:9
2023-03-14T02:31:10.846638988Z       std::panicking::rust_panic_with_hook
2023-03-14T02:31:10.846646204Z              at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:692:13
2023-03-14T02:31:10.846651435Z    2: std::panicking::begin_panic_handler::{{closure}}
2023-03-14T02:31:10.846656365Z              at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:577:13
2023-03-14T02:31:10.846661186Z    3: std::sys_common::backtrace::__rust_end_short_backtrace
2023-03-14T02:31:10.846669922Z              at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys_common/backtrace.rs:137:18
2023-03-14T02:31:10.846675231Z    4: rust_begin_unwind
2023-03-14T02:31:10.846679676Z              at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:575:5
2023-03-14T02:31:10.846683169Z    5: core::panicking::panic_fmt
2023-03-14T02:31:10.846689518Z              at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panicking.rs:64:14
2023-03-14T02:31:10.846692993Z    6: core::panicking::panic
2023-03-14T02:31:10.846695905Z              at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panicking.rs:114:5
2023-03-14T02:31:10.846698880Z    7: analytic_engine::space::Space::insert_table
2023-03-14T02:31:10.846701684Z              at ceresdb/analytic_engine/src/space.rs:159:9
2023-03-14T02:31:10.846704965Z    8: analytic_engine::instance::open::<impl analytic_engine::instance::Instance>::process_recover_table_command::{{closure}}
2023-03-14T02:31:10.846708301Z              at ceresdb/analytic_engine/src/instance/open.rs:200:9
2023-03-14T02:31:10.846711635Z       analytic_engine::instance::write_worker::WriteWorker::handle_recover_table::{{closure}}
2023-03-14T02:31:10.846714672Z              at ceresdb/analytic_engine/src/instance/write_worker.rs:828:13
2023-03-14T02:31:10.846717503Z       analytic_engine::instance::write_worker::WriteWorker::run::{{closure}}
2023-03-14T02:31:10.846720325Z              at ceresdb/analytic_engine/src/instance/write_worker.rs:762:51
2023-03-14T02:31:10.846723184Z    9: analytic_engine::instance::write_worker::WriteGroup::new::{{closure}}
2023-03-14T02:31:10.846725951Z              at ceresdb/analytic_engine/src/instance/write_worker.rs:619:29
2023-03-14T02:31:10.846729072Z   10: tokio::runtime::task::core::Core<T,S>::poll::{{closure}}
```

A similar failed case, I guess this is the steps cause this issue:
1. Restart server
2. Client insert table A with auto create table

Step 2 happens before server open table, then when step 1 open table A, it will panic at `assert`
```rs
    /// Insert table data into space memory state if the table is
    /// absent. For internal use only
    ///
    /// Panic if the table has already existed.
    pub(crate) fn insert_table(&self, table_data: TableDataRef) {
        let success = self
            .table_datas
            .write()
            .unwrap()
            .insert_if_absent(table_data);
        assert!(success);
    }
```

A quickfix is to disable write/read request before server open all shards.

```
ceresdb-server --version
CeresDB Server 
CeresDB version: 1.0.0
Git branch: main
Git commit: 95ea870
Build time: 2023-03-04T20:49:54.88119768Z
Rustc version: 1.69.0-nightly
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XdM34/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/561,https://api.github.com/repos/apache/horaedb/issues/561,horaedb,1529015693,561,HTTP API sql will truncate error message,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-01-11T13:13:22Z,2023-01-12T13:39:40Z,"### Describe this problem

When make SQL request via HTTP, the error message will be truncated.





### Steps to reproduce

```
curl -d '
select' 0:5446/sql
```
This request will return
```
{""code"":500,""message"":""Failed to handle request, err:Failed to parse sql, err:Invalid sql, sql:""}
```

And the server contains following error log

```
2023-01-11 21:09:05.401 ERRO [server/src/http.rs:152] Http service Failed to handle sql, err:Failed to parse sql, err:Invalid sql, sql:
select, err:sql parser error: Expected an expression:, found: EOF
```

### Expected behavior

Complete error message

### Additional Information

It seems this issue happen when sql begin with a newline, if make request like this, then error message is right

```
$ curl -d 'select' 0:5446/sql
{""code"":500,""message"":""Failed to handle request, err:Failed to parse sql, err:Invalid sql, sql:select, err:sql parser error: Expected an expression:, found: EOF""}
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/561/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/563,https://api.github.com/repos/apache/horaedb/issues/563,horaedb,1530207576,563,Physical plan optimize failed when time_bucket field is used in group by,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-01-12T07:01:32Z,2023-01-12T08:02:25Z,"### Describe this problem

Found this bug when test in my local env, server error log
```
2023-01-12 14:34:45.397 ERRO [server/src/http.rs:152] Http service Failed to handle sql, err:Failed to execute interpreter, query:
SELECT
    time_bucket (""PT1S"", timestamp) AS t,
    count(1)
FROM
    cpu
GROUP BY
    t
ORDER BY
    t
, err:Failed to execute select, err:Failed to execute logical plan, err:Failed to do physical optimization, err:DataFusion Failed to optimize physical plan, err:Error during planning: Coercion from [Utf8, Timestamp(Millisecond, None)] to the signature OneOf([Exact([Timestamp(Millisecond, None), Utf8]), Exact([Timestamp(Millisecond, None), Utf8, Utf8]), Exact([Timestamp(Millisecond, None), Utf8, Utf8, Utf8]), Exact([Timestamp(Millisecond, None), Utf8, Utf8, Utf8, Utf8])]) failed..
Backtrace:
 0 <snafu::backtrace_shim::Backtrace as snafu::GenerateBacktrace>::generate::h996ee016dfa35e37
   /ceresdb/ceresdb/query_engine/src/physical_optimizer/mod.rs:25
   <query_engine::physical_optimizer::DataFusionOptimize as snafu::IntoError<query_engine::physical_optimizer::Error>>::into_error::h9306985b8afeb6dd
   /ceresdb/ceresdb/query_engine/src/physical_optimizer/mod.rs:25
   <core::result::Result<T,E> as snafu::ResultExt<T,E>>::context::{{closure}}::he2518b2d5ea5bd63
   /root/.cargo/registry/src/mirrors.ustc.edu.cn-61ef6e0cd06fb9b8/snafu-0.6.10/src/lib.rs:318
   core::result::Result<T,E>::map_err::h23ba0d251c1d09eb
   /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/result.rs:855
   <core::result::Result<T,E> as snafu::ResultExt<T,E>>::context::h2bd53b23bcf18e12
   /root/.cargo/registry/src/mirrors.ustc.edu.cn-61ef6e0cd06fb9b8/snafu-0.6.10/src/lib.rs:318
   <query_engine::physical_optimizer::PhysicalOptimizerImpl as query_engine::physical_optimizer::PhysicalOptimizer>::optimize::{{closure}}::hfc9fa5a505ce3930
   /ceresdb/ceresdb/query_engine/src/physical_optimizer/mod.rs:62
   <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll::h4f43db1401e975b2
   /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/future/mod.rs:91
 1 <core::pin::Pin<P> as core::future::future::Future>::poll::h4431a0faf84ed4e2
   /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/future/future.rs:124
   query_engine::executor::optimize_plan::{{closure}}::h2e03f03faaa61315
   /ceresdb/ceresdb/query_engine/src/executor.rs:147
```

### Steps to reproduce

Execute following SQL

```sql
CREATE TABLE `cpu` (
  `timestamp` timestamp NOT NULL,
  `value` int,
  timestamp KEY (timestamp)) ENGINE=Analytic
WITH(
	 enable_ttl='false'
);


SELECT
    time_bucket (""PT1M"", timestamp) AS t,
    count(1)
FROM
    cpu
GROUP BY
    t
ORDER BY
    t
```

### Expected behavior

No Error

### Additional Information

It seems output type of time_bucket is wrong, change `time_bucket` to `date_trunc` works as expected

```sql
SELECT
    to_timestamp_millis(date_trunc(""minute"", timestamp)) AS t,
    count(1)
FROM
    cpu
GROUP BY
    t
ORDER BY
    t
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/563/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/563,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5SQEOM,horaedb,1379943308,563,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-01-12T08:01:59Z,2023-01-12T08:01:59Z,"Ooops, I pass wrong args order to time_bucket, SQL below is right

```sql
SELECT
    time_bucket (timestamp, ""PT1M"") AS t,
    count(1)
FROM
    cpu
GROUP BY
    t
ORDER BY
    t
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5SQEOM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/569,https://api.github.com/repos/apache/horaedb/issues/569,horaedb,1531910745,569,Format http return results to increase readability,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-01-13T08:32:05Z,2023-01-13T10:55:25Z,"### Describe This Problem

When I use the http service to get metadata, the returned result is as follows, which is not very readable.
```
curl --location --request POST 'http://127.0.0.1:8080/api/v1/getShardTables' \
--header 'Content-Type: application/json' \
--data-raw '{
    ""clusterName"":""defaultCluster"",
    ""nodeName"":""127.0.0.1:8832"",
    ""shardIDs"": [0,1,2,3,4,5,6,7]
}'

{
    ""status"": ""success"",
    ""data"": ""map[0:{Shard:{ID:0 Role:0 Version:0} Tables:[]} 1:{Shard:{ID:0 Role:0 Version:0} Tables:[]} 2:{Shard:{ID:0 Role:0 Version:0} Tables:[]} 3:{Shard:{ID:0 Role:0 Version:0} Tables:[]} 4:{Shard:{ID:4 Role:1 Version:2} Tables:[{ID:0 Name:demo SchemaID:0 SchemaName:public PartitionInfo:key:{partition_definitions:{name:\""0\""} partition_definitions:{name:\""1\""} partition_key:\""name\""}} {ID:1 Name:____demo_0 SchemaID:0 SchemaName:public PartitionInfo:key:{partition_definitions:{name:\""0\""} partition_definitions:{name:\""1\""} partition_key:\""name\""}}]} 5:{Shard:{ID:5 Role:1 Version:0} Tables:[]} 6:{Shard:{ID:6 Role:1 Version:0} Tables:[]} 7:{Shard:{ID:7 Role:1 Version:0} Tables:[]}]""
}
```

### Proposal

Use json to format http return results to increase readability.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/569/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/571,https://api.github.com/repos/apache/horaedb/issues/571,horaedb,1533681449,571,Delete `proto` dir and maintain pb in ceresdbproto,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-01-15T06:28:41Z,2023-02-14T15:48:22Z,"### Describe This Problem

Currently ceresdb `protobuf` is maintained in two places, one is https://github.com/CeresDB/ceresdbproto, and the other is https://github.com/CeresDB/ceresdb/tree/main/proto.
In the previous design, the `proto` dir in ceresdb ceresdb stored the pb used by the ceresdb server, but with more features developed, the `proto` and `ceresdbproto` would define the same elements, such as TableSchema and RequestHeader.

### Proposal

Delete `proto` dir and maintain pb in ceresdbproto.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/571/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/571,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5VO4kI,horaedb,1429965064,571,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2023-02-14T15:48:22Z,2023-02-14T15:48:22Z,Done,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5VO4kI/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/572,https://api.github.com/repos/apache/horaedb/issues/572,horaedb,1533685031,572,Support partition table opened in multi nodes in the case of non-distributed wal,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-01-15T06:45:28Z,2024-10-19T11:29:48Z,"### Describe This Problem

Now the partition table only supports opened in multi nodes when using distributed WAL. 
In order to support partition table opened in multi nodes in the case of non-distributed wal, we need to save the partition information in ceresemta.

### Proposal

- [ ] Store `PartitionInfo` in `ceresmeta` instead of `manifest`. https://github.com/CeresDB/ceresdb/pull/568
- [ ] Remove `PartitionInfo` in `manifest`.
- [ ] Open `PartitionTable` with PartitionInfo in `ceresmeta`.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/572/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/573,https://api.github.com/repos/apache/horaedb/issues/573,horaedb,1533707050,573,Table partition's design is contradictory,kamille126,122521912,,,CLOSED,2023-01-15T07:52:39Z,2024-10-19T11:14:50Z,"### Describe This Problem

As I see, we want to treat the `partitioned table` same as the `data table` like traditional standalone database.
But in `ceresmeta`, `data table` is not allowed for existing in multiple shards, and this is allowed for `partitioned table`(make sense by a hard code scheduled rule: two shards which has the same `partitioned table` can not be scheduled to the same node).
So, it make me confused: how we want to treat the `partitioned table` and `data table`, same or different?

### Proposal

I think we should make the target clear, and refactor according to our target.
1. treat them same
`Data table` can exist in multiple shards, too. And since we promote the concept `shard` to admin tables, we should admin the all tables following the level: node -> shard -> table, it makes no sense to admin the `partitioned table` directly according to nodes if we decide to treat them same.
+  In `ceresdb,` we add `shard` information in all the memory data structure for managing.
+ In `ceresmeta`, we try best to make the shards(has the `partitioned table`) no in the same nodes(but it is allowed to happen).
problems: because we need to write the `partitioned table`, we should sovle the multiple leaders problem in later development.

2. treat them different
`Partitioned table` is just a logical concept, it is not managed by the same way as the `data table` no matter in `ceresdb` or `ceresmeta`.
+ In `ceresdb,` we treat them different in the top level, for example, desgin a independent `catalog` for `Partitioned table`.
+ In `ceresmeta`, we don't schedule the `partitioned table` by the same way of the `data table`, new scheuling way is needed.
problems: the architecture is a bit messy.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/573/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/573,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ScDPc,horaedb,1383085020,573,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2023-01-15T08:14:07Z,2023-01-15T08:14:07Z,"Can we consider that `PartitionTable` is a logical concept, which is equivalent to a kind of calculation. 
We abstract a proxy role for stateless capabilities such as calculation, so PartitionTable can be scheduled on `proxy` nodes. 
The node where the actual data table resides is called `datanode`. 
`ceresmeta` maintains different logic for scheduling `proxy` and `datanode`.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ScDPc/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/573,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ScZx4,horaedb,1383177336,573,NA,kamille126,122521912,,,NA,2023-01-15T15:15:10Z,2023-01-15T15:15:10Z,"> Can we consider that `PartitionTable` is a logical concept, which is equivalent to a kind of calculation. We abstract a proxy role for stateless capabilities such as calculation, so PartitionTable can be scheduled on `proxy` nodes. The node where the actual data table resides is called `datanode`. `ceresmeta` maintains different logic for scheduling `proxy` and `datanode`.

Yep, I prefer the second way, too.
But should we place the table partition logic in `analytic_engine`?
We do it now for treating it a normal table, but some troubles has occurred:
+ place the `remote client` and `router` into `instance` that makes the initialization code hard to write.
+ reuse the table recovering logic that leads to the multiple manifest writing problem.

Maybe, we shouldn't place it in `analytic_engine` actually? As I see, if we feel strange to implement, there is a high probability that the design is bad.



","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ScZx4/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/577,https://api.github.com/repos/apache/horaedb/issues/577,horaedb,1535696295,577,Implement prometheus remote storage API,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-01-17T02:38:45Z,2023-01-30T08:38:46Z,"### Describe This Problem

Prometheus is a popular choice for monitoring, and it supports long-term storage via remote storage API. 

CeresDB can gain more popularity by integrating with Prometheus.

### Proposal

Implement remote storage API in CeresDB 

### Additional Context

- https://prometheus.io/docs/prometheus/latest/storage/#remote-storage-integrations
- https://crates.io/crates/prom-remote-api","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/577/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/579,https://api.github.com/repos/apache/horaedb/issues/579,horaedb,1535956638,579,Store snapshot of manifest to object store,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-01-17T08:16:14Z,2023-02-09T03:40:56Z,"### Describe This Problem

Currently, the snapshot and normal updates of manifest are appended to WAL, which leads to such problems:
- Complexity of processing the concurrent writing to WAL of snapshotting and normal updating;
- One giant snapshot may exceed the limit of the length of one WAL entry;

### Proposal

The basic idea of the proposal is simple: store the snapshot to the object store.

### Additional Context

This is a breaking change #402.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/579/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/579,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5UO9Xh,horaedb,1413207521,579,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-02-02T06:13:45Z,2023-02-02T06:13:45Z,"## Overview
Here is a diagram to describe the new storage of manifest combined with WAL and Object Storage
```
                      ┌─────────────────────────────────────────────────────────────────┐ 
                      │                                                                 │ 
                      │    ┌────────┐                                                   │ 
                      │    │    3   │  /manifest/snapshot/{space_id}/{table_id}/current │ 
┌──────┐              │    └────────┘                                                   │ 
│  4   │              │         │                                                       │ 
├──────┘              │         │                                                       │ 
   3   │─────────┐    │         ▼                                                       │ 
├ ─ ─ ─          │    │    ┌────────┐                                                   │ 
   2   │─────────┼────┼───▶│Snapshot│     /manifest/snapshot/{space_id}/{table_id}/3    │ 
├ ─ ─ ─          │    │    └────────┘                                                   │ 
   1   │─────────┘    │                                                                 │ 
└ ─ ─ ─               └─────────────────────────────────────────────────────────────────┘ 
                                                                                          
  WAL                                          Object Storage                             
                                                                                          
```

## Update
Just insert the new updates into the WAL.

## Snapshot
- Generate the latest snapshot with previous snapshot if any and updates after that snapshot;
- Store the latest snapshot to as the `/manifest/snapshot/{space_id}/{table_id}/{end_seq}` into Object Storage;
- Overwrite the `/manifest/snapshot/{space_id}/{table_id}/current` to map the current snapshot to the path of the latest snapshot;
- Delete the logs included by the latest snapshot and previous snapshot in the Object Storage, and it doesn't matter if any error ocurr in this stage.

## Recover
- Fetch the `/manifest/snapshot/{space_id}/{table_id}/current` file from Object Storage for the current snapshot file path;
- Fetch the current snapshot;
- Read the new logs after the current snapshot and combine the both two parts to generate the integrate meta data of the table;","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5UO9Xh/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/579,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5URHOx,horaedb,1413772209,579,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-02-02T13:46:56Z,2023-02-02T13:46:56Z,"> Overwrite the /manifest/snapshot/{space_id}/{table_id}/current to map the current snapshot to the path of the latest snapshot;

How will you ensure this operation is atomic?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5URHOx/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/579,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5UUtpO,horaedb,1414715982,579,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-02-03T02:39:50Z,2023-02-03T02:39:50Z,"> > Overwrite the /manifest/snapshot/{space_id}/{table_id}/current to map the current snapshot to the path of the latest snapshot;
> 
> How will you ensure this operation is atomic?

I guess the `ObjectStore` trait should ensure that operation to be atomic.

Currently, for the real object storage service, e.g. AWS3, Aliyun OSS, the consistency model guarantees the put/update is atomic. As for the current ""object store"" based local file system in CeresDB, the atomic put/update is actually not ensured yet, but I guess it should provide such guarantee to meet the requirements by the `ObjectStore` trait.

Reference: [S3 consistency model](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel)","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5UUtpO/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/589,https://api.github.com/repos/apache/horaedb/issues/589,horaedb,1560732388,589,More advanced techniques to read parquet files efficiently,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,OPEN,2023-01-28T07:17:06Z,2023-05-18T15:56:43Z,"### Describe This Problem

Usually the IO part of a query is the most time consuming, so reducing time spent on this would improve query latency quietly a lot.

In current implementation, we have already applies some tricks to optimize this, to name a few:
1. concurrent reads even for one file
2. min/max prune
3. custom bloom filter prune

There is an [awesome blog](https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/) written by @tustvold and @alamb introducing some more advanced techniques to further improve read speed, which is definitely a must-read for developer in Arrow ecosystem.


### Proposal

Explore ideas introduced in [Querying Parquet with Millisecond Latency](https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/), Some notable ideas are:
- Page prune
- Late materialization
- Decode optimization, especially dictionary encoding




### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/589/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/590,https://api.github.com/repos/apache/horaedb/issues/590,horaedb,1560777876,590,Refactor manifest module,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-01-28T09:14:36Z,2023-08-01T05:59:29Z,"### Describe This Problem

For current `manifest` implementation, following problems exist:
+ Complexity of snapshot generation, and problem led by giant wal entry #579 
+ Pull logs from storage repeatedly while building snapshot.


### Proposal

For solving above problem, we plan to refactor manifest module:

- [x] Store the snapshot in oss rather than putting it back to wal storage.
#579 by @ShiKaiWi 
- [x] #801

### Additional Context
This is a breaking change https://github.com/CeresDB/ceresdb/issues/402.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/590/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/592,https://api.github.com/repos/apache/horaedb/issues/592,horaedb,1561109864,592,panic in  `disk_cache::test::test_disk_cache_store_get_range`,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-01-29T03:40:04Z,2023-06-28T09:07:11Z,"### Describe this problem

When I run `codecov` I get the following error:
```
failures:
---- disk_cache::test::test_disk_cache_store_get_range stdout ----
thread 'main' panicked at 'range end out of bounds: 1 <= 0', /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/bytes-1.2.1/src/bytes.rs:261:9
stack backtrace:
   0: rust_begin_unwind
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/std/src/panicking.rs:584:5
   1: core::panicking::panic_fmt
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/panicking.rs:142:14
   2: <bytes::bytes::Bytes>::slice::<core::ops::range::Range<usize>>
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/bytes-1.2.1/src/bytes.rs:261:9
   3: <object_store::disk_cache::DiskCacheStore as object_store::ObjectStore>::get_range::{closure#0}
             at ./src/disk_cache.rs:459:26
   4: <core::future::from_generator::GenFuture<<object_store::disk_cache::DiskCacheStore as object_store::ObjectStore>::get_range::{closure#0}> as core::future::future::Future>::poll
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/future/mod.rs:91:19
   5: <core::pin::Pin<alloc::boxed::Box<dyn core::future::future::Future<Output = core::result::Result<bytes::bytes::Bytes, object_store::Error>> + core::marker::Send>> as core::future::future::Future>::poll
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/future/future.rs:124:9
   6: object_store::disk_cache::test::test_disk_cache_store_get_range::{closure#0}
             at ./src/disk_cache.rs:610:56
   7: <core::future::from_generator::GenFuture<object_store::disk_cache::test::test_disk_cache_store_get_range::{closure#0}> as core::future::future::Future>::poll
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/future/mod.rs:91:19
   8: <core::pin::Pin<&mut core::future::from_generator::GenFuture<object_store::disk_cache::test::test_disk_cache_store_get_range::{closure#0}>> as core::future::future::Future>::poll
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/future/future.rs:124:9
   9: <tokio::runtime::scheduler::current_thread::CoreGuard>::block_on::<core::pin::Pin<&mut core::future::from_generator::GenFuture<object_store::disk_cache::test::test_disk_cache_store_get_range::{closure#0}>>>::{closure#0}::{closure#0}::{closure#0}
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.24.1/src/runtime/scheduler/current_thread.rs:541:57
  10: tokio::runtime::coop::with_budget::<core::task::poll::Poll<()>, <tokio::runtime::scheduler::current_thread::CoreGuard>::block_on<core::pin::Pin<&mut core::future::from_generator::GenFuture<object_store::disk_cache::test::test_disk_cache_store_get_range::{closure#0}>>>::{closure#0}::{closure#0}::{closure#0}>
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.24.1/src/runtime/coop.rs:102:5
  11: tokio::runtime::coop::budget::<core::task::poll::Poll<()>, <tokio::runtime::scheduler::current_thread::CoreGuard>::block_on<core::pin::Pin<&mut core::future::from_generator::GenFuture<object_store::disk_cache::test::test_disk_cache_store_get_range::{closure#0}>>>::{closure#0}::{closure#0}::{closure#0}>
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.24.1/src/runtime/coop.rs:68:5
  12: <tokio::runtime::scheduler::current_thread::CoreGuard>::block_on::<core::pin::Pin<&mut core::future::from_generator::GenFuture<object_store::disk_cache::test::test_disk_cache_store_get_range::{closure#0}>>>::{closure#0}::{closure#0}
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.24.1/src/runtime/scheduler/current_thread.rs:541:25
  13: <tokio::runtime::scheduler::current_thread::Context>::enter::<core::task::poll::Poll<()>, <tokio::runtime::scheduler::current_thread::CoreGuard>::block_on<core::pin::Pin<&mut core::future::from_generator::GenFuture<object_store::disk_cache::test::test_disk_cache_store_get_range::{closure#0}>>>::{closure#0}::{closure#0}>
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.24.1/src/runtime/scheduler/current_thread.rs:350:19
  14: <tokio::runtime::scheduler::current_thread::CoreGuard>::block_on::<core::pin::Pin<&mut core::future::from_generator::GenFuture<object_store::disk_cache::test::test_disk_cache_store_get_range::{closure#0}>>>::{closure#0}
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.24.1/src/runtime/scheduler/current_thread.rs:540:36
  15: <tokio::runtime::scheduler::current_thread::CoreGuard>::enter::<<tokio::runtime::scheduler::current_thread::CoreGuard>::block_on<core::pin::Pin<&mut core::future::from_generator::GenFuture<object_store::disk_cache::test::test_disk_cache_store_get_range::{closure#0}>>>::{closure#0}, core::option::Option<()>>::{closure#0}
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.24.1/src/runtime/scheduler/current_thread.rs:615:57
  16: <tokio::macros::scoped_tls::ScopedKey<tokio::runtime::scheduler::current_thread::Context>>::set::<<tokio::runtime::scheduler::current_thread::CoreGuard>::enter<<tokio::runtime::scheduler::current_thread::CoreGuard>::block_on<core::pin::Pin<&mut core::future::from_generator::GenFuture<object_store::disk_cache::test::test_disk_cache_store_get_range::{closure#0}>>>::{closure#0}, core::option::Option<()>>::{closure#0}, (alloc::boxed::Box<tokio::runtime::scheduler::current_thread::Core>, core::option::Option<()>)>
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.24.1/src/macros/scoped_tls.rs:61:9
  17: <tokio::runtime::scheduler::current_thread::CoreGuard>::enter::<<tokio::runtime::scheduler::current_thread::CoreGuard>::block_on<core::pin::Pin<&mut core::future::from_generator::GenFuture<object_store::disk_cache::test::test_disk_cache_store_get_range::{closure#0}>>>::{closure#0}, core::option::Option<()>>
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.24.1/src/runtime/scheduler/current_thread.rs:615:27
  18: <tokio::runtime::scheduler::current_thread::CoreGuard>::block_on::<core::pin::Pin<&mut core::future::from_generator::GenFuture<object_store::disk_cache::test::test_disk_cache_store_get_range::{closure#0}>>>
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.24.1/src/runtime/scheduler/current_thread.rs:530:19
  19: <tokio::runtime::scheduler::current_thread::CurrentThread>::block_on::<core::future::from_generator::GenFuture<object_store::disk_cache::test::test_disk_cache_store_get_range::{closure#0}>>
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.24.1/src/runtime/scheduler/current_thread.rs:154:24
  20: <tokio::runtime::runtime::Runtime>::block_on::<core::future::from_generator::GenFuture<object_store::disk_cache::test::test_disk_cache_store_get_range::{closure#0}>>
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.24.1/src/runtime/runtime.rs:282:47
  21: object_store::disk_cache::test::test_disk_cache_store_get_range
             at ./src/disk_cache.rs:630:9
  22: object_store::disk_cache::test::test_disk_cache_store_get_range::{closure#0}
             at ./src/disk_cache.rs:584:11
  23: <object_store::disk_cache::test::test_disk_cache_store_get_range::{closure#0} as core::ops::function::FnOnce<()>>::call_once
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/ops/function.rs:248:5
  24: core::ops::function::FnOnce::call_once
             at /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/ops/function.rs:248:5
note: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.
```

### Steps to reproduce

Refer to https://github.com/CeresDB/ceresdb/actions/runs/4034840337/jobs/6936355253

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/592/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/592,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gBpbb,horaedb,1611044571,592,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-06-28T09:07:10Z,2023-06-28T09:07:10Z,"It may have been fixed, and if it reappears, the issue will reopen.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gBpbb/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/596,https://api.github.com/repos/apache/horaedb/issues/596,horaedb,1561989163,596,Remove unnecessary type conversion in MysqlWorker,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-01-30T08:45:26Z,2023-02-03T04:05:39Z,"### Describe This Problem

When I'm implement #578, I find query in MysqlWorker return type used for JSON, which is not necessary, and may cost more resources.

### Proposal

Remove this conversion
https://github.com/CeresDB/ceresdb/blob/43a70fb4fa3462eeb760ba5b671c708e20a22aa4/server/src/mysql/worker.rs#L119

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/596/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/597,https://api.github.com/repos/apache/horaedb/issues/597,horaedb,1563596729,597,Support create/drop database,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-01-31T03:44:50Z,2024-10-19T11:14:50Z,"### Describe This Problem

In current implementation, there is only one database(called `schema` in src), we should support database creation to allow one server holding multiple databases.

### Proposal

Support those SQL

```sql
create database NAME;
drop database NAME;
```

As for distributed deployment, ceresmeta need to distribute newly-created database to all servers.



### Additional Context

For distributed mode, the default database is created when server start up, it should first communicate with ceresmeta to check whether this database is already existing.

https://github.com/CeresDB/ceresmeta/issues/136","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/597/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/597,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5s0bCr,horaedb,1825681579,597,NA,jackbit,150449,Yacobus Reinhart,yacobus.reinhart@gmail.com,NA,2023-11-24T13:30:51Z,2023-11-24T13:30:51Z,"Hi, is it done? any documentation for it? how to implement it?

```
//->#go-sdk

client, err := ceresdb.NewClient(endpoint, ceresdb.Direct)

if err != nil {
  return err
}

resp, err := client.SQLQuery(context.Background(), ceresdb.SQLQueryRequest{
  Tables: []string{},
  SQL:    ""create database NAME;"",
})

```

is it correct? i got this error with those script:

```
err: add request ctx: no database selected, you can use database in client initial options or WriteRequest/SqlQueryRequest
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5s0bCr/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/597,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5s1A0r,horaedb,1825836331,597,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-11-24T15:40:28Z,2023-11-24T15:40:28Z,"Hi, this feature hasn't got implemented yet.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5s1A0r/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/605,https://api.github.com/repos/apache/horaedb/issues/605,horaedb,1567352982,605,Query stale data in overwritten mode when filter match the old one,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-02-02T05:34:03Z,2023-02-07T09:30:46Z,"### Describe this problem

Two records with a same primary key may exist in two ssts even in overwritten mode(we will do the dedup work while querying according to primary key).

In the following case, the overwritten logic mentioned above will fail. Assume that we have record definition with two fields in it : key(primary key), and is_deleted(a normal field, not tag, not primary key). And two record with the same key are flushed into two different sst in overwritten mode. 

Obviously the new one should overwrite the old one in the right logic, and we should never get the old one while querying. But we can still get the old one in theory, and that happened actually.
```

  ┌────────────────────────┐
  │        Storage         │
  │                        │            ┌─────────────────────────┐
  │   ┌─────────────────┐  │            │                         │
  │   │     SST1        │  │            │                         │
  │   │                 │  │            │                         │
  │   │  ┌────┬───────┐ │  │            │    Filter:              │
  │   │  │ pk │ false │ │  │            │   ┌────────────────┐    │
  │   │  └────┴───────┘ │  ◄────────────┼───┤ deleted = false│    │
  │   │                 │  │            │   └────────────────┘    │
  │   └─────────────────┘  │            │                         │
  │                        │            │                         │
  │   ┌─────────────────┐  │            │    Result:              │
  │   │      SST2       │  │            │   ┌────┬───────┐        │
  │   │                 │  ├────────────┼───► pk │ false │        │
  │   │  ┌────┬──────┐  │  │            │   └────┴───────┘        │
  │   │  │ pk │ true │  │  │            │                         │
  │   │  └────┴──────┘  │  │            │                         │
  │   │                 │  │            │                         │
  │   └─────────────────┘  │            │                         │
  │                        │            └─────────────────────────┘
  └────────────────────────┘
```   


### Steps to reproduce

I am trying to reproduce it in a simple way.

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/605/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/608,https://api.github.com/repos/apache/horaedb/issues/608,horaedb,1569304630,608,Use OpenDAL to access object store,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-02-03T06:36:44Z,2023-03-10T06:55:17Z,"### Describe This Problem

[OpenDAL](https://github.com/datafuselabs/opendal) is widely adopted in Rust community to access files in many backends, such as S3, OSS, etc...

By integrate with it, we can gain more features from community. 

### Proposal

Replace AliyunOSS implementation with OpenDAL
- https://github.com/CeresDB/ceresdb/blob/main/components/object_store/src/aliyun.rs#L93

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/608/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/608,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5VQ3no,horaedb,1430485480,608,NA,tustvold,1781103,Raphael Taylor-Davies,,NA,2023-02-14T22:40:55Z,2023-02-14T22:40:55Z,"FWIW DataFusion has first party support for [object_store](https://docs.rs/object_store/latest/object_store/), which is part of the Apache Arrow project. Just thought I would mention it","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5VQ3no/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/608,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5V5eb2,horaedb,1441130230,608,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-02-23T02:05:36Z,2023-02-23T02:05:36Z,"@tustvold We already use object_store in our codebase, opendal here is meant to implement object_store trait for aliyun OSS.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5V5eb2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/608,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5V5wOg,horaedb,1441203104,608,NA,tustvold,1781103,Raphael Taylor-Davies,,NA,2023-02-23T04:27:48Z,2023-02-23T04:27:48Z,I _think_ aliyun OSS might be supported by object_store natively - https://github.com/apache/arrow-rs/issues/2777,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5V5wOg/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/608,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5V6yBe,horaedb,1441472606,608,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-02-23T09:54:34Z,2023-02-23T09:54:34Z,"That sounds reasonable to me, we will try implement OSS directly in object_store.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5V6yBe/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/612,https://api.github.com/repos/apache/horaedb/issues/612,horaedb,1572055400,612,Move config wal_path under RocksDB of WalStorageConfig,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-02-06T07:21:34Z,2023-02-09T11:05:32Z,"### Describe This Problem

Currently, `wal_path` is in `analytic`, which is not reasonable, 

```
[analytic]
wal_path = ""/tmp/xx""
scan_batch_size = 100
```

### Proposal

Put this field in `wal_config` section.

```
[analytic.wal_storage]
type = ""RocksDB""

[analytic.wal_storage.rocksdb_config]
path = ""/tmp/xx""
```

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/612/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/616,https://api.github.com/repos/apache/horaedb/issues/616,horaedb,1573590642,616,Use column id instead of index to refer column when persist to storage,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-02-07T02:50:46Z,2023-03-02T13:36:33Z,"### Describe This Problem

When persist schema info, we use column index to refer corresponding column, this works fine if schema is not updated, and will block us to implement some DDL, such as add column after another.

### Proposal

Use column id to refer column when need to persist to underlying storage.

### Additional Context

AFAIK, primary keys/bloom filters in encoded using index now.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/616/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/616,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Up9bX,horaedb,1420285655,616,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-02-07T06:58:21Z,2023-02-07T06:58:21Z,"Assuming that **schema info** mentioned here is the one in the SST, I guess this is not necessary, because the existing sst won't be updated even if the schema is updated.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Up9bX/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/616,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5UrnT0,horaedb,1420719348,616,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-02-07T12:49:41Z,2023-02-07T12:49:41Z,"I grep TableSchema, schema is not only used inside SST, but also in meta_update.

```
7 candidates:
./common.proto:46:message TableSchema {
./common.proto:67:  common.TableSchema table_schema = 1;
./meta_update.proto:43:  common.TableSchema schema = 4;
./meta_update.proto:98:  common.TableSchema schema = 3;
./remote_engine.proto:64:  common.TableSchema table_schema = 1;
./sst.proto:34:  common.TableSchema schema = 5;
./table_requests.proto:14:  common.TableSchema schema = 2;
```
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5UrnT0/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/617,https://api.github.com/repos/apache/horaedb/issues/617,horaedb,1573722296,617,"When upgrade to datafusion 17, ""thetasketch_distinct"" test results in integration tests is different",chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-02-07T05:44:13Z,2024-10-19T11:14:50Z,"### Describe this problem

After #601 , `integration_tests/cases/local/02_function/thetasketch_distinct.sql` test results in integration tests is different.

### Steps to reproduce

cd integration_tests && make run


### Expected behavior

Old result in `integration_tests/cases/local/02_function/thetasketch_distinct.result` is
```
arch,thetasketch_distinct(02_function_thetasketch_distinct_table.value),
String(StringBytes(b""x86"")),Int64(115),
String(StringBytes(b""arm"")),Int64(114),
```
New result is 
```
arch,thetasketch_distinct(02_function_thetasketch_distinct_table.value),
String(StringBytes(b""x86"")),Int64(115),
String(StringBytes(b""arm"")),Int64(117),
```

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/617/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/620,https://api.github.com/repos/apache/horaedb/issues/620,horaedb,1574591048,620,General wal deletion model,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-02-07T16:00:26Z,2024-10-19T11:14:50Z,"### Describe This Problem

In fact, I want to refactor the whole wal module finally, reasons:
+ hard to implement master-follower model based on current wal implementation.
+ hard to introduce new component(such as HBase, HDFS...) as the wal storage base.
+ messy architecture, we can divide logics  to: wal and wal's storage, and all logic in abstract wal can be reused.

However, this is a massive work, I am impossible finish in short term. 
And I want to just refactor the `deletion part`(the hardest part in wal and in the general wal model in my mind) in `Kafka based WAL` as a start.

### Proposal

Steps in this deletion model can also be divided into two part: mark deleted and clean.

In mark deleted part:
+ we need to create an new `wal file` first (we will simulate this in Kafka) 
  + If something wrong,  we just ignore the error and still use the old file.
  + If every thing is ok , we switch to the new file.
+ mark the flushed file to the flushed table.

In clean part:
+ we just scan all table metadatas in the region, compare the flushed files of them and find the one with the smallest file number.
+ make snapshot of table metadatas first.
+ remove all files whose file nubmer less than the smallest file number then.

### Additional Context
The development project:
- [ ] impl page manager first.
- [ ] make use of the page manager.

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/620/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/620,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Xesr3,horaedb,1467665143,620,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2023-03-14T08:55:23Z,2023-03-14T08:55:23Z,"Hello, I want to challenge this seemingly difficult job. Is there anything I can do right now?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Xesr3/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/620,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XtaVX,horaedb,1471522135,620,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2023-03-16T08:34:53Z,2023-03-16T08:34:53Z,"> Hello, I want to challenge this seemingly difficult job. Is there anything I can do right now?

We haven't finished the plan because of the low priority of it.
In fact, the most annoying thing is how to make it compatible with the old wal module... because ceresdb has release the 1.0 version, the breaking changes are unacceptable in short term...

We are doing works about supporting influxql now, if you are interested welcome to join it. ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XtaVX/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/620,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Xtckg,horaedb,1471531296,620,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2023-03-16T08:42:47Z,2023-03-16T08:42:47Z,"Of course, I'm interested in this and will try to do some work after I implement #558 . ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Xtckg/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/622,https://api.github.com/repos/apache/horaedb/issues/622,horaedb,1575752559,622,Use ConfigOptions to transfer custom settings,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-02-08T09:12:00Z,2023-02-13T06:09:07Z,"### Describe This Problem

In datafusion, configOptions is used to store internal configuration and user configuration.
Remove `read_parallelism` in `ContextProviderAdapter`. 
Refer to https://github.com/CeresDB/ceresdb/blob/d6f5e774cadfaa2d73defe297881e11b0ce5236c/sql/src/provider.rs#L150

### Proposal

Remove `read_parallelism` in `ContextProviderAdapter`. 
Refer to https://github.com/CeresDB/ceresdb/blob/d6f5e774cadfaa2d73defe297881e11b0ce5236c/sql/src/provider.rs#L150

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/622/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/623,https://api.github.com/repos/apache/horaedb/issues/623,horaedb,1576234734,623,Optimize build speed for tools,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-02-08T14:35:03Z,2024-10-19T11:14:51Z,"### Describe This Problem

`tools` module currently depend on `analytic_engine`, which is a heavy module.

Ideally we could move sst related logic in a new module to get rid of `analytic_engine`

```
   Compiling common_util v1.0.0-alpha02 (/ceresdb/ceresdb/common_util)
   Compiling df_operator v1.0.0-alpha02 (/ceresdb/ceresdb/df_operator)
   Compiling parquet_ext v1.0.0-alpha02 (/ceresdb/ceresdb/components/parquet_ext)
   Compiling table_kv v1.0.0-alpha02 (/ceresdb/ceresdb/components/table_kv)
   Compiling message_queue v0.1.0 (/ceresdb/ceresdb/components/message_queue)
   Compiling object_store v1.0.0-alpha02 (/ceresdb/ceresdb/components/object_store)
   Compiling table_engine v1.0.0-alpha02 (/ceresdb/ceresdb/table_engine)
   Compiling wal v1.0.0-alpha02 (/ceresdb/ceresdb/wal)
   Compiling meta_client v1.0.0-alpha02 (/ceresdb/ceresdb/meta_client)
   Compiling cluster v1.0.0-alpha02 (/ceresdb/ceresdb/cluster)
   Compiling router v1.0.0-alpha02 (/ceresdb/ceresdb/router)
   Compiling remote_engine_client v1.0.0-alpha02 (/ceresdb/ceresdb/remote_engine_client)
   Compiling analytic_engine v1.0.0-alpha02 (/ceresdb/ceresdb/analytic_engine)
   Compiling tools v1.0.0-alpha02 (/ceresdb/ceresdb/tools)
    Finished release-slim [optimized] target(s) in 5m 51s

```

### Proposal

Extract sst related logic to a single module.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/623/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/623,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WKQq_,horaedb,1445530303,623,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2023-02-27T01:05:29Z,2023-02-27T01:05:29Z,Maybe I can finish this. Is the `sst` not dependent by the `tools`?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WKQq_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/623,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WKlVt,horaedb,1445614957,623,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-02-27T02:57:58Z,2023-02-27T02:57:58Z,"This module is a little messy now, I plan to do some polish work, maybe you can try other issue first?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WKlVt/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/623,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WKlg5,horaedb,1445615673,623,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2023-02-27T02:58:53Z,2023-02-27T02:58:53Z,ok,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WKlg5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/626,https://api.github.com/repos/apache/horaedb/issues/626,horaedb,1577281779,626,Too tired to create a generic error,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-02-09T05:46:38Z,2023-02-09T07:21:09Z,"### Describe This Problem

Now we create the generic error : Box<dyn std::error::Error + Send + Sync + 'static> by `map_err(|e| Box::new(e) as _)`.

It is too verbose and make me tired to write it again and again.

I want to make impl  a combinator for `Result` to finish it.

### Proposal

See above.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/626/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/628,https://api.github.com/repos/apache/horaedb/issues/628,horaedb,1577406577,628,Separate the customized index from the parquet file,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-02-09T07:49:48Z,2023-12-04T03:24:56Z,"### Describe This Problem

Currently, our customized bloom filter index are saved in the metadata of the parquet file, which leads to:
- decoding overhead when such index is unnecessary, e.g. scan for compaction;
- limit on indexing across multiple parquet files;



### Proposal

Separate the index from the parquet file and keep only a file path of the index in the parquet file.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/628/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/628,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tikrJ,horaedb,1837779657,628,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-12-04T03:24:55Z,2023-12-04T03:24:55Z,I guess some PR has address this.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tikrJ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/633,https://api.github.com/repos/apache/horaedb/issues/633,horaedb,1578884192,633,Support for promql queries,jiwliu,20149434,jin,,OPEN,2023-02-10T02:22:01Z,2023-02-10T03:20:39Z,"### Describe This Problem

It is hoped that ceresdb can support promql query, allowing users to select and aggregate time series data in real time

### Proposal

support for promql queries

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/633/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/633,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5U8VNy,horaedb,1425101682,633,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-02-10T02:47:58Z,2023-02-10T02:47:58Z,"We already support PromQL via its remote storage API.
- https://docs.ceresdb.io/ecosystem/prometheus.html","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5U8VNy/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/633,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5U8aB1,horaedb,1425121397,633,NA,jiwliu,20149434,jin,,NA,2023-02-10T03:20:38Z,2023-02-10T03:20:38Z,"> We already support PromQL via its remote storage API.
> 
> * https://docs.ceresdb.io/ecosystem/prometheus.html

It is best that ceresdb can support pql natively and can be used through sdk.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5U8aB1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/634,https://api.github.com/repos/apache/horaedb/issues/634,horaedb,1578896479,634,Supports automatic field creation,jiwliu,20149434,jin,,CLOSED,2023-02-10T02:41:35Z,2023-03-22T09:04:17Z,"### Describe This Problem

When ceresdb writes data for the first time, it supports automatic table creation, but when the data written next contains new fields, it does not support automatic creation of new fields. It is hoped that new fields can be automatically created in this scenario

### Proposal

Supports automatic field creation

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/634/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/634,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5U8YNM,horaedb,1425113932,634,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-02-10T03:06:39Z,2023-02-10T03:06:39Z,"This sounds a good feature. 

A pre-task of this feature is to support alter table schema, I will create another issue to track this.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5U8YNM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/635,https://api.github.com/repos/apache/horaedb/issues/635,horaedb,1578915507,635,Support multi-tenant,jiwliu,20149434,jin,,CLOSED,2023-02-10T03:09:43Z,2023-06-28T09:05:56Z,"### Describe This Problem

In the case of deploying a single cluster, support read and write isolation between different tenants

### Proposal

Support multi-tenant

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/635/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/635,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gBpBQ,horaedb,1611042896,635,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-06-28T09:05:55Z,2023-06-28T09:05:55Z,Tracked in https://github.com/CeresDB/ceresdb/issues/929,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gBpBQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/637,https://api.github.com/repos/apache/horaedb/issues/637,horaedb,1579394866,637,"Unsupported SQL data type, err:Unsupported SQL data type, type:VARBINARY.",jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-02-10T10:07:37Z,2023-02-17T06:53:00Z,"### Describe this problem

Create table with varbinary will throw error.
```
2023-02-10 18:04:16.977 ERRO [server/src/grpc/storage_service/mod.rs:257] Failed to handle request, mod:sql_query, handler:handle_query, err:Rpc error, code:500, message:Failed to create plan, qu
ery:CREATE TABLE IF NOT EXISTS ceresdb (                                                                                                                                                           
                str_tag string TAG,                                                                                                                                                                
                int_tag int32 TAG,                                                                                                                                                                 
                var_tag VARBINARY TAG,                                                                                                                                                             
                str_field string,                                                                                                                                                                  
                int_field int32,                                                                                                                                                                   
                bin_field string,                                                                                                                                                                  
                t timestamp NOT NULL,                                                                                                                                                              
                TIMESTAMP KEY(t)) ENGINE=Analytic with                                                                                                                                             
(enable_ttl='false'), cause:Failed to create plan, err:Unsupported SQL data type, err:Unsupported SQL data type, type:VARBINARY.                                                                   
Backtrace:                                                                                       
 0 <snafu::backtrace_shim::Backtrace as snafu::GenerateBacktrace>::generate::hae5493024171cea8                                                                                                     
   /home/chenxiang/.cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/backtrace_shim.rs:15
   common_types::datum::UnsupportedDataType<__T0>::build::hd328396a801bd9d0                                                                                                                        
   /home/chenxiang/ceresdb/common_types/src/datum.rs:14                                     
 1 common_types::datum::UnsupportedDataType<__T0>::fail::hf24c888d994be662                                                                                                                         
   /home/chenxiang/ceresdb/common_types/src/datum.rs:14                             
 2 <common_types::datum::DatumKind as core::convert::TryFrom<&sqlparser::ast::data_type::DataType>>::try_from::h52db0aa41eec82d4
   /home/chenxiang/ceresdb/common_types/src/datum.rs:235                                                                                                                                           
 3 sql::planner::parse_column::h61a0a174a1cff1ca                                                                                                                                                   
   /home/chenxiang/ceresdb/sql/src/planner.rs:951                                                                                                                                                  
 4 sql::planner::PlannerDelegate<P>::create_table_to_plan::{{closure}}::h0cc4f7e8384aa3d1                                                                                                          
   /home/chenxiang/ceresdb/sql/src/planner.rs:478                                                                                                                                                  
 5 core::iter::adapters::map::map_try_fold::{{closure}}::h4b9fbdbaf06092f6                                                                                                                         
   /rustc/d394408fb38c4de61f765a3ed5189d2731a1da91/library/core/src/iter/adapters/map.rs:91                                                                                                        
 6 core::iter::traits::iterator::Iterator::try_fold::hd566d40d047a9498 
```

### Steps to reproduce

See create table statement above

### Expected behavior

No error

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/637/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/638,https://api.github.com/repos/apache/horaedb/issues/638,horaedb,1581153845,638,How about using the struct defined in proto directly?,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-02-12T07:19:43Z,2023-02-14T02:17:47Z,"### Describe This Problem

When using the struct defined in proto, we will define a struct with same fields and convert the proto one to it.

It make sense before replacing the proto processing crate from `grpcio`'s to `tonic`'s, becasue it is hard to do this with the one in `grpcio`.

However, it is so convenient to do this now. 

### Proposal

Directly use the struct defined in proto.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/638/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/638,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5VEfnA,horaedb,1427241408,638,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-02-13T02:32:54Z,2023-02-13T02:32:54Z,"For simple case, I think we can re-export types defined in proto.

However, types defined in proto may not what we want in upper layer, for example 
```rs
pub space_id: SpaceId,
pub table_id: TableId,
```
We have type alias for table_id and space_id, but in proto those fields are all u64, so in this case a mapping is required.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5VEfnA/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/638,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5VGrhQ,horaedb,1427814480,638,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-02-13T11:54:45Z,2023-02-13T11:54:45Z,"Besides the problem mentioned by @jiacai2050, another dilemma is that the custom structs generated by `prost` are always wrapped in an `Option`, which may be very inconvenient for use.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5VGrhQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/638,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5VIf64,horaedb,1428291256,638,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2023-02-13T16:57:28Z,2023-02-13T16:57:28Z,"> Besides the problem mentioned by @jiacai2050, another dilemma is that the custom structs generated by `prost` are always wrapped in an `Option`, which may be very inconvenient for use.

Yes, the nested or oneof struct has this problem... ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5VIf64/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/647,https://api.github.com/repos/apache/horaedb/issues/647,horaedb,1584164013,647,Re-enable some clippy rules,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-02-14T13:23:49Z,2023-02-21T11:18:29Z,"### Describe This Problem

When doing #641, lots of clippy error arise, I disable some rules for quick dev. We should reenable those rules.

### Proposal

Remove ad-hoc clippy fix

```
clippy:
	cd $(DIR); cargo clippy --all-targets --all-features --workspace -- -D warnings
```

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/647/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/648,https://api.github.com/repos/apache/horaedb/issues/648,horaedb,1585158258,648,Support route by sql,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-02-15T03:39:16Z,2023-05-31T06:11:50Z,"### Describe This Problem

It can not build the auto integration test for cluster mode now, due to unable to get the table which is the routing base for test sql.

In fact, the user not so careful for performance will feel annoyed about filling the table name manually too.

### Proposal

Server parse sql before routing if found table name not exist in request.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/648/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/650,https://api.github.com/repos/apache/horaedb/issues/650,horaedb,1585709723,650,Flushed sequence number in manifest will be reset after compaction,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-02-15T11:42:30Z,2023-02-16T06:12:22Z,"### Describe this problem

After compaction, the flushed sequence number will be reset.

### Steps to reproduce

- Write some data to generate multiple sst files;
- Wait for a compaction;

After the compaction finished, the flushed sequence number of the relevant sst files in the manifest wil be reset 0.

### Expected behavior

The flushed sequence number of the new sst file should be set to the max one before the compaction.

### Additional Information
_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/650/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/654,https://api.github.com/repos/apache/horaedb/issues/654,horaedb,1587324268,654,Failed to write data through HTTP interface.,ZuLiangWang,11689031,CooooolFrog,coolfrog@apache.org,CLOSED,2023-02-16T09:21:45Z,2023-03-06T03:00:21Z,"### Describe this problem

I try to execute insert request by HTTP interface:
```
curl --location --request POST 'http://127.0.0.1:5440/sql' \
--data-raw '
INSERT INTO `02_function_thetasketch_distinct_table`
(`timestamp`, `arch`, `datacenter`, `value`)
VALUES
    (1658304762, 'x86', 'china', 75)
'
```
And it will throw an error:
```
{""code"":500,""message"":""Failed to handle request, err:Failed to create plan, query:\nINSERT INTO `02_function_thetasketch_distinct_table`\n(`timestamp`, `arch`, `datacenter`, `value`)\nVALUES\n    (1658304762, x86, china, 75)\n, err:Failed to create plan, err:Invalid insert stmt, source expr is not value, source_expr:Identifier(Ident { value: \""x86\"", quote_style: None }).""}%
```
But when I try to same sql though grpc request, it will work normally.
I guess there is a problem with the data parsing of the HTTP request.

### Steps to reproduce

curl --location --request POST 'http://127.0.0.1:5440/sql' \
--data-raw '
CREATE TABLE `02_function_thetasketch_distinct_table` (
    `timestamp` timestamp NOT NULL,
    `arch` string TAG,
    `datacenter` string TAG,
    `value` int,
    timestamp KEY (timestamp)) ENGINE=Analytic
WITH(
	 enable_ttl='false'
);
'

curl --location --request POST 'http://127.0.0.1:5440/sql' \
--data-raw '
INSERT INTO `02_function_thetasketch_distinct_table`
(`timestamp`, `arch`, `datacenter`, `value`)
VALUES
    (1658304762, 'x86', 'china', 75)
'

### Expected behavior

_No response_

### Additional Information
```
curl --location --request POST 'http://127.0.0.1:5440/sql' \
--data-raw '
INSERT INTO `02_function_thetasketch_distinct_table`
(`timestamp`, `arch`, `datacenter`, `value`)
VALUES
    (1658304762, ""x86"", ""china"", 75)
'
```
The string enclosed by double quotation marks can be written normally.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/654/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/654,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Vp3qZ,horaedb,1437039257,654,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-02-20T13:38:02Z,2023-02-20T13:38:02Z,"It seems you have used mixed single quote, try this

```curl
curl --location --request POST 'http://127.0.0.1:5440/sql' \
--data-raw '
INSERT INTO `02_function_thetasketch_distinct_table`
(`timestamp`, `arch`, `datacenter`, `value`)
VALUES
    (1658304762, ""x86"", ""china"", 75)
'
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Vp3qZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/662,https://api.github.com/repos/apache/horaedb/issues/662,horaedb,1596353890,662,Remove native-tls dependence,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-02-23T07:27:53Z,2023-03-09T10:24:09Z,"### Describe This Problem

native-tls is a heavy dep, which depend on openssl on local system, which may cause issue when build host and target host have different openssl version.
```
/usr/bin/ceresdb-server: error while loading shared libraries: libssl.so.3: cannot open shared object file: No such file or directory
```
### Proposal

Replace native-tls with [rustls](https://github.com/rustls/rustls)

### Additional Context

Using `cargo tree`, we can see which deps are relying on native-tls

```
$ cargo tree -i -p native-tls

native-tls v0.2.10
├── hyper-tls v0.3.2
│   └── reqwest v0.9.24
│       └── obkv-table-client-rs v0.1.0 (https://github.com/oceanbase/obkv-table-client-rs.git?rev=211e5718630577a7f8c1a2d74055bad4d31dea57#211e5718)
│           └── table_kv v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/components/table_kv)
│               ├── analytic_engine v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/analytic_engine)
│               │   ├── benchmarks v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/benchmarks)
│               │   ├── ceresdb v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB)
│               │   ├── server v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/server)
│               │   │   └── ceresdb v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB)
│               │   │   [dev-dependencies]
│               │   │   └── catalog_impls v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/catalog_impls)
│               │   │       └── ceresdb v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB)
│               │   │       [dev-dependencies]
│               │   │       └── interpreters v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/interpreters)
│               │   │           ├── ceresdb v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB)
│               │   │           └── server v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/server) (*)
│               │   └── tools v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/tools)
│               │   [dev-dependencies]
│               │   ├── catalog_impls v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/catalog_impls) (*)
│               │   └── interpreters v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/interpreters) (*)
│               ├── benchmarks v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/benchmarks)
│               └── wal v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/wal)
│                   ├── analytic_engine v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/analytic_engine) (*)
│                   └── benchmarks v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/benchmarks)
│                   [dev-dependencies]
│                   └── analytic_engine v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/analytic_engine) (*)
├── hyper-tls v0.5.0
│   └── reqwest v0.11.13
│       ├── meta_client v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/meta_client)
│       │   ├── catalog_impls v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/catalog_impls) (*)
│       │   ├── ceresdb v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB)
│       │   ├── cluster v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/cluster)
│       │   │   ├── catalog_impls v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/catalog_impls) (*)
│       │   │   ├── ceresdb v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB)
│       │   │   ├── router v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/router)
│       │   │   │   ├── analytic_engine v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/analytic_engine) (*)
│       │   │   │   ├── ceresdb v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB)
│       │   │   │   ├── remote_engine_client v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/remote_engine_client)
│       │   │   │   │   └── analytic_engine v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/analytic_engine) (*)
│       │   │   │   └── server v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/server) (*)
│       │   │   └── server v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/server) (*)
│       │   ├── interpreters v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/interpreters) (*)
│       │   ├── router v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/router) (*)
│       │   └── server v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/server) (*)
│       └── oss-rust-sdk v0.6.1
│           └── object_store v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/components/object_store)
│               ├── analytic_engine v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/analytic_engine) (*)
│               ├── benchmarks v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/benchmarks)
│               └── tools v1.0.0-alpha02 (/Users/jiacai/code/misc/CeresDB/tools)
├── reqwest v0.9.24 (*)
├── reqwest v0.11.13 (*)
└── tokio-native-tls v0.3.0
    ├── hyper-tls v0.5.0 (*)
    └── reqwest v0.11.13 (*)

```

Direct dep is reqwest, which already have rustls support via features. Similar changes:
- https://github.com/NoXF/oss-rust-sdk/pull/17","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/662/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/662,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WKmMB,horaedb,1445618433,662,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2023-02-27T03:02:19Z,2023-02-27T03:02:19Z,may i do this?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WKmMB/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/662,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WK2F2,horaedb,1445683574,662,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-02-27T04:19:36Z,2023-02-27T04:19:36Z,"Yeah, go ahead. 👍","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WK2F2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/662,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Wl9Tl,horaedb,1452791013,662,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-03T01:08:20Z,2023-03-03T01:08:20Z,"Just think of https://github.com/CeresDB/ceresdb/issues/63

I try cross compile it on my m1 macbook with this command:
```
cargo build --target aarch64-unknown-linux-gnu
```

The first error is openssl related

```bash
  run pkg_config fail: ""pkg-config has not been configured to support cross-compilation.\n\nInstall a sysroot for the target platform and configure it via\nPKG_CONFIG_SYSROOT_DIR and PKG_CONFIG_PATH, or install a\ncross-compiling wrapper for pkg-config and set it via\nPKG_CONFIG environment variable.""

  --- stderr
  thread 'main' panicked at '

  Could not find directory of OpenSSL installation, and this `-sys` crate cannot
  proceed without this knowledge. If OpenSSL is installed and this crate had
  trouble finding it,  you can set the `OPENSSL_DIR` environment variable for the
  compilation process.

  Make sure you also have the development packages of openssl installed.
  For example, `libssl-dev` on Ubuntu or `openssl-devel` on Fedora.

  If you're in a situation where you think the directory *should* be found
  automatically, please open a bug at https://github.com/sfackler/rust-openssl
  and include information about your system as well as this message.

  $HOST = aarch64-apple-darwin
  $TARGET = aarch64-unknown-linux-gnu
  openssl-sys = 0.9.75
```

So fix this issue is the first step to support cross compile.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Wl9Tl/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/662,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Wwec0,horaedb,1455548212,662,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-06T06:53:06Z,2023-03-06T06:53:06Z,"

I open https://github.com/oceanbase/obkv-table-client-rs/issues/12 to track reqwest in obkv-table-client-rs.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Wwec0/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/662,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Ww1gJ,horaedb,1455642633,662,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2023-03-06T07:56:44Z,2023-03-06T07:56:44Z,got,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Ww1gJ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/663,https://api.github.com/repos/apache/horaedb/issues/663,horaedb,1596472472,663,Support shard based table recovering,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-02-23T09:05:18Z,2023-03-31T12:17:16Z,"### Describe This Problem

We have supported shard based reading and writing in wal, the further target is to support shard based table recovering.

### Proposal

It may be a big work, and it should be split into targets:
+ Implement shard based manifest recovering.
+ Implement shard based tables recovering (problem is how we support single table's recovering after refactoring?)
+ Maybe refactor the wal module?

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/663/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/663,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Y67H_,horaedb,1491841535,663,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2023-03-31T12:17:12Z,2023-03-31T12:17:12Z,Deprecated.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Y67H_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/665,https://api.github.com/repos/apache/horaedb/issues/665,horaedb,1598271193,665,Simplify the logs for the compaction and flushing,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2023-02-24T09:41:16Z,2023-05-31T03:41:15Z,"### Describe This Problem

In my CeresDB server ,there  are more than 10k tables;
Sometimes, too many log entries for compaction and flushing make it difficult to locate other information.

```
2023-02-24 17:33:52.407 INFO [analytic_engine/src/instance/flush_compaction.rs:521] Instance flush memtables to output, table:MMM_1891000126_INFLUENCE_DEFAULT, table_id:3298534887502, request_id:3159191, mems_to_flush:FlushableMemTables { sampling_mem: None, memtables: [MemTableState { time_range: TimeRange { inclusive_start: Timestamp(1677225600000), exclusive_end: Timestamp(1677232800000) }, id: 25, last_sequence: 18096 }] }, files_to_level0:[AddFile { level: 0, file: FileMeta { id: 32, meta: SstMetaData { min_key: b""\x03\x80\0\x01\x86\x82\xc5\x0f0\x04\x16\xd8\xea\xd4&\x9dd\x86\xff\xff\xff\xff\xff\xff\xbbf\xff\xff\xff\xfd"", max_key: b""\x03\x80\0\x01\x86\x82\xc5\x90\x18\x04\xe3I\xd2+\xe4WnR\xff\xff\xff\xff\xff\xff\xb9a\xff\xff\xff\xcc"", time_range: TimeRange { inclusive_start: Timestamp(1677225600000), exclusive_end: Timestamp(1677232800000) }, max_sequence: 18096, schema: Schema { num_key_columns: 2, timestamp_index: 0, tsid_index: Some(1), enable_tsid_primary_key: true, column_schemas: ColumnSchemas { columns: [ColumnSchema { id: 1, name: ""period"", data_type: Timestamp, is_nullable: false, is_tag: false, comment: """", escaped_name: ""period"", default_value: None }, ColumnSchema { id: 2, name: ""tsid"", data_type: UInt64, is_nullable: false, is_tag: false, comment: """", escaped_name: ""tsid"", default_value: None }, ColumnSchema { id: 3, name: ""TraceId"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""TraceId"", default_value: None }, ColumnSchema { id: 4, name: ""_result"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""_result"", default_value: None }, ColumnSchema { id: 5, name: ""groupbyIndex0"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex0"", default_value: None }, ColumnSchema { id: 6, name: ""groupbyIndex1"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex1"", default_value: None }, ColumnSchema { id: 7, name: ""groupbyIndex2"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex2"", default_value: None }, ColumnSchema { id: 8, name: ""groupbyIndex3"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex3"", default_value: None }, ColumnSchema { id: 9, name: ""groupbyIndex4"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex4"", default_value: None }, ColumnSchema { id: 10, name: ""groupbyIndex5"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex5"", default_value: None }, ColumnSchema { id: 11, name: ""groupbyIndex6"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex6"", default_value: None }, ColumnSchema { id: 12, name: ""idc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""idc"", default_value: None }, ColumnSchema { id: 13, name: ""ldc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""ldc"", default_value: None }, ColumnSchema { id: 14, name: ""logSample"", data_type: String, is_nullable: true, is_tag: false, comment: """", escaped_name: ""logSample"", default_value: None }, ColumnSchema { id: 15, name: ""server"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""server"", default_value: None }] }, version: 1 }, size: 2050423, row_num: 20772, storage_format_opts: StorageFormatOptions { format: Columnar, collapsible_cols_idx: [] } } } }], flushed_sequence:18096
2023-02-24 17:33:52.407 INFO [analytic_engine/src/meta/details.rs:185] Manifest store update, update:VersionEdit(VersionEditMeta { space_id: 3, table_id: TableId(3298534887502), flushed_sequence: 18096, files_to_add: [AddFile { level: 0, file: FileMeta { id: 32, meta: SstMetaData { min_key: b""\x03\x80\0\x01\x86\x82\xc5\x0f0\x04\x16\xd8\xea\xd4&\x9dd\x86\xff\xff\xff\xff\xff\xff\xbbf\xff\xff\xff\xfd"", max_key: b""\x03\x80\0\x01\x86\x82\xc5\x90\x18\x04\xe3I\xd2+\xe4WnR\xff\xff\xff\xff\xff\xff\xb9a\xff\xff\xff\xcc"", time_range: TimeRange { inclusive_start: Timestamp(1677225600000), exclusive_end: Timestamp(1677232800000) }, max_sequence: 18096, schema: Schema { num_key_columns: 2, timestamp_index: 0, tsid_index: Some(1), enable_tsid_primary_key: true, column_schemas: ColumnSchemas { columns: [ColumnSchema { id: 1, name: ""period"", data_type: Timestamp, is_nullable: false, is_tag: false, comment: """", escaped_name: ""period"", default_value: None }, ColumnSchema { id: 2, name: ""tsid"", data_type: UInt64, is_nullable: false, is_tag: false, comment: """", escaped_name: ""tsid"", default_value: None }, ColumnSchema { id: 3, name: ""TraceId"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""TraceId"", default_value: None }, ColumnSchema { id: 4, name: ""_result"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""_result"", default_value: None }, ColumnSchema { id: 5, name: ""groupbyIndex0"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex0"", default_value: None }, ColumnSchema { id: 6, name: ""groupbyIndex1"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex1"", default_value: None }, ColumnSchema { id: 7, name: ""groupbyIndex2"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex2"", default_value: None }, ColumnSchema { id: 8, name: ""groupbyIndex3"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex3"", default_value: None }, ColumnSchema { id: 9, name: ""groupbyIndex4"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex4"", default_value: None }, ColumnSchema { id: 10, name: ""groupbyIndex5"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex5"", default_value: None }, ColumnSchema { id: 11, name: ""groupbyIndex6"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex6"", default_value: None }, ColumnSchema { id: 12, name: ""idc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""idc"", default_value: None }, ColumnSchema { id: 13, name: ""ldc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""ldc"", default_value: None }, ColumnSchema { id: 14, name: ""logSample"", data_type: String, is_nullable: true, is_tag: false, comment: """", escaped_name: ""logSample"", default_value: None }, ColumnSchema { id: 15, name: ""server"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""server"", default_value: None }] }, version: 1 }, size: 2050423, row_num: 20772, storage_format_opts: StorageFormatOptions { format: Columnar, collapsible_cols_idx: [] } } } }], files_to_delete: [] })
2023-02-24 17:33:52.425 INFO [analytic_engine/src/meta/details.rs:449] Store snapshot to region, region_id:3298534887502, snapshot_end_seq:256
2023-02-24 17:33:52.425 INFO [analytic_engine/src/instance/flush_compaction.rs:421] Instance flush memtables done, table:MMM_1891000126_INFLUENCE_DEFAULT, table_id:3298534887502, request_id:3159191
2023-02-24 17:33:52.426 INFO [analytic_engine/src/compaction/picker.rs:140] Compaction strategy: Default picker pick files to compact, input_files:CompactionInputFiles { level: 0, files: [FileHandle { meta: FileMeta { id: 31, meta: SstMetaData { min_key: b""\x03\x80\0\x01\x86\x82\xc4\xdch\x04\xee>*\xb4\xe3\xc5`\x8c\xff\xff\xff\xff\xff\xff\xbd\xd6\xff\xff\xff\xff"", max_key: b""\x03\x80\0\x01\x86\x82\xc5Q\x98\x04\xfdk\x17BZ1\x04\xde\xff\xff\xff\xff\xff\xff\xbc\xc0\xff\xff\xff\xda"", time_range: TimeRange { inclusive_start: Timestamp(1677225600000), exclusive_end: Timestamp(1677232800000) }, max_sequence: 17334, schema: Schema { num_key_columns: 2, timestamp_index: 0, tsid_index: Some(1), enable_tsid_primary_key: true, column_schemas: ColumnSchemas { columns: [ColumnSchema { id: 1, name: ""period"", data_type: Timestamp, is_nullable: false, is_tag: false, comment: """", escaped_name: ""period"", default_value: None }, ColumnSchema { id: 2, name: ""tsid"", data_type: UInt64, is_nullable: false, is_tag: false, comment: """", escaped_name: ""tsid"", default_value: None }, ColumnSchema { id: 3, name: ""TraceId"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""TraceId"", default_value: None }, ColumnSchema { id: 4, name: ""_result"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""_result"", default_value: None }, ColumnSchema { id: 5, name: ""groupbyIndex0"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex0"", default_value: None }, ColumnSchema { id: 6, name: ""groupbyIndex1"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex1"", default_value: None }, ColumnSchema { id: 7, name: ""groupbyIndex2"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex2"", default_value: None }, ColumnSchema { id: 8, name: ""groupbyIndex3"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex3"", default_value: None }, ColumnSchema { id: 9, name: ""groupbyIndex4"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex4"", default_value: None }, ColumnSchema { id: 10, name: ""groupbyIndex5"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex5"", default_value: None }, ColumnSchema { id: 11, name: ""groupbyIndex6"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex6"", default_value: None }, ColumnSchema { id: 12, name: ""idc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""idc"", default_value: None }, ColumnSchema { id: 13, name: ""ldc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""ldc"", default_value: None }, ColumnSchema { id: 14, name: ""logSample"", data_type: String, is_nullable: true, is_tag: false, comment: """", escaped_name: ""logSample"", default_value: None }, ColumnSchema { id: 15, name: ""server"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""server"", default_value: None }] }, version: 1 }, size: 2044367, row_num: 20786, storage_format_opts: StorageFormatOptions { format: Columnar, collapsible_cols_idx: [] } } }, being_compacted: false, metrics: SstMetrics { read_meter: 0.0, key_num: 0 } }, FileHandle { meta: FileMeta { id: 30, meta: SstMetaData { min_key: b""\x03\x80\0\x01\x86\x82\xc4\xb1p\x046-\xdeT|\x01d\xc8\xff\xff\xff\xff\xff\xff\xc2Q\xff\xff\xff\xff"", max_key: b""\x03\x80\0\x01\x86\x82\xc5*\x88\x04\xa3n\xc7\xec$\xf5(\x9f\xff\xff\xff\xff\xff\xff\xbff\xff\xff\xff\xcc"", time_range: TimeRange { inclusive_start: Timestamp(1677225600000), exclusive_end: Timestamp(1677232800000) }, max_sequence: 16568, schema: Schema { num_key_columns: 2, timestamp_index: 0, tsid_index: Some(1), enable_tsid_primary_key: true, column_schemas: ColumnSchemas { columns: [ColumnSchema { id: 1, name: ""period"", data_type: Timestamp, is_nullable: false, is_tag: false, comment: """", escaped_name: ""period"", default_value: None }, ColumnSchema { id: 2, name: ""tsid"", data_type: UInt64, is_nullable: false, is_tag: false, comment: """", escaped_name: ""tsid"", default_value: None }, ColumnSchema { id: 3, name: ""TraceId"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""TraceId"", default_value: None }, ColumnSchema { id: 4, name: ""_result"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""_result"", default_value: None }, ColumnSchema { id: 5, name: ""groupbyIndex0"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex0"", default_value: None }, ColumnSchema { id: 6, name: ""groupbyIndex1"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex1"", default_value: None }, ColumnSchema { id: 7, name: ""groupbyIndex2"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex2"", default_value: None }, ColumnSchema { id: 8, name: ""groupbyIndex3"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex3"", default_value: None }, ColumnSchema { id: 9, name: ""groupbyIndex4"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex4"", default_value: None }, ColumnSchema { id: 10, name: ""groupbyIndex5"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex5"", default_value: None }, ColumnSchema { id: 11, name: ""groupbyIndex6"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex6"", default_value: None }, ColumnSchema { id: 12, name: ""idc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""idc"", default_value: None }, ColumnSchema { id: 13, name: ""ldc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""ldc"", default_value: None }, ColumnSchema { id: 14, name: ""logSample"", data_type: String, is_nullable: true, is_tag: false, comment: """", escaped_name: ""logSample"", default_value: None }, ColumnSchema { id: 15, name: ""server"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""server"", default_value: None }] }, version: 1 }, size: 2048995, row_num: 20809, storage_format_opts: StorageFormatOptions { format: Columnar, collapsible_cols_idx: [] } } }, being_compacted: false, metrics: SstMetrics { read_meter: 0.0, key_num: 0 } }, FileHandle { meta: FileMeta { id: 32, meta: SstMetaData { min_key: b""\x03\x80\0\x01\x86\x82\xc5\x0f0\x04\x16\xd8\xea\xd4&\x9dd\x86\xff\xff\xff\xff\xff\xff\xbbf\xff\xff\xff\xfd"", max_key: b""\x03\x80\0\x01\x86\x82\xc5\x90\x18\x04\xe3I\xd2+\xe4WnR\xff\xff\xff\xff\xff\xff\xb9a\xff\xff\xff\xcc"", time_range: TimeRange { inclusive_start: Timestamp(1677225600000), exclusive_end: Timestamp(1677232800000) }, max_sequence: 18096, schema: Schema { num_key_columns: 2, timestamp_index: 0, tsid_index: Some(1), enable_tsid_primary_key: true, column_schemas: ColumnSchemas { columns: [ColumnSchema { id: 1, name: ""period"", data_type: Timestamp, is_nullable: false, is_tag: false, comment: """", escaped_name: ""period"", default_value: None }, ColumnSchema { id: 2, name: ""tsid"", data_type: UInt64, is_nullable: false, is_tag: false, comment: """", escaped_name: ""tsid"", default_value: None }, ColumnSchema { id: 3, name: ""TraceId"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""TraceId"", default_value: None }, ColumnSchema { id: 4, name: ""_result"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""_result"", default_value: None }, ColumnSchema { id: 5, name: ""groupbyIndex0"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex0"", default_value: None }, ColumnSchema { id: 6, name: ""groupbyIndex1"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex1"", default_value: None }, ColumnSchema { id: 7, name: ""groupbyIndex2"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex2"", default_value: None }, ColumnSchema { id: 8, name: ""groupbyIndex3"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex3"", default_value: None }, ColumnSchema { id: 9, name: ""groupbyIndex4"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex4"", default_value: None }, ColumnSchema { id: 10, name: ""groupbyIndex5"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex5"", default_value: None }, ColumnSchema { id: 11, name: ""groupbyIndex6"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex6"", default_value: None }, ColumnSchema { id: 12, name: ""idc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""idc"", default_value: None }, ColumnSchema { id: 13, name: ""ldc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""ldc"", default_value: None }, ColumnSchema { id: 14, name: ""logSample"", data_type: String, is_nullable: true, is_tag: false, comment: """", escaped_name: ""logSample"", default_value: None }, ColumnSchema { id: 15, name: ""server"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""server"", default_value: None }] }, version: 1 }, size: 2050423, row_num: 20772, storage_format_opts: StorageFormatOptions { format: Columnar, collapsible_cols_idx: [] } } }, being_compacted: false, metrics: SstMetrics { read_meter: 0.0, key_num: 0 } }, FileHandle { meta: FileMeta { id: 29, meta: SstMetaData { min_key: b""\x03\x80\0\x01\x86\x82\xc05\0\x04p\xea\xfe\x84\xdf\x1e\xcf\xc7\xff\xff\xff\xff\xff\xff\xfd\x9a\xff\xff\xff\xf3"", max_key: b""\x03\x80\0\x01\x86\x82\xc4\xfb\xa8\x04\xee\xec\xf6\xc3\xce\x85\x80\xad\xff\xff\xff\xff\xff\xff\xc2\x88\xff\xff\xff\xee"", time_range: TimeRange { inclusive_start: Timestamp(1677225600000), exclusive_end: Timestamp(1677232800000) }, max_sequence: 15768, schema: Schema { num_key_columns: 2, timestamp_index: 0, tsid_index: Some(1), enable_tsid_primary_key: true, column_schemas: ColumnSchemas { columns: [ColumnSchema { id: 1, name: ""period"", data_type: Timestamp, is_nullable: false, is_tag: false, comment: """", escaped_name: ""period"", default_value: None }, ColumnSchema { id: 2, name: ""tsid"", data_type: UInt64, is_nullable: false, is_tag: false, comment: """", escaped_name: ""tsid"", default_value: None }, ColumnSchema { id: 3, name: ""TraceId"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""TraceId"", default_value: None }, ColumnSchema { id: 4, name: ""_result"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""_result"", default_value: None }, ColumnSchema { id: 5, name: ""groupbyIndex0"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex0"", default_value: None }, ColumnSchema { id: 6, name: ""groupbyIndex1"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex1"", default_value: None }, ColumnSchema { id: 7, name: ""groupbyIndex2"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex2"", default_value: None }, ColumnSchema { id: 8, name: ""groupbyIndex3"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex3"", default_value: None }, ColumnSchema { id: 9, name: ""groupbyIndex4"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex4"", default_value: None }, ColumnSchema { id: 10, name: ""groupbyIndex5"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex5"", default_value: None }, ColumnSchema { id: 11, name: ""groupbyIndex6"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex6"", default_value: None }, ColumnSchema { id: 12, name: ""idc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""idc"", default_value: None }, ColumnSchema { id: 13, name: ""ldc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""ldc"", default_value: None }, ColumnSchema { id: 14, name: ""logSample"", data_type: String, is_nullable: true, is_tag: false, comment: """", escaped_name: ""logSample"", default_value: None }, ColumnSchema { id: 15, name: ""server"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""server"", default_value: None }] }, version: 1 }, size: 42788111, row_num: 437018, storage_format_opts: StorageFormatOptions { format: Columnar, collapsible_cols_idx: [] } } }, being_compacted: false, metrics: SstMetrics { read_meter: 0.0, key_num: 0 } }], output_level: 0 }
2023-02-24 17:33:52.426 INFO [analytic_engine/src/instance/flush_compaction.rs:865] Instance try to compact table, table:MMM_1891000126_INFLUENCE_DEFAULT, table_id:3298534887502, request_id:3159701, input_files:[FileHandle { meta: FileMeta { id: 31, meta: SstMetaData { min_key: b""\x03\x80\0\x01\x86\x82\xc4\xdch\x04\xee>*\xb4\xe3\xc5`\x8c\xff\xff\xff\xff\xff\xff\xbd\xd6\xff\xff\xff\xff"", max_key: b""\x03\x80\0\x01\x86\x82\xc5Q\x98\x04\xfdk\x17BZ1\x04\xde\xff\xff\xff\xff\xff\xff\xbc\xc0\xff\xff\xff\xda"", time_range: TimeRange { inclusive_start: Timestamp(1677225600000), exclusive_end: Timestamp(1677232800000) }, max_sequence: 17334, schema: Schema { num_key_columns: 2, timestamp_index: 0, tsid_index: Some(1), enable_tsid_primary_key: true, column_schemas: ColumnSchemas { columns: [ColumnSchema { id: 1, name: ""period"", data_type: Timestamp, is_nullable: false, is_tag: false, comment: """", escaped_name: ""period"", default_value: None }, ColumnSchema { id: 2, name: ""tsid"", data_type: UInt64, is_nullable: false, is_tag: false, comment: """", escaped_name: ""tsid"", default_value: None }, ColumnSchema { id: 3, name: ""TraceId"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""TraceId"", default_value: None }, ColumnSchema { id: 4, name: ""_result"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""_result"", default_value: None }, ColumnSchema { id: 5, name: ""groupbyIndex0"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex0"", default_value: None }, ColumnSchema { id: 6, name: ""groupbyIndex1"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex1"", default_value: None }, ColumnSchema { id: 7, name: ""groupbyIndex2"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex2"", default_value: None }, ColumnSchema { id: 8, name: ""groupbyIndex3"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex3"", default_value: None }, ColumnSchema { id: 9, name: ""groupbyIndex4"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex4"", default_value: None }, ColumnSchema { id: 10, name: ""groupbyIndex5"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex5"", default_value: None }, ColumnSchema { id: 11, name: ""groupbyIndex6"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex6"", default_value: None }, ColumnSchema { id: 12, name: ""idc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""idc"", default_value: None }, ColumnSchema { id: 13, name: ""ldc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""ldc"", default_value: None }, ColumnSchema { id: 14, name: ""logSample"", data_type: String, is_nullable: true, is_tag: false, comment: """", escaped_name: ""logSample"", default_value: None }, ColumnSchema { id: 15, name: ""server"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""server"", default_value: None }] }, version: 1 }, size: 2044367, row_num: 20786, storage_format_opts: StorageFormatOptions { format: Columnar, collapsible_cols_idx: [] } } }, being_compacted: true, metrics: SstMetrics { read_meter: 0.0, key_num: 0 } }, FileHandle { meta: FileMeta { id: 30, meta: SstMetaData { min_key: b""\x03\x80\0\x01\x86\x82\xc4\xb1p\x046-\xdeT|\x01d\xc8\xff\xff\xff\xff\xff\xff\xc2Q\xff\xff\xff\xff"", max_key: b""\x03\x80\0\x01\x86\x82\xc5*\x88\x04\xa3n\xc7\xec$\xf5(\x9f\xff\xff\xff\xff\xff\xff\xbff\xff\xff\xff\xcc"", time_range: TimeRange { inclusive_start: Timestamp(1677225600000), exclusive_end: Timestamp(1677232800000) }, max_sequence: 16568, schema: Schema { num_key_columns: 2, timestamp_index: 0, tsid_index: Some(1), enable_tsid_primary_key: true, column_schemas: ColumnSchemas { columns: [ColumnSchema { id: 1, name: ""period"", data_type: Timestamp, is_nullable: false, is_tag: false, comment: """", escaped_name: ""period"", default_value: None }, ColumnSchema { id: 2, name: ""tsid"", data_type: UInt64, is_nullable: false, is_tag: false, comment: """", escaped_name: ""tsid"", default_value: None }, ColumnSchema { id: 3, name: ""TraceId"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""TraceId"", default_value: None }, ColumnSchema { id: 4, name: ""_result"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""_result"", default_value: None }, ColumnSchema { id: 5, name: ""groupbyIndex0"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex0"", default_value: None }, ColumnSchema { id: 6, name: ""groupbyIndex1"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex1"", default_value: None }, ColumnSchema { id: 7, name: ""groupbyIndex2"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex2"", default_value: None }, ColumnSchema { id: 8, name: ""groupbyIndex3"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex3"", default_value: None }, ColumnSchema { id: 9, name: ""groupbyIndex4"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex4"", default_value: None }, ColumnSchema { id: 10, name: ""groupbyIndex5"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex5"", default_value: None }, ColumnSchema { id: 11, name: ""groupbyIndex6"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex6"", default_value: None }, ColumnSchema { id: 12, name: ""idc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""idc"", default_value: None }, ColumnSchema { id: 13, name: ""ldc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""ldc"", default_value: None }, ColumnSchema { id: 14, name: ""logSample"", data_type: String, is_nullable: true, is_tag: false, comment: """", escaped_name: ""logSample"", default_value: None }, ColumnSchema { id: 15, name: ""server"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""server"", default_value: None }] }, version: 1 }, size: 2048995, row_num: 20809, storage_format_opts: StorageFormatOptions { format: Columnar, collapsible_cols_idx: [] } } }, being_compacted: true, metrics: SstMetrics { read_meter: 0.0, key_num: 0 } }, FileHandle { meta: FileMeta { id: 32, meta: SstMetaData { min_key: b""\x03\x80\0\x01\x86\x82\xc5\x0f0\x04\x16\xd8\xea\xd4&\x9dd\x86\xff\xff\xff\xff\xff\xff\xbbf\xff\xff\xff\xfd"", max_key: b""\x03\x80\0\x01\x86\x82\xc5\x90\x18\x04\xe3I\xd2+\xe4WnR\xff\xff\xff\xff\xff\xff\xb9a\xff\xff\xff\xcc"", time_range: TimeRange { inclusive_start: Timestamp(1677225600000), exclusive_end: Timestamp(1677232800000) }, max_sequence: 18096, schema: Schema { num_key_columns: 2, timestamp_index: 0, tsid_index: Some(1), enable_tsid_primary_key: true, column_schemas: ColumnSchemas { columns: [ColumnSchema { id: 1, name: ""period"", data_type: Timestamp, is_nullable: false, is_tag: false, comment: """", escaped_name: ""period"", default_value: None }, ColumnSchema { id: 2, name: ""tsid"", data_type: UInt64, is_nullable: false, is_tag: false, comment: """", escaped_name: ""tsid"", default_value: None }, ColumnSchema { id: 3, name: ""TraceId"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""TraceId"", default_value: None }, ColumnSchema { id: 4, name: ""_result"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""_result"", default_value: None }, ColumnSchema { id: 5, name: ""groupbyIndex0"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex0"", default_value: None }, ColumnSchema { id: 6, name: ""groupbyIndex1"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex1"", default_value: None }, ColumnSchema { id: 7, name: ""groupbyIndex2"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex2"", default_value: None }, ColumnSchema { id: 8, name: ""groupbyIndex3"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex3"", default_value: None }, ColumnSchema { id: 9, name: ""groupbyIndex4"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex4"", default_value: None }, ColumnSchema { id: 10, name: ""groupbyIndex5"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex5"", default_value: None }, ColumnSchema { id: 11, name: ""groupbyIndex6"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex6"", default_value: None }, ColumnSchema { id: 12, name: ""idc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""idc"", default_value: None }, ColumnSchema { id: 13, name: ""ldc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""ldc"", default_value: None }, ColumnSchema { id: 14, name: ""logSample"", data_type: String, is_nullable: true, is_tag: false, comment: """", escaped_name: ""logSample"", default_value: None }, ColumnSchema { id: 15, name: ""server"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""server"", default_value: None }] }, version: 1 }, size: 2050423, row_num: 20772, storage_format_opts: StorageFormatOptions { format: Columnar, collapsible_cols_idx: [] } } }, being_compacted: true, metrics: SstMetrics { read_meter: 0.0, key_num: 0 } }, FileHandle { meta: FileMeta { id: 29, meta: SstMetaData { min_key: b""\x03\x80\0\x01\x86\x82\xc05\0\x04p\xea\xfe\x84\xdf\x1e\xcf\xc7\xff\xff\xff\xff\xff\xff\xfd\x9a\xff\xff\xff\xf3"", max_key: b""\x03\x80\0\x01\x86\x82\xc4\xfb\xa8\x04\xee\xec\xf6\xc3\xce\x85\x80\xad\xff\xff\xff\xff\xff\xff\xc2\x88\xff\xff\xff\xee"", time_range: TimeRange { inclusive_start: Timestamp(1677225600000), exclusive_end: Timestamp(1677232800000) }, max_sequence: 15768, schema: Schema { num_key_columns: 2, timestamp_index: 0, tsid_index: Some(1), enable_tsid_primary_key: true, column_schemas: ColumnSchemas { columns: [ColumnSchema { id: 1, name: ""period"", data_type: Timestamp, is_nullable: false, is_tag: false, comment: """", escaped_name: ""period"", default_value: None }, ColumnSchema { id: 2, name: ""tsid"", data_type: UInt64, is_nullable: false, is_tag: false, comment: """", escaped_name: ""tsid"", default_value: None }, ColumnSchema { id: 3, name: ""TraceId"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""TraceId"", default_value: None }, ColumnSchema { id: 4, name: ""_result"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""_result"", default_value: None }, ColumnSchema { id: 5, name: ""groupbyIndex0"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex0"", default_value: None }, ColumnSchema { id: 6, name: ""groupbyIndex1"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex1"", default_value: None }, ColumnSchema { id: 7, name: ""groupbyIndex2"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex2"", default_value: None }, ColumnSchema { id: 8, name: ""groupbyIndex3"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex3"", default_value: None }, ColumnSchema { id: 9, name: ""groupbyIndex4"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex4"", default_value: None }, ColumnSchema { id: 10, name: ""groupbyIndex5"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex5"", default_value: None }, ColumnSchema { id: 11, name: ""groupbyIndex6"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""groupbyIndex6"", default_value: None }, ColumnSchema { id: 12, name: ""idc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""idc"", default_value: None }, ColumnSchema { id: 13, name: ""ldc"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""ldc"", default_value: None }, ColumnSchema { id: 14, name: ""logSample"", data_type: String, is_nullable: true, is_tag: false, comment: """", escaped_name: ""logSample"", default_value: None }, ColumnSchema { id: 15, name: ""server"", data_type: String, is_nullable: true, is_tag: true, comment: """", escaped_name: ""server"", default_value: None }] }, version: 1 }, size: 42788111, row_num: 437018, storage_format_opts: StorageFormatOptions { format: Columnar, collapsible_cols_idx: [] } } }, being_compacted: true, metrics: SstMetrics { read_meter: 0.0, key_num: 0 } }]
```

### Proposal

simplify the compaction and flushing log. Maybe we can change the log mode to `debug`;

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/665/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/665,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5di-aZ,horaedb,1569449625,665,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-05-31T03:41:14Z,2023-05-31T03:41:14Z,"Much of this log is simplified, feel free to reopen when this issue still exists.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5di-aZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/670,https://api.github.com/repos/apache/horaedb/issues/670,horaedb,1600718808,670,Timeout expired when creating table for the first time,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2023-02-27T08:33:26Z,2024-10-19T11:17:42Z,"### Describe this problem

Error Occurred When Creating the Table for the first time:
```
{""code"":500,""message"":""Failed to handle request, err:Failed to execute interpreter, query:CREATE TABLE `demo4` ( `the_datetime` timestamp NOT NULL, `ext_props` string, `factor` string TAG, `gmt_create` timestamp, `gmt_modified` timestamp, `is_dfd_del` bigint, `period` string TAG, `symbol` string TAG, `value` double, TIMESTAMP KEY(the_datetime)) ENGINE=Analytic WITH(arena_block_size='1024', compaction_strategy='default', compression='ZSTD', enable_ttl='false', num_rows_per_row_group='8192', segment_duration='', storage_format='COLUMNAR', ttl='7d', update_mode='OVERWRITE', write_buffer_size='335544320'), err:Failed to execute create table, err:Failed to create table by table manipulator, err:Failed to create table, msg:failed to create table by meta client, req:CreateTableRequest { schema_name: \""public\"", name: \""demo4\"", encoded_schema: [0, 10, 10, 10, 4, 116, 115, 105, 100, 16, 5, 32, 1, 10, 18, 10, 12, 116, 104, 101, 95, 100, 97, 116, 101, 116, 105, 109, 101, 16, 1, 32, 2, 10, 17, 10, 9, 101, 120, 116, 95, 112, 114, 111, 112, 115, 16, 4, 24, 1, 32, 3, 10, 16, 10, 6, 102, 97, 99, 116, 111, 114, 16, 4, 24, 1, 32, 4, 40, 1, 10, 18, 10, 10, 103, 109, 116, 95, 99, 114, 101, 97, 116, 101, 16, 1, 24, 1, 32, 5, 10, 20, 10, 12, 103, 109, 116, 95, 109, 111, 100, 105, 102, 105, 101, 100, 16, 1, 24, 1, 32, 6, 10, 18, 10, 10, 105, 115, 95, 100, 102, 100, 95, 100, 101, 108, 16, 7, 24, 1, 32, 7, 10, 16, 10, 6, 112, 101, 114, 105, 111, 100, 16, 4, 24, 1, 32, 8, 40, 1, 10, 16, 10, 6, 115, 121, 109, 98, 111, 108, 16, 4, 24, 1, 32, 9, 40, 1, 10, 13, 10, 5, 118, 97, 108, 117, 101, 16, 2, 24, 1, 32, 10, 16, 1, 24, 2, 34, 2, 1, 2], engine: \""Analytic\"", create_if_not_exist: false, options: {\""write_buffer_size\"": \""335544320\"", \""arena_block_size\"": \""1024\"", \""enable_ttl\"": \""false\"", \""update_mode\"": \""OVERWRITE\"", \""ttl\"": \""7d\"", \""segment_duration\"": \""\"", \""compression\"": \""ZSTD\"", \""compaction_strategy\"": \""default\"", \""num_rows_per_row_group\"": \""8192\"", \""storage_format\"": \""COLUMNAR\""}, partition_table_info: None }, err:Failed to alloc table id, err:status: Cancelled, message: \""Timeout expired\"", details: [], metadata: MetadataMap { headers: {} }""}
```

### Steps to reproduce

My env is:
shard_num: 512
wal: Oceanbase
storage: Aliyun Oss
version info:
```
CeresDB Version: 1.0.0-alpha02
Git branch: main
Git commit: 9e06dcc
Build: 2023-02-24T09:49:37.459728854+00:00
```

Steps to reproduce the error:
1. I install and init cluster;
2. Create a table
```
curl --location --request POST 'http://127.0.0.1:5000/sql' \
-d'
CREATE TABLE `MMM_demo` (
    `name` string TAG,
    `value` double NOT NULL,
    `t` timestamp NOT NULL,
    timestamp KEY (t))
ENGINE=Analytic
  with
(enable_ttl=""false"")
'
```

### Expected behavior

Create table successfully.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/670/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/670,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WbvXn,horaedb,1450112487,670,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-01T12:58:32Z,2023-03-01T12:58:32Z,"This failure seems caused by `CeresMeta`, @ZuLiangWang can help troubleshoot this problem.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WbvXn/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/675,https://api.github.com/repos/apache/horaedb/issues/675,horaedb,1604110144,675,Make integration tests easier to run locally,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-03-01T02:20:03Z,2023-03-20T11:31:38Z,"### Describe This Problem

Running integration tests locally requires too many preconditions.
* Add ceresdb*ceresmeta in `/etc/hosts`.
* Install docker compose.
* Build docker image.
* Modify docker compose config.
* Configuration files are scattered in several places.

### Proposal

Complete manual operations in one step with scripts.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/675/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/675,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WmHwf,horaedb,1452833823,675,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-03T02:04:32Z,2023-03-03T02:04:32Z,"Another choice is for integration test to allow run local(or cluster) test only.
- https://github.com/CeresDB/sqlness/issues/46","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WmHwf/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/675,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5X-uvi,horaedb,1476062178,675,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2023-03-20T11:31:37Z,2023-03-20T11:31:37Z,#740 ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5X-uvi/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/681,https://api.github.com/repos/apache/horaedb/issues/681,horaedb,1604967642,681,Tracking issue: Introduce proxy module,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-03-01T13:21:14Z,2023-05-18T08:08:29Z,"### Describe This Problem

Currently, it is just a simple module called `forwarder` that forwards the sql query to the right server, but more work can't be handled by the `forwarder`, including:
- Forward according to the sql rather than the table hints in the sql query;
- Forward all the requests besides the sql query;
- Able to finish query/write about the partitioned table;

### Proposal

Design a proxy module which can serve as a separate service or a module in the ceresdb-server.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/681/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/681,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Wf0k2,horaedb,1451182390,681,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2023-03-02T02:31:34Z,2023-03-02T02:31:34Z,Support automatic table creation in proxy.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Wf0k2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/681,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WmHXt,horaedb,1452832237,681,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-03T02:02:51Z,2023-03-03T02:02:51Z,"> Forward according to the sql rather than the table hints in the sql query;

SQL may not enough, other query language should also be supported.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WmHXt/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/681,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XAQPZ,horaedb,1459684313,681,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2023-03-08T07:56:08Z,2023-03-08T07:56:08Z,"I am going to introduce the proxy module, and all protocol layers request the proxy module. The proxy module supports `Forward`, `Auth`, `PartitionTable` query.

<img width=""732"" alt=""image"" src=""https://user-images.githubusercontent.com/15178480/223653720-d92e0992-a61b-457f-b58f-25254fdc1964.png"">
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XAQPZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/681,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XARM6,horaedb,1459688250,681,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2023-03-08T07:59:42Z,2023-03-08T07:59:42Z,"I split the work into the following tasks:

- [x] The proxy module first supports storage service. #732 
- [x] Integrate forward in proxy. #732 
- [x] Support forward write request in grpc service.  #844 
- [x] The proxy module supports prom service(including forward query). #833 
- [x] The proxy module supports influxdb service. #889
- [x] The proxy module supports mysql service. #896
- [x] The proxy module supports http service(including forward query). #807 
- [x] Support query partition table in grpc query. #802 
- [x] Support write into partition table in grpc write. #828 
- [x] Support query partition table in http query. #857 
- [x] Support query partition table in http write. #857 
- [x] Support query partition table in prom query. #857 
- [x] Support write into partition table in prom write. #889
- [x] Support query partition table in influxdb query. #896
- [x] Support write into partition table in influxdb write. #889","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XARM6/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/682,https://api.github.com/repos/apache/horaedb/issues/682,horaedb,1604986714,682,Tracking issue: InfluxQL support,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-03-01T13:34:28Z,2023-03-09T05:41:15Z,"### Describe This Problem

InfluxQL is widely used in the timeseries DBMS field, and CeresDB should support such a protocol for integration into the ecosystem.

### Proposal

The work includes three parts:
- Write: the parser of line protocol;
- Query: 
  - The parser for InfluxQL;
  - The planner for InfluxQL;

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/682/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/682,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WcIrh,horaedb,1450216161,682,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2023-03-01T14:10:46Z,2023-03-01T14:10:46Z,"Works of query part may can be divided into three part:
+ Support statements compatible with sql statments (may be need some rewrites)
   - Wildcard and regrex in projection, from clause, group by clause.
   - From multiple tables.
+ Support statements can be represent by `udf`
    Selector aggregate function.
   + Some special udf.
+ Support statemetns only can be represent by `extension logical node`
   + Some special clause, such as slimit clause, soffset clause, fill clause.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WcIrh/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/682,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XGfwc,horaedb,1461320732,682,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-09T05:41:06Z,2023-03-09T05:41:06Z,In favor of #698 ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XGfwc/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/685,https://api.github.com/repos/apache/horaedb/issues/685,horaedb,1606045900,685,add gRPC integration test,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-03-02T03:28:24Z,2023-03-30T11:32:23Z,"### Describe This Problem

Current integration test is mainly for SQL compatibility of server, there is no test for gRPC communication between server and SDK.

### Proposal

Add integration test for different SDK.

Basic cases should include:
- Create Table (Auto create table)
- Write
- SQL Query
- Delete table

- [x] Go
- [x] Rust
- [x] Java
- [ ] Python

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/685/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/685,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XHhcR,horaedb,1461589777,685,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-09T08:51:49Z,2023-03-09T08:51:49Z,"I guess there is no need to build test for rust sdk, because current integration tests is using this sdk.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XHhcR/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/685,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XYONF,horaedb,1465967429,685,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-13T11:24:22Z,2023-03-13T11:24:22Z,"> I guess there is no need to build test for rust sdk, because current integration tests is using this sdk.

I'm afraid existing integration tests is not suitable for this issue, mainly because integration tests only use `query` API of gRPC, `write` is not covered.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XYONF/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/685,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Y0cd1,horaedb,1490143093,685,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-30T11:32:23Z,2023-03-30T11:32:23Z,"Since python sdk is based on rust, so it’s not required.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Y0cd1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/688,https://api.github.com/repos/apache/horaedb/issues/688,horaedb,1606274027,688,Add downsapmling for ceresdb,chenmudu,36784140,Chen,chenmudu@gmail.com,OPEN,2023-03-02T07:47:18Z,2023-03-06T02:30:02Z,"### Describe This Problem

At this stage, downsampling has become one of the basic capabilities in the field of time-series databases, which can solve problems such as excessive data volume returns under certain special circumstances (large time interval, high data set density).

CeresDB should support such a capability, at least in the immediate plans.


### Proposal

The engine layer supports downsampling, and supports dynamic downsampling and configuration downsampling. Matching, clients in each language should also support such an API.

Notice：
- dynamic downsampling：Support can automatically adjust the sampling granularity according to the time interval.
- configuration downsampling：Rely on the user to provide the accuracy of the downsampling.


### Additional Context

[OpenTSDB Downsampling](http://opentsdb.net/docs/build/html/user_guide/query/downsampling.html)","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/688/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/688,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Wg7Fy,horaedb,1451471218,688,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-02T08:20:27Z,2023-03-02T08:20:27Z,"Thanks for suggestion.

Downsample is indeed important for time-series user-case, to support downsample an [RFC](https://github.com/CeresDB/ceresdb/blob/main/docs/rfcs/YYYYmmDD-template.md) maybe required to discuss 
- pros and cons of downsample
- when query will use downsampled data
- how to run downsample job(in main server or some other background components).
- ....","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Wg7Fy/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/688,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WhZWv,horaedb,1451595183,688,NA,chenmudu,36784140,Chen,chenmudu@gmail.com,NA,2023-03-02T09:55:07Z,2023-03-02T09:55:07Z,"> * pros and cons of downsample

In most people's opinion, downsampling should have no disadvantages for time series data. Maybe I'm missing something, can anyone add here.

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5WhZWv/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/693,https://api.github.com/repos/apache/horaedb/issues/693,horaedb,1607997931,693,Http debug api for showing the config used by server,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-03-03T06:00:35Z,2023-03-15T12:46:59Z,"### Describe This Problem

Now, the config used by ceresdb server is only logged when start, and it is a little difficult to find out the config content when server runs for a long time.

### Proposal

Support a new http api whose url is  `/debug/config` to show the config content when running.

### Additional Context

The implementation can refer to the  implementation of `/debug/log_level`.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/693/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/696,https://api.github.com/repos/apache/horaedb/issues/696,horaedb,1608121900,696,Collect scan metrics and feed them into the `datafusion` analyze framework,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-03-03T07:51:02Z,2023-03-14T12:59:14Z,"### Describe This Problem

By `datafusion` analyze utilities, we can use `explain analyze [sql]` to retrieve a description for the costs of every execution stage of a sql, e.g.:

```
""CoalescePartitionsExec, metrics=[output_rows=1, elapsed_compute=21.596µs, spill_count=0, spilled_bytes=0, mem_used=0]
  ProjectionExec: expr=[tsid@0 as tsid, opdate@1 as opdate, f_info_delistdate@2 as f_info_delistdate, f_info_corp_fundmanagementid@3 as f_info_corp_fundmanagementid, f_sales_service_rate@4 as f_sales_service_rate, f_info_custodianbankid@5 as f_info_custodianbankid, max_num_holder@6 as max_num_holder, f_investment_area@7 as f_investment_area, max_num_coltarget@8 as max_num_coltarget, investstrategy@9 as investstrategy, close_institu_oef_down@10 as close_institu_oef_down, risk_return@11 as risk_return, f_pchredm_pchminamt@12 as f_pchredm_pchminamt, close_institu_oef_up@13 as close_institu_oef_up, f_pchredm_pchminamt_ex@14 as f_pchredm_pchminamt_ex, f_info_investobject@15 as f_info_investobject, s_fellow_distor@16 as s_fellow_distor, f_info_investconception@17 as f_info_investconception, f_info_trustee@18 as f_info_trustee, f_info_firstinvesttype@19 as f_info_firstinvesttype, f_info_decision_basis@20 as f_info_decision_basis, f_personal_subtype@21 as f_personal_subtype, is_indexfund@22 as is_indexfund, f_pchredm_pchstartdate@23 as f_pchredm_pchstartdate, f_info_setupdate@24 as f_info_setupdate, f_info_type@25 as f_info_type, f_info_redmstartdate@26 as f_info_redmstartdate, f_info_maturitydate@27 as f_info_maturitydate, close_institu_subtype@28 as close_institu_subtype, f_info_isinitial@29 as f_info_isinitial, f_info_minbuyamount@30 as f_info_minbuyamount, f_info_pinyin@31 as f_info_pinyin, f_issue_totalunit@32 as f_issue_totalunit, f_info_investscope@33 as f_info_investscope, f_issue_oef_startdateinst@34 as f_issue_oef_startdateinst, f_info_managementfeeratio@35 as f_info_managementfeeratio, f_info_registrant@36 as f_info_registrant, opmode@37 as opmode, f_issue_oef_dnddateinst@38 as f_issue_oef_dnddateinst, f_personal_startdateind@39 as f_personal_startdateind, f_info_custodianfeeratio@40 as f_info_custodianfeeratio, f_personal_enddateind@41 as f_personal_enddateind, f_info_parvalue@42 as f_info_parvalue, f_info_fund_id@43 as f_info_fund_id, crny_code@44 as crny_code, f_info_trusttype@45 as f_info_trusttype, f_info_listdate@46 as f_info_listdate, f_info_restrictedornot@47 as f_info_restrictedornot, f_info_ptmyear@48 as f_info_ptmyear, f_info_anndate@49 as f_info_anndate, f_info_expectedrateofreturn@50 as f_info_expectedrateofreturn, f_info_structuredornot@51 as f_info_structuredornot, f_closed_operation_period@52 as f_closed_operation_period, f_info_fullname@53 as f_info_fullname, f_info_issuingplace@54 as f_info_issuingplace, f_closed_operation_interval@55 as f_closed_operation_interval, f_info_exchmarket@56 as f_info_exchmarket, f_info_name@57 as f_info_name, object_id@58 as object_id, f_info_benchmark@59 as f_info_benchmark, f_info_firstinveststyle@60 as f_info_firstinveststyle, f_info_windcode@61 as f_info_windcode, f_info_corp_fundmanagementcomp@62 as f_info_corp_fundmanagementcomp, f_info_status@63 as f_info_status, f_info_front_code@64 as f_info_front_code, f_info_issuedate@65 as f_info_issuedate, f_info_custodianbank@66 as f_info_custodianbank, f_info_backend_code@67 as f_info_backend_code, is_dfd_del@68 as is_dfd_del], metrics=[output_rows=1, elapsed_compute=10.073µs, spill_count=0, spilled_bytes=0, mem_used=0]
    CoalesceBatchesExec: target_batch_size=4096, metrics=[output_rows=1, elapsed_compute=139.837µs, spill_count=0, spilled_bytes=0, mem_used=0]
      FilterExec: is_dfd_del@68 IS NULL OR is_dfd_del@68 = 0 AND f_info_windcode@61 = 002298.OF AND opdate@1 >= 0 AND opdate@1 <= 1677665081864, metrics=[output_rows=1, elapsed_compute=70.81µs, spill_count=0, spilled_bytes=0, mem_used=0]
        ScanTable: table=vanguardwind_chinamutualfunddescription_v1, parallelism=8, order=None, , metrics=[]
```

However, the metrics of `ScanTable` which is provided by CeresDB is empty, and the most of the performance issues happen in this stage, so without the metrics, it is hard to troubleshoot performance problems.

### Proposal

Collect the metrics of `ScanTable` stage, and feed them into the `datafusion` analyze framework.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/696/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/696,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Wvqpr,horaedb,1455336043,696,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-06T02:27:29Z,2023-03-06T02:27:29Z,"Some metrics I think of
- Number of SST to read
- Read cost of each SST
- Pushdowned filters
- ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Wvqpr/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/698,https://api.github.com/repos/apache/horaedb/issues/698,horaedb,1610682049,698,Tracking Issue: Support influxql raw query,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-03-06T05:12:46Z,2023-04-04T06:17:59Z,"### Describe This Problem

We plan to  support influxql mainly by converting it to sql in the statement level. 

So the first thing to do is to process the difference between influxql and sql.

Difference is listed as following：
 
## General
- [x] Only string type tag, and string、int64、uint64、float64、boolean type field allowed.

## Projection clause
- [x] Automatically include some selected columns
  - [x] `timestamp key`. 
  - [x] `table(measurement) name`.
  - [x] `group by key`.
- [x] Only fields can be parameters in function.
- [x] Regex and special wilcards, like:  #683 
```
select *::tag;
``` 
- [x] use ::type to represent cast, if faield, null will be filled.
- [x] Regex and special wilcards about tag may be covered by tags in `group by`. #683 
- [x] Column name can be quoted in double quotes.
- [x] Fill NULL to not exist column.
```
select * group by *

no tags will be in the project in the result.
``` 
- [x] Regex and special wilcards in function call.
- [x] All above in the subquery.

## From clause
- [x] Support regex.
- [x] Support from multiple measurements(Not supported in short term).

## Group by clause
- [x] Regex and wilcard in `group by` like:  #683 
```
select a from t group by *::tag / *
``` 
- [x] Group by without aggregation.

## Order by clause
- [x] Only can order by time.  https://docs.influxdata.com/influxdb/v1.8/query_language/explore-data/#basic-group-by-time-syntax

### Proposal

+ We should process the wildcard and regex by rewriting the influxql statement.
+ Then we convert it to sql statement.
+ Finally, throw it into datafusion.

### Additional Context
- [x] Integration tests: #716 
_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/698/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/698,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5W4UGd,horaedb,1457602973,698,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-07T06:12:48Z,2023-03-07T06:12:48Z,"Other difference I found

# Project clause
Column name can be quoted in double quotes
```
SELECT ""level description"",""location"",""water_level"" FROM ""h2o_feet""
```
It will output string directly in SQL.



Source: https://docs.influxdata.com/influxdb/v1.8/query_language/explore-data/","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5W4UGd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/698,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5W4aH0,horaedb,1457627636,698,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-07T06:41:47Z,2023-03-07T06:41:47Z,"As for `time()` in groupby 
- https://docs.influxdata.com/influxdb/v1.8/query_language/explore-data/#group-query-results-into-12-minutes-intervals-and-by-a-tag-key
```sql
> SELECT COUNT(""water_level"") FROM ""h2o_feet"" WHERE time >= '2015-08-18T00:00:00Z' AND time <= '2015-08-18T00:30:00Z' GROUP BY time(12m),""location""

name: h2o_feet
tags: location=coyote_creek
time                   count
----                   -----
2015-08-18T00:00:00Z   2
2015-08-18T00:12:00Z   2
2015-08-18T00:24:00Z   2

name: h2o_feet
tags: location=santa_monica
time                   count
----                   -----
2015-08-18T00:00:00Z   2
2015-08-18T00:12:00Z   2
2015-08-18T00:24:00Z   2
```

We can rewrite it to SQL, something like
```sql
SELECT
  to_timestamp_millis(DATE_BIN (INTERVAL '12' minute, timestamp, 0::TIMESTAMP) ) AS groupby_time,
  location,
  COUNT(`water_level`)  
FROM
    ""h2o_feet""
WHERE
    timestamp >= '2015-08-18T00:00:00Z'
    AND timestamp <= '2015-08-18T00:30:00Z'
GROUP BY
    groupby_time,
    location;
    
```

Plan to write an optimize rule to do this.

Question: does sql parser support `time(12m)` in group by clause?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5W4aH0/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/698,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5W_iFM,horaedb,1459495244,698,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2023-03-08T05:03:35Z,2023-03-08T05:03:35Z,"> As for `time()` in groupby
> 
>     * https://docs.influxdata.com/influxdb/v1.8/query_language/explore-data/#group-query-results-into-12-minutes-intervals-and-by-a-tag-key
> 
> 
> ```sql
> > SELECT COUNT(""water_level"") FROM ""h2o_feet"" WHERE time >= '2015-08-18T00:00:00Z' AND time <= '2015-08-18T00:30:00Z' GROUP BY time(12m),""location""
> 
> name: h2o_feet
> tags: location=coyote_creek
> time                   count
> ----                   -----
> 2015-08-18T00:00:00Z   2
> 2015-08-18T00:12:00Z   2
> 2015-08-18T00:24:00Z   2
> 
> name: h2o_feet
> tags: location=santa_monica
> time                   count
> ----                   -----
> 2015-08-18T00:00:00Z   2
> 2015-08-18T00:12:00Z   2
> 2015-08-18T00:24:00Z   2
> ```
> 
> We can rewrite it to SQL, something like
> 
> ```sql
> SELECT
>   to_timestamp_millis(DATE_BIN (INTERVAL '12' minute, timestamp, 0::TIMESTAMP) ) AS groupby_time,
>   location,
>   COUNT(`water_level`)  
> FROM
>     ""h2o_feet""
> WHERE
>     timestamp >= '2015-08-18T00:00:00Z'
>     AND timestamp <= '2015-08-18T00:30:00Z'
> GROUP BY
>     groupby_time,
>     location;
>     
> ```
> 
> Plan to write an optimize rule to do this.
> 
> Question: does sql parser support `time(12m)` in group by clause?

OK, I add them to the diffrence list.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5W_iFM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/698,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ZIeCw,horaedb,1495392432,698,NA,aierui,16207137,YIXIAO SHI,aieruishi@gmail.com,NA,2023-04-04T05:55:38Z,2023-04-04T05:55:38Z,"This thing is the same as what [6112](https://github.com/influxdata/influxdb_iox/issues/6112) they are doing.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ZIeCw/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/698,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ZIig1,horaedb,1495410741,698,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-04-04T06:17:59Z,2023-04-04T06:17:59Z,"Thanks for your tips. 

We have forked influxql related logical(parser and planner) into an independent crate already.
- https://github.com/CeresDB/influxql","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ZIig1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/702,https://api.github.com/repos/apache/horaedb/issues/702,horaedb,1612955129,702,Should CeresDB support interface route in No-Meta mode?,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2023-03-07T08:24:32Z,2023-03-07T09:52:54Z,"### Describe This Problem

In With-Meta mode, We can use route interface to query table's location as follow:
```
curl --location --request POST 'http://{CeresMetaAddr}:8080/api/v1/route' \
--header 'Content-Type: application/json' \
--data-raw '{
    ""clusterName"":""defaultCluster"",
    ""schemaName"":""public"",
    ""table"":[""demo""]
}'

```

But there is no one in No-Meta mode.

### Proposal

Should we support interface route in No-Meta mode?

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/702/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/702,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5W44uK,horaedb,1457752970,702,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-07T08:28:03Z,2023-03-07T08:28:03Z,Duplicate of #146 ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5W44uK/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/702,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5W48dN,horaedb,1457768269,702,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-07T08:40:52Z,2023-03-07T08:40:52Z,"If #146 fixed, we can retrieve routing information from CeresDB rather than CeresMeta no matter which mode is chosen.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5W48dN/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/702,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5W5A5r,horaedb,1457786475,702,NA,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,NA,2023-03-07T08:54:49Z,2023-03-07T08:54:49Z,"> If #146 fixed, we can retrieve routing information from CeresDB rather than CeresMeta no matter which mode is chosen.

Yes, I think it is more reasonable to retrieve routing information from CeresDB. It is not necessary for users to be aware of Meta.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5W5A5r/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/705,https://api.github.com/repos/apache/horaedb/issues/705,horaedb,1613576020,705,Cleanup unused dependencies,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-03-07T14:26:40Z,2023-06-01T08:16:09Z,"### Describe This Problem

There are some unused dependencies in our Cargo.toml as pointed out in #701.

### Proposal


- Cleanup those unused dependencies
- Add CI to ensure this.

### Additional Context

https://github.com/est31/cargo-udeps

This is what I get when run this tool against CeresDB
```sh
unused dependencies:
`benchmarks v1.0.0 (/Users/jiacai/code/misc/CeresDB/benchmarks)`
└─── dependencies
     └─── ""pprof""
`cluster v1.0.0 (/Users/jiacai/code/misc/CeresDB/cluster)`
└─── dependencies
     └─── ""rust-fsm""
`logger v1.0.0 (/Users/jiacai/code/misc/CeresDB/components/logger)`
└─── dependencies
     └─── ""slog_derive""
`meta_client v1.0.0 (/Users/jiacai/code/misc/CeresDB/meta_client)`
└─── dependencies
     └─── ""reqwest""
`parquet_ext v1.0.0 (/Users/jiacai/code/misc/CeresDB/components/parquet_ext)`
└─── dependencies
     ├─── ""lru""
     └─── ""parquet-format""
`profile v1.0.0 (/Users/jiacai/code/misc/CeresDB/components/profile)`
└─── dependencies
     └─── ""tempfile""
`remote_engine_client v1.0.0 (/Users/jiacai/code/misc/CeresDB/remote_engine_client)`
└─── dependencies
     └─── ""clru""
`server v1.0.0 (/Users/jiacai/code/misc/CeresDB/server)`
└─── dependencies
     ├─── ""analytic_engine""
     └─── ""system_catalog""
`tools v1.0.0 (/Users/jiacai/code/misc/CeresDB/tools)`
└─── dependencies
     └─── ""env_logger""
Note: These dependencies might be used by other targets.
      To find dependencies that are not used by any target, enable `--all-targets`.
Note: They might be false-positive.
      For example, `cargo-udeps` cannot detect usage of crates that are only used in doc-tests.
      To ignore some dependencies, write `package.metadata.cargo-udeps.ignore` in Cargo.toml.


```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/705/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/705,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5W-LFj,horaedb,1459138915,705,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-08T01:44:34Z,2023-03-08T01:44:34Z,Maybe we can add such a check in github workflows.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5W-LFj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/705,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5drF8y,horaedb,1571577650,705,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-06-01T08:16:09Z,2023-06-01T08:16:09Z,"cargo-udeps may lead to a very slow ci, so let's do it manually.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5drF8y/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/711,https://api.github.com/repos/apache/horaedb/issues/711,horaedb,1614930612,711,Support PostgreSQL wire protocol,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-03-08T09:27:16Z,2023-08-08T02:12:11Z,"### Describe This Problem

MySQL wire protocol is already supported by CeresDB, it would be great to support PostgreSQL also.

### Proposal

Support PostgreSQL protocol like MySQL

### Additional Context

https://github.com/sunng87/pgwire","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/711/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/711,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5i8DHg,horaedb,1659908576,711,NA,holicc,19146591,Joe,longshanluu@gmail.com,NA,2023-08-01T09:14:48Z,2023-08-01T09:14:48Z,"i'd like to do this, but i'm new to Rust, can i take a stab at this ?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5i8DHg/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/711,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5i9KED,horaedb,1660199171,711,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-08-01T12:18:07Z,2023-08-01T12:18:07Z,"Thanks for you interests, let us know when you have any trouble.

You can refer how MySQL is implemented.
https://github.com/CeresDB/ceresdb/blob/ee10f2f8f84fd75c6f680c34c2f2f15dc59eed4e/server/src/mysql/service.rs#L15","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5i9KED/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/711,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jPJWI,horaedb,1664914824,711,NA,holicc,19146591,Joe,longshanluu@gmail.com,NA,2023-08-04T03:18:39Z,2023-08-04T03:18:39Z,"@jiacai2050 I found something confused:

```sql 
CREATE TABLE `demo`(`name`string TAG,`id` int TAG,`value` double NOT NULL,`t` timestamp NOT NULL,TIMESTAMP KEY(t)) ENGINE = Analytic with(enable_ttl=false);

insert into demo (name,value,t)values(""ceresdb"",1,1691116127622);
```

When i try to insert value with column `t`, ceresDB treat `t` as `UInt64(12128845460635970325)`.  Shouldn't it be a `Timestamp` type ?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jPJWI/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/711,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jQzI3,horaedb,1665348151,711,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-08-04T09:55:59Z,2023-08-04T09:55:59Z,"
@holicc  This is what I got by `select * from demo;`;
```
tsid	t	name	id	value
12128845460635970325	1691116127622	ceresdb	NULL	1
```
`12128845460635970325 ` is `tsid`, not `t`.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jQzI3/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/711,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jUn8g,horaedb,1666350880,711,NA,holicc,19146591,Joe,longshanluu@gmail.com,NA,2023-08-05T02:09:20Z,2023-08-05T02:09:20Z,"@jiacai2050 Sorry, my bad. But PostgreSQL doesn't support unsigned int yet. Can i just cast uint to int ?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jUn8g/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/716,https://api.github.com/repos/apache/horaedb/issues/716,horaedb,1616480275,716,Integration tests for InfluxQL,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-03-09T06:00:36Z,2023-03-10T06:25:36Z,"### Describe This Problem

#698 is supporting InfluxQL, and current implementation has supported some basic query. For the following intensive development on this feature, an integration tests are necessary.

### Proposal

Build the integration tests for InfluxQL similar to the sql integration tests.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/716/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/718,https://api.github.com/repos/apache/horaedb/issues/718,horaedb,1616716641,718,Simplify the metrics,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-03-09T08:48:34Z,2023-03-20T05:42:35Z,"### Describe This Problem

Currently, the metrics has been recorded for every single table, which leading to massive metrics (The size of the metrics may reach up to 100MB+ when the table number is 10K+).

### Proposal

Simplify the metrics and use the aggregated metrics rather than the table level metrics, which should be replaced with the trace.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/718/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/721,https://api.github.com/repos/apache/horaedb/issues/721,horaedb,1617360622,721,Open shard failed because of OB_BUF_NOT_ENOUGH,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2023-03-09T14:25:36Z,2023-03-15T06:04:59Z,"### Describe this problem

env：
* Wal is based on OBKV
* replay_batch_size is 100

error msg is :
```
2023-03-09 21:30:10.684 INFO [server/src/grpc/meta_event_service/mod.rs:142] Finish handling request from meta, resp:OpenShardResponse { header: Some(ResponseHeader { code: 500, error: ""fail to open table, open_request:OpenTableRequest { catalog_name: \""ceresdb\"", schema_name: \""public\"", schema_id: SchemaId(0), table_name: \""MMM_2196651627_INFLUENCE_DEFAULT\"", table_id: TableId(31801), engine: \""Analytic\"", shard_id: 54, cluster_version: 1 }. Caused by: Failed to open table, source:Unexpected error, err:Failed to operate table through write worker, space_id:0, table:MMM_2196651627_INFLUENCE_DEFAULT, table_id:31801, err:Channel error, err:Failed to read wal, err:Failed to read log entries, err:Failed to open region, namespace:wal, region_id:54, table_id:31801, err:Failed to scan table, err:Failed to iter result set, table:wal_wal_20230309000000_000054, err:Common error, code:ObException(OB_BUF_NOT_ENOUGH), err:rcode:OB_BUF_NOT_ENOUGH, message:, addr:33.143.20.185:2882, trace_id:Y7BC76F78-0000000000000AA2, server_trace_id:Y7BC76F78-0000000000000AA2."" }) }
```

### Steps to reproduce

None

### Expected behavior

Open shard successfully.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/721/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/721,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XJtLm,horaedb,1462162150,721,NA,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,NA,2023-03-09T14:31:10Z,2023-03-09T14:31:10Z,"I think there are two ways to solve the problem:
1. continue to open other tables instead of stoping to open shard
2. decrease the replay_batch_size automatically","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XJtLm/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/721,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XNdEE,horaedb,1463144708,721,NA,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,NA,2023-03-10T02:35:03Z,2023-03-10T02:35:03Z,Still open shard fail after decrease `replay_batch_size` to 5,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XNdEE/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/721,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XNntz,horaedb,1463188339,721,NA,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,NA,2023-03-10T03:06:48Z,2023-03-10T03:06:48Z,We found the reason is one WAL log is too large.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XNntz/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/724,https://api.github.com/repos/apache/horaedb/issues/724,horaedb,1618699758,724,Reuse the logical planner in influxdb_iox,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-03-10T10:09:26Z,2023-03-16T03:18:23Z,"### Describe This Problem

Influxdb_iox has done many works in planner of influxql, maybe it's not reasonable to do the duplicated works rather than resuing and building it together.


### Proposal

- [ ] Extract the logical planner from influxdb_iox
  - [x] Make `Schema` a trait and make it able to build.
  - [ ] Make effort to pass the exist uts.
- [ ] Make use of it in CeresDB by impl the related traits.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/724/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/729,https://api.github.com/repos/apache/horaedb/issues/729,horaedb,1622797844,729,Thread 'ceres-bg' panicked: 'attempt to divide by zero' ,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2023-03-14T06:09:36Z,2023-03-15T03:28:14Z,"### Describe this problem


Here is the error msg:

```
2023-03-14 13:42:59.877 ERRO [common_util/src/panic.rs:42] thread 'ceres-bg' panicked 'attempt to divide by zero' at ""analytic_engine/src/compaction/picker.rs:189""
   0: common_util::panic::set_panic_hook::{{closure}}
             at /xunming/ceresdb/common_util/src/panic.rs:41:18
   1: <alloc::boxed::Box<F,A> as core::ops::function::Fn<Args>>::call
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:2002:9
      std::panicking::rust_panic_with_hook
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:692:13
   2: std::panicking::begin_panic_handler::{{closure}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:577:13
   3: std::sys_common::backtrace::__rust_end_short_backtrace
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys_common/backtrace.rs:137:18
   4: rust_begin_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:575:5
   5: core::panicking::panic_fmt
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panicking.rs:64:14
   6: core::panicking::panic
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panicking.rs:114:5
   7: analytic_engine::compaction::picker::Bucket::with_files
      analytic_engine::compaction::picker::SizeTieredPicker::trim_to_threshold_with_hotness
             at /xunming/ceresdb/analytic_engine/src/compaction/picker.rs:413:10
      analytic_engine::compaction::picker::SizeTieredPicker::most_interesting_bucket
             at /xunming/ceresdb/analytic_engine/src/compaction/picker.rs:328:17
   8: analytic_engine::compaction::picker::TimeWindowPicker::newest_bucket
             at /xunming/ceresdb/analytic_engine/src/compaction/picker.rs:500:33
      <analytic_engine::compaction::picker::TimeWindowPicker as analytic_engine::compaction::picker::LevelPicker>::pick_candidates_at_level
             at /xunming/ceresdb/analytic_engine/src/compaction/picker.rs:596:9
   9: analytic_engine::compaction::picker::CommonCompactionPicker::pick_compact_candidates
             at /xunming/ceresdb/analytic_engine/src/compaction/picker.rs:106:34
      <analytic_engine::compaction::picker::CommonCompactionPicker as analytic_engine::compaction::picker::CompactionPicker>::pick_compaction
             at /xunming/ceresdb/analytic_engine/src/compaction/picker.rs:138:13
  10: analytic_engine::table::version::TableVersion::pick_for_compaction
             at /xunming/ceresdb/analytic_engine/src/table/version.rs:739:9
  11: analytic_engine::compaction::scheduler::ScheduleWorker::handle_table_compaction_request::{{closure}}
             at /xunming/ceresdb/analytic_engine/src/compaction/scheduler.rs:559:31
  12: analytic_engine::compaction::scheduler::ScheduleWorker::handle_schedule_task::{{closure}}
             at /xunming/ceresdb/analytic_engine/src/compaction/scheduler.rs:436:74
      analytic_engine::compaction::scheduler::ScheduleWorker::schedule_loop::{{closure}}
             at /xunming/ceresdb/analytic_engine/src/compaction/scheduler.rs:391:61
      analytic_engine::compaction::scheduler::SchedulerImpl::new::{{closure}}
             at /xunming/ceresdb/analytic_engine/src/compaction/scheduler.rs:315:35
  13: tokio::runtime::task::core::Core<T,S>::poll::{{closure}}
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/core.rs:223:17
      tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/loom/std/unsafe_cell.rs:14:9
      tokio::runtime::task::core::Core<T,S>::poll
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/core.rs:212:13
      tokio::runtime::task::harness::poll_future::{{closure}}
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:476:19
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271:9
      std::panicking::try::do_call
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483:40
      std::panicking::try
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447:19
      std::panic::catch_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140:14
      tokio::runtime::task::harness::poll_future
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:464:18
      tokio::runtime::task::harness::Harness<T,S>::poll_inner
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:198:27
      tokio::runtime::task::harness::Harness<T,S>::poll
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:152:15
      tokio::runtime::task::raw::poll
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/raw.rs:255:5
  14: tokio::runtime::task::raw::RawTask::poll
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/raw.rs:200:18
      tokio::runtime::task::LocalNotified<S>::run
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/mod.rs:394:9
      tokio::runtime::scheduler::multi_thread::worker::Context::run_task::{{closure}}
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/multi_thread/worker.rs:464:13
      tokio::runtime::coop::with_budget

```

### Steps to reproduce

None

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/729/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/729,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Xe_3m,horaedb,1467743718,729,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-14T09:40:17Z,2023-03-14T09:40:17Z,"Please add server version when you meet this bug.

I will update issue template to add this section.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Xe_3m/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/735,https://api.github.com/repos/apache/horaedb/issues/735,horaedb,1624630788,735,Tracking issue: streaming building/reading SST,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-03-15T02:48:26Z,2023-03-31T06:03:32Z,"### Describe This Problem

Current building/reading SST is not in a streaming way, that is to say, all the content will be buffered in the memory before dumping into the underlying storage for building procedure and all the content will be fetched into memory before feeding to the upside merge iterators. Such way to bulid/read sst may lead to high memory consumption.

### Proposal

- [x] Write SST in a stream way #486
- [x] Read SST in a stream way.

### Additional Context","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/735/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/735,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XlVHo,horaedb,1469403624,735,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-15T06:14:23Z,2023-03-15T06:14:23Z,Write is duplicated with https://github.com/CeresDB/ceresdb/issues/486,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XlVHo/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/735,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XlWiR,horaedb,1469409425,735,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-15T06:19:49Z,2023-03-15T06:19:49Z,"The hard part of this new procedure is partial failure, how will you record partial failed tasks to avoid leave temp files?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XlWiR/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/735,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XlXbB,horaedb,1469413057,735,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-15T06:24:17Z,2023-03-15T06:24:17Z,"> Write is duplicated with #486

Ok. #486 has been linked to this tracking issue as one sub task issue.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XlXbB/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/735,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XmLvQ,horaedb,1469627344,735,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-15T09:12:07Z,2023-03-15T09:12:07Z,"> The hard part of this new procedure is partial failure, how will you record partial failed tasks to avoid leave temp files?

Do clearing jobs, e.g. removing temp files or abort multipart upload, in the drop method. As for the leaked temp files, I guess we can just let it go for now.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XmLvQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/736,https://api.github.com/repos/apache/horaedb/issues/736,horaedb,1624686747,736,Add new error code instead of `Table Not Found` for client routing table refresh When shard is opening,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2023-03-15T03:52:21Z,2024-10-19T11:29:38Z,"### Describe This Problem

Recently CeresDb always return `Table Not Found` In different error cases. Maybe it is useful to add some new error codes to distinguish.

### Proposal

- return Shard_Not_Open before open a shard 
- return Shard Is Opening when shard is opening

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/736/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/736,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XldL_,horaedb,1469436671,736,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-15T06:46:47Z,2023-03-15T06:46:47Z,"This problem may bring massive route requests because java sdk will refresh its routing tables when any error happens, which will overwhelm `CeresMeta` because there is no cache on the CeresDB server.

In order to solve this problem, two actions can be adopted:
- Return a specific error telling the client whether to refresh its routing table (this is actually this issue).
- Add routing cache to avoid fowarding all route requests to the ceresmeta (another issue #738  will be filed to track this problem).","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5XldL_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/738,https://api.github.com/repos/apache/horaedb/issues/738,horaedb,1624870098,738,Avoid route request penetration into CeresMeta,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-03-15T06:53:15Z,2023-03-22T02:23:40Z,"### Describe This Problem

Current route request implementation is just forwarding the request to CeresMeta, which is very dangerous to CeresMeta when massive route requests are received.

### Proposal

Add a cache for route requests, which should be refreshed periodically. With the cache, the route requests from the client won't be forwarded to CeresMeta.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/738/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/746,https://api.github.com/repos/apache/horaedb/issues/746,horaedb,1627002194,746,add macro to help write error message,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-03-16T09:01:52Z,2024-05-09T14:16:28Z,"```
        #[snafu(display(
            ""Failed to build influxql plan with no cause, msg:{}.\nBacktrace:\n{}"",
            msg,
            backtrace
        ))]
        BuildPlanNoCause { msg: String, backtrace: Backtrace },
```

Add a `display_backtrace` macro to simplify error message, something like:
```
display_backtrace!(""Failed to build influxql plan with no cause, msg:{}."", msg)
```
It will rewrite to line above.

_Originally posted by @jiacai2050 in https://github.com/CeresDB/ceresdb/pull/745#discussion_r1138301614_
            ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/746/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/746,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5yVhcH,horaedb,1918244615,746,NA,yuhaoran1214,75669303,Haoran Yu,,NA,2024-01-31T02:15:36Z,2024-01-31T02:15:36Z,"Hi, is this issue available?if so, can I try on this?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5yVhcH/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/746,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5yWRGS,horaedb,1918439826,746,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-01-31T05:59:54Z,2024-01-31T05:59:54Z,"Thanks, assigned. ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5yWRGS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/746,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM52rei9,horaedb,1991108797,746,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-03-12T09:10:42Z,2024-03-12T09:10:42Z,@Apricity001 Any progress? It has been quiet for a while.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM52rei9/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/746,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM52rx-3,horaedb,1991188407,746,NA,yuhaoran1214,75669303,Haoran Yu,,NA,2024-03-12T09:42:17Z,2024-03-12T09:42:17Z,"Sorry, I'm a little busy recently,I want to give up it.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM52rx-3/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/746,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM52sbrM,horaedb,1991359180,746,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-03-12T10:54:05Z,2024-03-12T10:54:05Z,"Thanks for response, unassigned.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM52sbrM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/746,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59VW6o,horaedb,2102750888,746,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-05-09T14:16:28Z,2024-05-09T14:16:28Z,"Not need any more, see #1513 ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59VW6o/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/752,https://api.github.com/repos/apache/horaedb/issues/752,horaedb,1633146722,752,Ignore backtrace for 4xx error ,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-03-21T02:59:32Z,2024-10-19T11:17:17Z,"### Describe This Problem

When testing, I found 400 errors are logged with backtrace:

```bash
2023-03-21 02:56:11.289 ERRO [server/src/http.rs:608] handle error: Rejection(GRPCWriteError { source: ErrWithCause { code: 403, msg: ""Insert is blocked"", source: BlockedTable { table: ""cse_rpc_duration_seconds_bucket"", backtrace: Backtrace(   0: <snafu::backtrace_shim::Backtrace as snafu::GenerateBacktrace>::generate
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/backtrace_shim.rs:15:19
      server::limiter::BlockedTable<__T0>::build
             at ceresdb/server/src/limiter.rs:10:10
      server::limiter::BlockedTable<__T0>::fail
             at ceresdb/server/src/limiter.rs:10:10
      server::limiter::Limiter::try_limit_by_block_list
             at ceresdb/server/src/limiter.rs:126:21
      server::limiter::Limiter::try_limit
             at ceresdb/server/src/limiter.rs:152:9
   1: server::grpc::storage_service::write::execute_plan::{{closure}}
             at ceresdb/server/src/grpc/storage_service/write.rs:157:5
   2: <server::handlers::prom::CeresDBStorage<Q> as prom_remote_api::types::RemoteStorage>::write::{{closure}}
             at ceresdb/server/src/handlers/prom.rs:269:13
   3: <core::pin::Pin<P> as core::future::future::Future>::poll
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125:9
      prom_remote_api::web::warp::write::{{closure}}
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/prom-remote-api-0.2.1/src/web/warp.rs:30:9
      <F as futures_core::future::TryFuture>::try_poll
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.25/src/future.rs:82:9
      <warp::filter::and_then::State<T,F> as core::future::future::Future>::poll
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/and_then.rs:99:44
   4: <warp::filter::and_then::AndThenFuture<T,F> as core::future::future::Future>::poll
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/and_then.rs:74:9
      <F as futures_core::future::TryFuture>::try_poll
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.25/src/future.rs:82:9
```

### Proposal

- Those error logged at warn level
- Remove backtrace when logging

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/752/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/760,https://api.github.com/repos/apache/horaedb/issues/760,horaedb,1637564493,760,"Too annoying to write unit tests when relating to some structs(schema, record batch, etc)",Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-03-23T13:38:51Z,2024-10-19T11:18:39Z,"### Describe This Problem

As I see, write test related to the structs mentioned in title is too annonying.
It make me mad to find the way of building them.

### Proposal

Write global test utils to ease the pain of building uts.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/760/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/762,https://api.github.com/repos/apache/horaedb/issues/762,horaedb,1638930258,762,question about FixedSizeArena's implementation ,guoxiangCN,30590325,mzygdeaq@163.com,,CLOSED,2023-03-24T08:16:49Z,2023-03-28T08:20:54Z,"### Describe this problem

point1:

https://github.com/CeresDB/ceresdb/blob/main/components/arena/src/fixed_size.rs#L55
 layout.pad_to_align() only expand and pad the size, it does not guarantee that the returned address is aligned according to the user's needs.  Currently, its normal operation depends entirely on the alignment property of the core. ptr returned by the operating system malloc during allocation of the entire block

point2:
https://github.com/CeresDB/ceresdb/blob/main/components/arena/src/fixed_size.rs#L64
why not self.ptr.as_ptr().add(self.len())?

### Server version

Omitted 

### Steps to reproduce

need inject some code to the core.ptr alloc.
for example:
1. core.ptr = malloc().add(1);
2. in Core::drop, free(core.ptr.sub(1))

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/762/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/762,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YW-y9,horaedb,1482419389,762,NA,guoxiangCN,30590325,mzygdeaq@163.com,,NA,2023-03-24T08:19:49Z,2023-03-24T08:19:49Z,it's my fault,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YW-y9/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/762,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YXAFw,horaedb,1482424688,762,NA,guoxiangCN,30590325,mzygdeaq@163.com,,NA,2023-03-24T08:25:02Z,2023-03-24T08:25:02Z,"https://github.com/CeresDB/ceresdb/blob/main/components/arena/src/fixed_size.rs#L64
why not self.ptr.as_ptr().add(self.len())?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YXAFw/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/762,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YXV7-,horaedb,1482514174,762,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-24T09:39:59Z,2023-03-24T09:39:59Z,"> point1:
>
>https://github.com/CeresDB/ceresdb/blob/main/components/arena/src/fixed_size.rs#L55
layout.pad_to_align() only expand and pad the size, it does not guarantee that the returned address is aligned according to the user's needs. Currently, its normal operation depends entirely on the alignment property of the core. ptr returned by the operating system malloc during allocation of the entire block
>
>point2:
https://github.com/CeresDB/ceresdb/blob/main/components/arena/src/fixed_size.rs#L64
why not self.ptr.as_ptr().add(self.len())?

@guoxiangCN Thanks for your reports. Actually, it looks like the `FixedSizeArena` is just a demo in the codebase, because it is not used at all. So it may have some bugs.

As for your two points, I agree on them. And as for the point2, I guess the answer to it is that it is a bug.

Maybe we should remove the `FixedSizeArena` from the codebase.

@guoxiangCN Are you reading the source code? It will be appreciated if you give more suggestions when you dig into the codebase.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YXV7-/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/762,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YXXjU,horaedb,1482520788,762,NA,guoxiangCN,30590325,mzygdeaq@163.com,,NA,2023-03-24T09:44:49Z,2023-03-24T09:44:49Z,"I want to learn more about Rust and database, and I have seen this open source project through some publicity, so I plan to learn about it. I wonder that if the open source version is consistent with your internal version?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YXXjU/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/762,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YXXns,horaedb,1482521068,762,NA,guoxiangCN,30590325,mzygdeaq@163.com,,NA,2023-03-24T09:45:03Z,2023-03-24T09:45:03Z,thanks for your reply!,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YXXns/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/762,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YXejH,horaedb,1482549447,762,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-24T10:06:39Z,2023-03-24T10:06:39Z,"> I want to learn more about Rust and database, and I have seen this open source project through some publicity, so I plan to learn about it. I wonder that if the open source version is consistent with your internal version?

Actually, there is no internal version at all.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YXejH/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/762,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YX0y4,horaedb,1482640568,762,NA,guoxiangCN,30590325,mzygdeaq@163.com,,NA,2023-03-24T11:16:15Z,2023-03-24T11:16:15Z,"> > I want to learn more about Rust and database, and I have seen this open source project through some publicity, so I plan to learn about it. I wonder that if the open source version is consistent with your internal version?
> 
> Actually, there is no internal version at all.

Got it","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YX0y4/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/764,https://api.github.com/repos/apache/horaedb/issues/764,horaedb,1639031783,764,`test_disk_cache_store_get_range` failed,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-03-24T09:29:40Z,2023-06-28T09:04:43Z,"### Describe this problem

https://github.com/CeresDB/ceresdb/actions/runs/4509558746/jobs/7939465824?pr=763
```
---- disk_cache::test::test_disk_cache_store_get_range stdout ----
thread 'disk_cache::test::test_disk_cache_store_get_range' panicked at 'range end out of bounds: 1 <= 0', /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/bytes-1.4.0/src/bytes.rs:261:9
stack backtrace:
   0: rust_begin_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:575:5
   1: core::panicking::panic_fmt
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panicking.rs:64:14
   2: bytes::bytes::Bytes::slice
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/bytes-1.4.0/src/bytes.rs:261:9
   3: <object_store::disk_cache::DiskCacheStore as object_store::ObjectStore>::get_range::{{closure}}
             at ./src/disk_cache.rs:459:26
   4: <core::pin::Pin<P> as core::future::future::Future>::poll
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125:9
   5: object_store::disk_cache::test::test_disk_cache_store_get_range::{{closure}}
             at ./src/disk_cache.rs:610:56
   6: <core::pin::Pin<P> as core::future::future::Future>::poll
             at /rustc/11d96b59307b1702fffe[871](https://github.com/CeresDB/ceresdb/actions/runs/4509558746/jobs/7939465824?pr=763#step:8:872)bfc2d0145d070881e/library/core/src/future/future.rs:125:9
   7: tokio::runtime::scheduler::current_thread::CoreGuard::block_on::{{closure}}::{{closure}}::{{closure}}
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/current_thread.rs:541:57
   8: tokio::runtime::coop::with_budget
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/coop.rs:102:5
   9: tokio::runtime::coop::budget
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/coop.rs:68:5
  10: tokio::runtime::scheduler::current_thread::CoreGuard::block_on::{{closure}}::{{closure}}
error: test failed, to rerun pass `-p object_store --lib`
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/current_thread.rs:541:25
  11: tokio::runtime::scheduler::current_thread::Context::enter
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/current_thread.rs:350:19
  12: tokio::runtime::scheduler::current_thread::CoreGuard::block_on::{{closure}}
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/current_thread.rs:540:36
  13: tokio::runtime::scheduler::current_thread::CoreGuard::enter::{{closure}}
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/current_thread.rs:615:57
  14: tokio::macros::scoped_tls::ScopedKey<T>::set
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/macros/scoped_tls.rs:61:9
  15: tokio::runtime::scheduler::current_thread::CoreGuard::enter
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/current_thread.rs:615:27
  16: tokio::runtime::scheduler::current_thread::CoreGuard::block_on
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/current_thread.rs:530:19
  17: tokio::runtime::scheduler::current_thread::CurrentThread::block_on
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/current_thread.rs:154:24
  18: tokio::runtime::runtime::Runtime::block_on
             at /home/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/runtime.rs:282:47
  19: object_store::disk_cache::test::test_disk_cache_store_get_range
             at ./src/disk_cache.rs:630:9
  20: object_store::disk_cache::test::test_disk_cache_store_get_range::{{closure}}
             at ./src/disk_cache.rs:584:48
  21: core::ops::function::FnOnce::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070[881](https://github.com/CeresDB/ceresdb/actions/runs/4509558746/jobs/7939465824?pr=763#step:8:882)e/library/core/src/ops/function.rs:250:5
  22: core::ops::function::FnOnce::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/ops/function.rs:250:5
note: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.


failures:
    disk_cache::test::test_disk_cache_store_get_range
```

### Server version

refer https://github.com/CeresDB/ceresdb/pull/763

### Steps to reproduce

Run CI.

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/764/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/764,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gBoIQ,horaedb,1611039248,764,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-06-28T09:04:43Z,2023-06-28T09:04:43Z,"It may have been fixed, and if it reappears, the issue will reopen","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gBoIQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/769,https://api.github.com/repos/apache/horaedb/issues/769,horaedb,1640994957,769,Buffer underflow when executing query,zouxiang1993,26276281,zouxiang,,CLOSED,2023-03-26T17:14:55Z,2023-03-30T13:36:09Z,"### Describe this problem

Error message is:
```text
{""code"":500,""message"":""Failed to handle request, err:Failed to execute interpreter, query:select count(*) from perflog where namespace=\""grpc.server.cost\"" and subtag=\""video-conductor-proxy-service.task-service/BatchAckTask\"" limit 10, err:Failed to execute select, err:Failed to execute logical plan, err:Failed to collect record batch stream, err:Stream error, msg:convert from arrow record batch, err:External error: Stream error, msg:poll read response failed, err:Failed to query from table in server, table_ident:TableIdentifier { catalog: \""ceresdb\"", schema: \""public\"", table: \""__perflog_9\"" }, code:500, msg:record batch failed. Caused by: Stream error, msg:read record batch, err:Failed to read data from the sub iterator, err:PullRecordBatch { source: DecodeRecordBatch { source: ParquetError { source: General(\""Failed to fetch ranges from object store, err:Generic DiskCacheStore error: Failed to decode cache pb value, file:/data/ceresdb/sst_cache/0-14-357.sst-0-4194304, source:failed to decode Protobuf message: Bytes.value: buffer underflow.\\nbacktrace:\\n 0 <snafu::backtrace_shim::""}
```

### Server version

CeresDB Server 
CeresDB version: 1.0.0
Git branch: main
Git commit: 95ea870
Build time: 2023-03-06T03:30:09.845055021Z
Rustc version: 1.69.0-nightly

### Steps to reproduce

None

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/769/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/769,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Ydr91,horaedb,1484177269,769,NA,zouxiang1993,26276281,zouxiang,,NA,2023-03-26T18:15:50Z,2023-03-26T18:15:50Z,"More details in server's log : 
```text
ERRO [analytic_engine/src/compaction/scheduler.rs:480] Failed to compact table, table_name:__perflog_9, table_id:14, request_id:8116, err:Failed to write sst, file_path:0/14/865.sst, source:Failed to poll record batch, err:Failed to read data from the sub iterator, err:PullRecordBatch { source: DecodeRecordBatch { source: ParquetError { source: General(""Failed to fetch ranges from object store, err:Generic DiskCacheStore error: Failed to decode cache pb value, file:/data/ceresdb/sst_cache/0-14-357.sst-0-4194304, source:failed to decode Protobuf message: Bytes.value: buffer underflow.
backtrace:
 0 <snafu::backtrace_shim::Backtrace as snafu::GenerateBacktrace>::generate::he03e4c3ab1c80600
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/backtrace_shim.rs:15
   <object_store::disk_cache::DecodeCache<__T0> as snafu::IntoError<object_store::disk_cache::Error>>::into_error::h6766085f94d56f24
   /ceresdb/components/object_store/src/disk_cache.rs:36
   <core::result::Result<T,E> as snafu::ResultExt<T,E>>::with_context::{{closure}}::h60332f29a31f65c1
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/lib.rs:329
   core::result::Result<T,E>::map_err::h7ec61b0bfdb27f49
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/result.rs:860
   <core::result::Result<T,E> as snafu::ResultExt<T,E>>::with_context::h9715893581ea9770
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/lib.rs:327
   object_store::disk_cache::DiskCache::read_bytes::{{closure}}::h302692f6d333d14f
   /ceresdb/components/object_store/src/disk_cache.rs:223
   object_store::disk_cache::DiskCache::get::{{closure}}::hb627edb9a7c406e0
   /ceresdb/components/object_store/src/disk_cache.rs:176
   <object_store::disk_cache::DiskCacheStore as object_store::ObjectStore>::get_range::{{closure}}::hb03eb5ecaf902e42
   /ceresdb/components/object_store/src/disk_cache.rs:440
 1 <core::pin::Pin<P> as core::future::future::Future>::poll::hb42f0fa5134d0b52
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125
   object_store::mem_cache::MemCacheStore::get_range_with_ro_cache::{{closure}}::hfd6a236affcdd47a
   /ceresdb/components/object_store/src/mem_cache.rs:215
   <object_store::mem_cache::MemCacheStore as object_store::ObjectStore>::get_range::{{closure}}::h8f0a058260f55760
   /ceresdb/components/object_store/src/mem_cache.rs:259
 2 <core::pin::Pin<P> as core::future::future::Future>::poll::ha4f6106f055e31d0
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125
   <futures_util::stream::futures_ordered::OrderWrapper<T> as core::future::future::Future>::poll::hb938642749ffd829
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/stream/futures_ordered.rs:55
   <futures_util::stream::futures_unordered::FuturesUnordered<Fut> as futures_core::stream::Stream>::poll_next::h2d6ad3345857ad00
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/stream/futures_unordered/mod.rs:515
   futures_util::stream::stream::StreamExt::poll_next_unpin::h0a0b71e2f0c1d421
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/stream/stream/mod.rs:1626
   <futures_util::stream::futures_ordered::FuturesOrdered<Fut> as futures_core::stream::Stream>::poll_next::h13e662b4eccfe494
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/stream/futures_ordered.rs:194
   futures_util::stream::stream::StreamExt::poll_next_unpin::h338ae9e5478f2ce0
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/stream/stream/mod.rs:1626
 3 <futures_util::stream::stream::buffered::Buffered<St> as futures_core::stream::Stream>::poll_next::h82c609b30075f805
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/stream/stream/buffered.rs:73
   <S as futures_core::stream::TryStream>::try_poll_next::h741b56be3668f582
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.25/src/stream.rs:196
   <futures_util::stream::try_stream::try_collect::TryCollect<St,C> as core::future::future::Future>::poll::h525a35c904a937ad
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/stream/try_stream/try_collect.rs:46
   object_store::util::coalesce_ranges::{{closure}}::h5bc881faf9037bfa
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/object_store-0.5.3/src/util.rs:131
   object_store::ObjectStore::get_ranges::{{closure}}::h1300ace6bf3c229b
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/object_store-0.5.3/src/lib.rs:328
 4 <core::pin::Pin<P> as core::future::future::Future>::poll::hed6a9454d7feebf9
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125
   <F as futures_core::future::TryFuture>::try_poll::hf916a937875176dc
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.25/src/future.rs:82
   <futures_util::future::try_future::into_future::IntoFuture<Fut> as core::future::future::Future>::poll::h74b61a8d4929db83
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/future/try_future/into_future.rs:34
   <futures_util::future::future::map::Map<Fut,F> as core::future::future::Future>::poll::hc65e758e6aa18e6f
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/future/future/map.rs:55
   <futures_util::future::future::Map<Fut,F> as core::future::future::Future>::poll::h69b36077f1cf1b9c
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/lib.rs:91
   <futures_util::future::try_future::MapErr<Fut,F> as core::future::future::Future>::poll::h3c6c793540c031db
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/lib.rs:91
   <analytic_engine::sst::parquet::async_reader::ObjectStoreReader as parquet::arrow::async_reader::AsyncFileReader>::get_byte_ranges::{{closure}}::h7439ff1d42706a38
   /ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:428
 5 <core::pin::Pin<P> as core::future::future::Future>::poll::habde48208b839890
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125
   parquet::arrow::async_reader::InMemoryRowGroup::fetch::{{closure}}::hcb8805ceeee3c974
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/parquet-32.0.0/src/arrow/async_reader/mod.rs:663
 6 parquet::arrow::async_reader::ReaderFactory<T>::read_row_group::{{closure}}::hea330ebf48a894b2
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/parquet-32.0.0/src/arrow/async_reader/mod.rs:435
 7 <core::pin::Pin<P> as core::future::future::Future>::poll::h223e8e21bd20206b
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125
   futures_util::future::future::FutureExt::poll_unpin::h4fcbba251d73fe48
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/future/future/mod.rs:562
   <parquet::arrow::async_reader::ParquetRecordBatchStream<T> as futures_core::stream::Stream>::poll_next::h683512bf01ccde9c
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/parquet-32.0.0/src/arrow/async_reader/mod.rs:556
   <futures_util::stream::stream::map::Map<St,F> as futures_core::stream::Stream>::poll_next::h977b000a451a251f
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/stream/stream/map.rs:58
 8 <core::pin::Pin<P> as futures_core::stream::Stream>::poll_next::h2621e98428624aa3
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.25/src/stream.rs:120
   futures_util::stream::stream::StreamExt::poll_next_unpin::hea46f1a59cf1779a
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/stream/stream/mod.rs:1626
   <analytic_engine::sst::parquet::async_reader::RecordBatchProjector as futures_core::stream::Stream>::poll_next::h8f232a2ef4c0da11
   /ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:497
 9 futures_core::stream::if_alloc::<impl futures_core::stream::Stream for alloc::boxed::Box<S>>::poll_next::h67749b5950120e9e
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.25/src/stream.rs:209
   futures_util::stream::stream::StreamExt::poll_next_unpin::h493c08bb9d694ea5
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/stream/stream/mod.rs:1626
   <futures_util::stream::stream::next::Next<St> as core::future::future::Future>::poll::h03bf8fc1a4794704
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/stream/stream/next.rs:32
   analytic_engine::sst::parquet::async_reader::ThreadedReader::read_record_batches_from_sub_reader::{{closure}}::h761b72d30a484fe4
   /ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:634
10 tokio::runtime::task::core::Core<T,S>::poll::{{closure}}::ha77ba507aa1a6c6a
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/core.rs:223
   tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut::h1480c780c84c5987
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/loom/std/unsafe_cell.rs:14
   tokio::runtime::task::core::Core<T,S>::poll::h8dbca0be4565a9f1
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/core.rs:212
   tokio::runtime::task::harness::poll_future::{{closure}}::h5274d59ab545e337
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:476
   <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once::h45c7a41203f6f644
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271
   std::panicking::try::do_call::h662f3bd66f827194
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483
   std::panicking::try::h826161d6f844ff6e
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447
   std::panic::catch_unwind::he45b4968c5b77539
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140
   tokio::runtime::task::harness::poll_future::h1e6c88e4f73ab72f
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:464
   tokio::runtime::task::harness::Harness<T,S>::poll_inner::hc0d7cabb859976a3
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:198
   tokio::runtime::task::harness::Harness<T,S>::poll::h1c6bdc856802656f
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:152
   tokio::runtime::task::raw::poll::h2dbd3e5dc09ec6af
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/raw.rs:255
11 tokio::runtime::task::raw::RawTask::poll::hee1eeac52e93dc2f
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/raw.rs:200
   tokio::runtime::task::LocalNotified<S>::run::hcd55b9f1199cdd62
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/mod.rs:394
   tokio::runtime::scheduler::multi_thread::worker::Context::run_task::{{closure}}::h0c53b75fa50b944d
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/multi_thread/worker.rs:464
   tokio::runtime::coop::with_budget::hfdc83963a844eaa7
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/coop.rs:102
   tokio::runtime::coop::budget::h618fd14970b005a9
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/coop.rs:68
   tokio::runtime::scheduler::multi_thread::worker::Context::run_task::hab70c980d6876801
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/multi_thread/worker.rs:463
12 tokio::runtime::scheduler::multi_thread::worker::Context::run::h24427a6b47ebfa79
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/multi_thread/worker.rs:426
   tokio::runtime::scheduler::multi_thread::worker::run::{{closure}}::h5a799797e20bccc0
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/multi_thread/worker.rs:406
   tokio::macros::scoped_tls::ScopedKey<T>::set::h3b4d1836e1621479
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/macros/scoped_tls.rs:61
   tokio::runtime::scheduler::multi_thread::worker::run::hb14797ff3f8425bc
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/multi_thread/worker.rs:403
13 tokio::runtime::scheduler::multi_thread::worker::Launch::launch::{{closure}}::h7cc83be1b17975b1
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/multi_thread/worker.rs:365
   <tokio::runtime::blocking::task::BlockingTask<T> as core::future::future::Future>::poll::h0a5f18076b0ca1cc
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/blocking/task.rs:42
   tokio::runtime::task::core::Core<T,S>::poll::{{closure}}::h7b7b658625db7fe4
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/core.rs:223
   tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut::h8a8bbc22e527cd7a
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/loom/std/unsafe_cell.rs:14
   tokio::runtime::task::core::Core<T,S>::poll::h34d4508722e4ba21
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/core.rs:212
   tokio::runtime::task::harness::poll_future::{{closure}}::h1e3d1452d648fdc5
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:476
   <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once::h4a728758beaa058e
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271
   std::panicking::try::do_call::hc236a41348235ea7
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483
   std::panicking::try::hdcea4ada6944ce3d
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447
   std::panic::catch_unwind::hdfdd7e6cdf3831d2
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140
   tokio::runtime::task::harness::poll_future::hce14bb430b77a087
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:464
   tokio::runtime::task::harness::Harness<T,S>::poll_inner::hb6c5dd6740168a88
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:198
   tokio::runtime::task::harness::Harness<T,S>::poll::h55415e04c80e331c
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:152
   tokio::runtime::task::raw::poll::hd22e917b536ed7f8
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/raw.rs:255
14 tokio::runtime::task::raw::RawTask::poll::hee1eeac52e93dc2f
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/raw.rs:200
   tokio::runtime::task::UnownedTask<S>::run::h58fd606513e73ddb
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/mod.rs:431
   tokio::runtime::blocking::pool::Task::run::h9890a5808360982f
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/blocking/pool.rs:159
   tokio::runtime::blocking::pool::Inner::run::h9c0e0c521fb2e6f9
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/blocking/pool.rs:511
   tokio::runtime::blocking::pool::Spawner::spawn_thread::{{closure}}::h18b2e71018da51d1
   /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/blocking/pool.rs:469
   std::sys_common::backtrace::__rust_begin_short_backtrace::h8516ed23210cf795
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys_common/backtrace.rs:121
15 std::thread::Builder::spawn_unchecked_::{{closure}}::{{closure}}::h0601890e5bbaea04
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/thread/mod.rs:558
   <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once::h7b1241f8fb072088
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271
   std::panicking::try::do_call::he655ac3b8c52908c
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483
   std::panicking::try::h9a85ae35c2c9342a
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447
   std::panic::catch_unwind::h56fd0ce15d2a37b2
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140
   std::thread::Builder::spawn_unchecked_::{{closure}}::h86790adc1395a6d6
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/thread/mod.rs:557
   core::ops::function::FnOnce::call_once{{vtable.shim}}::h5edf909cb71465eb
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/ops/function.rs:250
16 <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once::hc8beb91c5e39b692
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:1988
   <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once::h20e58dce1054acc4
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:1988
   std::sys::unix::thread::Thread::new::thread_start::h848946a57aa81736
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys/unix/thread.rs:108
17 start_thread
   /build/glibc-SzIz7B/glibc-2.31/nptl/pthread_create.c:477
18 clone
   /build/glibc-SzIz7B/glibc-2.31/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:95
""), backtrace: Backtrace(   0: <snafu::backtrace_shim::Backtrace as snafu::GenerateBacktrace>::generate
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/backtrace_shim.rs:15:19
      <analytic_engine::sst::reader::error::ParquetError as snafu::IntoError<analytic_engine::sst::reader::error::Error>>::into_error
             at ceresdb/analytic_engine/src/sst/reader.rs:15:21
      <core::result::Result<T,E> as snafu::ResultExt<T,E>>::with_context::{{closure}}
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/lib.rs:329:13
      core::result::Result<T,E>::map_err
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/result.rs:860:27
      <core::result::Result<T,E> as snafu::ResultExt<T,E>>::with_context
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/lib.rs:327:9
      analytic_engine::sst::parquet::async_reader::Reader::fetch_record_batch_streams::{{closure}}::{{closure}}
             at ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:222:30
      <T as futures_util::fns::FnMut1<A>>::call_mut
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/fns.rs:28:9
      <futures_util::stream::stream::map::Map<St,F> as futures_core::stream::Stream>::poll_next::{{closure}}
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/stream/stream/map.rs:59:33
      core::option::Option<T>::map
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/option.rs:970:29
      <futures_util::stream::stream::map::Map<St,F> as futures_core::stream::Stream>::poll_next
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/stream/stream/map.rs:59:21
   1: <core::pin::Pin<P> as futures_core::stream::Stream>::poll_next
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.25/src/stream.rs:120:9
      futures_util::stream::stream::StreamExt::poll_next_unpin
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/stream/stream/mod.rs:1626:9
      <analytic_engine::sst::parquet::async_reader::RecordBatchProjector as futures_core::stream::Stream>::poll_next
             at ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:497:15
   2: futures_core::stream::if_alloc::<impl futures_core::stream::Stream for alloc::boxed::Box<S>>::poll_next
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.25/src/stream.rs:209:13
      futures_util::stream::stream::StreamExt::poll_next_unpin
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/stream/stream/mod.rs:1626:9
      <futures_util::stream::stream::next::Next<St> as core::future::future::Future>::poll
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.25/src/stream/stream/next.rs:32:9
      analytic_engine::sst::parquet::async_reader::ThreadedReader::read_record_batches_from_sub_reader::{{closure}}
             at ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:634:50
   3: tokio::runtime::task::core::Core<T,S>::poll::{{closure}}
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/core.rs:223:17
      tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/loom/std/unsafe_cell.rs:14:9
      tokio::runtime::task::core::Core<T,S>::poll
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/core.rs:212:13
      tokio::runtime::task::harness::poll_future::{{closure}}
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:476:19
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271:9
      std::panicking::try::do_call
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483:40
      std::panicking::try
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447:19
      std::panic::catch_unwind
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140:14
      tokio::runtime::task::harness::poll_future
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:464:18
      tokio::runtime::task::harness::Harness<T,S>::poll_inner
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:198:27
      tokio::runtime::task::harness::Harness<T,S>::poll
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:152:15
      tokio::runtime::task::raw::poll
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/raw.rs:255:5
   4: tokio::runtime::task::raw::RawTask::poll
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/raw.rs:200:18
      tokio::runtime::task::LocalNotified<S>::run
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/mod.rs:394:9
      tokio::runtime::scheduler::multi_thread::worker::Context::run_task::{{closure}}
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/multi_thread/worker.rs:464:13
      tokio::runtime::coop::with_budget
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/coop.rs:102:5
      tokio::runtime::coop::budget
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/coop.rs:68:5
      tokio::runtime::scheduler::multi_thread::worker::Context::run_task
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/multi_thread/worker.rs:463:9
   5: tokio::runtime::scheduler::multi_thread::worker::Context::run
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/multi_thread/worker.rs:426:24
      tokio::runtime::scheduler::multi_thread::worker::run::{{closure}}
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/multi_thread/worker.rs:406:17
      tokio::macros::scoped_tls::ScopedKey<T>::set
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/macros/scoped_tls.rs:61:9
      tokio::runtime::scheduler::multi_thread::worker::run
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/multi_thread/worker.rs:403:5
   6: tokio::runtime::scheduler::multi_thread::worker::Launch::launch::{{closure}}
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/scheduler/multi_thread/worker.rs:365:45
      <tokio::runtime::blocking::task::BlockingTask<T> as core::future::future::Future>::poll
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/blocking/task.rs:42:21
      tokio::runtime::task::core::Core<T,S>::poll::{{closure}}
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/core.rs:223:17
      tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/loom/std/unsafe_cell.rs:14:9
      tokio::runtime::task::core::Core<T,S>::poll
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/core.rs:212:13
      tokio::runtime::task::harness::poll_future::{{closure}}
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:476:19
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271:9
      std::panicking::try::do_call
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483:40
      std::panicking::try
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447:19
      std::panic::catch_unwind
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140:14
      tokio::runtime::task::harness::poll_future
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:464:18
      tokio::runtime::task::harness::Harness<T,S>::poll_inner
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:198:27
      tokio::runtime::task::harness::Harness<T,S>::poll
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/harness.rs:152:15
      tokio::runtime::task::raw::poll
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/raw.rs:255:5
   7: tokio::runtime::task::raw::RawTask::poll
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/raw.rs:200:18
      tokio::runtime::task::UnownedTask<S>::run
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/task/mod.rs:431:9
      tokio::runtime::blocking::pool::Task::run
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/blocking/pool.rs:159:9
      tokio::runtime::blocking::pool::Inner::run
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/blocking/pool.rs:511:17
      tokio::runtime::blocking::pool::Spawner::spawn_thread::{{closure}}
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.25.0/src/runtime/blocking/pool.rs:469:13
      std::sys_common::backtrace::__rust_begin_short_backtrace
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys_common/backtrace.rs:121:18
   8: std::thread::Builder::spawn_unchecked_::{{closure}}::{{closure}}
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/thread/mod.rs:558:17
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271:9
      std::panicking::try::do_call
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483:40
      std::panicking::try
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447:19
      std::panic::catch_unwind
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140:14
      std::thread::Builder::spawn_unchecked_::{{closure}}
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/thread/mod.rs:557:30
      core::ops::function::FnOnce::call_once{{vtable.shim}}
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/ops/function.rs:250:5
   9: <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:1988:9
      <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:1988:9
      std::sys::unix::thread::Thread::new::thread_start
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys/unix/thread.rs:108:17
  10: start_thread
             at build/glibc-SzIz7B/glibc-2.31/nptl/pthread_create.c:477:8
  11: clone
             at build/glibc-SzIz7B/glibc-2.31/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:95
) } } }

```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Ydr91/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/769,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Yeept,horaedb,1484384877,769,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2023-03-27T02:21:47Z,2023-03-27T02:21:47Z,"Can you send me your configuration file? Currently, `diskcache` is not frequently used and there may be bugs.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Yeept/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/769,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YehPr,horaedb,1484395499,769,NA,zouxiang1993,26276281,zouxiang,,NA,2023-03-27T02:33:41Z,2023-03-27T02:33:41Z,"config.toml: 
```text
[node]
addr=""xxxx""

[server]
bind_addr = ""xxxx""
http_port = 5440
grpc_port = 8831

[logger]
level = ""info""

[runtime]
read_thread_num = 30
write_thread_num = 16
background_thread_num = 12

[cluster_deployment]
mode = ""WithMeta""

[cluster_deployment.meta_client]
cluster_name = 'defaultCluster'
meta_addr = 'http://xxxx.:2379'
lease = ""10s""
timeout = ""5s""

[analytic]
write_group_worker_num = 16
replay_batch_size = 100
max_replay_tables_per_batch = 128
write_group_command_channel_cap = 1024
sst_background_read_parallelism = 8

[analytic.manifest]
scan_batch_size = 100
snapshot_every_n_updates = 10000
scan_timeout = ""5s""
store_timeout = ""5s""

[analytic.wal]
type = ""RocksDB""
data_dir = ""/data/ceresdb""

[analytic.storage]
mem_cache_capacity = ""20GB""
# 1<<8=256
mem_cache_partition_bits = 8
disk_cache_dir = ""/data/ceresdb/""
disk_cache_capacity = '2G'
disk_cache_page_size = '4M'

[analytic.storage.object_store]
type = ""Local""
data_dir = ""/data/ceresdb/""

[analytic.table_opts]
arena_block_size = 2097152
write_buffer_size = 33554432

[analytic.compaction_config]
schedule_channel_len = 16
schedule_interval = ""30m""
max_ongoing_tasks = 8
memory_limit = ""4G""

```

server log : 
```
INFO [src/setup.rs:83] Server starts up, config:Config {
    node: NodeInfo {
        addr: ""xxxx"",
        zone: """",
        idc: """",
        binary_version: """",
    },
    server: ServerConfig {
        bind_addr: ""xxxx"",
        mysql_port: 3307,
        http_port: 5440,
        grpc_port: 8831,
        timeout: None,
        http_max_body_size: 65536,
        grpc_server_cq_count: 20,
        resp_compress_min_length: ReadableSize(
            4194304,
        ),
        forward: Config {
            enable: false,
            thread_num: 4,
            max_send_msg_len: 20971520,
            max_recv_msg_len: 1073741824,
            keep_alive_interval: 600s,
            keep_alive_timeout: 3s,
            keep_alive_while_idle: true,
            connect_timeout: 3s,
            forward_timeout: 60s,
        },
    },
    runtime: RuntimeConfig {
        read_thread_num: 30,
        write_thread_num: 16,
        meta_thread_num: 2,
        background_thread_num: 12,
    },
    logger: Config {
        level: ""info"",
        enable_async: true,
        async_channel_len: 102400,
    },
    tracing: Config {
        prefix: ""tracing"",
        dir: ""/tmp/ceresdb"",
        level: ""info"",
    },
    analytic: Config {
        storage: StorageOptions {
            mem_cache_capacity: ReadableSize(
                21474836480,
            ),
            mem_cache_partition_bits: 8,
            disk_cache_capacity: ReadableSize(
                2147483648,
            ),
            disk_cache_page_size: ReadableSize(
                4194304,
            ),
            disk_cache_dir: ""/data/ceresdb/"",
            object_store: Local(
                LocalOptions {
                    data_dir: ""/data/ceresdb/"",
                },
            ),
        },
        replay_batch_size: 100,
        max_replay_tables_per_batch: 128,
        write_group_worker_num: 16,
        write_group_command_channel_cap: 1024,
        table_opts: TableOptions {
            segment_duration: None,
            update_mode: Overwrite,
            storage_format_hint: Auto,
            enable_ttl: true,
            ttl: ReadableDuration(
                604800s,
            ),
            arena_block_size: 2097152,
            write_buffer_size: 33554432,
            compaction_strategy: Default,
            num_rows_per_row_group: 8192,
            compression: Zstd,
        },
        compaction_config: SchedulerConfig {
            schedule_channel_len: 16,
            schedule_interval: ReadableDuration(
                1800s,
            ),
            max_ongoing_tasks: 8,
            max_unflushed_duration: ReadableDuration(
                18000s,
            ),
            memory_limit: ReadableSize(
                4294967296,
            ),
        },
        sst_meta_cache_cap: Some(
            1000,
        ),
        sst_data_cache_cap: Some(
            1000,
        ),
        manifest: Options {
            snapshot_every_n_updates: 10000,
            scan_timeout: ReadableDuration(
                5s,
            ),
            scan_batch_size: 100,
            store_timeout: ReadableDuration(
                5s,
            ),
        },
        space_write_buffer_size: 0,
        db_write_buffer_size: 0,
        scan_batch_size: 500,
        sst_background_read_parallelism: 8,
        wal: RocksDB(
            RocksDBConfig {
                data_dir: ""/data/ceresdb"",
            },
        ),
        remote_engine_client: Config {
            connect_timeout: ReadableDuration(
                3s,
            ),
            channel_pool_max_size_per_partition: 16,
            channel_pool_partition_num: 16,
            channel_keep_alive_while_idle: true,
            channel_keep_alive_timeout: ReadableDuration(
                3s,
            ),
            channel_keep_alive_interval: ReadableDuration(
                600s,
            ),
            route_cache_max_size_per_partition: 16,
            route_cache_partition_num: 16,
        },
    },
    query_engine: Config {
        read_parallelism: 8,
    },
    cluster_deployment: Some(
        WithMeta(
            ClusterConfig {
                cmd_channel_buffer_size: 0,
                meta_client: MetaClientConfig {
                    cluster_name: ""defaultCluster"",
                    meta_addr: ""http://xxxx:2379"",
                    lease: ReadableDuration(
                        10s,
                    ),
                    timeout: ReadableDuration(
                        5s,
                    ),
                    cq_count: 8,
                },
            },
        ),
    ),
    limiter: LimiterConfig {
        write_block_list: [],
        read_block_list: [],
        rules: [],
    },
}

```

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YehPr/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/769,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Yeih6,horaedb,1484400762,769,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2023-03-27T02:44:43Z,2023-03-27T02:44:43Z,"You can disable `diskcache` with following setting.
```
disk_cache_capacity = '0G'
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Yeih6/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/769,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YsqqL,horaedb,1488104075,769,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-29T07:49:23Z,2023-03-29T07:49:23Z,"After some digging, I guess `buffer underflow` is caused by cache file corrupted. And I guess we should ignore the decode error from the disk cache because it can't be ensured always right.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YsqqL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/770,https://api.github.com/repos/apache/horaedb/issues/770,horaedb,1641261844,770,Simplify the logs for query,zouxiang1993,26276281,zouxiang,,CLOSED,2023-03-27T02:38:50Z,2023-03-28T08:16:51Z,"### Describe This Problem

Now there are too many logs in the query path: 
```
2023-03-26 17:54:30.510 INFO [analytic_engine/src/sst/parquet/async_reader.rs:482] RecordBatchProjector dropped, path:0/14/761.sst rows:2103, cost:199ms.
2023-03-26 17:54:30.539 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-14-686.sst-0-2484243.
2023-03-26 17:54:30.637 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-8-969.sst-243269632-247463936.
2023-03-26 17:54:30.652 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-8-969.sst-247463936-251658240.
2023-03-26 17:54:30.742 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-6-949.sst-239075328-243269632.
2023-03-26 17:54:30.771 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-9-959.sst-138412032-142606336.
2023-03-26 17:54:30.825 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-997.sst-12582912-16777216.
2023-03-26 17:54:30.876 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-983.sst-33554432-37748736.
2023-03-26 17:54:31.004 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-514.sst-243269632-247463936.
2023-03-26 17:54:31.037 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-9-959.sst-142606336-146800640.
2023-03-26 17:54:31.063 INFO [analytic_engine/src/sst/parquet/async_reader.rs:174] Reader fetch record batches, path:0/14/763.sst, row_groups total:4, after prune:4
2023-03-26 17:54:31.063 INFO [analytic_engine/src/sst/parquet/async_reader.rs:194] Reader fetch record batches parallelly, parallelism suggest:8, real:4, chunk_size:2
2023-03-26 17:54:31.069 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-6-949.sst-243269632-247463936.
2023-03-26 17:54:31.084 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-983.sst-37748736-41943040.
2023-03-26 17:54:31.143 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-514.sst-247463936-251658240.
2023-03-26 17:54:31.248 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-9-483.sst-549453824-553648128.
2023-03-26 17:54:31.283 INFO [analytic_engine/src/sst/parquet/async_reader.rs:482] RecordBatchProjector dropped, path:0/14/763.sst rows:8192, cost:219ms.
2023-03-26 17:54:31.286 INFO [analytic_engine/src/sst/parquet/async_reader.rs:482] RecordBatchProjector dropped, path:0/14/763.sst rows:8192, cost:222ms.
2023-03-26 17:54:31.288 INFO [analytic_engine/src/sst/parquet/async_reader.rs:482] RecordBatchProjector dropped, path:0/14/763.sst rows:8192, cost:224ms.
2023-03-26 17:54:31.291 INFO [analytic_engine/src/sst/parquet/async_reader.rs:482] RecordBatchProjector dropped, path:0/14/763.sst rows:2311, cost:227ms.
2023-03-26 17:54:31.291 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-8-969.sst-251658240-255852544.
2023-03-26 17:54:31.355 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-999.sst-0-2395290.
2023-03-26 17:54:31.396 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-9-959.sst-146800640-150994944.
2023-03-26 17:54:31.464 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-997.sst-16777216-20971520.
2023-03-26 17:54:31.502 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-983.sst-41943040-46137344.
2023-03-26 17:54:31.549 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-514.sst-251658240-255852544.
2023-03-26 17:54:31.564 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-14-688.sst-0-2488352.
2023-03-26 17:54:31.648 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-9-483.sst-553648128-557842432.
2023-03-26 17:54:31.708 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-8-969.sst-255852544-260046848.
2023-03-26 17:54:31.887 INFO [analytic_engine/src/sst/parquet/async_reader.rs:174] Reader fetch record batches, path:0/14/765.sst, row_groups total:4, after prune:4
2023-03-26 17:54:31.887 INFO [analytic_engine/src/sst/parquet/async_reader.rs:194] Reader fetch record batches parallelly, parallelism suggest:8, real:4, chunk_size:2
2023-03-26 17:54:31.905 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-6-949.sst-247463936-251658240.
2023-03-26 17:54:31.954 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-997.sst-20971520-25165824.
2023-03-26 17:54:31.974 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-983.sst-46137344-50331648.
2023-03-26 17:54:32.069 INFO [analytic_engine/src/sst/parquet/async_reader.rs:482] RecordBatchProjector dropped, path:0/14/765.sst rows:8192, cost:182ms.
2023-03-26 17:54:32.072 INFO [analytic_engine/src/sst/parquet/async_reader.rs:482] RecordBatchProjector dropped, path:0/14/765.sst rows:8192, cost:185ms.
2023-03-26 17:54:32.074 INFO [analytic_engine/src/sst/parquet/async_reader.rs:482] RecordBatchProjector dropped, path:0/14/765.sst rows:8192, cost:186ms.
2023-03-26 17:54:32.075 INFO [analytic_engine/src/sst/parquet/async_reader.rs:482] RecordBatchProjector dropped, path:0/14/765.sst rows:2031, cost:188ms.
2023-03-26 17:54:32.075 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-9-959.sst-150994944-155189248.
2023-03-26 17:54:32.106 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-9-483.sst-557842432-562036736.
2023-03-26 17:54:32.149 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-6-949.sst-251658240-255852544.
2023-03-26 17:54:32.203 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-983.sst-50331648-54525952.
2023-03-26 17:54:32.288 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-514.sst-255852544-260046848.
2023-03-26 17:54:32.330 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-9-959.sst-155189248-159383552.
2023-03-26 17:54:32.391 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-8-969.sst-260046848-264241152.
2023-03-26 17:54:32.405 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-9-483.sst-562036736-566231040.
2023-03-26 17:54:32.475 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-6-949.sst-255852544-260046848.
2023-03-26 17:54:32.518 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-997.sst-25165824-29360128.
2023-03-26 17:54:32.617 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-983.sst-54525952-58720256.
2023-03-26 17:54:32.646 INFO [analytic_engine/src/sst/parquet/async_reader.rs:174] Reader fetch record batches, path:0/14/767.sst, row_groups total:4, after prune:4
2023-03-26 17:54:32.646 INFO [analytic_engine/src/sst/parquet/async_reader.rs:194] Reader fetch record batches parallelly, parallelism suggest:8, real:4, chunk_size:2
2023-03-26 17:54:32.685 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-514.sst-260046848-264241152.
2023-03-26 17:54:32.752 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-14-690.sst-0-2429693.
2023-03-26 17:54:32.845 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-8-969.sst-264241152-268435456.
2023-03-26 17:54:32.861 INFO [analytic_engine/src/sst/parquet/async_reader.rs:482] RecordBatchProjector dropped, path:0/14/767.sst rows:8192, cost:215ms.
2023-03-26 17:54:32.863 INFO [analytic_engine/src/sst/parquet/async_reader.rs:482] RecordBatchProjector dropped, path:0/14/767.sst rows:8192, cost:217ms.
2023-03-26 17:54:32.865 INFO [analytic_engine/src/sst/parquet/async_reader.rs:482] RecordBatchProjector dropped, path:0/14/767.sst rows:8192, cost:219ms.
2023-03-26 17:54:32.868 INFO [analytic_engine/src/sst/parquet/async_reader.rs:482] RecordBatchProjector dropped, path:0/14/767.sst rows:2155, cost:221ms.
2023-03-26 17:54:32.885 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-9-483.sst-566231040-570425344.
2023-03-26 17:54:32.968 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-9-959.sst-159383552-163577856.
2023-03-26 17:54:32.982 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-997.sst-29360128-33554432.
2023-03-26 17:54:33.060 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-6-949.sst-260046848-264241152.
2023-03-26 17:54:33.094 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-514.sst-264241152-268435456.
2023-03-26 17:54:33.140 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-8-969.sst-268435456-272629760.
2023-03-26 17:54:33.150 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-9-483.sst-570425344-574619648.
2023-03-26 17:54:33.214 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-9-959.sst-163577856-167772160.
2023-03-26 17:54:33.293 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-997.sst-33554432-37748736.
2023-03-26 17:54:33.427 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-6-949.sst-264241152-268435456.
2023-03-26 17:54:33.449 INFO [analytic_engine/src/sst/parquet/async_reader.rs:174] Reader fetch record batches, path:0/14/769.sst, row_groups total:4, after prune:4
2023-03-26 17:54:33.449 INFO [analytic_engine/src/sst/parquet/async_reader.rs:194] Reader fetch record batches parallelly, parallelism suggest:8, real:4, chunk_size:2
2023-03-26 17:54:33.479 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-514.sst-268435456-272629760.
2023-03-26 17:54:33.496 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-8-969.sst-272629760-276824064.
2023-03-26 17:54:33.571 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-997.sst-37748736-41943040.
2023-03-26 17:54:33.590 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-983.sst-58720256-62914560.
2023-03-26 17:54:33.616 INFO [analytic_engine/src/sst/parquet/async_reader.rs:482] RecordBatchProjector dropped, path:0/14/769.sst rows:8192, cost:167ms.
2023-03-26 17:54:33.618 INFO [analytic_engine/src/sst/parquet/async_reader.rs:482] RecordBatchProjector dropped, path:0/14/769.sst rows:8192, cost:169ms.
2023-03-26 17:54:33.621 INFO [analytic_engine/src/sst/parquet/async_reader.rs:482] RecordBatchProjector dropped, path:0/14/769.sst rows:8192, cost:172ms.
2023-03-26 17:54:33.623 INFO [analytic_engine/src/sst/parquet/async_reader.rs:482] RecordBatchProjector dropped, path:0/14/769.sst rows:2037, cost:174ms.
2023-03-26 17:54:33.685 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-9-959.sst-167772160-171966464.
2023-03-26 17:54:33.771 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-14-692.sst-0-2463524.
2023-03-26 17:54:33.787 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-9-483.sst-574619648-578813952.
2023-03-26 17:54:33.896 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-6-949.sst-268435456-272629760.
2023-03-26 17:54:33.943 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-8-969.sst-276824064-281018368.
2023-03-26 17:54:33.975 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-983.sst-62914560-67108864.
2023-03-26 17:54:34.035 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-12-514.sst-272629760-276824064.
2023-03-26 17:54:34.142 INFO [components/object_store/src/disk_cache.rs:150] Remove disk cache, filename:/data/ceresdb/sst_cache/0-9-959.sst-171966464-176160768.
```

### Proposal

Maybe we can change the log level to DEBUG

### Additional Context

None","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/770/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/770,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Ye0v5,horaedb,1484475385,770,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-27T04:31:38Z,2023-03-27T04:31:38Z,"Oops. The disk cache is disabled in most cases, so we didn't find these logs before. Would you like to submit a PR to fix it by changing its log level to DEBUG?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Ye0v5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/771,https://api.github.com/repos/apache/horaedb/issues/771,horaedb,1641349880,771,'Table is not found ' in PartitionTable,zouxiang1993,26276281,zouxiang,,CLOSED,2023-03-27T04:29:12Z,2023-11-03T06:48:31Z,"### Describe this problem

Error message is:
```
{""code"":500,""message"":""Failed to handle request, err:Failed to execute interpreter, query:\nselect count(*) from perflog\n, err:Failed to execute select, err:Failed to execute logical plan, err:Failed to collect record batch stream, err:Stream error, msg:convert from arrow record batch, err:External error: Stream error, msg:poll read response failed, err:Failed to query from table in server, table_ident:TableIdentifier { catalog: \""ceresdb\"", schema: \""public\"", table: \""__perflog_1\"" }, code:404, msg:table is not found, table:__perflog_1""}
```

### Server version

CeresDB Server
CeresDB version: 1.0.0
Git branch: main
Git commit: https://github.com/CeresDB/ceresdb/commit/95ea87054a940ba830973aba58e8476479d23680
Build time: 2023-03-06T03:30:09.845055021Z
Rustc version: 1.69.0-nightly

### Steps to reproduce

1. create a partition table : CREATE TABLE perflog (...) PARTITION BY KEY(host) PARTITIONS 10
2. DROP TABLE perflog
3. create the same table with a different partition number: CREATE TABLE perflog (...) PARTITION BY KEY(host) PARTITIONS 128
4. SELECT count(*) FROM xxx. Query fails, the error message is mentioned above.

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/771/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/771,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Ye1Al,horaedb,1484476453,771,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-27T04:33:55Z,2023-03-27T04:33:55Z,@chunshao90 Would you like to help troubleshoot this problem? I guess the table partitioning has not been fully supported yet.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Ye1Al/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/771,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Ye3qQ,horaedb,1484487312,771,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2023-03-27T04:55:48Z,2023-03-27T04:55:48Z,"Is there any error during the table creation process?
I cannot reproduce the error locally. Two `ceresdb-server` with one `ceresmeta-server`.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Ye3qQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/771,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YtY3A,horaedb,1488293312,771,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2023-03-29T09:56:06Z,2023-03-29T09:56:06Z,"This issue may be caused by ceresmeta.
Refer to https://github.com/CeresDB/ceresmeta/issues/152 .","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YtY3A/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/771,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qzxXw,horaedb,1791956464,771,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-11-03T06:48:31Z,2023-11-03T06:48:31Z,Fixed already.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qzxXw/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/775,https://api.github.com/repos/apache/horaedb/issues/775,horaedb,1641434944,775,sst not found during compaction,zouxiang1993,26276281,zouxiang,,CLOSED,2023-03-27T06:02:08Z,2023-03-27T06:16:02Z,"### Describe this problem

I create a partition table and write points to it. Then I execute a query and it fails with the response: 
```
{""code"":500,""message"":""Failed to handle request, err:Failed to execute interpreter, query:\nselect count(*) from perflog\n, err:Failed to execute select, err:Failed to execute logical plan, err:Failed to collect record batch stream, err:Stream error, msg:convert from arrow record batch, err:External error: Stream error, msg:poll read response failed, err:Failed to query from table in server, table_ident:TableIdentifier { catalog: \""ceresdb\"", schema: \""public\"", table: \""__perflog_95\"" }, code:500, msg:record batch failed. Caused by: Stream error, msg:read record batch, err:Failed to read data from the sub iterator, err:PullRecordBatch { source: DecodeRecordBatch { source: ParquetError { source: General(\""Failed to fetch ranges from object store, err:Object at location /data/ceresdb/store/0/96/182.sst not found: No such file or directory (os error 2)\""), backtrace:""}
```

I retry the query after a few seconds and it success.
There is no useful information in the server log.

It seems that the sst has been removed during compaction but the query gets the expired version.

### Server version

CeresDB Server
CeresDB version: 1.0.0
Git branch: main
Git commit: https://github.com/CeresDB/ceresdb/commit/95ea87054a940ba830973aba58e8476479d23680
Build time: 2023-03-06T03:30:09.845055021Z
Rustc version: 1.69.0-nightly

### Steps to reproduce

None

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/775/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/775,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YfGwt,horaedb,1484549165,775,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2023-03-27T06:07:22Z,2023-03-27T06:07:22Z,#699 has fixed this issue.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YfGwt/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/775,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YfIxx,horaedb,1484557425,775,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-27T06:16:02Z,2023-03-27T06:16:02Z,"@zouxiang1993 If you want to use the latest ceresdb server, the nightly image may be a good choice: https://github.com/CeresDB/ceresdb/pkgs/container/ceresdb-server","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YfIxx/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/779,https://api.github.com/repos/apache/horaedb/issues/779,horaedb,1641926913,779,"Not allow to add a not null column, name:timestamp",zouxiang1993,26276281,zouxiang,,CLOSED,2023-03-27T11:19:39Z,2023-03-30T03:43:31Z,"### Describe this problem

Error message is:
```
2023-03-27 08:58:03.127 INFO [server/src/proxy/grpc/write.rs:576] Add columns start, request_id:475, table:perflog, columns:[ColumnSchema { id: 2, name: ""timestamp"", data_type: Timestamp, is_nullable: false, is_tag: false, comment: """", escaped_name: ""timestamp"", default_value: None }]
2023-03-27 08:58:03.127 ERRO [server/src/proxy/grpc/write.rs:75] Failed to handle write, err:Rpc error, code:500, message:Failed to execute interpreter, err:Failed to execute alter table, err:Not allow to add a not null column, name:timestamp
```

### Server version

CeresDB Server 
CeresDB version: 1.0.0
Git branch: main
Git commit: 50471c0
Build time: 2023-03-26T20:55:13.872269184Z
Rustc version: 1.69.0-nightly

### Steps to reproduce

1. Create a table, use column ""ts"" as a TIMESTAMP KEY.  
```
CREATE TABLE xxx (
    ts timestamp not null, 
    ... 
    TIMESTAMP KEY(ts)
)
```

2. Write points through java SDK: 
```
    Point.PointBuilder builder = Point.newPointBuilder(""perflog"")
            .setTimestamp(msg.getTime())
            .addTag(""tag"", msg.getTag())
```

3. The ""timestamp"" column from SDK will be treated as a new column and then execute ALTER TABLE.

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/779/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/779,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YgxSa,horaedb,1484985498,779,NA,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,NA,2023-03-27T11:35:29Z,2023-03-27T11:35:29Z,Do you have any interest in fixing this bug? We would be very grateful!,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YgxSa/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/779,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Yg0d2,horaedb,1484998518,779,NA,zouxiang1993,26276281,zouxiang,,NA,2023-03-27T11:46:23Z,2023-03-27T11:46:23Z,OK. I will have a try.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Yg0d2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/779,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Yg1gY,horaedb,1485002776,779,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-27T11:49:32Z,2023-03-27T11:49:32Z,"It seems your write doesn't match table's schema, so it go to `auto add column` feature, which is introduced in #749.

Could you reproduce this issue when they are the same?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Yg1gY/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/779,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YhCXt,horaedb,1485055469,779,NA,zouxiang1993,26276281,zouxiang,,NA,2023-03-27T12:38:58Z,2023-03-27T12:38:58Z,"My write matches table's schema except the special timestamp column.  
The column name in the table's schema can be any string, but the column name from Java SDK is a constant in Schema Config:
https://github.com/CeresDB/ceresdb/blob/50471c0980d4b74a7d3b7491a10d9e10cf87b1b0/sql/src/planner.rs#L469
<br>
To fix this, maybe we can add a parameter to 'build_schema_from_write_table_request' and discard the timestamp column when auto add column: 
https://github.com/CeresDB/ceresdb/blob/50471c0980d4b74a7d3b7491a10d9e10cf87b1b0/sql/src/planner.rs#L371
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YhCXt/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/779,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YhHmn,horaedb,1485076903,779,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-27T13:15:33Z,2023-03-27T13:15:33Z,"> My write matches table's schema except the special timestamp column.

That's the problem. 

>  but the column name from Java SDK is a constant in Schema Config:

AFAIK, timestamp is a special field in protobuf, column name for it is not required.
https://github.com/CeresDB/ceresdbproto/blob/37ba6214b131bba1169bbdc6862be4f7408ff803/protos/storage.proto#L70

To fix this, I think we can reuse timestamp name in original schema when invoke `find_new_columns`, something like this

```rs
build_schema_from_write_table_request(
    schema_config: &SchemaConfig,
    write_table_req: &WriteTableRequest,
    timestamp_name: Option<&str> // Some(_) when call from find_new_columns
)
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YhHmn/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/779,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YrzA0,horaedb,1487876148,779,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-03-29T02:56:46Z,2023-03-29T02:56:46Z,"@zouxiang1993 Have you already started working on this issue?

We plan to release 1.1 this Friday, if you haven't started, we will fix this today.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YrzA0/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/779,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YyhrM,horaedb,1489640140,779,NA,zouxiang1993,26276281,zouxiang,,NA,2023-03-30T03:43:31Z,2023-03-30T03:43:31Z,"Sorry, I was going to fix it on the weekend.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5YyhrM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/781,https://api.github.com/repos/apache/horaedb/issues/781,horaedb,1643348144,781,SnappyDecode error when write via prom remote protocol,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-03-28T07:17:03Z,2023-05-10T02:28:01Z,"### Describe this problem

I found this when test prometheus remote write protocol, error message:

```
ts=2023-03-27T11:33:31.873Z caller=dedupe.go:112 component=remote level=warn remote_name=a1b1d7 url=http://localhost:4444/prom/v1/write msg=""Failed to send batch, retrying"" err=""server returned HTTP status 500 Internal Server Error: {\""code\"":500,\""message\"":\""UNKNOWN_ERROR: Rejection(SnappyDecode(Literal { len: 1, src_len: 0, dst_len: 29345 }))\""}""
ts=2023-03-27T11:33:34.088Z caller=dedupe.go:112 component=remote level=warn remote_name=a1b1d7 url=http://localhost:4444/prom/v1/write msg=""Failed to send batch, retrying"" err=""server returned HTTP status 500 Internal Server Error: {\""code\"":500,\""message\"":\""UNKNOWN_ERROR: Rejection(SnappyDecode(Literal { len: 7, src_len: 0, dst_len: 74523 }))\""}""
ts=2023-03-27T11:33:34.229Z caller=dedupe.go:112 component=remote level=warn remote_name=a1b1d7 url=http://localhost:4444/prom/v1/write msg=""Failed to send batch, retrying"" err=""server returned HTTP status 500 Internal Server Error: {\""code\"":500,\""message\"":\""UNKNOWN_ERROR: Rejection(SnappyDecode(HeaderMismatch { expected_len: 78004, got_len: 63920 }))\""}""
ts=2023-03-27T11:33:34.263Z caller=dedupe.go:112 component=remote level=warn remote_name=a1b1d7 url=http://localhost:4444/prom/v1/write msg=""Failed to send batch, retrying"" err=""server returned HTTP status 500 Internal Server Error: {\""code\"":500,\""mes^Cts=2023-03-27T11:33:48.980Z caller=dedupe.go:112 component=remote level=warn remote_name=a1b1d7 url=http://localhost:4444/prom/v1/write msg=""Failed to send batch, retrying"" err=""server returned HTTP status 500 Internal Server Error: {\""code\"":500,\""message\"":\""UNKNOWN_ERROR: Rejection(SnappyDecode(Literal { len: 4, src_len: 0, dst_len: 75607 }))\""}""
ts=2023-03-27T11:33:49.276Z caller=dedupe.go:112 component=remote level=warn remote_name=a1b1d7 url=http://localhost:4444/prom/v1/write msg=""Failed to send batch, retrying"" err=""server returned HTTP status 500 Internal Server Error: {\""code\"":500,\""message\"":\""UNKNOWN_ERROR: Rejection(SnappyDecode(Literal { len: 13, src_len: 8, dst_len: 59909 }))\""}""


```

### Server version

CeresDB version: 1.0.0                                                                                                                                                                                                                
Git branch: main                                                                                                                                                                                                                      Git commit: 32bd51e                                                                                                                                                                                                                   
Build time: 2023-03-27T21:01:30.051952251Z                                                                                                                                                                                            
Rustc version: 1.69.0-nightly  


### Steps to reproduce

Setup remote write described in https://docs.ceresdb.io/ecosystem/prometheus.html

Then after a while, error above will arise.

### Expected behavior

No decode error

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/781/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/785,https://api.github.com/repos/apache/horaedb/issues/785,horaedb,1643889702,785,Still open table when table create failed,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2023-03-28T12:53:21Z,2023-11-03T06:48:10Z,"### Describe this problem

Here is the error log, i found CeresDb still open table after table create failed, I guess there are some dirty data in CeresMeta.
And I can not create table again because table meta data exists in CeresMeta.
 
```
2023-03-27 23:49:36.262 INFO [server/src/grpc/meta_event_service/mod.rs:146] Receive request from meta, req:CreateTableOnShardRequest { update_shard_info: Some(UpdateShardInfo { curr_shard_info: Some(ShardInfo { id: 49, role: Leader, version: 708 }), prev_version: 707 }), table_info: Some(TableInfo { id: 54458, name: ""SPM_546165999_INFLUENCE_DEFAULT"", schema_id: 0, schema_name: ""public"", partition_info: None }), encoded_schema: [0, 10, 10, 10, 4, 116, 115, 105, 100, 16, 5, 32, 1, 10, 12, 10, 6, 112, 101, 114, 105, 111, 100, 16, 1, 32, 2, 10, 23, 10, 13, 103, 114, 111, 117, 112, 98, 121, 73, 110, 100, 101, 120, 48, 16, 4, 24, 1, 32, 3, 40, 1, 10, 23, 10, 13, 103, 114, 111, 117, 112, 98, 121, 73, 110, 100, 101, 120, 49, 16, 4, 24, 1, 32, 4, 40, 1, 10, 23, 10, 13, 103, 114, 111, 117, 112, 98, 121, 73, 110, 100, 101, 120, 50, 16, 4, 24, 1, 32, 5, 40, 1, 10, 17, 10, 7, 84, 114, 97, 99, 101, 73, 100, 16, 4, 24, 1, 32, 6, 40, 1, 10, 17, 10, 7, 95, 114, 101, 115, 117, 108, 116, 16, 4, 24, 1, 32, 7, 40, 1, 10, 16, 10, 6, 115, 101, 114, 118, 101, 114, 16, 4, 24, 1, 32, 8, 40, 1, 10, 13, 10, 3, 105, 100, 99, 16, 4, 24, 1, 32, 9, 40, 1, 10, 13, 10, 3, 108, 100, 99, 16, 4, 24, 1, 32, 10, 40, 1, 16, 1, 24, 2, 34, 2, 1, 2], engine: ""Analytic"", create_if_not_exist: false, options: {""ttl"": ""3d"", ""update_mode"": ""APPEND""} }
2023-03-27 23:49:36.262 ERRO [server/src/grpc/meta_event_service/mod.rs:146] Fail to process request from meta, err:Server error, code:Internal, message:fail to create table on shard in cluster, req:CreateTableOnShardRequest { update_shard_info: Some(UpdateShardInfo { curr_shard_info: Some(ShardInfo { id: 49, role: Leader, version: 708 }), prev_version: 707 }), table_info: Some(TableInfo { id: 54458, name: ""SPM_546165999_INFLUENCE_DEFAULT"", schema_id: 0, schema_name: ""public"", partition_info: None }), encoded_schema: [0, 10, 10, 10, 4, 116, 115, 105, 100, 16, 5, 32, 1, 10, 12, 10, 6, 112, 101, 114, 105, 111, 100, 16, 1, 32, 2, 10, 23, 10, 13, 103, 114, 111, 117, 112, 98, 121, 73, 110, 100, 101, 120, 48, 16, 4, 24, 1, 32, 3, 40, 1, 10, 23, 10, 13, 103, 114, 111, 117, 112, 98, 121, 73, 110, 100, 101, 120, 49, 16, 4, 24, 1, 32, 4, 40, 1, 10, 23, 10, 13, 103, 114, 111, 117, 112, 98, 121, 73, 110, 100, 101, 120, 50, 16, 4, 24, 1, 32, 5, 40, 1, 10, 17, 10, 7, 84, 114, 97, 99, 101, 73, 100, 16, 4, 24, 1, 32, 6, 40, 1, 10, 17, 10, 7, 95, 114, 101, 115, 117, 108, 116, 16, 4, 24, 1, 32, 7, 40, 1, 10, 16, 10, 6, 115, 101, 114, 118, 101, 114, 16, 4, 24, 1, 32, 8, 40, 1, 10, 13, 10, 3, 105, 100, 99, 16, 4, 24, 1, 32, 9, 40, 1, 10, 13, 10, 3, 108, 100, 99, 16, 4, 24, 1, 32, 10, 40, 1, 16, 1, 24, 2, 34, 2, 1, 2], engine: ""Analytic"", create_if_not_exist: false, options: {""ttl"": ""3d"", ""update_mode"": ""APPEND""} }, cause:Shard not found, msg:insert table to a non-existent shard, shard_id:49.
2023-03-27 23:49:36.262 INFO [server/src/grpc/meta_event_service/mod.rs:146] Finish handling request from meta, resp:CreateTableOnShardResponse { header: Some(ResponseHeader { code: 500, error: ""fail to create table on shard in cluster, req:CreateTableOnShardRequest { update_shard_info: Some(UpdateShardInfo { curr_shard_info: Some(ShardInfo { id: 49, role: Leader, version: 708 }), prev_version: 707 }), table_info: Some(TableInfo { id: 54458, name: \""SPM_546165999_INFLUENCE_DEFAULT\"", schema_id: 0, schema_name: \""public\"", partition_info: None }), encoded_schema: [0, 10, 10, 10, 4, 116, 115, 105, 100, 16, 5, 32, 1, 10, 12, 10, 6, 112, 101, 114, 105, 111, 100, 16, 1, 32, 2, 10, 23, 10, 13, 103, 114, 111, 117, 112, 98, 121, 73, 110, 100, 101, 120, 48, 16, 4, 24, 1, 32, 3, 40, 1, 10, 23, 10, 13, 103, 114, 111, 117, 112, 98, 121, 73, 110, 100, 101, 120, 49, 16, 4, 24, 1, 32, 4, 40, 1, 10, 23, 10, 13, 103, 114, 111, 117, 112, 98, 121, 73, 110, 100, 101, 120, 50, 16, 4, 24, 1, 32, 5, 40, 1, 10, 17, 10, 7, 84, 114, 97, 99, 101, 73, 100, 16, 4, 24, 1, 32, 6, 40, 1, 10, 17, 10, 7, 95, 114, 101, 115, 117, 108, 116, 16, 4, 24, 1, 32, 7, 40, 1, 10, 16, 10, 6, 115, 101, 114, 118, 101, 114, 16, 4, 24, 1, 32, 8, 40, 1, 10, 13, 10, 3, 105, 100, 99, 16, 4, 24, 1, 32, 9, 40, 1, 10, 13, 10, 3, 108, 100, 99, 16, 4, 24, 1, 32, 10, 40, 1, 16, 1, 24, 2, 34, 2, 1, 2], engine: \""Analytic\"", create_if_not_exist: false, options: {\""ttl\"": \""3d\"", \""update_mode\"": \""APPEND\""} }. Caused by: Shard not found, msg:insert table to a non-existent shard, shard_id:49."" }) }
2023-03-27 23:51:35.137 INFO [analytic_engine/src/engine.rs:124] Table engine impl open table, space_id:0, request:OpenTableRequest { catalog_name: ""ceresdb"", schema_name: ""public"", schema_id: SchemaId(0), table_name: ""SPM_546165999_INFLUENCE_DEFAULT"", table_id: TableId(54458), engine: ""Analytic"", shard_id: 49, cluster_version: 1 }
2023-03-27 23:51:35.141 ERRO [server/src/grpc/meta_event_service/mod.rs:265] no table is opened, open_request:OpenTableRequest { catalog_name: ""ceresdb"", schema_name: ""public"", schema_id: SchemaId(0), table_name: ""SPM_546165999_INFLUENCE_DEFAULT"", table_id: TableId(54458), engine: ""Analytic"", shard_id: 49, cluster_version: 1 }
```

### Server version

CeresDB Server 
CeresDB version: 1.0.0
Git branch: main
Git commit: 32bd51e
Build time: 2023-03-27T08:55:26.394167676Z
Rustc version: 1.69.0-nightly

### Steps to reproduce

Create massive table, there are 20k tables in my cluster.

### Expected behavior

No dirty table meta data in CeresMeta, and create table successfully when retry.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/785/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/785,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qzxUO,horaedb,1791956238,785,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-11-03T06:48:10Z,2023-11-03T06:48:10Z,Maybe fixed already on the lastest version.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qzxUO/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/792,https://api.github.com/repos/apache/horaedb/issues/792,horaedb,1646892740,792,"Unsupported arrow data type, type:Timestamp(Nanosecond, None)",zouxiang1993,26276281,zouxiang,,CLOSED,2023-03-30T04:30:45Z,2023-04-11T11:57:03Z,"### Describe this problem

None

### Server version

CeresDB Server
CeresDB version: 1.0.0
Git branch: main
Git commit: https://github.com/CeresDB/ceresdb/commit/95ea87054a940ba830973aba58e8476479d23680
Build time: 2023-03-06T03:30:09.845055021Z
Rustc version: 1.69.0-nightly

### Steps to reproduce

1. Create a table:
```
curl --location --request POST 'http://127.0.0.1:5440/sql' --data-raw '
CREATE TABLE `demo` (
    `name` string TAG,
    `value` double NOT NULL,
    `t` timestamp NOT NULL,
    timestamp KEY (t))
ENGINE=Analytic
  with
(enable_ttl=""false"")
'
```

2. Insert a row:
```
curl --location --request POST 'http://127.0.0.1:5440/sql' --data-raw '
INSERT INTO demo (t, name, value)
    VALUES (1651737067000, ""ceresdb"", 100)
'
```

3. Query 1:
```
curl --location --request POST 'http://127.0.0.1:5440/sql' --data-raw '
SELECT to_timestamp_millis(date_trunc(""minute"", t))
FROM demo
'
```
Response:
```
{""rows"":[{""totimestampmillis(datetrunc(Utf8(\""minute\""),demo.t))"":1651737060000}]}
```

4. Query 2:
```
curl --location --request POST 'http://127.0.0.1:5440/sql' --data-raw '
SELECT date_trunc(""minute"", t)
FROM demo
'
```
Error message:
```
{""code"":500,""message"":""Failed to handle request, err:Failed to execute interpreter, query:\nSELECT date_trunc(\""minute\"", t)\nFROM demo\n, err:Failed to execute select, err:Failed to execute logical plan, err:Failed to execute physical plan, err:Failed to convert datafusion stream, err:Stream error, msg:convert record schema, err:Invalid arrow field, field_name:datetrunc(Utf8(\""minute\""),demo.t), arrow_schema:Schema { fields: [Field { name: \""datetrunc(Utf8(\\\""minute\\\""),demo.t)\"", data_type: Timestamp(Nanosecond, None), nullable: true, dict_id: 0, dict_is_ordered: false, metadata: {} }], metadata: {\""schema::timestamp_index\"": \""1\"", \""schema::version\"": \""1\"", \""schema::primary_key_indexes\"": \""0,1\""} }, err:Unsupported arrow data type, type:Timestamp(Nanosecond, None).""}
```

5. Query 3:
```
curl --location --request POST 'http://127.0.0.1:5440/sql' --data-raw '
SELECT date_trunc(""minute"", to_timestamp_millis(t))
FROM demo
'
```
Error message:
```
{""code"":500,""message"":""Failed to handle request, err:Failed to execute interpreter, query:\nSELECT date_trunc(\""minute\"", to_timestamp_millis(t))\nFROM demo\n, err:Failed to execute select, err:Failed to execute logical plan, err:Failed to execute physical plan, err:Failed to convert datafusion stream, err:Stream error, msg:convert record schema, err:Invalid arrow field, field_name:datetrunc(Utf8(\""minute\""),totimestampmillis(demo.t)), arrow_schema:Schema { fields: [Field { name: \""datetrunc(Utf8(\\\""minute\\\""),totimestampmillis(demo.t))\"", data_type: Timestamp(Nanosecond, None), nullable: true, dict_id: 0, dict_is_ordered: false, metadata: {} }], metadata: {\""schema::timestamp_index\"": \""1\"", \""schema::version\"": \""1\"", \""schema::primary_key_indexes\"": \""0,1\""} }, err:Unsupported arrow data type, type:Timestamp(Nanosecond, None).""}
```

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/792/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/792,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Y4mkM,horaedb,1491233036,792,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-31T03:17:34Z,2023-03-31T03:17:34Z,"This is caused by that `date_trunc`generates timestamp in nanosecond, which has not supported yet by CeresDB. Two methods to fix this:
- Support nanosecond in CeresDB;
- Try to convert the data type of `date_trunc` output to millisecond, that is to say, make the output of `date_trunc` follows the type of the input source;","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Y4mkM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/792,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Y4qwo,horaedb,1491250216,792,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-03-31T03:54:43Z,2023-03-31T03:54:43Z,"Currently, the issue about the output type of the `date_trunc` has been filed: https://github.com/apache/arrow-datafusion/issues/5750.

Before this issue fixed, a work-around way is to use `to_timestamp_millis` convert its output to millisecond. @zouxiang1993 ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5Y4qwo/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/792,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ZmOey,horaedb,1503193010,792,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-04-11T11:57:02Z,2023-04-11T11:57:02Z,#780 should fix this.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ZmOey/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/799,https://api.github.com/repos/apache/horaedb/issues/799,horaedb,1648668071,799,Shard based recovery,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-03-31T04:57:42Z,2024-03-12T09:00:04Z,"### Describe This Problem

Now table recovery in on table level but wal's storing in on shard level.
The recovery performance may be unsatisfied especially in kafka based wal.  

### Proposal

- Split actual table recovery from schema, and refactor table engine
- Impl shard based table meta recovery
- Impl shard based table data recovery

#### 1. Split actual table recovery from schema, and refactor table engine
We should begin at modifying the high level interface(Schema and TableEngine) for adapting to the new recovery process.

Now the path about `Schema` and `TableEngine` when opening tables on shard is like:
<img src=""https://raw.githubusercontent.com/Rachelint/drawio-store/main/now.drawio.svg?sanitize=true"">

For modify interfaces above to open whole tables on shard together rather than respectively, the most troublesome place is: 
+ Tables on the same shard may belong to different schema, so we are unable to add the api like `open_tables_on_shard` to `Schema`.

My solution about this is: 
- [x]  Split actual table recovery from schema, we just call the `TableEngine` directly, and just register the opened tables to `Schema` #808 

In this stage, we still keep the origin interface of `TableEngine`, the path may be like:

<img src=""https://raw.githubusercontent.com/Rachelint/drawio-store/main/stage1.drawio.svg?sanitize=true"">

- [x]  Refactor the `TableEngine` interface to support shard level opening
tbc...
- [ ] Furthermore, split `Schema` and `TableEngine` completely? 

+ Just keep `register_table` and `unregister_table` in `Schema`
+ Remove all table operation(create,drop,open,close), and call them directly in `TableEngine`

#### 2. Impl shard based table meta recovery
- [x] Refactor manifest module
    - Rename the original `TableData` to `TableContext`, and extract members which will be recovered from manifest as the new `TableData`.
    - Register the `TableData` into manifest when `create table` and `open table`, and unregister it when `drop table` and `close table`.
    - When do snapshot in `Manifest`, we just make use of the hold `TableData` rather than scanning the persist wals.
- [x] Place `TableData`s into `Manifest`, and we update the memory and storage in just one place.
- [ ] Shard based manifest recovery

#### 3. Impl shard based table data recovery
- [x] Region based wal replay  #976 
- [ ] Make wal replay more concurrently

#### 4. other
- [x] Add integration test about recovery #996 
- [ ] [WIP] retry when open shard failed
- [ ] Support limited retry in kafka client #1005 
- [ ] Fix logs deleting in wal on kafka

### Additional Context




_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/799/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/799,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ZBgsT,horaedb,1493568275,799,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-04-03T02:58:43Z,2023-04-03T02:58:43Z,"@Rachelint In the #800, the separate runtime for recovery has been mentioned, and I guess it should be taken into considerations together with this.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ZBgsT/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/799,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM52rX_m,horaedb,1991081958,799,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-03-12T09:00:04Z,2024-03-12T09:00:04Z,"Most tasks are finished, so closing.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM52rX_m/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/800,https://api.github.com/repos/apache/horaedb/issues/800,horaedb,1648757927,800,Tracking Issue: accelerate recover speed,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-03-31T06:30:05Z,2023-11-03T06:46:17Z,"### Describe This Problem

Current mechanism of recovering when starting up costs too much time when the number of tables belonging to the ceresdb server instance is large. And there are two known reasons accounting for the slow recovering:
- Recovering work is processed table by table, but data is actually organized by shard;
- Recovering work is handled in the same runtime with write work, that is to say, recovering work has intensive contention with the writing work.

### Proposal

Here is a simple version of the proposal to resolve the two problems:
- [ ] Recover data shard by shard. #799 
- [x] Recover data in a separate runtime from write work. Solved by #814 

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/800/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/801,https://api.github.com/repos/apache/horaedb/issues/801,horaedb,1651225988,801,Build the snapshot according to data structure in memory rather than pulling logs from storage repeatedly.,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-04-03T02:30:22Z,2023-04-25T08:35:04Z,"## Problems
Currently, in order to build a snapshot of manifest, it is necessary to scan the wal to fetch the latest changes. However, the data in wal is organized by shards, that is to say, it may cost too much to scan manifest data table by table.

## Proposal
Maintain a memory state of table's manifest in the memory, and do snapshot by it.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/801/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/813,https://api.github.com/repos/apache/horaedb/issues/813,horaedb,1658547009,813,Should CeresDB support `Decimal` datatype?,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2023-04-07T08:45:18Z,2024-12-18T04:43:53Z,"### Describe This Problem

Datatype `Decimal` is popular used in finance scene, should we support it?
And I found datafusion had support `Decimal` in https://github.com/apache/arrow-datafusion/issues/122

### Proposal

Support `Decimal` datatype

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/813/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/816,https://api.github.com/repos/apache/horaedb/issues/816,horaedb,1660189073,816,Optimize the response message for creating an already existing table,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-04-10T03:53:32Z,2023-07-12T08:15:55Z,"### Describe This Problem

When i create an existing table, the response is confusing.
```
{
    ""code"": 500,
    ""message"": ""Failed to handle request, err:Rpc error, code:500, message:Failed to execute interpreter, err:Failed to execute create table, err:Failed to create table by table manipulator, err:Failed to create table, msg:failed to create table by meta client, req:CreateTableRequest { schema_name: \""public\"", name: \""demo2\"", encoded_schema: [0, 10, 10, 10, 4, 116, 115, 105, 100, 16, 5, 32, 1, 10, 7, 10, 1, 116, 16, 1, 32, 2, 10, 14, 10, 4, 110, 97, 109, 101, 16, 4, 24, 1, 32, 3, 40, 1, 10, 12, 10, 2, 105, 100, 16, 8, 24, 1, 32, 4, 40, 1, 10, 11, 10, 5, 118, 97, 108, 117, 101, 16, 2, 32, 5, 16, 1, 24, 2, 34, 2, 1, 2], engine: \""Analytic\"", create_if_not_exist: false, options: {\""enable_ttl\"": \""false\""}, partition_table_info: None }, err:Missing table info, msg:created table is not found in the create table response.""
}
```

### Proposal

Optimize the response message.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/816/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/816,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5hLNGI,horaedb,1630327176,816,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-07-11T07:50:53Z,2023-07-11T07:50:53Z,"This is what I got now

```
2023-07-11 15:49:55.526 ERRO [server/src/mysql/worker.rs:86] MysqlWorker on_query failed. err:Failed to handle sql:CREATE TABLE `demo2` ( `timestamp` timestamp NOT NULL, `name` string TAG, `value
` double NOT NULL, PRIMARY KEY(tsid,timestamp), TIMESTAMP KEY(timestamp)), err:Rpc error, code:500, message:Failed to execute plan, sql:CREATE TABLE `demo2` ( `timestamp` timestamp NOT NULL, `nam
e` string TAG, `value` double NOT NULL, PRIMARY KEY(tsid,timestamp), TIMESTAMP KEY(timestamp)), err:Internal error, msg:Failed to execute interpreter, err:Failed to execute create table, err:Fail
ed to create table by table manipulator, err:Failed to operate table, err:Failed to operate table, msg:Some(""failed to create table on shard, request:CreateTableRequest { catalog_name: \""ceresdb\
"", schema_name: \""public\"", table_name: \""demo2\"", table_id: None, table_schema: Schema { timestamp_index: 1, tsid_index: Some(0), column_schemas: ColumnSchemas { columns: [ColumnSchema { id: 1, 
name: \""tsid\"", data_type: UInt64, is_nullable: false, is_tag: false, comment: \""\"", escaped_name: \""tsid\"", default_value: None }, ColumnSchema { id: 2, name: \""timestamp\"", data_type: Timestamp
, is_nullable: false, is_tag: false, comment: \""\"", escaped_name: \""timestamp\"", default_value: None }, ColumnSchema { id: 3, name: \""name\"", data_type: String, is_nullable: true, is_tag: true, c
omment: \""\"", escaped_name: \""name\"", default_value: None }, ColumnSchema { id: 4, name: \""value\"", data_type: Double, is_nullable: false, is_tag: false, comment: \""\"", escaped_name: \""value\"", d
efault_value: None }] }, version: 1, primary_key_indexes: [0, 1] }, engine: \""Analytic\"", options: {}, state: Stable, shard_id: 0, partition_info: None }""), err:Failed to create table, table alre
ady exists, table:demo2.
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5hLNGI/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/818,https://api.github.com/repos/apache/horaedb/issues/818,horaedb,1660522698,818,Can't build with cmake >=2.25.0,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-04-10T09:42:08Z,2023-04-11T04:35:50Z,"### Describe this problem

It reports failure when building the ceresdb with cmake = 2.26:
```
    Policy CMP0066 is not set: Honor per-config flags in try_compile()
    source-file signature.  Run ""cmake --help-policy CMP0066"" for policy
    details.  Use the cmake_policy command to set the policy and suppress this
    warning.

    For compatibility with older versions of CMake, try_compile is not honoring
    caller config-specific compiler flags (e.g.  CMAKE_C_FLAGS_DEBUG) in the
    test project.
```

### Server version

Built on macos 13.3, Apple M1 Pro.

### Steps to reproduce

Execute `cargo build` on the root directory of ceresdb project.

### Expected behavior

Successful building.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/818/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/819,https://api.github.com/repos/apache/horaedb/issues/819,horaedb,1660524262,819,Add write batch size metrics,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-04-10T09:43:52Z,2023-04-12T02:42:41Z,"### Describe This Problem

Write batch size is important to improve write throughtput, we should record it to help debug performance issue.

### Proposal

Add prometheus metrics to records it.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/819/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/820,https://api.github.com/repos/apache/horaedb/issues/820,horaedb,1660673408,820,Impl `show create table` of partition table,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-04-10T11:49:09Z,2023-04-20T10:03:44Z,"### Describe This Problem

The ""show create table"" command for partition table dose not include all the table options in the returned output.

### Proposal

Retrieve options from the first sub-table.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/820/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/830,https://api.github.com/repos/apache/horaedb/issues/830,horaedb,1663646166,830,Tracking issue for influxql support,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-04-12T03:22:43Z,2023-04-24T07:48:59Z,"### Describe This Problem

After #824, influxql support is almost finished, users can query ceresdb via influxdb datasource in grafana. 

This issue track some improvements:

- [x] In InfluxQLSchemaProvider we hold all tables in HashMap, this is expensive and in theory we could load tables in lazy way.
- [x] Currently depend on iox directly, maybe we could extract them to https://github.com/CeresDB/influxql, or better to contribute to iox project.


### Proposal

See above.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/830/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/830,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5akp0t,horaedb,1519557933,830,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2023-04-24T07:48:49Z,2023-04-24T07:48:49Z,"All tasks finished, close it until new tasks came.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5akp0t/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/836,https://api.github.com/repos/apache/horaedb/issues/836,horaedb,1665616391,836,Found some bugs when using PQL,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-04-13T03:17:26Z,2023-04-24T05:16:18Z,"### Describe this problem

When i use PQL query data, i found some bugs:
* PQL does not support query label types other than strings.
* Panic occurs when query tag is null.
```
2023-04-12 17:50:17.061 ERRO [common_util/src/panic.rs:42] thread 'ceres-default' panicked 'checked in try_new' at ""server/src/proxy/http/prom.rs:242""
   0: backtrace::backtrace::libunwind::trace
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/backtrace-0.3.67/src/backtrace/libunwind.rs:93:5
      backtrace::backtrace::trace_unsynchronized
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/backtrace-0.3.67/src/backtrace/mod.rs:66:5
   1: backtrace::backtrace::trace
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/backtrace-0.3.67/src/backtrace/mod.rs:53:14
   2: backtrace::capture::Backtrace::create
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/backtrace-0.3.67/src/capture.rs:176:9
   3: backtrace::capture::Backtrace::new
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/backtrace-0.3.67/src/capture.rs:140:22
   4: common_util::panic::set_panic_hook::{{closure}}
             at common_util/src/panic.rs:41:18
   5: <alloc::boxed::Box<F,A> as core::ops::function::Fn<Args>>::call
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:2002:9
      std::panicking::rust_panic_with_hook
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:692:13
   6: std::panicking::begin_panic_handler::{{closure}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:579:13
   7: std::sys_common::backtrace::__rust_end_short_backtrace
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys_common/backtrace.rs:137:18
   8: rust_begin_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:575:5
   9: core::panicking::panic_fmt
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panicking.rs:64:14
  10: core::panicking::panic_display
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panicking.rs:147:5
  11: core::panicking::panic_str
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panicking.rs:131:5
  12: core::option::expect_failed
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/option.rs:1924:5
  13: core::option::Option<T>::expect
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/option.rs:786:21
  14: server::proxy::http::prom::Converter::convert::{{closure}}::{{closure}}
             at server/src/proxy/http/prom.rs:242:49
  15: core::iter::adapters::map::map_fold::{{closure}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/iter/adapters/map.rs:84:28
  16: <core::iter::adapters::enumerate::Enumerate<I> as core::iter::traits::iterator::Iterator>::fold::enumerate::{{closure}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/iter/adapters/enumerate.rs:106:27
  17: core::iter::traits::iterator::Iterator::fold
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/iter/traits/iterator.rs:2438:21
  18: <core::iter::adapters::enumerate::Enumerate<I> as core::iter::traits::iterator::Iterator>::fold
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/iter/adapters/enumerate.rs:112:9
  19: <core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/iter/adapters/map.rs:124:9
  20: core::iter::traits::iterator::Iterator::for_each
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/iter/traits/iterator.rs:837:9
  21: alloc::vec::Vec<T,A>::extend_trusted
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/vec/mod.rs:2856:17
  22: <alloc::vec::Vec<T,A> as alloc::vec::spec_extend::SpecExtend<T,I>>::spec_extend
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/vec/spec_extend.rs:26:9
  23: <alloc::vec::Vec<T> as alloc::vec::spec_from_iter_nested::SpecFromIterNested<T,I>>::from_iter
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/vec/spec_from_iter_nested.rs:62:9
  24: <alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/vec/spec_from_iter.rs:33:9
  25: <alloc::vec::Vec<T> as core::iter::traits::collect::FromIterator<T>>::from_iter
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/vec/mod.rs:2724:9
  26: core::iter::traits::iterator::Iterator::collect
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/iter/traits/iterator.rs:1860:9
  27: server::proxy::http::prom::Converter::convert::{{closure}}
             at server/src/proxy/http/prom.rs:236:42
  28: std::collections::hash::map::Entry<K,V>::or_insert_with
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/collections/hash/map.rs:2559:43
  29: server::proxy::http::prom::Converter::convert
             at server/src/proxy/http/prom.rs:233:17
  30: server::proxy::http::prom::convert_query_result
             at server/src/proxy/http/prom.rs:386:5
  31: server::proxy::http::prom::<impl server::proxy::Proxy<Q>>::prom_process_query::{{closure}}
             at server/src/proxy/http/prom.rs:122:9
  32: server::proxy::http::prom::<impl prom_remote_api::types::RemoteStorage for server::proxy::Proxy<Q>>::process_query::{{closure}}
             at server/src/proxy/http/prom.rs:140:43
  33: <core::pin::Pin<P> as core::future::future::Future>::poll
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125:9
  34: prom_remote_api::types::RemoteStorage::read::{{closure}}::{{closure}}::{{closure}}
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/prom-remote-api-0.2.1/src/types.rs:81:61
  35: <futures_util::future::maybe_done::MaybeDone<Fut> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.28/src/future/maybe_done.rs:95:38
  36: <futures_util::future::join_all::JoinAll<F> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.28/src/future/join_all.rs:142:24
  37: prom_remote_api::types::RemoteStorage::read::{{closure}}
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/prom-remote-api-0.2.1/src/types.rs:83:9
  38: <core::pin::Pin<P> as core::future::future::Future>::poll
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125:9
  39: prom_remote_api::web::warp::read::{{closure}}
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/prom-remote-api-0.2.1/src/web/warp.rs:41:27
  40: <F as futures_core::future::TryFuture>::try_poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.28/src/future.rs:82:9
  41: <warp::filter::and_then::State<T,F> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/and_then.rs:99:44
  42: <warp::filter::and_then::AndThenFuture<T,F> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/and_then.rs:74:9
  43: <F as futures_core::future::TryFuture>::try_poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.28/src/future.rs:82:9
  44: <warp::filter::or::EitherFuture<T,U> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/or.rs:87:44
  45: <warp::filter::and::State<T,TE,U> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/and.rs:88:38
  46: <warp::filter::and::AndFuture<T,U> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/and.rs:65:9
  47: <F as futures_core::future::TryFuture>::try_poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.28/src/future.rs:82:9
  48: <warp::filter::or::EitherFuture<T,U> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/or.rs:87:44
  49: <F as futures_core::future::TryFuture>::try_poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.28/src/future.rs:82:9
  50: <warp::filter::or::EitherFuture<T,U> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/or.rs:77:65
  51: <F as futures_core::future::TryFuture>::try_poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.28/src/future.rs:82:9
  52: <warp::filter::or::EitherFuture<T,U> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/or.rs:77:65
  53: <F as futures_core::future::TryFuture>::try_poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.28/src/future.rs:82:9
  54: <warp::filter::or::EitherFuture<T,U> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/or.rs:77:65
  55: <F as futures_core::future::TryFuture>::try_poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.28/src/future.rs:82:9
  56: <warp::filter::or::EitherFuture<T,U> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/or.rs:77:65
  57: <F as futures_core::future::TryFuture>::try_poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.28/src/future.rs:82:9
  58: <warp::filter::or::EitherFuture<T,U> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/or.rs:77:65
  59: <F as futures_core::future::TryFuture>::try_poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.28/src/future.rs:82:9
  60: <warp::filter::or::EitherFuture<T,U> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/or.rs:77:65
  61: <F as futures_core::future::TryFuture>::try_poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.28/src/future.rs:82:9
  62: <warp::filter::or::EitherFuture<T,U> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/or.rs:77:65
  63: <F as futures_core::future::TryFuture>::try_poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.28/src/future.rs:82:9
  64: <warp::filter::or::EitherFuture<T,U> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/or.rs:77:65
  65: <F as futures_core::future::TryFuture>::try_poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.28/src/future.rs:82:9
  66: <warp::filter::recover::RecoverFuture<T,F> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/recover.rs:91:65
  67: <F as futures_core::future::TryFuture>::try_poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.28/src/future.rs:82:9
  68: <warp::filter::service::FilteredFuture<F> as core::future::future::Future>::poll::{{closure}}
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/service.rs:128:40
  69: scoped_tls::ScopedKey<T>::set
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/scoped-tls-1.0.1/src/lib.rs:137:9
  70: warp::route::set
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/route.rs:16:5
  71: <warp::filter::service::FilteredFuture<F> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/warp-0.3.3/src/filter/service.rs:128:15
  72: <hyper::proto::h1::dispatch::Server<S,hyper::body::body::Body> as hyper::proto::h1::dispatch::Dispatch>::poll_msg
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.25/src/proto/h1/dispatch.rs:491:35
  73: hyper::proto::h1::dispatch::Dispatcher<D,Bs,I,T>::poll_write
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.25/src/proto/h1/dispatch.rs:297:43
  74: hyper::proto::h1::dispatch::Dispatcher<D,Bs,I,T>::poll_loop
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.25/src/proto/h1/dispatch.rs:161:21
  75: hyper::proto::h1::dispatch::Dispatcher<D,Bs,I,T>::poll_inner
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.25/src/proto/h1/dispatch.rs:137:16
  76: hyper::proto::h1::dispatch::Dispatcher<D,Bs,I,T>::poll_catch
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.25/src/proto/h1/dispatch.rs:120:28
  77: <hyper::proto::h1::dispatch::Dispatcher<D,Bs,I,T> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.25/src/proto/h1/dispatch.rs:424:9
  78: <hyper::server::conn::ProtoServer<T,B,S,E> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.25/src/server/conn.rs:969:47
  79: <hyper::server::conn::upgrades::UpgradeableConnection<I,S,E> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.25/src/server/conn.rs:1029:30
  80: <hyper::common::drain::Watching<F,FN> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.25/src/common/drain.rs:95:36
  81: <hyper::server::server::new_svc::NewSvcTask<I,N,S,E,W> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/hyper-0.14.25/src/server/server.rs:751:36
  82: <core::pin::Pin<P> as core::future::future::Future>::poll
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125:9
  83: tokio::runtime::task::core::Core<T,S>::poll::{{closure}}
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/core.rs:223:17
  84: tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/loom/std/unsafe_cell.rs:14:9
  85: tokio::runtime::task::core::Core<T,S>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/core.rs:212:13
  86: tokio::runtime::task::harness::poll_future::{{closure}}
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:476:19
  87: <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271:9
  88: std::panicking::try::do_call
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483:40
  89: ___rust_try
  90: std::panicking::try
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447:19
  91: std::panic::catch_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140:14
  92: tokio::runtime::task::harness::poll_future
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:464:18
  93: tokio::runtime::task::harness::Harness<T,S>::poll_inner
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:198:27
  94: tokio::runtime::task::harness::Harness<T,S>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:152:15
  95: tokio::runtime::task::raw::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/raw.rs:255:5
  96: tokio::runtime::task::raw::RawTask::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/raw.rs:200:18
  97: tokio::runtime::task::LocalNotified<S>::run
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/mod.rs:394:9
  98: tokio::runtime::scheduler::multi_thread::worker::Context::run_task::{{closure}}
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/scheduler/multi_thread/worker.rs:487:21
  99: tokio::runtime::coop::with_budget
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/coop.rs:107:5
      tokio::runtime::coop::budget
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/coop.rs:73:5
      tokio::runtime::scheduler::multi_thread::worker::Context::run_task
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/scheduler/multi_thread/worker.rs:463:9
 100: tokio::runtime::scheduler::multi_thread::worker::Context::run
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/scheduler/multi_thread/worker.rs:426:24
 101: tokio::runtime::scheduler::multi_thread::worker::run::{{closure}}
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/scheduler/multi_thread/worker.rs:406:17
 102: tokio::macros::scoped_tls::ScopedKey<T>::set
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/macros/scoped_tls.rs:61:9
 103: tokio::runtime::scheduler::multi_thread::worker::run
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/scheduler/multi_thread/worker.rs:403:5
 104: tokio::runtime::scheduler::multi_thread::worker::Launch::launch::{{closure}}
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/scheduler/multi_thread/worker.rs:365:45
 105: <tokio::runtime::blocking::task::BlockingTask<T> as core::future::future::Future>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/blocking/task.rs:42:21
 106: tokio::runtime::task::core::Core<T,S>::poll::{{closure}}
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/core.rs:223:17
 107: tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/loom/std/unsafe_cell.rs:14:9
 108: tokio::runtime::task::core::Core<T,S>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/core.rs:212:13
 109: tokio::runtime::task::harness::poll_future::{{closure}}
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:476:19
 110: <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271:9
 111: std::panicking::try::do_call
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483:40
 112: ___rust_try
 113: std::panicking::try
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447:19
 114: std::panic::catch_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140:14
 115: tokio::runtime::task::harness::poll_future
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:464:18
 116: tokio::runtime::task::harness::Harness<T,S>::poll_inner
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:198:27
 117: tokio::runtime::task::harness::Harness<T,S>::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:152:15
 118: tokio::runtime::task::raw::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/raw.rs:255:5
 119: tokio::runtime::task::raw::RawTask::poll
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/raw.rs:200:18
 120: tokio::runtime::task::UnownedTask<S>::run
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/mod.rs:431:9
 121: tokio::runtime::blocking::pool::Task::run
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/blocking/pool.rs:159:9
 122: tokio::runtime::blocking::pool::Inner::run
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/blocking/pool.rs:513:17
 123: tokio::runtime::blocking::pool::Spawner::spawn_thread::{{closure}}
             at /Users/chunshao.rcs/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/blocking/pool.rs:471:13
 124: std::sys_common::backtrace::__rust_begin_short_backtrace
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys_common/backtrace.rs:121:18
 125: std::thread::Builder::spawn_unchecked_::{{closure}}::{{closure}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/thread/mod.rs:558:17
 126: <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271:9
 127: std::panicking::try::do_call
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483:40
 128: ___rust_try
 129: std::panicking::try
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447:19
 130: std::panic::catch_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140:14
 131: std::thread::Builder::spawn_unchecked_::{{closure}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/thread/mod.rs:557:30
 132: core::ops::function::FnOnce::call_once{{vtable.shim}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/ops/function.rs:250:5
 133: <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:1988:9
      <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:1988:9
      std::sys::unix::thread::Thread::new::thread_start
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys/unix/thread.rs:108:17
 134: __pthread_start
```

### Server version

#833 .

### Steps to reproduce

todo.

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/836/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/846,https://api.github.com/repos/apache/horaedb/issues/846,horaedb,1672459911,846,Automatic table creation bug,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-04-18T06:46:17Z,2023-05-18T03:29:33Z,"### Describe this problem

Automatic table creation has a bug that may cause the first attempt to write data to fail.

### Server version

In PR #844.

### Steps to reproduce

In `integration-test`, `make run-rust`.

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/846/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/856,https://api.github.com/repos/apache/horaedb/issues/856,horaedb,1682581388,856,Failed to operate partition table by http api,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-04-25T07:19:46Z,2023-04-26T03:42:37Z,"### Describe this problem

Currently, the partition table can be operated in the ceresdb cluster via sdk. However, error will be thrown when do it thorough http api.

### Server version

commit id:d127005a7ac199026eb65e36f856ce5dae027763

### Steps to reproduce

After start the ceresdb cluster, execute the commands below:
```bash
curl -X POST \
  'http://127.0.0.1:5440/sql' \
  --header 'Accept: */*' \
  --header 'User-Agent: Thunder Client (https://www.thunderclient.com)' \
  --header 'Content-Type: text/plain' \
  --data-raw 'CREATE TABLE `partition_table_t`(
                                    `name`string TAG,
                                    `id` int TAG,
                                    `value` double NOT NULL,
                                    `t` timestamp NOT NULL,
                                    TIMESTAMP KEY(t)
) PARTITION BY KEY(name) PARTITIONS 4 ENGINE = Analytic with (enable_ttl='false');'

curl -X POST \
  'http://127.0.0.1:5440/sql' \
  --header 'Accept: */*' \
  --header 'User-Agent: Thunder Client (https://www.thunderclient.com)' \
  --header 'Content-Type: text/plain' \
  --data-raw 'INSERT INTO partition_table_t (t, name, value)
VALUES (1651737067000, ""ceresdb0"", 100),
       (1651737067000, ""ceresdb1"", 101),
       (1651737067000, ""ceresdb2"", 102),
       (1651737067000, ""ceresdb3"", 103),
       (1651737067000, ""ceresdb4"", 104),
       (1651737067000, ""ceresdb5"", 105),
       (1651737067000, ""ceresdb6"", 106),
       (1651737067000, ""ceresdb7"", 107),
       (1651737067000, ""ceresdb8"", 108),
       (1651737067000, ""ceresdb9"", 109),
       (1651737067000, ""ceresdb10"", 110);'
```

### Expected behavior

Expect that the rows can be inserted successfully.

### Additional Information

After digging into the code, I find that the forwarding logic in the http is different with grpc service, that is to say, the partition table is not considered at all.

Here is log error in the server:
<code>2023-04-25 15:10:14.020 INFO [server/src/proxy/http/query.rs:51] Query handler try to process request, request_id:5, request:Sql(Request { query: ""INSERT INTO partition_table_t (t, name, value)\nVALUES (1651737067000, \""ceresdb0\"", 100),\n       (1651737067000, \""ceresdb1\"", 101),\n       (1651737067000, \""ceresdb2\"", 102),\n       (1651737067000, \""ceresdb3\"", 103),\n       (1651737067000, \""ceresdb4\"", 104),\n       (1651737067000, \""ceresdb5\"", 105),\n       (1651737067000, \""ceresdb6\"", 106),\n       (1651737067000, \""ceresdb7\"", 107),\n       (1651737067000, \""ceresdb8\"", 108),\n       (1651737067000, \""ceresdb9\"", 109),\n       (1651737067000, \""ceresdb10\"", 110);"" })
2023-04-25 15:10:14.023 WARN [server/src/proxy/forward.rs:269] Fail to forward request for multiple or empty route results, routes result:[Route { table: ""partition_table_t"", endpoint: None }], req:Request { metadata: MetadataMap { headers: {} }, message: SqlQueryRequest { context: Some(RequestContext { database: ""public"" }), tables: [], sql: ""INSERT INTO partition_table_t (t, name, value)\nVALUES (1651737067000, \""ceresdb0\"", 100),\n       (1651737067000, \""ceresdb1\"", 101),\n       (1651737067000, \""ceresdb2\"", 102),\n       (1651737067000, \""ceresdb3\"", 103),\n       (1651737067000, \""ceresdb4\"", 104),\n       (1651737067000, \""ceresdb5\"", 105),\n       (1651737067000, \""ceresdb6\"", 106),\n       (1651737067000, \""ceresdb7\"", 107),\n       (1651737067000, \""ceresdb8\"", 108),\n       (1651737067000, \""ceresdb9\"", 109),\n       (1651737067000, \""ceresdb10\"", 110);"" }, extensions: Extensions }
2023-04-25 15:10:14.024 ERRO [server/src/http.rs:253] Http service Failed to handle sql, err:Rpc error, code:400, message:Failed to build plan, query:INSERT INTO partition_table_t (t, name, value)
</code>","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/856/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/858,https://api.github.com/repos/apache/horaedb/issues/858,horaedb,1684219372,858,InfluxQL support forward,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-04-26T03:52:46Z,2024-10-19T11:17:09Z,"### Describe This Problem

In cluster deployment, request forward is required for all requests, SQL/PQL already support this.

### Proposal

InfluxQL should also support request foward.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/858/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/874,https://api.github.com/repos/apache/horaedb/issues/874,horaedb,1699636949,874,Failed to write data because of GetShardNodesByTableIDs,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2023-05-08T06:46:05Z,2023-11-03T06:53:11Z,"### Describe this problem

Failed to write data , here are the error msg:
```
2023-05-08 14:22:19.763 ERRO [proxy/src/grpc/write.rs:63] Failed to handle write, err:Rpc error, code:500, message:Failed to execute interpreter, err:Failed to execute insert, err:Failed to write table, err:Failed to write tables in batch, tables:[""antmonitor_mid_table_3750""], err:Failed to write to remote, err:Failed to route table, table_ident:TableIdentifier { catalog: ""ceresdb"", schema: ""public"", table: ""__antmonitor_mid_table_3750_19"" }, err:Failure caused by others, msg:Failed to route tables by cluster, req:RouteTablesRequest { schema_name: ""public"", table_names: [""__antmonitor_mid_table_3750_19""] }, err:Meta client execute failed, err:Bad response, resp code:404, msg:grpc routeTablescluster route tables: topology get shard nodes by table ids: (#404)shard not found, cause:shard id:82
github.com/CeresDB/ceresmeta/pkg/coderr.(*codeError).WithCausef
        /Users/zulliangwang/code/ceres/ceresmeta/pkg/coderr/error.go:73
github.com/CeresDB/ceresmeta/server/cluster/metadata.(*TopologyManagerImpl).GetShardNodesByTableIDs
        /Users/zulliangwang/code/ceres/ceresmeta/server/cluster/metadata/topology_manager.go:374
github.com/CeresDB/ceresmeta/server/cluster/metadata.(*ClusterMetadata).RouteTables
        /Users/zulliangwang/code/ceres/ceresmeta/server/cluster/metadata/cluster_metadata.go:495
github.com/CeresDB/ceresmeta/server/cluster.(*managerImpl).RouteTables
        /Users/zulliangwang/code/ceres/ceresmeta/server/cluster/manager.go:329
github.com/CeresDB/ceresmeta/server/service/grpc.(*Service).RouteTables
        /Users/zulliangwang/code/ceres/ceresmeta/server/service/grpc/service.go:290
github.com/CeresDB/ceresdbproto/golang/pkg/metaservicepb._CeresmetaRpcService_RouteTables_Handler.func1
        /Users/zulliangwang/go/pkg/mod/github.com/!ceres!d!b/ceresdbproto/golang@v0.0.0-20230420112830-0aee7927400c/pkg/metaservicepb/meta_service_grpc.pb.go:242
github.com/grpc-ecosystem/go-grpc-prometheus.(*ServerMetrics).UnaryServerInterceptor.func1
        /Users/zulliangwang/go/pkg/mod/github.com/grpc-ecosystem/go-grpc-prometheus@v1.2.0/server_metrics.go:107
github.com/grpc-ecosystem/go-grpc-middleware.ChainUnaryServer.func1.1.1
        /Users/zulliangwang/go/pkg/mod/github.com/grpc-ecosystem/go-grpc-middleware@v1.3.0/chain.go:25
go.etcd.io/etcd/server/v3/etcdserver/api/v3rpc.newUnaryInterceptor.func1
        /Users/zulliangwang/go/pkg/mod/go.etcd.io/etcd/server/v3@v3.5.4/etcdserver/api/v3rpc/interceptor.go:71
github.com/grpc-ecosystem/go-grpc-middleware.ChainUnaryServer.func1.1.1
        /Users/zulliangwang/go/pkg/mod/github.com/grpc-ecosystem/go-grpc-middleware@v1.3.0/chain.go:25
go.etcd.io/etcd/server/v3/etcdserver/api/v3rpc.newLogUnaryInterceptor.func1
        /Users/zulliangwang/go/pkg/mod/go.etcd.io/etcd/server/v3@v3.5.4/etcdserver/api/v3rpc/interceptor.go:78
github.com/grpc-ecosystem/go-grpc-middleware.ChainUnaryServer.func1.1.1
        /Users/zulliangwang/go/pkg/mod/github.com/grpc-ecosystem/go-grpc-middleware@v1.3.0/chain.go:25
github.com/grpc-ecosystem/go-grpc-middleware.ChainUnaryServer.func1
        /Users/zulliangwang/go/pkg/mod/github.com/grpc-ecosystem/go-grpc-middleware@v1.3.0/chain.go:34
github.com/CeresDB/ceresdbproto/golang/pkg/metaservicepb._CeresmetaRpcService_RouteTables_Handler
        /Users/zulliangwang/go/pkg/mod/github.com/!ceres!d!b/ceresdbproto/golang@v0.0.0-20230420112830-0aee7927400c/pkg/metaservicepb/meta_service_grpc.pb.go:244
google.golang.org/grpc.(*Server).processUnaryRPC
        /Users/zulliangwang/go/pkg/mod/google.golang.org/grpc@v1.47.0/server.go:1283
google.golang.org/grpc.(*Server).handleStream
        /Users/zulliangwang/go/pkg/mod/google.golang.org/grpc@v1.47.0/server.go:1620
google.golang.org/grpc.(*Server).serveStreams.func1.2
        /Users/zulliangwang/go/pkg/mod/google.golang.org/grpc@v1.47.0/server.go:922
runtime.goexit
        /usr/local/go/src/runtime/asm_amd64.s:1594
github.com/CeresDB/ceresmeta/pkg/coderr.(*codeError).WithCausef
        /Users/zulliangwang/code/ceres/ceresmeta/pkg/coderr/error.go:73
github.com/CeresDB/ceresmeta/server/cluster/metadata.(*TopologyManagerImpl).GetShardNodesByTableIDs
        /Users/zulliangwang/code/ceres/ceresmeta/server/cluster/metadata/topology_manager.go:374
github.com/CeresDB/ceresmeta/server/cluster/metadata.(*ClusterMetadata).RouteTables
        /Users/zulliangwang/code/ceres/ceresmeta/server/cluster/metadata/cluster_metadata.go:495
github.com/CeresDB/ceresmeta/server/cluster.(*managerImpl).RouteTables
        /Users/zulliangwang/code/ceres/ceresmeta/server/cluster/manager.go:329
github.com/CeresDB/ceresmeta/server/service/grpc.(*Service).RouteTables
        /Users/zulliangwang/code/ceres/ceresmeta/server/service/grpc/service.go:290
github.com/CeresDB/ceresdbproto/golang/pkg/metaservicepb._CeresmetaRpcService_RouteTables_Handler.func1
        /Users/zulliangwang/go/pkg/mod/github.com/!ceres!d!b/ceresdbproto/golang@v0.0.0-20230420112830-0aee7927400c/pkg/metaservicepb/meta_service_grpc.pb.go:242
github.com/grpc-ecosystem/go-grpc-prometheus.(*ServerMetrics).UnaryServerInterceptor.func1
        /Users/zulliangwang/go/pkg/mod/github.com/grpc-ecosystem/go-grpc-prometheus@v1.2.0/server_metrics.go:107
github.com/grpc-ecosystem/go-grpc-middleware.ChainUnaryServer.func1.1.1
        /Users/zulliangwang/go/pkg/mod/github.com/grpc-ecosystem/go-grpc-middleware@v1.3.0/chain.go:25
go.etcd.io/etcd/server/v3/etcdserver/api/v3rpc.newUnaryInterceptor.func1
        /Users/zulliangwang/go/pkg/mod/go.etcd.io/etcd/server/v3@v3.5.4/etcdserver/api/v3rpc/interceptor.go:71
github.com/grpc-ecosystem/go-grpc-middleware.ChainUnaryServer.func1.1.1
        /Users/zulliangwang/go/pkg/mod/github.com/grpc-ecosystem/go-grpc-middleware@v1.3.0/chain.go:25
go.etcd.io/etcd/server/v3/etcdserver/api/v3rpc.newLogUnaryInterceptor.func1
        /Users/zulliangwang/go/pkg/mod/go.etcd.io/etcd/server/v3@v3.5.4/etcdserver/api/v3rpc/interceptor.go:78
github.com/grpc-ecosystem/go-grpc-middleware.ChainUnaryServer.func1.1.1
        /Users/zulliangwang/go/pkg/mod/github.com/grpc-ecosystem/go-grpc-middleware@v1.3.0/chain.go:25
github.com/grpc-ecosystem/go-grpc-middleware.ChainUnaryServer.func1
        /Users/zulliangwang/go/pkg/mod/github.com/grpc-ecosystem/go-grpc-middleware@v1.3.0/chain.go:34
github.com/CeresDB/ceresdbproto/golang/pkg/metaservicepb._CeresmetaRpcService_RouteTables_Handler
        /Users/zulliangwang/go/pkg/mod/github.com/!ceres!d!b/ceresdbproto/golang@v0.0.0-20230420112830-0aee7927400c/pkg/metaservicepb/meta_service_grpc.pb.go:244
google.golang.org/grpc.(*Server).processUnaryRPC
        /Users/zulliangwang/go/pkg/mod/google.golang.org/grpc@v1.47.0/server.go:1283
google.golang.org/grpc.(*Server).handleStream
        /Users/zulliangwang/go/pkg/mod/google.golang.org/grpc@v1.47.0/server.go:1620
google.golang.org/grpc.(*Server).serveStreams.func1.2
        /Users/zulliangwang/go/pkg/mod/google.golang.org/grpc@v1.47.0/server.go:922
runtime.goexit
        /usr/local/go/src/runtime/asm_amd64.s:1594.
Backtrace:
```

Route table response:
$ curl --location --request POST 'http://xxx:8080/api/v1/route' \
> --header 'Content-Type: application/json' \
> --data '{
>     ""clusterName"":""defaultCluster"",
>     ""schemaName"":""public"",
>     ""table"":[""__antmonitor_mid_table_3750_19""]
> }'
{""status"":""error"",""error"":""(#500)route table, cause:\u003cnil\u003e"",""msg"":""route tables failed""}

### Server version

$ ceresdb-server --version
CeresDB Server 
CeresDB version: 1.1.0
Git branch: main
Git commit: f891e2e
Build time: 2023-05-06T08:20:00.304670994Z
Rustc version: 1.69.0-nightly

### Steps to reproduce

None

### Expected behavior

Write data successfully.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/874/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/874,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qzyG0,horaedb,1791959476,874,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-11-03T06:53:11Z,2023-11-03T06:53:11Z,Fixed already,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qzyG0/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/892,https://api.github.com/repos/apache/horaedb/issues/892,horaedb,1706810273,892,Support `delete` sql,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-05-12T02:06:06Z,2024-12-18T04:42:07Z,"### Describe This Problem

Currently, the `delete` sql is not supported yet. However, this is a very important feature in some cases.

### Proposal

#### Feature Details
CeresDB's model is LSM, that is to say, we can't do deletion in place and any way to support deletion must be processed when querying or compaction.

There are two cases where delete may happen:
- Delete the rows specified by primary keys;
- Delete the rows specified by a predicate;

#### By predicate for deletion
A sound solution is to save the delete predicate somewhere, and filter the results with this predicate when querying or compaction. However, it will cost too much in this way if user deletes massive rows by massive primary keys.

#### By marking rows deleted
An alternative solution is to persist all the deleted rows in the sst just like the normal rows but with an extra marker for deletion. When querying or compaction, dedup procedure will filter out the deleted rows. This way is good at handling the first case, but for the second case, the rows to delete are not provided directly by the user, that is to say, we have to collect all the primary keys according to the delete predicate and then mark them all deleted.

#### Conclusion
I'm in favor of the first solution because I guess in the first cases, the number of the provided primary keys won't be too large and it is easy to convert into a simple predicate.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/892/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/892,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5nu7p_,horaedb,1740356223,892,NA,fredrikIOT,43440995,,,NA,2023-09-29T06:04:41Z,2023-09-29T06:04:41Z,Any plan when this will be implemented?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5nu7p_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/892,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5xxeOz,horaedb,1908794291,892,NA,jackbit,150449,Yacobus Reinhart,yacobus.reinhart@gmail.com,NA,2024-01-24T19:37:18Z,2024-01-24T19:37:18Z,+1 We need something like this:  `DELETE FROM tableName WHERE price != 100`,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5xxeOz/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/893,https://api.github.com/repos/apache/horaedb/issues/893,horaedb,1707116943,893,Auto refresh route cache,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-05-12T07:36:24Z,2024-12-18T04:43:23Z,"### Describe This Problem

Currently, the route information in the CeresDB instance is configured with a TTL for auto refreshing. And here are some problems of this mechanism:
- The route entry for a dropped won't expire in time;
- TTL is hard to be determined: a too short TTL will impose a large route traffic to CeresMeta, and a too long TTL will make invalid route entry exist for a long time;


### Proposal

Replace the TTL mechanism with auto refreshing with Shard Version:
- Let CeresDB instance fetches the versions of its shards from CeresMeta;
- And fetch the tables belonging to the shard and updating the route entry in the cache;

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/893/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/893,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5cyakG,horaedb,1556719878,893,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-05-22T07:56:08Z,2023-05-22T07:56:08Z,"The problem is that the route cache used for proxy is involved with all the tables of the whole cluster, so we can't cache routing information of all the tables.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5cyakG/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/898,https://api.github.com/repos/apache/horaedb/issues/898,horaedb,1711809879,898,Improve PQL regex query performance,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-05-16T11:17:44Z,2023-11-03T06:35:23Z,"### Describe This Problem

The following regex query is very slow.
```
sum (rate(http_requests_counter{url=~""/api/query|/plugin/prometheus/api/v1/query.*""}[3m])) 
```

But the performance is very good after splitting the query.
```
sum (rate(http_requests_counter{url=""/plugin/prometheus/api/v1/query""}[3m])) 
sum (rate(http_requests_counter{url=""/api/query""}[3m])) 
```


### Proposal

Improve PQL regex query performance.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/898/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/898,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5dXaAC,horaedb,1566416898,898,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-05-29T02:46:31Z,2023-05-29T02:46:31Z,"For simple case, I think we can add a datafusion [OptimizerRule](https://docs.rs/datafusion-optimizer/25.0.0/datafusion_optimizer/optimizer/trait.OptimizerRule.html) to convert regex filter to equal filter. Eg:

```sql
select * from demo where name ~ '1|2';
select * from demo where name in ('1', '2');
```

Related changes:
https://github.com/CeresDB/ceresdb/blob/17b40c63b50e7ea50b22be651f4a8c07a48dbc00/query_engine/src/context.rs#L89","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5dXaAC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/898,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5dXakm,horaedb,1566419238,898,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-05-29T02:50:55Z,2023-05-29T02:50:55Z,"https://crates.io/crates/regex-syntax

This crate allow us to program based on regex syntax.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5dXakm/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/898,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5dsKUl,horaedb,1571857701,898,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-06-01T11:16:39Z,2023-06-01T11:16:39Z,https://github.com/apache/arrow-datafusion/pull/6487 partially solved this issue.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5dsKUl/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/898,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jOWjh,horaedb,1664706785,898,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-08-03T22:11:31Z,2023-08-03T22:11:31Z,"https://github.com/apache/arrow-datafusion/issues/7185 Now we can support string which include `_`.
And https://github.com/CeresDB/ceresdb/pull/1128 make the PQL regex expression optimizeable.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jOWjh/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/904,https://api.github.com/repos/apache/horaedb/issues/904,horaedb,1715212345,904,Support write/query via OpenTSDB API,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-05-18T08:10:44Z,2024-12-18T04:43:06Z,"### Describe This Problem

Currently there are three query language supported in CeresDB: SQL/InfluxQL/PromQL. It would be great to also support OpenTSBD query API.

### Proposal

There are many API in OpenTSDB, the following two API are most important, so we can first implement those.
- [ ] http://opentsdb.net/docs/build/html/api_http/query/index.html
- [x] http://opentsdb.net/docs/build/html/api_http/put.html

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/904/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/904,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jBu7x,horaedb,1661398769,904,NA,parkma99,84610851,,park-ma@hotmail.com,NA,2023-08-02T02:34:06Z,2023-08-02T02:34:06Z,"I would like to work `query` api, I am newer fo this project, Could you give me some suggestion to start?😊","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jBu7x/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/904,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jB-8B,horaedb,1661464321,904,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-08-02T04:15:17Z,2023-08-02T04:15:17Z,"Thanks for you interests, let us know when you have any problems.

Generally speaking, you need to convert opentsdb query syntax to datafusion plan, you can refer how InfluxQL is implemented here:
- https://github.com/CeresDB/ceresdb/blob/b6a0563226035bfa704a69fed685d030dea6f2ef/proxy/src/influxdb/mod.rs#L39


","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jB-8B/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/904,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jFYrD,horaedb,1662356163,904,NA,parkma99,84610851,,park-ma@hotmail.com,NA,2023-08-02T14:50:52Z,2023-08-02T14:50:52Z,I find it's very hard to me current 😢😢，I give up to others this issue.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jFYrD/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/904,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jXhqT,horaedb,1667111571,904,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-08-07T02:55:39Z,2023-08-07T02:55:39Z,"@parkma99 😅 Thanks anyway, you can try other issue first.
- https://github.com/CeresDB/ceresdb/labels/contributor%20friendly","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jXhqT/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/904,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5lmQi7,horaedb,1704528059,904,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-09-04T02:32:04Z,2023-09-04T02:32:04Z,"[This docs](http://opentsdb.net/docs/build/html/user_guide/query/index.html) explain how opentsdb query is executed:

    Filtering

    Grouping

    Downsampling

    Interpolation

    Aggregation

    Rate Conversion

    Functions

    Expressions



Understanding the order of operations is important. When returning query results the following is the order in which processing takes place.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5lmQi7/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/912,https://api.github.com/repos/apache/horaedb/issues/912,horaedb,1719667593,912,Implement object store on Obkv,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2023-05-22T13:12:13Z,2023-05-24T08:42:25Z,"### Describe This Problem

OceanBase is a highly reliable, distributed relational database. Currently, it can be used as a write-ahead log (WAL) for CeresDB. If OceanBase can be used as an object store for CeresDB, it would decrease CeresDB's dependencies and improve stability.

### Proposal

This work can be split into 3 parts:
* Define a trait `CloudMultiPartUploadImpl ` that can be implemented by cloud-based objectstores; This part refer to `object_store` in `arrow_rs`;  See detail in https://github.com/CeresDB/ceresdb/pull/913
* Implement the the trait CloudMultiPartUploadImpl on obkv; See  detail in https://github.com/CeresDB/ceresdb/pull/887
* Implement the trait ObjectStore on obkv: See detail in https://github.com/CeresDB/ceresdb/pull/887

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/912/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/914,https://api.github.com/repos/apache/horaedb/issues/914,horaedb,1721071035,914,Use partitioned lock to optimize disk cache,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-05-23T03:09:03Z,2023-06-07T08:37:15Z,"### Describe This Problem

Partitioned lock is a common trick to reduce lock contention, disk cache could use it to improve perf.

https://github.com/CeresDB/ceresdb/blob/da6899c4c97089d4fbd8aa8a01db98ab8366d5bc/components/object_store/src/disk_cache.rs#L114

### Proposal

Use PartitionedRwLock to replace `cache` in `DiskCache`

https://github.com/CeresDB/ceresdb/blob/da6899c4c97089d4fbd8aa8a01db98ab8366d5bc/common_util/src/partitioned_lock.rs#L17 

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/914/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/914,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5c5Alc,horaedb,1558448476,914,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-05-23T03:12:44Z,2023-05-23T03:12:44Z,please assign to me.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5c5Alc/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/916,https://api.github.com/repos/apache/horaedb/issues/916,horaedb,1721387010,916,Shard lock won't be released after open failure,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-05-23T07:15:26Z,2023-08-02T09:38:18Z,"### Describe this problem

The shard lock won't be released after open failure.

### Server version

dcae0e086a8406ea57c36887eabf7ac27eb5ae4d

### Steps to reproduce

- Connect to the CeresMeta;
- When ceresdb receives an open shard request, it fails to open shard;
- The shard lock won't be released after that, that is to say, the shard won't be able to be opened any more.

### Expected behavior

The shard lock should be released if failed to open shard totally.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/916/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/916,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jDjzk,horaedb,1661877476,916,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-08-02T09:38:18Z,2023-08-02T09:38:18Z,Already fixed in https://github.com/CeresDB/ceresdb/pull/1104,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jDjzk/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/920,https://api.github.com/repos/apache/horaedb/issues/920,horaedb,1721585118,920,A large number of '\' in Java SDK error message,xzchaoo,8319940,xzchaoo,xzchaoo@gmail.com,CLOSED,2023-05-23T09:06:04Z,2024-10-19T11:16:50Z,"### Describe this problem

The error message of the CeresDB SDK contains a large number of '\\'.

I create a table with 'value' field initialized with 'double' data type.
Then I write a record with value '{... my biz json content ...}', which is a json string.
CeresDB Java SDK reports an error. Ok. I know it's normal to report errors.
But, the error content contains a large number of '\\'.
**The longest line reaches 12MB.** But the length of my json is only a few thousand.

![image](https://github.com/CeresDB/ceresdb/assets/8319940/251ef2d0-1e36-416f-9b14-c3828785eb4c)

![image](https://github.com/CeresDB/ceresdb/assets/8319940/f4ea36a4-caa3-4f90-ab84-e0998ecf5962)


### Server version

Docker image ceresdb/ceresdb-server:v1.0.0
Java SDK: io.ceresdb:ceresdb-all:1.0.3


### Steps to reproduce

I create a table with 'value' field initialized with 'double' data type.
Then I write a record with value '{... my biz json content ...}', which is a json string.
CeresDB Java SDK reports an error. Ok. I know it's normal to report errors.
But, the error content contains a large number of '\\'.
**The longest line reaches 12MB.** But the length of my json is only a few thousand.


### Expected behavior

CeresDB Java SDK reports an error with better error message.


### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/920/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/920,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5c6tbd,horaedb,1558894301,920,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-05-23T09:24:33Z,2023-05-23T09:24:33Z,I guess this seems a bug in the java SDK. Let's try to reproduce it. @xzchaoo Could please provide your table schema or the create table SQL?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5c6tbd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/921,https://api.github.com/repos/apache/horaedb/issues/921,horaedb,1723104929,921,Implement a Hasher based on murmur3,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-05-24T03:08:46Z,2023-05-24T11:46:18Z,"### Describe This Problem

We now use `DefaultHasher` inside partition lock, maybe we can change to a different Hasher based on murmur3 to improve perf.

### Proposal

https://github.com/CeresDB/ceresdb/blob/27267b55faf1bce3c263b49d2fb61bcd5ce55c00/common_types/src/hash.rs#L7

### Additional Context

It's better to do a simple benchmark about those Hasher.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/921/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/923,https://api.github.com/repos/apache/horaedb/issues/923,horaedb,1723122589,923,Write table fail when field name is `timestamp`,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2023-05-24T03:38:02Z,2024-10-19T11:31:03Z,"### Describe this problem

Write table fail when field name is `timestamp`， I know `timestamp` is a keyword in CeresDB, but can we remove this restrict in sdk. 
```
[2023-05-24 11:20:44 ERROR] [data-async-executor] {com.alipay.pontus.service.storage.DataAsyncWriter:114}-Invalid name, not allow keyword `timestamp`
java.lang.IllegalArgumentException: Invalid name, not allow keyword `timestamp`
        at io.shaded.ceresdb.util.Utils.ensureNotKeyword(Utils.java:421)
        at io.shaded.ceresdb.util.Utils.checkKeywords(Utils.java:415)
        at io.shaded.ceresdb.models.Point$PointBuilder.check(Point.java:84)
        at io.shaded.ceresdb.models.Point$PointBuilder.build(Point.java:78)
```

### Server version

* 1.2.0

### Steps to reproduce

* Create a table with a field named `timestamp`
* Use java sdk to write data

### Expected behavior

* Write data successfully 

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/923/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/924,https://api.github.com/repos/apache/horaedb/issues/924,horaedb,1723289162,924,Tracking Issue: improve query,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-05-24T06:48:20Z,2024-12-18T04:44:13Z,"## Performance
- [x] #664 
- [x] #898
- [x] #997 
- [x] #1001 
- [x] #1102
- [x] #1105 

## Stability
- [x] #959 

## Observability
- [x] #468 

## Tools
- [x] #925 ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/924/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/924,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tikbq,horaedb,1837778666,924,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-12-04T03:23:23Z,2023-12-04T03:23:23Z,Stale.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tikbq/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/925,https://api.github.com/repos/apache/horaedb/issues/925,horaedb,1723307543,925,Support load data from a bunch of sst,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-05-24T07:00:14Z,2024-10-19T11:25:14Z,"### Describe This Problem

For some data persisted as ssts in production environment, it is hoped that download the data and load it into a CeresDB instance for test and development.

### Proposal

#### Load by write API
A simple way is to provide a tool to decode the ssts, and load the data into CeresDB by write API.
- Pros:
  - Simple;
  - No need to worry about the format compatibility, that is to say, we can load data files in not only CeresDB's format(Parquet);
- Cons:
  - Loading by write may cost too much time;

#### Bulk load
Use CeresDB to read the ssts, and generates the corresponding meta data in manifest directly.

- Pros:
  - The speed must be very fast;
- Cons:
  - A little bit complex;
  - Only can support load data files in CeresDB's file format;


### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/925/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/925,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5dHNSJ,horaedb,1562170505,925,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-05-25T02:29:24Z,2023-05-25T02:29:24Z,"When troubleshoot query performance issues, the generated ssts by compaction matters most so I vote for the second way to load the sst directly.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5dHNSJ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/926,https://api.github.com/repos/apache/horaedb/issues/926,horaedb,1723366293,926,Provide tools to generate the best config for ceresdb,ShiKaiWi,8605990,WEI Xikai,,OPEN,2023-05-24T07:36:07Z,2023-05-24T07:36:07Z,"### Describe This Problem

Current config of CeresDB is a little complex, for users, it is hard to decide a proper config to deploy ceresdb on the production environment.

### Proposal

Maybe we should provide a tool to suggest a config for ceresdb according to user's machine, just like [timescaledb-tune](https://github.com/timescale/timescaledb-tune).

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/926/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/929,https://api.github.com/repos/apache/horaedb/issues/929,horaedb,1725016055,929,Tenant management and authentication,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-05-25T03:21:06Z,2024-05-15T09:04:39Z,"### Describe This Problem

Tenant management and authentication.

### Proposal
#### Simple solution
- [ ] config files, something like
```toml
[server.auth]
enable = true
# available values: file/ceresmeta
source = ""file""

[server.auth.file]
username = ""admin""
password = ""secret-token""
```
#### Complex
- [ ] `ceresmeta` supports creating tenants and generating tokens.
- [ ] `ceresdb` supports identity verification.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/929/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/929,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gBqN-,horaedb,1611047806,929,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-06-28T09:08:48Z,2023-06-28T09:08:48Z,"Besides store tenants in ceresmeta, we should also support this via config files
- https://github.com/CeresDB/ceresdb/issues/1016#issuecomment-1605831993","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gBqN-/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/929,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5hF0DZ,horaedb,1628913881,929,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-07-10T13:00:26Z,2023-07-10T13:00:26Z,"https://www.postgresql.org/docs/current/sql-createrole.html

PG will store password according to https://www.postgresql.org/docs/current/runtime-config-connection.html#GUC-PASSWORD-ENCRYPTION, and users can't get original password.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5hF0DZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/929,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59TFO_,horaedb,2102154175,929,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-05-09T07:59:35Z,2024-05-09T07:59:35Z,"For gRPC, we can use Bearer auth
- https://github.com/hyperium/tonic/blob/master/examples/src/authentication/client.rs","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59TFO_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/930,https://api.github.com/repos/apache/horaedb/issues/930,horaedb,1725024850,930,SDK tests support distributed tables,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-05-25T03:36:10Z,2024-12-18T04:42:37Z,"### Describe This Problem

Currently sdk tests only contains normal table against a standalone server, it should also test distributed tables write/query.

### Proposal

Beside standalone server tests, SDK tests also support full clustered server.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/930/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/933,https://api.github.com/repos/apache/horaedb/issues/933,horaedb,1726915325,933,Use `tracing` as our logger rather than `slog`,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-05-26T05:08:08Z,2024-11-04T02:51:23Z,"### Describe This Problem
`slog` is an outdated crate maintained by nobody now. And the most popular one is tokio's `tracing`. In fact, we have used `tracing` in our project but not include its logger, how about let's  make use of it. 

### Proposal

Maybe can make it referring to [influxdb_iox](https://github.com/influxdata/influxdb)

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/933/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/933,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5dYw9J,horaedb,1566773065,933,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-05-29T08:44:41Z,2023-05-29T08:44:41Z,I'll vote for it if there is no performance issue.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5dYw9J/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/934,https://api.github.com/repos/apache/horaedb/issues/934,horaedb,1726940678,934,Parquet error when compaction,MichaelLeeHZ,21232632,MichaelLee,michael.lee.hz@gmail.com,CLOSED,2023-05-26T05:40:10Z,2023-07-27T09:16:48Z,"### Describe this problem

Here is the stack:
```
2023-05-26 13:30:16.590 ERRO [analytic_engine/src/compaction/scheduler.rs:509] Failed to compact table, table_name:__LEGO_SPLIT_3001_DEFAULT_27, table_id:2180, request_id:99, err:Failed to build merge iterator, table:__LEGO_SPLIT_3001_DEFAULT_27, err:Failed to build record batch from sst, err:Fail to read sst meta, err:Failed to decode sst meta data, file_path:0/2180/30.sst, err:Parquet error: Invalid Parquet file. Corrupt footer.
Backtrace:
Backtrace(   0: <snafu::backtrace_shim::Backtrace as snafu::GenerateBacktrace>::generate
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/backtrace_shim.rs:15:19
      <analytic_engine::sst::reader::error::FetchAndDecodeSstMeta<__T0> as snafu::IntoError<analytic_engine::sst::reader::error::Error>>::into_error
             at /root/code/xunming/ceresdb/analytic_engine/src/sst/reader.rs:15:21
      <core::result::Result<T,E> as snafu::ResultExt<T,E>>::with_context::{{closure}}
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/lib.rs:329:13
      core::result::Result<T,E>::map_err
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/result.rs:860:27
      <core::result::Result<T,E> as snafu::ResultExt<T,E>>::with_context
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/lib.rs:327:9
      analytic_engine::sst::parquet::async_reader::Reader::load_meta_data_from_storage::{{closure}}
             at /root/code/xunming/ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:303:13
      analytic_engine::sst::parquet::async_reader::Reader::read_sst_meta::{{closure}}
             at /root/code/xunming/ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:333:71
      analytic_engine::sst::parquet::async_reader::Reader::init_if_necessary::{{closure}}
             at /root/code/xunming/ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:268:49
   1: <analytic_engine::sst::parquet::async_reader::Reader as analytic_engine::sst::reader::SstReader>::meta_data::{{closure}}
             at /root/code/xunming/ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:537:33
   2: <core::pin::Pin<P> as core::future::future::Future>::poll
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125:9
      <analytic_engine::sst::parquet::async_reader::ThreadedReader as analytic_engine::sst::reader::SstReader>::meta_data::{{closure}}
             at /root/code/xunming/ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:652:31
   3: <core::pin::Pin<P> as core::future::future::Future>::poll
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125:9
      analytic_engine::row_iter::record_batch_stream::stream_from_sst_file::{{closure}}
             at /root/code/xunming/ceresdb/analytic_engine/src/row_iter/record_batch_stream.rs:333:38
      analytic_engine::row_iter::record_batch_stream::filtered_stream_from_sst_file::{{closure}}
             at /root/code/xunming/ceresdb/analytic_engine/src/row_iter/record_batch_stream.rs:291:5
   4: analytic_engine::row_iter::merge::MergeBuilder::build::{{closure}}
             at /root/code/xunming/ceresdb/analytic_engine/src/row_iter/merge.rs:218:17
   5: analytic_engine::instance::flush_compaction::<impl analytic_engine::instance::SpaceStore>::compact_input_files::{{closure}}
             at /root/code/xunming/ceresdb/analytic_engine/src/instance/flush_compaction.rs:827:28
      analytic_engine::instance::flush_compaction::<impl analytic_engine::instance::SpaceStore>::compact_table::{{closure}}
             at /root/code/xunming/ceresdb/analytic_engine/src/instance/flush_compaction.rs:724:13
   6: analytic_engine::compaction::scheduler::ScheduleWorker::do_table_compaction_task::{{closure}}
             at /root/code/xunming/ceresdb/analytic_engine/src/compaction/scheduler.rs:503:17
   7: tokio::runtime::task::core::Core<T,S>::poll::{{closure}}
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/core.rs:223:17
      tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/loom/std/unsafe_cell.rs:14:9
      tokio::runtime::task::core::Core<T,S>::poll
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/core.rs:212:13
      tokio::runtime::task::harness::poll_future::{{closure}}
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:476:19
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271:9
      std::panicking::try::do_call
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483:40
      std::panicking::try
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447:19
      std::panic::catch_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140:14
      tokio::runtime::task::harness::poll_future
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:464:18
      tokio::runtime::task::harness::Harness<T,S>::poll_inner
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:198:27
      tokio::runtime::task::harness::Harness<T,S>::poll
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:152:15
      tokio::runtime::task::raw::poll
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/raw.rs:255:5
   8: tokio::runtime::task::raw::RawTask::poll
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/raw.rs:200:18
      tokio::runtime::task::LocalNotified<S>::run
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/mod.rs:394:9
      tokio::runtime::scheduler::multi_thread::worker::Context::run_task::{{closure}}
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/scheduler/multi_thread/worker.rs:487:21
      tokio::runtime::coop::with_budget
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/coop.rs:107:5
      tokio::runtime::coop::budget
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/coop.rs:73:5
      tokio::runtime::scheduler::multi_thread::worker::Context::run_task
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/scheduler/multi_thread/worker.rs:463:9
   9: tokio::runtime::scheduler::multi_thread::worker::Context::run
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/scheduler/multi_thread/worker.rs:426:24
      tokio::runtime::scheduler::multi_thread::worker::run::{{closure}}
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/scheduler/multi_thread/worker.rs:406:17
      tokio::macros::scoped_tls::ScopedKey<T>::set
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/macros/scoped_tls.rs:61:9
      tokio::runtime::scheduler::multi_thread::worker::run
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/scheduler/multi_thread/worker.rs:403:5
  10: tokio::runtime::scheduler::multi_thread::worker::Launch::launch::{{closure}}
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/scheduler/multi_thread/worker.rs:365:45
      <tokio::runtime::blocking::task::BlockingTask<T> as core::future::future::Future>::poll
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/blocking/task.rs:42:21
      tokio::runtime::task::core::Core<T,S>::poll::{{closure}}
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/core.rs:223:17
      tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/loom/std/unsafe_cell.rs:14:9
      tokio::runtime::task::core::Core<T,S>::poll
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/core.rs:212:13
      tokio::runtime::task::harness::poll_future::{{closure}}
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:476:19
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271:9
      std::panicking::try::do_call
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483:40
      std::panicking::try
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447:19
      std::panic::catch_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140:14
      tokio::runtime::task::harness::poll_future
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:464:18
      tokio::runtime::task::harness::Harness<T,S>::poll_inner
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:198:27
      tokio::runtime::task::harness::Harness<T,S>::poll
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:152:15
      tokio::runtime::task::raw::poll
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/raw.rs:255:5
  11: tokio::runtime::task::raw::RawTask::poll
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/raw.rs:200:18
      tokio::runtime::task::UnownedTask<S>::run
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/mod.rs:431:9
      tokio::runtime::blocking::pool::Task::run
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/blocking/pool.rs:159:9
      tokio::runtime::blocking::pool::Inner::run
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/blocking/pool.rs:513:17
      tokio::runtime::blocking::pool::Spawner::spawn_thread::{{closure}}
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/blocking/pool.rs:471:13
      std::sys_common::backtrace::__rust_begin_short_backtrace
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys_common/backtrace.rs:121:18
  12: std::thread::Builder::spawn_unchecked_::{{closure}}::{{closure}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/thread/mod.rs:558:17
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271:9
      std::panicking::try::do_call
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483:40
      std::panicking::try
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447:19
      std::panic::catch_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140:14
      std::thread::Builder::spawn_unchecked_::{{closure}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/thread/mod.rs:557:30
      core::ops::function::FnOnce::call_once{{vtable.shim}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/ops/function.rs:250:5
  13: <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:1988:9
      <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:1988:9
      std::sys::unix::thread::Thread::new::thread_start
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys/unix/thread.rs:108:17
  14: start_thread
  15: clone
```

### Server version

CeresDB version: 1.2.0
commit:b2b21e1477774d9939026f126695b743b08f8609

### Steps to reproduce

None

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/934/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/934,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5di8AQ,horaedb,1569439760,934,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-05-31T03:23:48Z,2023-05-31T03:23:48Z,"It seems this parquet file is corrupt, I'm afraid we can do nothing is this case.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5di8AQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/939,https://api.github.com/repos/apache/horaedb/issues/939,horaedb,1729908872,939,When querying data exceeding the ttl,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-05-29T02:52:00Z,2023-07-11T07:25:50Z,"### Describe This Problem

When querying data exceeding the ttl, return additional information.

### Proposal

Return additional information, for example: querying data exceeding the storage time, the ttl of the table is 7d.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/939/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/945,https://api.github.com/repos/apache/horaedb/issues/945,horaedb,1731423000,945,Avoid `*join_all`,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-05-30T02:47:45Z,2023-06-01T07:47:32Z,"### Describe This Problem

The `try_join_all` and `join_all` has poor performance compared to `FuturesUnordered` or `FuturesOrdered` even if `join_all` will replace the underlying implement with `FuturesOrdered`.

### Proposal
There are three alternatives:
- `FuturesOrdered` can replace `*join_all`  totally, but with just a little performance improvement;
- `FuturesUnordered` can replace `*join_all` if we don't care about the order of the sub-futures' order, and with a fairly good performance;
- Use `forloop` to wait for all the sub-futures to be done if the sub-futures has been spawned in another runtime, and in this case, the performance is best;

### Additional Context

References:
- `join_all` performance issues: https://github.com/tokio-rs/tokio/issues/2401
- `FuturesUnordered` vs `JoinSet`: https://github.com/tokio-rs/tokio/issues/5564

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/945/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/945,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5dcrUL,horaedb,1567798539,945,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-05-30T06:01:41Z,2023-05-30T06:01:41Z,#940 is an example for this issue.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5dcrUL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/950,https://api.github.com/repos/apache/horaedb/issues/950,horaedb,1733263040,950,Use a new custom struct for efficient encoding in the write procedure,ShiKaiWi,8605990,WEI Xikai,,OPEN,2023-05-31T02:02:48Z,2023-05-31T02:44:00Z,"### Describe This Problem

In the current write procedure:
- `RowGroup` is used for `write` method of `Table` trait;
- Above the `Table.write`, there are two sources converted to `RowGroup`:
  - `RemoteEngineService.write_batch` receives the raw bytes of arrow record batch and converts the record batches to `RowGroup`;
  - `StorageService.write` receives the raw bytes of custom protobuf struct and converts the protobuf struct to `RowGroup`;
- Under the `Table.write`, the `RowGroup` will be encoded into raw bytes for wal logs and memtable rows, and the wal log payload doesn't have any special requirement for the encoding method while the memtable rows require that the `RowGroup` must be encoded in rows to keep all rows in primary key order;

### Proposal

From the description above, it can be found that there are too many conversions during the write procedure, leading to high CPU utilization, which has been proven in the production environment.

Maybe we can use only one struct for the whole write procedure to avoid extra conversions. And for the wal and memetable, I guess we can let the wal log payload shares the same encoded bytes used by memtable. And such struct must be designed for writing, that is to say, there is no need to include complex schema information.

### Additional Context
The encoding and decoding of the arrow ipc performs very well, and I guess it should a benchmark for the new struct designed for write procedure.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/950/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/953,https://api.github.com/repos/apache/horaedb/issues/953,horaedb,1733451728,953,Add integration test for alter table options,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-05-31T06:07:44Z,2023-12-08T02:21:12Z,"### Describe This Problem

Related with https://github.com/CeresDB/docs/pull/82


### Proposal

- https://github.com/CeresDB/ceresdb/blob/5ea6c0f3b87011d2e6802c9bb27916f853084a08/integration_tests/cases/env/cluster/ddl/alter_table.sql#L1

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/953/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/953,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tr9rg,horaedb,1840241376,953,NA,caicancai,77189278,Cancai Cai,,NA,2023-12-05T08:22:35Z,2023-12-05T08:22:35Z,"Hello, I'm interested in this. let me try","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tr9rg/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/953,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tsTNU,horaedb,1840329556,953,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-12-05T09:13:40Z,2023-12-05T09:13:40Z,"Thanks, assigned.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tsTNU/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/953,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tsdL5,horaedb,1840370425,953,NA,caicancai,77189278,Cancai Cai,,NA,2023-12-05T09:28:58Z,2023-12-05T09:28:58Z,"> Thanks, assigned.

Thank you for assigning this pr to me, I would like to ask if there is such a specification or reference","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tsdL5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/953,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tsrzj,horaedb,1840430307,953,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-12-05T10:06:07Z,2023-12-05T10:06:07Z,Already described above. See https://github.com/CeresDB/docs/pull/82,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tsrzj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/953,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tsv4B,horaedb,1840446977,953,NA,caicancai,77189278,Cancai Cai,,NA,2023-12-05T10:16:04Z,2023-12-05T10:16:04Z,"> Already described above. See [CeresDB/docs#82](https://github.com/CeresDB/docs/pull/82)

So I just need to add test to alter_table.sql? I thought I needed to write test code from scratch🤔","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tsv4B/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/955,https://api.github.com/repos/apache/horaedb/issues/955,horaedb,1733797648,955,Decrease SST metadata size,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-05-31T09:48:21Z,2023-08-24T09:51:14Z,"### Describe This Problem

We currently use parquet as our underlying file format, and we find the metadata of one SST is very large. Eg:
```
size:348.675M, metadata:40.142M, kv:38.175M, filter:28.525M, row_num:7038949
```

Ideally SST's metadata should be cached to improve query perf, so we need to find solutions to decrease metadata size to avoid OOM in production.

### Proposal

- Reduce xor filter size
  - [x] In practical double/bool column and key column could skip xor filter, since mix/max filter works well for them. #958
  - [ ]  Choose another filter when distinct values are less than 10.
- [ ] Reduce unnecessary base64 encoding when write metadata, it can cause size increase more than 25%. This task depends on https://github.com/apache/arrow-rs/issues/4317

### Additional Context

Xor filter size for different key num
```
key_num:0, len:54, byte_per_key:54
key_num:1, len:57, byte_per_key:57
key_num:5, len:63, byte_per_key:12.6
key_num:10, len:69, byte_per_key:6.9
key_num:100, len:177, byte_per_key:1.77
key_num:1000, len:1284, byte_per_key:1.284
key_num:2000, len:2514, byte_per_key:1.257
key_num:3000, len:3744, byte_per_key:1.248
key_num:4000, len:4974, byte_per_key:1.2435
key_num:8000, len:9894, byte_per_key:1.23675
```
This snippet show when key are less than 10, the cost of per key is relatively high, maybe we could use hash(u16) to store them.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/955/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/959,https://api.github.com/repos/apache/horaedb/issues/959,horaedb,1735565992,959,OOM caused by query,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-06-01T06:20:12Z,2023-07-18T06:50:17Z,"### Describe This Problem

Currently, there is no limit over the memory usage of the query, and OOM may happen frequently if query involves too much data.

### Proposal

Some methods can be adopted to prevent such OOM, and here are some of them I can come up with.
#### Memory limiter
Introduce a memory limiter during table scan.

#### Reject large table scan
Reject a scan involving too many row groups.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/959/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/959,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5em3Uw,horaedb,1587246384,959,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-06-12T12:30:20Z,2023-06-12T12:30:20Z,"After some digging, it is proven that the memory consumption is caused by massive ssts reading, that is to say, massive small ssts may be generated by flush and the following query may introduce many parts of these ssts into the memory because there is no limit on the number of all the involved in-flight ssts of a query.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5em3Uw/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/959,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fJw9u,horaedb,1596395374,959,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-06-19T02:31:26Z,2023-06-19T02:31:26Z,"> After some digging, it is proven that the memory consumption is caused by massive ssts reading, that is to say, massive small ssts may be generated by flush and the following query may introduce many parts of these ssts into the memory because there is no limit on the number of all the involved in-flight ssts of a query.

However, the merge procedure over all the ssts in the same segment requires the first row group of all the ssts, that is to say, the memory can't be limited to smaller than the consumption of all these first row group.

In conclusion, we should find a way to skip the unnecessary first row groups during merge procedure. And I guess another issue #1001 should be filed to describe how to skip unnecessary fetching.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fJw9u/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/959,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5hRxfd,horaedb,1632049117,959,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-07-12T08:11:52Z,2023-07-12T08:11:52Z,And the reason leading to high memory usage is that the data prefetching from ssts is not limited and started once the read procedure is initialized. I'll introduce a new mechanism to make this prefetch procedure be able to be triggered.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5hRxfd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/962,https://api.github.com/repos/apache/horaedb/issues/962,horaedb,1737381112,962,Only apply predicate for the first row in specific time series when filtering record batch,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-06-02T03:00:02Z,2024-10-19T11:24:53Z,"### Describe This Problem

# Data organization
When using the primary key `tsid + timestmap`, the data will be ordered like:
```
tag1, tag2, tag3, timestamp1, field1 
tag1, tag2, tag3, timestamp2, field2
tag1, tag2, tag3, timestamp3, field3
tag1, tag2, tag3, timestamp4, field4
...
```
# Filter in memory
However the order information is not used when filtering the pulled record batch.
We just simply apply the predicate to the record batch, that means something like `tag1, tag2, tag3` is possible to compute multiple times.

We just do a simple experiment to estimate the cost about duplicated computation above:
+ Schema: `a: int32, b: string, c: dictionary`
+ A big set has multiple duplicated rows, a small set has just one row
+ Predicates: int32 eq(for a), string eq(for b,c), string like(for b,c), string regex(for b,c)
+ Predicate 10000 times.
 
Experimetn results(see detail in `Additional Context`):
+ As we can see, duplicated computation about strings become much slower when `big / set >= 500`.
+ For eq/not eq case, dict has better performance compared to string(but worse than string's small set).
+ For like/not like/regex case, dict has worse performance compared to string.

It is easy to understand:
+ For eq/not eq case, strings comparison changed to int8s comparison when using dict format, that led to a better performance.
```
original expr:c = Utf8(""100"")
rewritten expr:c = CAST(Utf8(""100"") AS Dictionary(Int8, Utf8))
```
+ However, for like/not like/regex case, we can't do so and need to replace the dict idx with real word before comparing.
```
original expr:c LIKE Utf8(""10%"")
rewritten expr:CAST(c AS Utf8) LIKE Utf8(""10%"")
```
# Conclusion
Maybe we should design to remove the dupliacte computation taking advantage of the order(especially for string type column).


### Proposal

# 

### Additional Context
```
# big set / small set = 10:
## 1.big set / small set in each case
### a@0 = 100
big cost:540 micro
small cost:444 micro
big / small: 1.2162162
### a@0 != 100
big cost:486 micro
small cost:443 micro
big / small: 1.0970654
### b@1 = 100
big cost:596 micro
small cost:509 micro
big / small: 1.1709234
### b@1 != 100
big cost:531 micro
small cost:463 micro
big / small: 1.1468682
### b@1 LIKE 10%
big cost:404 micro
small cost:340 micro
big / small: 1.1882353
### b@1 NOT LIKE 10%
big cost:343 micro
small cost:295 micro
big / small: 1.1627119
### regexpmatch(b@1, 1*0{2}[a|b|c]*)
big cost:94968 micro
small cost:56659 micro
big / small: 1.6761327
### c@2 = CAST(100 AS Dictionary(Int8, Utf8))
big cost:4226 micro
small cost:4006 micro
big / small: 1.0549176
### c@2 != CAST(100 AS Dictionary(Int8, Utf8))
big cost:3959 micro
small cost:3820 micro
big / small: 1.0363874
### CAST(c@2 AS Utf8) LIKE 10%
big cost:1659 micro
small cost:1449 micro
big / small: 1.1449275
### CAST(c@2 AS Utf8) NOT LIKE 10%
big cost:1534 micro
small cost:1613 micro
big / small: 0.9510229
### regexpmatch(CAST(c@2 AS Utf8), 1*0{2}[a|b|c]*)
big cost:95559 micro
small cost:56795 micro
big / small: 1.6825249
## 2.dict / string in cases
case_eq small dict / string:7.870334
case_eq big dict / string:7.090604
case_not_eq small dict / string:8.25054
case_not_eq big dict / string:7.455744
case_like small dict / string:4.2617645
case_like big dict / string:4.106436
case_not_like small dict / string:5.467797
case_not_like big dict / string:4.4723034
case_regex small dict / string:1.0024003
case_regex big dict / string:1.0062232

# big set / small set = 100:
## 1.big set / small set in each case
### a@0 = 100
big cost:510 micro
small cost:458 micro
big / small: 1.1135371
### a@0 != 100
big cost:469 micro
small cost:454 micro
big / small: 1.0330397
### b@1 = 100
big cost:972 micro
small cost:485 micro
big / small: 2.0041237
### b@1 != 100
big cost:982 micro
small cost:470 micro
big / small: 2.0893617
### b@1 LIKE 10%
big cost:852 micro
small cost:374 micro
big / small: 2.278075
### b@1 NOT LIKE 10%
big cost:841 micro
small cost:366 micro
big / small: 2.2978141
### regexpmatch(b@1, 1*0{2}[a|b|c]*)
big cost:455702 micro
small cost:59480 micro
big / small: 7.6614323
### c@2 = CAST(100 AS Dictionary(Int8, Utf8))
big cost:4154 micro
small cost:3857 micro
big / small: 1.0770029
### c@2 != CAST(100 AS Dictionary(Int8, Utf8))
big cost:4130 micro
small cost:4077 micro
big / small: 1.0129998
### CAST(c@2 AS Utf8) LIKE 10%
big cost:2928 micro
small cost:1637 micro
big / small: 1.7886378
### CAST(c@2 AS Utf8) NOT LIKE 10%
big cost:2924 micro
small cost:1579 micro
big / small: 1.851805
### regexpmatch(CAST(c@2 AS Utf8), 1*0{2}[a|b|c]*)
big cost:457516 micro
small cost:56681 micro
big / small: 8.07177
## 2.dict / string in cases
case_eq small dict / string:7.952577
case_eq big dict / string:4.2736626
case_not_eq small dict / string:8.674468
case_not_eq big dict / string:4.205703
case_like small dict / string:4.3770056
case_like big dict / string:3.4366198
case_not_like small dict / string:4.3142076
case_not_like big dict / string:3.4768133
case_regex small dict / string:0.9529422
case_regex big dict / string:1.0039806

# big set / small set = 500:
## 1.big set / small set in each case
### a@0 = 100
big cost:633 micro
small cost:472 micro
big / small: 1.3411016
### a@0 != 100
big cost:545 micro
small cost:453 micro
big / small: 1.2030905
### b@1 = 100
big cost:3008 micro
small cost:464 micro
big / small: 6.4827585
### b@1 != 100
big cost:3013 micro
small cost:466 micro
big / small: 6.4656653
### b@1 LIKE 10%
big cost:2763 micro
small cost:346 micro
big / small: 7.985549
### b@1 NOT LIKE 10%
big cost:2688 micro
small cost:302 micro
big / small: 8.900662
### regexpmatch(b@1, 1*0{2}[a|b|c]*)
big cost:1963855 micro
small cost:54656 micro
big / small: 35.931187
### c@2 = CAST(100 AS Dictionary(Int8, Utf8))
big cost:5112 micro
small cost:3817 micro
big / small: 1.3392717
### c@2 != CAST(100 AS Dictionary(Int8, Utf8))
big cost:4040 micro
small cost:3785 micro
big / small: 1.0673712
### CAST(c@2 AS Utf8) LIKE 10%
big cost:7107 micro
small cost:1530 micro
big / small: 4.645098
### CAST(c@2 AS Utf8) NOT LIKE 10%
big cost:6704 micro
small cost:1471 micro
big / small: 4.557444
### regexpmatch(CAST(c@2 AS Utf8), 1*0{2}[a|b|c]*)
big cost:1908174 micro
small cost:56650 micro
big / small: 33.683567
## 2.dict / string in cases
case_eq small dict / string:8.226294
case_eq big dict / string:1.6994681
case_not_eq small dict / string:8.122317
case_not_eq big dict / string:1.3408563
case_like small dict / string:4.421965
case_like big dict / string:2.572204
case_not_like small dict / string:4.870861
case_not_like big dict / string:2.4940476
case_regex small dict / string:1.0364827
case_regex big dict / string:0.9716471

# big set / small set = 1000:
## 1.big set / small set in each case
### a@0 = 100
big cost:668 micro
small cost:470 micro
big / small: 1.4212766
### a@0 != 100
big cost:606 micro
small cost:470 micro
big / small: 1.2893617
### b@1 = 100
big cost:5916 micro
small cost:478 micro
big / small: 12.376569
### b@1 != 100
big cost:5578 micro
small cost:494 micro
big / small: 11.291498
### b@1 LIKE 10%
big cost:5260 micro
small cost:372 micro
big / small: 14.139785
### b@1 NOT LIKE 10%
big cost:5252 micro
small cost:291 micro
big / small: 18.04811
### regexpmatch(b@1, 1*0{2}[a|b|c]*)
big cost:3802702 micro
small cost:54614 micro
big / small: 69.6287
### c@2 = CAST(100 AS Dictionary(Int8, Utf8))
big cost:6062 micro
small cost:3875 micro
big / small: 1.5643871
### c@2 != CAST(100 AS Dictionary(Int8, Utf8))
big cost:4476 micro
small cost:3847 micro
big / small: 1.163504
### CAST(c@2 AS Utf8) LIKE 10%
big cost:11937 micro
small cost:1502 micro
big / small: 7.9474034
### CAST(c@2 AS Utf8) NOT LIKE 10%
big cost:12094 micro
small cost:1435 micro
big / small: 8.427875
### regexpmatch(CAST(c@2 AS Utf8), 1*0{2}[a|b|c]*)
big cost:3786914 micro
small cost:57731 micro
big / small: 65.59585
## 2.dict / string in cases
case_eq small dict / string:8.106694
case_eq big dict / string:1.0246788
case_not_eq small dict / string:7.7874494
case_not_eq big dict / string:0.80243814
case_like small dict / string:4.0376344
case_like big dict / string:2.2693915
case_not_like small dict / string:4.9312716
case_not_like big dict / string:2.3027418
case_regex small dict / string:1.0570732
case_regex big dict / string:0.99584824

# big set / small set = 4000:
## 1.big set / small set in each case
### a@0 = 100
big cost:1183 micro
small cost:468 micro
big / small: 2.5277777
### a@0 != 100
big cost:1053 micro
small cost:466 micro
big / small: 2.2596567
### b@1 = 100
big cost:21375 micro
small cost:582 micro
big / small: 36.726803
### b@1 != 100
big cost:21263 micro
small cost:482 micro
big / small: 44.11411
### b@1 LIKE 10%
big cost:19797 micro
small cost:355 micro
big / small: 55.766197
### b@1 NOT LIKE 10%
big cost:19641 micro
small cost:303 micro
big / small: 64.821785
### regexpmatch(b@1, 1*0{2}[a|b|c]*)
big cost:14936214 micro
small cost:56598 micro
big / small: 263.90002
### c@2 = CAST(100 AS Dictionary(Int8, Utf8))
big cost:12052 micro
small cost:3855 micro
big / small: 3.1263294
### c@2 != CAST(100 AS Dictionary(Int8, Utf8))
big cost:6440 micro
small cost:3697 micro
big / small: 1.7419529
### CAST(c@2 AS Utf8) LIKE 10%
big cost:43414 micro
small cost:1550 micro
big / small: 28.009033
### CAST(c@2 AS Utf8) NOT LIKE 10%
big cost:43047 micro
small cost:1564 micro
big / small: 27.523657
### regexpmatch(CAST(c@2 AS Utf8), 1*0{2}[a|b|c]*)
big cost:14997344 micro
small cost:57108 micro
big / small: 262.6137
## 2.dict / string in cases
case_eq small dict / string:6.623711
case_eq big dict / string:0.5638363
case_not_eq small dict / string:7.6701245
case_not_eq big dict / string:0.30287352
case_like small dict / string:4.366197
case_like big dict / string:2.1929586
case_not_like small dict / string:5.161716
case_not_like big dict / string:2.191691
case_regex small dict / string:1.0090109
case_regex big dict / string:1.0040927

# big set / small set = 8000:
## 1.big set / small set in each case
### a@0 = 100
big cost:1499 micro
small cost:477 micro
big / small: 3.1425576
### a@0 != 100
big cost:1544 micro
small cost:484 micro
big / small: 3.1900826
### b@1 = 100
big cost:41883 micro
small cost:499 micro
big / small: 83.93387
### b@1 != 100
big cost:41595 micro
small cost:492 micro
big / small: 84.54269
### b@1 LIKE 10%
big cost:38935 micro
small cost:359 micro
big / small: 108.45404
### b@1 NOT LIKE 10%
big cost:38706 micro
small cost:338 micro
big / small: 114.51479
### regexpmatch(b@1, 1*0{2}[a|b|c]*)
big cost:29740878 micro
small cost:55639 micro
big / small: 534.53296
### c@2 = CAST(100 AS Dictionary(Int8, Utf8))
big cost:20559 micro
small cost:4004 micro
big / small: 5.1346154
### c@2 != CAST(100 AS Dictionary(Int8, Utf8))
big cost:9026 micro
small cost:4060 micro
big / small: 2.2231526
### CAST(c@2 AS Utf8) LIKE 10%
big cost:84157 micro
small cost:1520 micro
big / small: 55.366447
### CAST(c@2 AS Utf8) NOT LIKE 10%
big cost:84709 micro
small cost:1469 micro
big / small: 57.6644
### regexpmatch(CAST(c@2 AS Utf8), 1*0{2}[a|b|c]*)
big cost:29849912 micro
small cost:56893 micro
big / small: 524.66754
## 2.dict / string in cases
case_eq small dict / string:8.024048
case_eq big dict / string:0.4908674
case_not_eq small dict / string:8.252032
case_not_eq big dict / string:0.21699724
case_like small dict / string:4.2339835
case_like big dict / string:2.1614742
case_not_like small dict / string:4.3461537
case_not_like big dict / string:2.1885238
case_regex small dict / string:1.0225382
case_regex big dict / string:1.0036662
```
_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/962/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/965,https://api.github.com/repos/apache/horaedb/issues/965,horaedb,1740811770,965,Use dictionary type to store tags column,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-06-05T03:03:01Z,2023-07-05T09:19:50Z,"### Describe This Problem

When we store data ordered by primary key `tsid, timestamp`, tags with same value will likely be put together, in current design we use string to represent it, in theory we could use more efficient datatype like dictionary.

### Proposal

Use dictionary to store tags column.

### Additional Context

https://docs.rs/arrow/latest/arrow/datatypes/enum.DataType.html#variant.Dictionary
- #962
- #589 

### Tracking Issues
- [x]  #1039
- [x] #1050
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/965/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/973,https://api.github.com/repos/apache/horaedb/issues/973,horaedb,1744044379,973,Hash value produced by DefaultHasher is not fixed,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-06-06T14:55:06Z,2024-10-19T11:23:47Z,"### Describe this problem

[The docs](https://doc.rust-lang.org/std/collections/hash_map/struct.DefaultHasher.html) says

> The internal algorithm is not specified, and so it and its hashes should not be relied upon over releases.




### Server version

NA

### Steps to reproduce

NA

### Expected behavior

_No response_

### Additional Information

AFAIK, xor filter is using DefaultHasher.

Some alternatives:
- https://docs.rs/seahash/latest/seahash/","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/973/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/982,https://api.github.com/repos/apache/horaedb/issues/982,horaedb,1749047110,982,Optimize the handling of flush failure,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-06-09T03:27:25Z,2023-11-03T07:04:52Z,"### Describe This Problem

Now flush table failures are counted globally. If a table flush fails for a certain number of times, this table will not be able to continue writing, and `ceresdb-server` must be restarted to recover.

### Proposal

The flush task failure needs to be counted separately, and the retry needs to have a back-off strategy.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/982/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/984,https://api.github.com/repos/apache/horaedb/issues/984,horaedb,1749369817,984,Prevent continuous forwarding between multiple nodes,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-06-09T08:08:30Z,2023-06-16T10:11:18Z,"### Describe This Problem

Now the proxy module does not check the number of forwarding times when forwarding, which may cause continuous forwarding between multiple nodes.

### Proposal

Add a parameter to the headers of grpc to mark that it has been forwarded.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/984/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/987,https://api.github.com/repos/apache/horaedb/issues/987,horaedb,1751790205,987, Tracking Issue: improve compaction,Rachelint,34352236,kamille,kamille@apache.org,OPEN,2023-06-12T02:55:23Z,2023-07-06T03:10:56Z,"### Describe This Problem

Now the design of compaction in ceresdb is still so rough, we should make more efforts in it.
There are several improvements that can be made in the following areas:
+ Compaction strategy. Now we just impl TWSC actually, we define a level 1 but do nothing special for it.
+ How to do compaction more efficiently. Speed of compaction may important equally important to strategy.
+ Metrics and tests. We should have ways to check the correctness and effectiveness(especially in query improvement) about our compaction strategy.


### Proposal

#### 1. Compaction strategy
- [ ] Introduce `score mechanism` to integrate multiple rules.
- [x] Consider sequence(wal) when picking compacting files to ensure the correctness.
- [ ] Eliminate time range overlap of ssts in level 1.
- [ ] Take priority of respective table in consideration.

#### 2. Performace of compaction
- [ ] Keep more data in memtable and larger L0 flushed sst. #1029 
- [x] Optimize sst iterator and filter build to consume less CPU. #975 

#### 3. Metrics and tests
- [ ] Emulator for compaction strategy inspired by [iox](https://github.com/influxdata/influxdb_iox/issues/7687) 
- [x] Add metrics (like `read amplification`, `write  amplification`, `space  amplification`) to check the effectiveness of the strategy.


### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/987/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/987,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fKCtK,horaedb,1596468042,987,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-06-19T04:13:27Z,2023-06-19T04:13:27Z,">  Add metrics (like read amplification, write  amplification, space  amplification) to check the effectiveness of the strategy.

Current codebase already have basic metrics for compact:
1. Input sst size/row num
2. Output sst size/row num

https://github.com/CeresDB/ceresdb/blob/f873980175e46eb436fb316cabaa6911985794ef/analytic_engine/src/table/metrics.rs#L62","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fKCtK/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/990,https://api.github.com/repos/apache/horaedb/issues/990,horaedb,1752742818,990,Multiple writer for the same sst caused by `close shard`,Rachelint,34352236,kamille,kamille@apache.org,OPEN,2023-06-12T13:12:15Z,2023-07-07T09:36:34Z,"### Describe this problem

Shard will be moved from nodes when process panic because if for any reason, all operations related to such a shard should be stopped before moving(especially the `write operations`). 
However background works(flush, compaction, all of them are `writes`) will not be stoppend rightly before now. That caused a serious bug : multiple writers for one sst.

<img src=""https://raw.githubusercontent.com/Rachelint/drawio-store/main/compaction_bug.drawio.svg?sanitize=true"">

### Server version

CeresDB Server 
Version: 1.2.2
Git commit: 2e206650
Git branch: main
Opt level: 3
Rustc version: 1.69.0-nightly
Target: aarch64-apple-darwin
Build date: 2023-06-12T13:01:03.592984000Z

### Steps to reproduce

Hard to reproduce, if must do this, steps following may can work:
+ Setup a ceresdb cluster with ceresmeta.
+ Trigger compaction/flush work for a specific table of shard in one node manually.
+ Move the shard to another node by ceresmeta manually before comapction/flush work finishing.
+ Trigger compaction/flush work for  the table of shard manually in the new node.

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/990/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/990,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fCXWx,horaedb,1594455473,990,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-06-16T10:18:56Z,2023-06-16T10:18:56Z,"After #998, the updates following the closing shard will be forbidden. However, some ssts may be still being written when close the shard, while these ssts may share the same ids with the new ssts created by the new node, leading to the multiple writers on the same sst.

Let's fix this problem in another PR. @baojinri ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fCXWx/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/990,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5g3aLC,horaedb,1625137858,990,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-07-07T09:36:34Z,2023-07-07T09:36:34Z,"> After #998, the updates following the closing shard will be forbidden. However, some ssts may be still being written when close the shard, while these ssts may share the same ids with the new ssts created by the new node, leading to the multiple writers on the same sst.
> 
> Let's fix this problem in another PR. @baojinri

#1009 has fixed the problem. However, #998 actually didn't achieve the goal to prevent updates after table is closed. And #998 has been reverted, I guess I'll submit another change set to make all things work.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5g3aLC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/997,https://api.github.com/repos/apache/horaedb/issues/997,horaedb,1759973591,997,Limit query is slow,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-06-16T05:58:56Z,2024-10-19T11:24:03Z,"### Describe This Problem

In most cases, limit query is expected to be very fast. However, when there are too many segments, it will be very slow even if  the query is limit by 1.

### Proposal

Make it run as fast as possible for limit query.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/997/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/997,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fJvER,horaedb,1596387601,997,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-06-19T02:18:41Z,2023-06-19T02:18:41Z,"> In most cases, limit query is expected to be very fast.

I think you are meaning query without group by and order by.

For this to work, our executor should support pass result in a streaming way, which is what we have now, so this should be improved without too much work.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fJvER/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/999,https://api.github.com/repos/apache/horaedb/issues/999,horaedb,1762604356,999,Support show tag values,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-06-19T02:20:07Z,2024-10-19T11:16:18Z,"### Describe This Problem

Currently, `count(distinct(x))` is the only choice to fetch all the values of a tag, which requires scanning all the ssts and takes lots of time. However, this should be a very light-weight query for time series database, e.g. grafana shows suggestions about a specific label.

### Proposal

- Record all the values for a tag if possible;
- Do fast query when `count(distinct(x))` is performed on a tag column, that is to say, `x` column a tag column.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/999/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/999,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5iSJB0,horaedb,1648922740,999,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-07-25T02:41:50Z,2023-07-25T02:41:50Z,"This is a huge task, I'll split it into these sub-tasks:
- [ ] Support separate index file for the column values
- [ ] Table options for supporting configure value index on specific column
- [ ] Query optimize rule for `count(distinct(x))` where `x` has the value index
- [ ] Compaction support create index file for multiple ssts","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5iSJB0/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1001,https://api.github.com/repos/apache/horaedb/issues/1001,horaedb,1762656687,1001,Avoid unnecessary merge,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-06-19T03:20:26Z,2024-12-18T04:44:12Z,"### Describe This Problem

Currently, merge sort is applied over all the ssts in one specific segment, in order for following dedup procedure. However, what is required by dedup is much less than the global order of all the rows of all the ssts, that is to say, current merge sort is unecessary.

### Proposal

What we need for dedup is just to gather all the rows sharing the same primary key in the order of their sequence number. And that is to say, there is no need to do merge sort over the ssts whose key range don't overlap between each other.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1001/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1002,https://api.github.com/repos/apache/horaedb/issues/1002,horaedb,1762663516,1002,Introduce columnar layout in memtable,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-06-19T03:33:14Z,2023-11-03T07:05:08Z,"### Describe This Problem

Currently, the memtable is organized by rows, which is not suitable for query and persist.

### Proposal

Introduce columnar layout in memtable to accelerate the query and flush.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1002/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1002,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qz0Fz,horaedb,1791967603,1002,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-11-03T07:05:08Z,2023-11-03T07:05:08Z,Duplicate.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qz0Fz/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1008,https://api.github.com/repos/apache/horaedb/issues/1008,horaedb,1765688589,1008,panic when trying to profile cpu,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-06-20T15:50:12Z,2023-07-20T07:15:48Z,"### Describe this problem

I try to profile cpu with ceresdb's http interface, and unfortunately the process panic.
```
2023-06-20 23:46:37.296 ERRO [common_util/src/panic.rs:42] thread 'ceres-default' panicked 'attempt to add with overflow' at ""/root/.cargo/registry/src/github.com-1ecc6299db9ec823/findshlibs-0.10.2/src/lib.rs:261""
   0: common_util::panic::set_panic_hook::{{closure}}
             at /root/code/xunming/ceresdb/common_util/src/panic.rs:41:18
   1: <alloc::boxed::Box<F,A> as core::ops::function::Fn<Args>>::call
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:2002:9
      std::panicking::rust_panic_with_hook
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:692:13
   2: std::panicking::begin_panic_handler::{{closure}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:577:13
   3: std::sys_common::backtrace::__rust_end_short_backtrace
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys_common/backtrace.rs:137:18
   4: rust_begin_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:575:5
   5: core::panicking::panic_fmt
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panicking.rs:64:14
   6: core::panicking::panic
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panicking.rs:114:5
   7: pprof::profiler::ProfilerGuardBuilder::blocklist::{{closure}}
      findshlibs::linux::SharedLibrary::callback::{{closure}}
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/findshlibs-0.10.2/src/linux/mod.rs:285:13
      core::ops::function::FnOnce::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/ops/function.rs:250:5
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271:9
      std::panicking::try::do_call
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483:40
      std::panicking::try
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447:19
      std::panic::catch_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140:14
      findshlibs::linux::SharedLibrary::callback
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/findshlibs-0.10.2/src/linux/mod.rs:281:15
   8: __GI___dl_iterate_phdr
   9: <findshlibs::linux::SharedLibrary as findshlibs::SharedLibrary>::each
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/findshlibs-0.10.2/src/linux/mod.rs:351:13
      pprof::profiler::ProfilerGuardBuilder::blocklist
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/pprof-0.11.1/src/profiler.rs:88:13
      profile::Profiler::dump_cpu_prof
             at /root/code/xunming/ceresdb/components/profile/src/lib.rs:148:21
  10: server::http::Service<Q>::profile_cpu::{{closure}}::{{closure}}::{{closure}}
             at /root/code/xunming/ceresdb/server/src/http.rs:414:25
      <tokio::runtime::blocking::task::BlockingTask<T> as core::future::future::Future>::poll
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/blocking/task.rs:42:21
      tokio::runtime::task::core::Core<T,S>::poll::{{closure}}
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/core.rs:223:17
      tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/loom/std/unsafe_cell.rs:14:9
      tokio::runtime::task::core::Core<T,S>::poll
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/core.rs:212:13
      tokio::runtime::task::harness::poll_future::{{closure}}
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:476:19
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271:9
      std::panicking::try::do_call
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483:40
      std::panicking::try
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447:19
      std::panic::catch_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140:14
      tokio::runtime::task::harness::poll_future
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:464:18
      tokio::runtime::task::harness::Harness<T,S>::poll_inner
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:198:27
      tokio::runtime::task::harness::Harness<T,S>::poll
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:152:15
      tokio::runtime::task::raw::poll
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/raw.rs:255:5
  11: tokio::runtime::task::raw::RawTask::poll
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/raw.rs:200:18
      tokio::runtime::task::UnownedTask<S>::run
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/mod.rs:431:9
      tokio::runtime::blocking::pool::Task::run
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/blocking/pool.rs:159:9
      tokio::runtime::blocking::pool::Inner::run
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/blocking/pool.rs:513:17
      tokio::runtime::blocking::pool::Spawner::spawn_thread::{{closure}}
             at /root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/blocking/pool.rs:471:13
      std::sys_common::backtrace::__rust_begin_short_backtrace
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys_common/backtrace.rs:121:18
  12: std::thread::Builder::spawn_unchecked_::{{closure}}::{{closure}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/thread/mod.rs:558:17
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271:9
      std::panicking::try::do_call
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483:40
      std::panicking::try
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447:19
      std::panic::catch_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140:14
      std::thread::Builder::spawn_unchecked_::{{closure}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/thread/mod.rs:557:30
      core::ops::function::FnOnce::call_once{{vtable.shim}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/ops/function.rs:250:5
  13: <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:1988:9
      <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:1988:9
      std::sys::unix::thread::Thread::new::thread_start
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys/unix/thread.rs:108:17
  14: start_thread
  15: clone
```

### Server version

curl 0:5000/debug/profile/cpu/1

### Steps to reproduce

curl 0:5000/debug/profile/cpu/1

### Expected behavior

CeresDB Server
Version: 1.2.2
Git commit: d26e958
Git branch: main
Opt level: 3
Rustc version: 1.69.0-nightly
Target: x86_64-unknown-linux-gnu
Build date: 2023-06-19T09:29:34.376369881Z

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1008/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1008,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fZQBB,horaedb,1600454721,1008,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-06-21T08:55:11Z,2023-06-21T08:55:11Z,"https://github.com/gimli-rs/findshlibs/blob/0.10.2/src/lib.rs#L261

There is already an issue for this https://github.com/gimli-rs/findshlibs/issues/77

But unfortunately no progress on this.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fZQBB/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1012,https://api.github.com/repos/apache/horaedb/issues/1012,horaedb,1767137859,1012,The current CI process merges the code from the Pr branch into the main branch for testing,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,CLOSED,2023-06-21T09:05:48Z,2023-07-13T09:58:14Z,"### Describe this problem

The current CI process merges the code in the pr branch with the main branch. This doesn't reasonable and can lead to extremely confusing results when multiple contributors make changes to related code at the same time.
We need to use the code from the pr branch for CI and the project maintainer to ensure that the pr code is newer than the main branch code before it can be merged.
This has the advantage that each pr is isolated and will ensure that the pr does not produce different ci results due to changes in the main branch.
I recommend using this setting:
https://i.stack.imgur.com/doAEL.png
### Server version

9a9c0f79116dff6df2a09fc8766e6952fd5f4432

### Steps to reproduce
Any CI before 9a9c0f79116dff6df2a09fc8766e6952fd5f4432

### Expected behavior

The CI process runs only on pr's branch.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1012/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1016,https://api.github.com/repos/apache/horaedb/issues/1016,horaedb,1768775160,1016,Support password setting / 支持设置密码,xxaier,130831741,xxai.art,xxai.art@gmail.com,CLOSED,2023-06-22T01:34:08Z,2023-06-28T09:06:40Z,"### Describe This Problem

No password will add a lot of complexity to the configuration of firewalls and permissions. 
It is recommended to support passwords

无密码对部署来说会添加很多防火墙和权限的配置复杂度，建议可以支持密码

### Proposal

https://docs.victoriametrics.com/vmauth.html
Password authentication can be embedded, or consider using vauth as an authentication gateway
密码鉴权可以做成嵌入式的，也可以考虑参考vauth做一个鉴权的网关

### Additional Context

Access passwords are a basic function of the database
访问密码对数据库是一个基础功能","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1016/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1016,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ftw05,horaedb,1605831993,1016,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-06-25T02:44:56Z,2023-06-25T02:44:56Z,"Hi, we can add a simple user system based on config files, such as this syntax:
```toml
[server.auth]
enable = true
# available values: file/ceresmeta
source = ""file""

[server.auth.file]
username = ""admin""
token = ""secret-token""
```

Does this method solve your problem?

> PS: In a fully cluster deployment, we may store user info in ceresmeta.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ftw05/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1016,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gBnb3,horaedb,1611036407,1016,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-06-28T09:02:44Z,2023-06-28T09:02:44Z,Duplicate with #929,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gBnb3/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1017,https://api.github.com/repos/apache/horaedb/issues/1017,horaedb,1770878486,1017,support sql comment,xxaier,130831741,xxai.art,xxai.art@gmail.com,CLOSED,2023-06-23T06:43:36Z,2023-06-28T07:25:05Z,"### Describe This Problem

<img width=""1696"" alt=""image"" src=""https://github.com/CeresDB/ceresdb/assets/130831741/b715801f-911b-4e0c-96ec-3c9e17e3b64f"">

```
   let fav = r#""CREATE TABLE fav (
    ts TIMESTAMP NOT NULL, // 插入的时间戳
    id uint64 NOT NULL,  // 用户行为的时间戳(毫秒)
    uid uint64 NOT NULL,  // 用户 id
    action uint8 NOT NULL, // 用户行为 : 收藏、取消收藏
    kind uint8 NOT NULL, // 对象类型，比如图片，模型，视频
    rid uint64 NOT NULL, // 收藏对象的 id
    TIMESTAMP KEY(ts),
    PRIMARY KEY(id)
  ) ENGINE=Analytic WITH (
    compression='ZSTD',
    enable_ttl=false
  );
```

### Proposal

as title

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1017/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1017,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fuHh5,horaedb,1605924985,1017,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-06-25T08:08:39Z,2023-06-25T08:08:39Z,"SQL use `--` character to represent comments, not `//`.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fuHh5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1017,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fzX8a,horaedb,1607302938,1017,NA,zouxiang1993,26276281,zouxiang,,NA,2023-06-26T11:49:35Z,2023-06-26T11:49:35Z,"-- as a comment prefix has already supported. 

```
curl --location --request POST 'http://127.0.0.1:5440/sql' --data-ascii '
> SELECT -- some comment
> 1 -- another comment
> '

{""rows"":[{""Int64(1)"":1}]}% 
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fzX8a/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1017,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5f0rr5,horaedb,1607645945,1017,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-06-26T14:47:51Z,2023-06-26T14:47:51Z,"Yes, but partially. 

I think the original issue is to ask for `comment` support, like

```sql
`uid` int64 tag comment ""用户 id"",  
```
We haven't store comments, so when you execute `show create table `, the comments will be lost.

- https://thispointer.com/add-and-view-comments-on-columns-in-mysql/","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5f0rr5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1017,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5f_uTn,horaedb,1610540263,1017,NA,zouxiang1993,26276281,zouxiang,,NA,2023-06-28T02:18:56Z,2023-06-28T02:18:56Z,"comment in `show create table` is already supported.

https://github.com/CeresDB/ceresdb/blob/946b3f89e9f6b18c51716a6fe1c5ddb549488dd5/interpreters/src/show_create.rs#L97-L99","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5f_uTn/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1017,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gA1I6,horaedb,1610830394,1017,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-06-28T06:28:11Z,2023-06-28T06:28:11Z,"Oops, it seems we already support this,  and there is a test case for it.

https://github.com/CeresDB/ceresdb/blob/946b3f89e9f6b18c51716a6fe1c5ddb549488dd5/integration_tests/cases/env/local/ddl/create_tables.result#L135","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gA1I6/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1017,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gBHdN,horaedb,1610905421,1017,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-06-28T07:25:05Z,2023-06-28T07:25:05Z,"@xxaier As said above, current version already support your request, please have a try.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gBHdN/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1020,https://api.github.com/repos/apache/horaedb/issues/1020,horaedb,1771330637,1020,influxdb write api not correctly handle time field,ttys3,41882455,,,CLOSED,2023-06-23T11:54:32Z,2023-08-02T09:46:17Z,"### Describe this problem

1. doc issue:  seems the document did not **explicitly** mention that users from inlfuxdb, should use unix `mili` for the time field.
but influxdb uses `nano` time for this (ref to https://docs.influxdata.com/influxdb/v1.7/write_protocols/line_protocol_reference/  `Unix nanosecond timestamp`).

2. the db does not support store `nano` as time field. but when we do 

```shell
curl --location 'http://localhost:5440/influxdb/v1/write' \
--header 'Content-Type: text/plain' \
--data 'system,api_type=blocking,hostname=host_94,id=rack_4,vendor=AWS disk_free=910.1630013889787,disk_total=7500000000i,mem_free=14855517621364468624u,mem_total=7500000000i,temperature=20.642589504090367 1687513104655044842'
```
the data is in valid influx line protocol, and the time is always nano, like `1687513104655044842`
ceresdb should auto convert this to `1687513104655044842/1000000` or throw an error

ref to https://github.com/CeresDB/ceresdb/blob/9a9c0f79116dff6df2a09fc8766e6952fd5f4432/proxy/src/influxdb/types.rs#L483C47-L483C47

```rust
        let timestamp = match line.timestamp {
            Some(ts) => req.precision.try_normalize(ts).context(InternalNoCause {
                msg: ""time outside range -9223372036854775806 - 9223372036854775806"",
            })?,
            None => Timestamp::now().as_i64(),
        };
```

but if we only support mili seconds, which will make the write api not fully compatible with influxdb, and even will cause the lost of data. see https://github.com/CeresDB/ceresdb/issues/1020#issuecomment-1604306737


3. uses too much memory when query. it use **22GB** memory (yes, this crashed my machine once, since it increased too quickly, the OOM is too slow to trigger) in my machine for query like `SELECT * FROM ""system"" order by time desc limit 5;` for `10000 * 10000` rows data.

after fixed the write using unix `mili`, the memory usage decreased to **3GB** but this is only because we only got `9981906` rows inserted now (I was so glad at first, but reality drove me into the abyss again).

-----------------------------------------------

```sql
SELECT MEAN(""temperature"") FROM ""system"" GROUP BY time(2m) LIMIT 10
```

got error:

```json
{
    ""code"": 500,
    ""message"": ""Failed to handle request, err:Internal error, msg:Failed to execute interpreter, err:Failed to execute select, err:Failed to execute logical plan, err:Failed to collect record batch stream, err:Stream error, msg:convert from arrow record batch, err:External error: Arrow error: Compute error: Overflow happened on: 1687518609978329692 * 1000000""
}
```

the data is like this:

```sql
mysql> SELECT * FROM ""system"" order by time desc limit 5010000,5;
+----------------------+---------------------+----------+-------------------+---------------+----------+--------+----------------------+---------------+--------------------+--------+
| tsid                 | time                | api_type | disk_free         | disk_total    | hostname | id     | mem_free             | mem_total     | temperature        | vendor |
+----------------------+---------------------+----------+-------------------+---------------+----------+--------+----------------------+---------------+--------------------+--------+
|  8419334968377245646 | 1687519193500451180 | blocking | 884.5372079676212 | 9499000000000 | host_99  | rack_9 |  4732955575044594108 | 9499000000000 |  56.49539751972941 | AWS    |
|  3010413116408139964 | 1687519193500450676 | blocking | 934.4715556549916 | 9499000000000 | host_98  | rack_8 | 15719136822748330796 | 9499000000000 |   9.77510533199514 | AWS    |
|  8291545101785022959 | 1687519193500450157 | blocking | 584.5101426032528 | 9499000000000 | host_97  | rack_7 | 15624829521003432967 | 9499000000000 |  71.64944410901087 | AWS    |
| 16312321373030829732 | 1687519193500449674 | blocking |  251.161030856011 | 9499000000000 | host_96  | rack_6 |  7068395414942882107 | 9499000000000 | 51.326979117765106 | AWS    |
|  8407531866066528452 | 1687519193500449143 | blocking | 367.7945846371885 | 9499000000000 | host_95  | rack_5 | 11735992339675738527 | 9499000000000 |   66.0831191068601 | AWS    |
+----------------------+---------------------+----------+-------------------+---------------+----------+--------+----------------------+---------------+--------------------+--------+
5 rows in set (6.39 sec)
```

### Server version

```
CeresDB Server 
Version: 1.2.2
Git commit: VERGEN_IDEMPOTENT_OUTPUT
Git branch: VERGEN_IDEMPOTENT_OUTPUT
Opt level: 3
Rustc version: 1.69.0-nightly
Target: x86_64-unknown-linux-gnu
Build date: 2023-05-31T09:30:26.560795195Z
```

### Steps to reproduce

0. ensure `default_timestamp_column_name = ""time""` config

config file:

```toml
[server]
bind_addr = ""0.0.0.0""
http_port = 5440
grpc_port = 8831

[server.default_schema_config]
default_timestamp_column_name = ""time""

[logger]
level = ""info""

[tracing]
dir = ""/data""

[analytic.storage.object_store]
type = ""Local""
data_dir = ""/data""

[analytic.wal]
type = ""RocksDB""
data_dir = ""/data""
```

2. make a first request to create the table

```shell
curl --location 'http://localhost:5440/influxdb/v1/write' \
--header 'Content-Type: text/plain' \
--data 'system,api_type=blocking,hostname=host_94,id=rack_4,vendor=AWS disk_free=910.1630013889787,disk_total=7500000000i,mem_free=14855517621364468624u,mem_total=7500000000i,temperature=20.642589504090367 1687513104655044842'
```

the auto created schema should like:

```sql
CREATE TABLE `system`
             (
                          `tsid` UINT64 NOT NULL,
                          `time` TIMESTAMP NOT NULL,
                          `api_type` STRING tag,
                          `disk_free` DOUBLE,
                          `disk_total` BIGINT,
                          `hostname` string tag,
                          `id` string tag,
                          `mem_free` uint64,
                          `mem_total` BIGINT,
                          `temperature` DOUBLE,
                          `vendor` string tag,
                          PRIMARY KEY(tsid,time),
                          timestamp KEY(   time)
             )
```

check what we got:

```sql
mysql> SELECT * FROM ""system"" where mem_free=14855517621364468624 limit 1;
+---------------------+---------------------+----------+-------------------+------------+----------+--------+----------------------+------------+--------------------+--------+
| tsid                | time                | api_type | disk_free         | disk_total | hostname | id     | mem_free             | mem_total  | temperature        | vendor |
+---------------------+---------------------+----------+-------------------+------------+----------+--------+----------------------+------------+--------------------+--------+
| 1698832110102542182 | 1687513104655044842 | blocking | 910.1630013889787 | 7500000000 | host_94  | rack_4 | 14855517621364468624 | 7500000000 | 20.642589504090367 | AWS    |
+---------------------+---------------------+----------+-------------------+------------+----------+--------+----------------------+------------+--------------------+--------+
1 row in set (0.10 sec)
```

note that, the time is in nano: `1687513104655044842` and without touched.

3. batch insert data

```go
package main

import (
	""context""
	""fmt""
	""log""
	""math/rand""
	""time""

	ceresdb ""github.com/CeresDB/ceresdb-client-go/ceresdb""
	""github.com/cheggaaa/pb/v3""
)

func main() {
	endpoint := ""127.0.0.1:8831""
	client, err := ceresdb.NewClient(endpoint, ceresdb.Direct,
		ceresdb.WithDefaultDatabase(""public""),
	)
	if err != nil {
		panic(err)
	}
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	ceresdbDemoWriteBlocking(ctx, client)
}

func ceresdbDemoWriteBlocking(ctx context.Context, client ceresdb.Client) {
	batchSize := 5000
	totalRows := 10000 * 10000
	ts := time.Now()
	n := 0
	errNum := 0

	bar := pb.Full.Start(totalRows)
	defer bar.Finish()

	points := make([]ceresdb.Point, 0, 2)

out:
	for i := 0; i < totalRows; i++ {
		// Create point
		point, err := ceresdb.NewPointBuilder(""system"").
			SetTimestamp(time.Now().UnixNano()).
			AddTag(""id"", ceresdb.NewStringValue(fmt.Sprintf(""rack_%v"", i%10))).
			AddTag(""vendor"", ceresdb.NewStringValue(""AWS"")).
			AddTag(""hostname"", ceresdb.NewStringValue(fmt.Sprintf(""host_%v"", i%100))).
			AddTag(""api_type"", ceresdb.NewStringValue(""blocking"")).
			AddField(""temperature"", ceresdb.NewDoubleValue(rand.Float64()*80.0)).
			AddField(""disk_free"", ceresdb.NewDoubleValue(rand.Float64()*1000.0)).
			AddField(""disk_total"", ceresdb.NewInt64Value((int64(i/10+1)*1000000))).
			AddField(""mem_total"", ceresdb.NewInt64Value((int64(i/100+1)*10000000))).
			AddField(""mem_free"", ceresdb.NewUint64Value(rand.Uint64())).
			Build()
		if err != nil {
			panic(err)
		}

		// Add point to the batch
		points = append(points, point)

		select {
		case <-ctx.Done():
			break out
		default:

			// Flush the batch if it reaches the batch size
			if len(points) >= batchSize {
				resp, err := writeBatch(ctx, client, points)
				if err != nil {
					errNum += len(points)
					log.Printf(""write error: %s"", err.Error())
				} else {
					log.Printf(""writeBatch success, resp=%+v"", resp)
					n += len(points)
				}
				bar.Add(len(points))
				// reset points
				points = points[:0]
			}

		}
	}

	// Flush the remaining points in the batch
	resp, err := writeBatch(ctx, client, points)
	if err != nil {
		errNum += len(points)
		log.Printf(""write error: %s"", err.Error())
	} else {
		log.Printf(""writeBatch success, resp=%+v"", resp)
		n += len(points)
	}
	bar.Add(len(points))

	log.Printf(""success=%v error=%v cost_ms=%v speed/s=%.2f"", n, errNum, time.Now().Sub(ts).Milliseconds(), float64(n)/time.Now().Sub(ts).Seconds())
}

func writeBatch(ctx context.Context, client ceresdb.Client, batchPoints []ceresdb.Point) (ceresdb.WriteResponse, error) {
	req := ceresdb.WriteRequest{
		// ReqCtx: ctx,
		Points: batchPoints,
	}

	resp, err := client.Write(ctx, req)
	if err != nil {
		return resp, err
	}

	return resp, nil
}
```

5. query with influxql

```
http://localhost:5440/influxdb/v1/query?q=SELECT MEAN(""temperature"") FROM ""system"" GROUP BY time(2m) LIMIT 10
```

### Expected behavior

1. query no error
2. less memory usage  (it uses 22GB for something like `SELECT * FROM ""system"" order by time desc limit 5;`)

### Additional Information

see 
https://github.com/CeresDB/ceresdb/issues/1020#issuecomment-1604196366

and 
 https://github.com/CeresDB/ceresdb/issues/1020#issuecomment-1604306737","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1020/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1020,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fnhgO,horaedb,1604196366,1020,NA,ttys3,41882455,,,NA,2023-06-23T12:12:55Z,2023-06-23T12:12:55Z,"update: 
according the error:  `Arrow error: Compute error: Overflow happened on: 1687518609978329692 * 1000000`

I changed the code to use unix mili for the `time` field

```diff
-SetTimestamp(time.Now().UnixNano())
+SetTimestamp(time.Now().UnixMilli())
```

drop the table and regenerate the demo data (since I can not remove the `1687513104655044842`  time record)

the error is gone.

but the `/influxdb/v1/write` api should fix the compatible issue. as I mention in the issue above.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fnhgO/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1020,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fnrWI,horaedb,1604236680,1020,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-06-23T12:48:53Z,2023-06-23T12:48:53Z,"@ttys3 Thanks for your report. After the vacation, I guess @jiacai2050 will help solve this issue.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fnrWI/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1020,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fn8cx,horaedb,1604306737,1020,NA,ttys3,41882455,,,NA,2023-06-23T13:44:14Z,2023-06-23T13:44:14Z,"update again:

if I use mili seconds in `time`  field, it will make `Overflow` error disappear, but also caused most of the data missing (due to mili seconds the same)

```
mysql> SELECT count(*) FROM ""system"";
+-----------------+
| COUNT(UInt8(1)) |
+-----------------+
|         9981906 |
+-----------------+
1 row in set (1.50 sec)
```

but in influxdb, this is OK:

`q=SELECT count(*) FROM ""system""`

we got the expected result:

```
{
    ""results"": [
        {
            ""statement_id"": 0,
            ""series"": [
                {
                    ""name"": ""system"",
                    ""columns"": [
                        ""time"",
                        ""count_disk_free"",
                        ""count_disk_total"",
                        ""count_mem_free"",
                        ""count_mem_total"",
                        ""count_temperature""
                    ],
                    ""values"": [
                        [
                            ""1970-01-01T00:00:00Z"",
                            100000000,
                            100000000,
                            100000000,
                            100000000,
                            100000000
                        ]
                    ]
                }
            ]
        }
    ]
}
```
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fn8cx/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1020,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fsG7Y,horaedb,1605398232,1020,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-06-24T11:16:10Z,2023-06-24T11:16:10Z,"Hi, thanks for reporting.

For your case 1 and 2, currently only ms precision is supported by CeresDB, other precision will convert to ms when write.

But in order to convert time, users need to set `precision` param, which default to ms. So your case 2 could be fixed this way

```
curl --location 'http://localhost:5440/influxdb/v1/write?precision=ns' \
--header 'Content-Type: text/plain' \
--data 'system,api_type=blocking,hostname=host_94,id=rack_4,vendor=AWS disk_free=910.1630013889787,disk_total=7500000000i,mem_free=14855517621364468624u,mem_total=7500000000i,temperature=20.642589504090367 1687513104655044842'
```

Of course this will lost time precision if converted from ns, but this is how ceresdb is implemented now, if you want ns precision, welcome to open another issue, tell us your usercase.

> PS: I file an issue to update this docs. https://github.com/CeresDB/docs/issues/94

For your case 3, I need more time to reproduce this, will update here when I have more context.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fsG7Y/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1020,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ftCLS,horaedb,1605640914,1020,NA,ttys3,41882455,,,NA,2023-06-24T16:51:15Z,2023-06-24T16:51:15Z,"case 3 is reproducable.

I tested again, this time it uses about **19.7GB** memory:

```sql
mysql> SELECT * FROM ""system"" order by time desc limit 5;
+----------------------+---------------+----------+--------------------+---------------+----------+--------+----------------------+----------------+--------------------+--------+
| tsid                 | time          | api_type | disk_free          | disk_total    | hostname | id     | mem_free             | mem_total      | temperature        | vendor |
+----------------------+---------------+----------+--------------------+---------------+----------+--------+----------------------+----------------+--------------------+--------+
| 16783440087510120366 | 1790316217186 | blocking |  575.0676008151054 | 9999996000000 | host_58  | rack_8 |  8613071225237535455 | 10000000000000 | 37.376148430256194 | AWS    |
| 14903144881462136102 | 1790316057393 | blocking | 185.71644931843477 | 9999997000000 | host_60  | rack_0 | 12336037372503751218 | 10000000000000 |  15.41807464592409 | AWS    |
| 18036101797844328737 | 1790315980444 | blocking |  323.1994394796841 | 9999948000000 | host_71  | rack_1 |  5277818915129357015 |  9999950000000 | 3.0270463604499485 | AWS    |
|  3146672079379267971 | 1790315975444 | blocking |  712.1429296004604 | 9999981000000 | host_8   | rack_8 | 17916653639169366056 |  9999990000000 | 50.668711268872215 | AWS    |
|  7443242848413171290 | 1790315969714 | blocking |  762.1131729883782 | 9999942000000 | host_15  | rack_5 |  9737001516229547025 |  9999950000000 |  17.12921532169998 | AWS    |
+----------------------+---------------+----------+--------------------+---------------+----------+--------+----------------------+----------------+--------------------+--------+
5 rows in set (5.65 sec)
```

and, it generate lots of small sst files, is this expected?

yes, I use iotop to check and wait the compact done.

```shell
❯ ls -1 /var/lib/ceresdb/store/2/2199023255553 | wc -l
17881

❯ sudo du -sh /var/lib/ceresdb/store/2/2199023255553
4.5G	/var/lib/ceresdb/store/2/2199023255553
```

```shell
.rw-r--r-- root root 328 KB Sun Jun 25 00:21:19 2023  9865.sst
.rw-r--r-- root root 343 KB Sun Jun 25 00:21:19 2023  9866.sst
.rw-r--r-- root root 337 KB Sun Jun 25 00:21:19 2023  9867.sst
```

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ftCLS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1020,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ftv4Y,horaedb,1605828120,1020,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-06-25T02:27:49Z,2023-06-25T02:27:49Z,"> and, it generate lots of small sst files, is this expected?

This depends on table's write buffer(default 32M), however there is a bug in 1.2.0, which cause flush too frequently, would you mind try latest version? we have nightly image here:
- https://github.com/CeresDB/ceresdb/pkgs/container/ceresdb-server
```
docker pull ghcr.io/ceresdb/ceresdb-server:nightly-20230625-9a9c0f79
```

As for high memory consumption, it's mostly caused by too many small sst files, since there is a merge sort process inside one query, which will read head of all sst files. Related issue:
- https://github.com/CeresDB/ceresdb/issues/1001
- https://github.com/CeresDB/ceresdb/issues/997","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ftv4Y/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1020,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fwbAV,horaedb,1606529045,1020,NA,ttys3,41882455,,,NA,2023-06-26T03:12:50Z,2023-06-26T03:12:50Z,"tried latest `ghcr.io/ceresdb/ceresdb-server:nightly-20230626-03d9aa49` and  `ghcr.io/ceresdb/ceresdb-server:nightly-20230625-9a9c0f79`

sst file still small:

```
  264.0 KiB [################  ]  175.sst
  264.0 KiB [################  ]  243.sst
  264.0 KiB [################  ]  134.sst
  264.0 KiB [################  ]  80.sst
  264.0 KiB [################  ]  84.sst
  264.0 KiB [################  ]  79.sst
  264.0 KiB [################  ]  269.sst
  264.0 KiB [################  ]  394.sst
  264.0 KiB [################  ]  150.sst
```

write speed becomes slower and slower","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fwbAV/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1020,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fy299,horaedb,1607167869,1020,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-06-26T10:24:06Z,2023-06-26T10:24:06Z,"> tried latest `ghcr.io/ceresdb/ceresdb-server:nightly-20230626-03d9aa49` and `ghcr.io/ceresdb/ceresdb-server:nightly-20230625-9a9c0f79`
> 
> sst file still small:
> 
> ```
>   264.0 KiB [################  ]  175.sst
>   264.0 KiB [################  ]  243.sst
>   264.0 KiB [################  ]  134.sst
>   264.0 KiB [################  ]  80.sst
>   264.0 KiB [################  ]  84.sst
>   264.0 KiB [################  ]  79.sst
>   264.0 KiB [################  ]  269.sst
>   264.0 KiB [################  ]  394.sst
>   264.0 KiB [################  ]  150.sst
> ```
> 
> write speed becomes slower and slower

Actually, we found the massive small sst will be generated too when large small string or null values are inserted. The problem is introduced by the current memtable implementation, and #1029 is one solution to it. As for the null values, another fix I'm working on will use bitmap to reduce the memory consumption.

Finally, we are planning to replace the current memtable with a brand new one, but the work is still in design stage and will take some time to be delivered.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5fy299/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1020,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5f1QF7,horaedb,1607795067,1020,NA,ttys3,41882455,,,NA,2023-06-26T16:07:40Z,2023-06-26T16:07:40Z,"so, curently,  sst flush when table's write buffer(default 32M) is full.

currently the test data is like
```
system,api_type=blocking,hostname=host_94,id=rack_4,vendor=AWS disk_free=910.1630013889787,disk_total=7500000000i,mem_free=14855517621364468624u,mem_total=7500000000i,temperature=20.642589504090367 1687513104655044842
```
which has string ""blocking"", ""host_xx"",  ""rack_xx"" and ""AWS"" so this is the reason why sst flush so fast ?

this seems not too much string

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5f1QF7/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1020,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5f4Vpk,horaedb,1608604260,1020,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-06-27T02:17:55Z,2023-06-27T02:17:55Z,"I will try to reproduce your case 3 today, I have one question for your code:
```
SetTimestamp(time.Now().UnixNano()).
```
It seems you use ns to set timestamp, do you change to ms in your later tests?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5f4Vpk/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1020,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5f5bMb,horaedb,1608889115,1020,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-06-27T06:43:25Z,2023-06-27T06:43:25Z,"Hi, after some tests on my local dev, I found why there are so many small sst in your benchmark.

Your original code have lots of duplicated rows since you have code like this
```
AddTag(""id"", ceresdb.NewStringValue(fmt.Sprintf(""rack_%v"", i%10))).
AddTag(""hostname"", ceresdb.NewStringValue(fmt.Sprintf(""host_%v"", i%100)))
```
For now, memtable will save all duplicated rows in memory(tracked in https://github.com/CeresDB/ceresdb/issues/1035), and do the dedup when flush to SST, so you have got lots of small SST file.

After change your code like this, SST size is expected.
```go
point, err := ceresdb.NewPointBuilder(""system"").
    SetTimestamp(time.Now().UnixNano()/1e6).
    AddTag(""id"", ceresdb.NewStringValue(fmt.Sprintf(""rack_%v"", i))).
    AddTag(""vendor"", ceresdb.NewStringValue(""AWS"")).
    AddTag(""hostname"", ceresdb.NewStringValue(fmt.Sprintf(""host_%v"", i))).
    AddTag(""api_type"", ceresdb.NewStringValue(""blocking"")).
    AddField(""temperature"", ceresdb.NewDoubleValue(rand.Float64()*80.0)).
    AddField(""disk_free"", ceresdb.NewDoubleValue(rand.Float64()*1000.0)).
    AddField(""disk_total"", ceresdb.NewInt64Value((int64(i/10+1)*1000000))).
    AddField(""mem_total"", ceresdb.NewInt64Value((int64(i/100+1)*10000000))).
    AddField(""mem_free"", ceresdb.NewUint64Value(rand.Uint64())).
    Build()
```

```
$ll -ltrh /tmp/ceresdb/store/2/2199023255553/
total 103M
-rw-r--r-- 1 chenxiang.ljc users 6.7M Jun 27 14:24 1.sst
-rw-r--r-- 1 chenxiang.ljc users 6.8M Jun 27 14:24 2.sst
-rw-r--r-- 1 chenxiang.ljc users 6.8M Jun 27 14:24 3.sst
-rw-r--r-- 1 chenxiang.ljc users 6.8M Jun 27 14:24 4.sst
-rw-r--r-- 1 chenxiang.ljc users 6.8M Jun 27 14:24 5.sst
-rw-r--r-- 1 chenxiang.ljc users 6.8M Jun 27 14:24 6.sst
-rw-r--r-- 1 chenxiang.ljc users 6.8M Jun 27 14:25 7.sst
-rw-r--r-- 1 chenxiang.ljc users 6.8M Jun 27 14:25 8.sst
-rw-r--r-- 1 chenxiang.ljc users 6.5M Jun 27 14:25 9.sst
-rw-r--r-- 1 chenxiang.ljc users 6.5M Jun 27 14:25 10.sst
-rw-r--r-- 1 chenxiang.ljc users 6.5M Jun 27 14:25 11.sst
-rw-r--r-- 1 chenxiang.ljc users 6.5M Jun 27 14:25 12.sst
-rw-r--r-- 1 chenxiang.ljc users 6.5M Jun 27 14:25 13.sst
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5f5bMb/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1020,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5f5lf1,horaedb,1608931317,1020,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-06-27T07:16:28Z,2023-06-27T07:16:28Z,"After fix the too many small SST issue, I try execute your query
```sql
SELECT * FROM ""system"" order by timestamp desc limit 5;
```
It will cost memory(res) up to 13G, and we optimize those query in those issue:
- https://github.com/CeresDB/ceresdb/issues/997
- https://github.com/CeresDB/ceresdb/issues/1001","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5f5lf1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1020,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jDnhQ,horaedb,1661892688,1020,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-08-02T09:46:17Z,2023-08-02T09:46:17Z,"Closed since the original issue is explained above, feel free to open new issue when you have more questions.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5jDnhQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1022,https://api.github.com/repos/apache/horaedb/issues/1022,horaedb,1773127174,1022,"When the sql is empty comment, ceresdb will crash",tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,CLOSED,2023-06-25T08:02:13Z,2023-06-25T09:50:34Z,"### Describe this problem

When the sql is empty comment, ceresdb will crash.
Like this sql:

```
-- 
```

### Server version

9a9c0f79116dff6df2a09fc8766e6952fd5f4432

### Steps to reproduce

send sql:
```
-- 
```

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1022/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1029,https://api.github.com/repos/apache/horaedb/issues/1029,horaedb,1773769368,1029,Use dictionary to store string in memtable,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-06-26T03:31:36Z,2024-10-19T11:23:14Z,"### Describe This Problem

When I run tsbs with the default memtable size(30M), I found the flushed sst is too small(1.9M).
As inspecting the parquet file, the high compression ratio is mainly due to storing string in dictionary way.
I think we can use dictionary to store string in memtable for keeping more data in memory, and lead to the larger flushed sst.

### Proposal

+ Use dictionary to store string in memtable(I plan to do the poc work first).
+ Maybe we should do some statistics with the exist sampling memtable to decide which column should be store in dictionary way(store the high cardinality string columns in this way may waste more space in contrast).
+ Suggested by @tanruixiang, we can keep the mutable memtables in original way, and compress the strings in dictionary way when switching them to immutables. 

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1029/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1031,https://api.github.com/repos/apache/horaedb/issues/1031,horaedb,1774415083,1031,Invert the dependency relationship between `common_types` and `common_util` crate,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-06-26T10:09:08Z,2023-07-18T02:11:59Z,"### Describe This Problem

Currently,  `common_util` depends on `common_types`, and this is weird.

### Proposal

Make `common_types` crate depend on `common_util`.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1031/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1031,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5hs8ZD,horaedb,1639171651,1031,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-07-18T02:11:56Z,2023-07-18T02:11:56Z,"After #1077, `common_util` has been removed.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5hs8ZD/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1032,https://api.github.com/repos/apache/horaedb/issues/1032,horaedb,1774579860,1032,Support nullable tag in primary key,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-06-26T11:39:30Z,2024-10-19T11:22:52Z,"### Describe This Problem

Currently the primary key column cannot be null.
It is common for tag to be null in time-series situation.

### Proposal

Shall we support nullable tag in primary key?

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1032/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1035,https://api.github.com/repos/apache/horaedb/issues/1035,horaedb,1776198267,1035,Memtable support dedup rows with same primary keys ,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-06-27T06:42:05Z,2024-10-19T11:34:16Z,"### Describe This Problem

Current memtable implementation will always append row to inner list, which will take more memory than expected when there are duplicated rows, and this will cause SST in level0 is very small, which is unfriendly for query.

### Proposal

Dedup rows when insert to memtable for tables in overwritten mode

### Additional Context

https://github.com/CeresDB/ceresdb/issues/1020#issuecomment-1608889115","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1035/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1038,https://api.github.com/repos/apache/horaedb/issues/1038,horaedb,1778136817,1038,Support triggering flush by wal size,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-06-28T04:05:36Z,2024-10-19T11:22:17Z,"### Describe This Problem

I found wal size will become too large in some situations, that leads to the unacceptable long recovery time.
Maybe we should support triggering flush by wal size, this is a common flush triggering strategy implemented in many other dbs(e.g. rocksdb).

### Proposal

+ Support a method like `should_flush` in `WalMananger`.
+ Peridic checking this and trigger flush.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1038/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1039,https://api.github.com/repos/apache/horaedb/issues/1039,horaedb,1778453912,1039,Support write and read dictionary encoding column,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,CLOSED,2023-06-28T08:22:51Z,2023-07-05T07:43:10Z,"### Describe This Problem

Support write and read dictionary encoding column

### Proposal

see #965

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1039/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1040,https://api.github.com/repos/apache/horaedb/issues/1040,horaedb,1778534590,1040,Parquet page index cause server panic,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-06-28T09:13:10Z,2024-10-19T11:24:17Z,"### Describe this problem

This error arise from one of our test cluster, here are the backtrace 
```
2023-06-28 08:59:04.789 ERRO [common_util/src/panic.rs:42] thread 'ceres-compact' panicked 'called `Option::unwrap()` on a `None` value' at ""/usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/parquet-38.0.0/src/file/page_index/index_reader.rs:159""
   0: common_util::panic::set_panic_hook::{{closure}}
             at ceresdb/common_util/src/panic.rs:41:18
   1: <alloc::boxed::Box<F,A> as core::ops::function::Fn<Args>>::call
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:2002:9
      std::panicking::rust_panic_with_hook
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:692:13
   2: std::panicking::begin_panic_handler::{{closure}}
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:577:13
   3: std::sys_common::backtrace::__rust_end_short_backtrace
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys_common/backtrace.rs:137:18
   4: rust_begin_unwind
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:575:5
   5: core::panicking::panic_fmt
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panicking.rs:64:14
   6: core::panicking::panic
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panicking.rs:114:5
   7: core::option::Option<T>::unwrap
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/option.rs:823:21
      parquet::file::page_index::index_reader::get_location_offset_and_total_length::{{closure}}
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/parquet-38.0.0/src/file/page_index/index_reader.rs:159:42
      core::iter::adapters::map::map_fold::{{closure}}
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/iter/adapters/map.rs:84:28
      core::iter::traits::iterator::Iterator::fold
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/iter/traits/iterator.rs:2438:21
      <core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/iter/adapters/map.rs:124:9
      <i32 as core::iter::traits::accum::Sum>::sum
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/iter/traits/accum.rs:50:17
      core::iter::traits::iterator::Iterator::sum
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/iter/traits/iterator.rs:3408:9
      parquet::file::page_index::index_reader::get_location_offset_and_total_length
             at usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/parquet-38.0.0/src/file/page_index/index_reader.rs:157:24
   8: parquet::arrow::async_reader::<impl parquet::arrow::arrow_reader::ArrowReaderBuilder<parquet::arrow::async_reader::AsyncReader<T>>>::new_with_options::{{closure}}
             at usr/local/carg o/registry/src/github.com-1ecc6299db9ec823/parquet-38.0.0/src/arrow/async_reader/mod.rs:250:21
      parquet_ext::meta_data::meta_with_page_indexes::{{closure}}
             at ceresdb/components/parquet_ext/src/meta_data.rs:82:13
      analytic_engine::sst::parquet::async_reader::Reader::load_meta_data_from_storage::{{closure}}
             at ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:368:13
   9: analytic_engine::sst::parquet::async_reader::Reader::read_sst_meta::{{closure}}
             at ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:400:64
      analytic_engine::sst::parquet::async_reader::Reader::init_if_necessary::{{closure}}
             at ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:323:49
  10: <analytic_engine::sst::parquet::async_reader::Reader as analytic_engine::sst::reader::SstReader>::meta_data::{{closure}}
             at ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:533:33
  11: <core::pin::Pin<P> as core::future::future::Future>::poll
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125:9
      <analytic_engine::sst::parquet::async_reader::ThreadedReader as analytic_engine::sst::reader::SstReader>::meta_data::{{closure}}
             at ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:648:31
  12: <core::pin::Pin<P> as core::future::future::Future>::poll
             at rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125:9
      analytic_engine::row_iter::record_batch_stream::stream_from_sst_file::{{closure}}
             at ceresdb/analytic_engine/src/row_iter/record_batch_stream.rs:333:38
      analytic_engine::row_iter::record_batch_stream::filtered_stream_from_sst_file::{{closure}}
             at ceresdb/analytic_engine/src/row_iter/record_batch_stream.rs:291:5
  13: analytic_engine::row_iter::merge::MergeBuilder::build::{{closure}}
             at ceresdb/analytic_engine/src/row_iter/merge.rs:218:17
  14: analytic_engine::instance::flush_compaction::<impl analytic_engine::instance::SpaceStore>::compact_input_files::{{closure}}
             at ceresdb/analytic_engine/src/instance/flush_compaction.rs:812:28
      analytic_engine::instance::flush_compaction::<impl analytic_engine::instance::SpaceStore>::compact_table::{{closure}}
             at ceresdb/analytic_engine/src/instance/flush_compaction.rs:709:13
  15: analytic_engine::compaction::scheduler::ScheduleWorker::do_table_compaction_task::{{closure}}
             at ceresdb/analytic_engine/src/compaction/scheduler.rs:503:17
```

### Server version

main, commit id 946b3f89e9f6b18c51716a6fe1c5ddb549488dd5
### Steps to reproduce

N/A

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1040/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1040,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5g1XNy,horaedb,1624601458,1040,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2023-07-07T03:25:48Z,2023-07-07T03:25:48Z,"Maybe upgrade parquet can resolve this panic.
https://github.com/apache/arrow-rs/commit/1434d1f4ddbe50e7729b7b69bdb8b7e10934f806","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5g1XNy/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1040,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5iEHFF,horaedb,1645244741,1040,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-07-21T08:57:54Z,2023-07-21T08:57:54Z,"After https://github.com/CeresDB/ceresdb/pull/1086, this may be solved. @MachaelLee @Rachelint ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5iEHFF/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1040,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5iLXqW,horaedb,1647147670,1040,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-07-24T03:33:49Z,2023-07-24T03:33:49Z,">
> 2023-07-24 03:30:57.450 ERRO [analytic_engine/src/compaction/scheduler.rs:506] Failed to compact table, table_name:prometheus_sd_consul_rpc_duration_seconds, table_id:184, request_id:15054054, err:Failed to build merge iterator, table:prometheus_sd_consul_rpc_duration_seconds, err:Failed to build record batch from sst, err:Fail to read sst meta, err:Failed to decode page indexes for meta data, file_path:0/184/173.sst, err:Parquet error: failed to build page indexes in metadata, err:Parquet error: missing offset index.


After upgrade, the panic has become an Error.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5iLXqW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1044,https://api.github.com/repos/apache/horaedb/issues/1044,horaedb,1785737825,1044,Columnar memtable & Organize data in a columnar way in the write process,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-07-03T09:21:50Z,2024-10-19T11:22:09Z,"### Describe This Problem

Organize data in a columnar way in the write process: Refer to #950 
Columnar memtable to resolve following problems:
1. At present, in the case of memtable with a lot of columns, if there are many fields with nulls written, the null fields will also take up space.
2. At present, `skiplistmemtable` is implemented in ro-store, and the data compression effect is not good.

### Proposal

1. Impl a columnar memtable.
2. Organize data in a columnar way in the write process.

We can refer to [influxdb_iox](https://github.com/influxdata/influxdb_iox/blob/9f00c9c4ef77e1defd415ea5ab0a388c8cb08adf/mutable_batch/src/lib.rs#L69).

### Additional Context

I have done some poc.

## `row_group` vs `column_block` vs `influxdb_iox column`
|              | grpc write decode | partition table split |               wal encode              |   total  |
|:------------:|:-----------------:|:---------------------:|:-------------------------------------:|:--------:|
|   row group  |    177.456918ms   |       9.328081ms      |              87.680753ms              | 274.45ms |
| column block |    229.374211ms   |      40.403491ms      |              22.222504ms              | 291.99ms |
|  iox column  |    181.631366ms   |      37.563124ms      | 15.374956ms+（56.706159ms pb encode） | 291.26ms |

code branch: chunshao90/improve-encoding-in-write-procedure
```
cargo test --release --workspace test_write_entry_to_row_group_wal -- --nocapture

cargo test --release --workspace test_write_entry_to_column_block_wal -- --nocapture

cargo test --release --workspace test_write_entry_to_column_data_wal_pb -- --nocapture
```

## `columnar memtable` vs `skiplist memtable`
normal table
| memtable | rows/s |
|:--------:|:------:|
| row_group + skiplist memtable | 14.99W |
| row_group + columnar memtable| 15.39W |
| column + columnar memtable| 14.91W |

partition table
| memtable | rows/s |
|:--------:|:------:|
| row_group + skiplist memtable | 10.0W |
| row_group + columnar memtable| 9.8W |
| column + columnar memtable| 5.6W |

`colum + columnar memtbale` branch: chunshao90/impl-write-with-column commit_id:0ec16ebe7843ea1740d22292612a604aedddd441
`row_group + columnar memtbale` branch: chunshao90/impl-row-group-columnar-memtable commit_id:22d943cea5e99b048f8b4b218bbe23c018d3ce50

```
tsbs cmd:
/tsbs_load_ceresdb --ceresdb-addr=127.0.0.1:8831 --file ./data.out --batch-size 100 --workers 100 --primary-keys hostname,region,datacenter,rack,os,arch,team,service,service_version,service_environment,timestamp
time,per. metric/s,metric total,overall metric/s,per. row/s,row total,overall row/s

result:
skiplist memtable
time,per. metric/s,metric total,overall metric/s,per. row/s,row total,overall row/s
1688546946,1585641.79,1.585700E+07,1585641.79,158564.18,1.585700E+06,158564.18
1688546956,1592642.95,3.178300E+07,1589142.26,159264.30,3.178300E+06,158914.23
1688546966,1505925.05,4.684300E+07,1561402.36,150592.50,4.684300E+06,156140.24
1688546976,1393255.67,6.077500E+07,1519367.57,139325.57,6.077500E+06,151936.76

Summary:
loaded 72000000 metrics in 48.030sec with 100 workers (mean rate 1499063.87 metrics/sec)
loaded 7200000 rows in 48.030sec with 100 workers (mean rate 149906.39 rows/sec)


row_group columnar memtable
time,per. metric/s,metric total,overall metric/s,per. row/s,row total,overall row/s
1688546571,1612907.18,1.613100E+07,1612907.18,161290.72,1.613100E+06,161290.72
1688546581,1674263.27,3.287300E+07,1643582.81,167426.33,3.287300E+06,164358.28
1688546591,1617414.15,4.904600E+07,1634860.57,161741.42,4.904600E+06,163486.06
1688546601,1318711.13,6.223300E+07,1555823.93,131871.11,6.223300E+06,155582.39

Summary:
loaded 72000000 metrics in 46.755sec with 100 workers (mean rate 1539946.13 metrics/sec)
loaded 7200000 rows in 46.755sec with 100 workers (mean rate 153994.61 rows/sec)


columnar memtable
time,per. metric/s,metric total,overall metric/s,per. row/s,row total,overall row/s
1688547630,1602775.15,1.602800E+07,1602775.15,160277.51,1.602800E+06,160277.51
1688547640,1573511.39,3.176300E+07,1588143.43,157351.14,3.176300E+06,158814.34
1688547650,1484345.30,4.661500E+07,1553530.87,148434.53,4.661500E+06,155353.09
1688547660,1392702.56,6.053400E+07,1513347.04,139270.26,6.053400E+06,151334.70

Summary:
loaded 72000000 metrics in 48.277sec with 100 workers (mean rate 1491384.57 metrics/sec)
loaded 7200000 rows in 48.277sec with 100 workers (mean rate 149138.46 rows/sec)

Summary:
loaded 72000000 metrics in 50.053sec with 100 workers (mean rate 1438472.55 metrics/sec)
loaded 7200000 rows in 50.053sec with 100 workers (mean rate 143847.25 rows/sec)

partition table:
skiplist memtable
time,per. metric/s,metric total,overall metric/s,per. row/s,row total,overall row/s
1688544758,1047144.86,1.047500E+07,1047144.86,104714.49,1.047500E+06,104714.49
1688544768,982702.45,2.029900E+07,1014934.08,98270.25,2.029900E+06,101493.41
1688544778,1028836.24,3.059000E+07,1019568.87,102883.62,3.059000E+06,101956.89
1688544788,1036873.44,4.095600E+07,1023893.85,103687.34,4.095600E+06,102389.38
1688544798,971114.41,5.066700E+07,1013338.14,97111.44,5.066700E+06,101333.81
1688544808,896769.76,5.963500E+07,993909.56,89676.98,5.963500E+06,99390.96
1688544818,1034585.41,6.998100E+07,999720.43,103458.54,6.998100E+06,99972.04

Summary:
loaded 72000000 metrics in 71.941sec with 100 workers (mean rate 1000821.83 metrics/sec)
loaded 7200000 rows in 71.941sec with 100 workers (mean rate 100082.18 rows/sec)

row_group columnar memtable
time,per. metric/s,metric total,overall metric/s,per. row/s,row total,overall row/s
1688544157,1026895.59,1.027100E+07,1026895.59,102689.56,1.027100E+06,102689.56
1688544167,1027156.72,2.054300E+07,1027026.15,102715.67,2.054300E+06,102702.61
1688544177,897363.91,2.951500E+07,983814.14,89736.39,2.951500E+06,98381.41
1688544187,890427.96,3.841900E+07,960468.49,89042.80,3.841900E+06,96046.85
1688544197,977477.31,4.819400E+07,963870.30,97747.73,4.819400E+06,96387.03
1688544207,1005044.19,5.824400E+07,970732.30,100504.42,5.824400E+06,97073.23
1688544217,1031550.07,6.856100E+07,979421.63,103155.01,6.856100E+06,97942.16

Summary:
loaded 72000000 metrics in 73.337sec with 100 workers (mean rate 981764.83 metrics/sec)
loaded 7200000 rows in 73.337sec with 100 workers (mean rate 98176.48 rows/sec)

influxdb iox
time,per. metric/s,metric total,overall metric/s,per. row/s,row total,overall row/s
1688545755,673633.18,6.738000E+06,673633.18,67363.32,6.738000E+05,67363.32
1688545765,432773.08,1.106500E+07,553228.21,43277.31,1.106500E+06,55322.82
1688545775,557813.75,1.664300E+07,554756.66,55781.38,1.664300E+06,55475.67
1688545785,636098.43,2.300600E+07,575096.66,63609.84,2.300600E+06,57509.67
1688545795,492526.40,2.793100E+07,558584.54,49252.64,2.793100E+06,55858.45
1688545805,590577.06,3.383500E+07,563915.01,59057.71,3.383500E+06,56391.50
1688545815,345059.84,3.728700E+07,532639.18,34505.98,3.728700E+06,53263.92
1688545825,358519.76,4.087100E+07,510881.77,35851.98,4.087100E+06,51088.18
1688545835,504602.80,4.591800E+07,510183.99,50460.28,4.591800E+06,51018.40
1688545845,756105.06,5.347700E+07,534769.40,75610.51,5.347700E+06,53476.94
```
### normal_table_main
![normal_table_main](https://github.com/CeresDB/ceresdb/assets/15178480/d67db404-4523-4fed-bce1-9157cf66f508)

### normal_table_row_group_columnar_memtable
![normal_table_row_group_columnar_memtable](https://github.com/CeresDB/ceresdb/assets/15178480/1fb44fb2-09eb-48ab-aed2-222b152f9328)

### normal_table_column_columnar_memtable
![normal_table_column_columnar_memtable](https://github.com/CeresDB/ceresdb/assets/15178480/9a82fed5-0bd4-45f8-b84c-0a920cf51b5e)

### partition_table_main
![partition_table_main](https://github.com/CeresDB/ceresdb/assets/15178480/48db2a4b-2f27-4580-b62f-288aecf58606)

### partition_table_row_group_columnar_memtable
![partition_table_row_group_columnar_memtable](https://github.com/CeresDB/ceresdb/assets/15178480/a515bcf2-68cd-4267-9bc7-743ef8fbdacc)

### partition_table_column_columnar_memtable
![partition_table_column_columnar_memtable](https://github.com/CeresDB/ceresdb/assets/15178480/818db809-af3d-413e-9dc1-b164ba6c58d6)


","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1044/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1044,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gjshQ,horaedb,1619970128,1044,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-07-04T10:19:31Z,2023-07-04T10:19:31Z,"From your experiments, there is no any obvious advantage (seems worse) to choose columnar layout in write procedure.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gjshQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1044,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gp0Gl,horaedb,1621574053,1044,NA,zouxiang1993,26276281,zouxiang,,NA,2023-07-05T11:31:55Z,2023-07-05T11:31:55Z,"I think columnar layout in write procedure will helps a lot. The flamegraph shows that `Message::decode` and `write_table_request_to_insert_plan` consumes a lot of CPU。

if use a columnar layout in write procedure, 
1. we can simplify the proto buffer message `WriteTableRequest` by using a few Arrow column, which can simplify the PB decode procedure.
2. there is no need to convert `WriteTableRequest` to columnar layout in `write_table_request_to_insert_plan`


<img width=""1416"" alt=""image"" src=""https://github.com/CeresDB/ceresdb/assets/26276281/0ae16233-17b9-46c0-b70d-44fecd7ed022"">
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gp0Gl/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1047,https://api.github.com/repos/apache/horaedb/issues/1047,horaedb,1788699433,1047,Use `interleave` to optimize merge iterator,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-07-05T03:32:17Z,2024-10-19T11:21:25Z,"### Describe This Problem

In merge iterator, we extract row by row from the src record batch, and append row by row to the dst record batch now.
However, arrow expose `interleave` function for mergine multiple record batch to single batch.

Demo design:
+ mixed schema:
```
    let schema = Arc::new(Schema::new(vec![
        Field::new(""id"", DataType::Int64, false),
        Field::new(""city"", DataType::Utf8, false),
        Field::new(""weight"", DataType::Float64, false),
    ]));
```
+ pure string schema:
```
    let schema = Arc::new(Schema::new(vec![
        Field::new(""city1"", DataType::Utf8, false),
        Field::new(""city2"", DataType::Utf8, false),
        Field::new(""city3"", DataType::Utf8, false),
    ]));
```
+ data:
```
    let string = vec![shanghai; 8192];
    let long_string = vec![""shanghaishanghaishanghai""; 8192];
    let f64 = vec![42.0; 8192];
    let i64 = vec![1234; capacity];
```
The demo generates 10 record batch with 8129 rows based on schema and data above, and randomly merge them to a single record batch.
Loop it 20000 times.

This is the test result for my simple demo:

| | row by row  | interleave |
|---|---|---|
| mixed  | 2.19s  | 1.76s |
| mixed(long string)  | 2.54s | 1.95s |
| pure string | 3.67s | 2.87s |
| pure long string | 9.66s | 5.12s |

### Proposal

See title.
I think maybe we can use datafusion to merge record batch(has do much optimization work about merging), I will test it in later.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1047/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1050,https://api.github.com/repos/apache/horaedb/issues/1050,horaedb,1789036270,1050,Sql support use dictionary encoding column,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,CLOSED,2023-07-05T08:20:04Z,2023-07-05T09:19:49Z,"### Describe This Problem

Sql support use dictionary encoding column.

### Proposal

see #965

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1050/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1051,https://api.github.com/repos/apache/horaedb/issues/1051,horaedb,1790342111,1051,Consider using Profile-Guided Optimization (PGO) ,zamazan4ik,7355383,Alexander Zaitsev,zamazan4ik@tut.by,CLOSED,2023-07-05T21:16:10Z,2024-10-19T11:21:02Z,"### Describe This Problem

Not a problem - just an idea of how to improve the performance of the database.

### Proposal

Hi!

According to my tests, Profile-Guided Optimization (PGO) usually helps a lot with database - see my tests [here](https://github.com/zamazan4ik/awesome-pgo/). I think can interesting to test this approach on CeresDB as well.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1051/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1051,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gun1G,horaedb,1622834502,1051,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-07-06T02:04:41Z,2023-07-06T02:04:41Z,@zamazan4ik It looks great! I'll spend some time to figure out how to use it.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5gun1G/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1053,https://api.github.com/repos/apache/horaedb/issues/1053,horaedb,1790974155,1053,Relation between `Cluster` and `Shard` seems messy,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-07-06T07:24:31Z,2023-07-11T03:56:08Z,"### Describe This Problem

As I see, `Cluster` can  be seemed as `Space` in storage engine, and `Shard` can be seemed as `Table`.
So call relationship between `Cluster` and `Shard` should be similar as `Space` and `Table`'s, like:
<img src=""https://raw.githubusercontent.com/Rachelint/drawio-store/main/cluster.drawio.svg?sanitize=true"">

However the operation levels seems messy in current design.

### Proposal

Refactor the call path between `Cluster` and `Shard` to be similar as `Space` and `Table`.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1053/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1055,https://api.github.com/repos/apache/horaedb/issues/1055,horaedb,1792650058,1055,Use varint to encode string/bytes length in memtable,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-07-07T03:03:05Z,2023-07-11T13:23:58Z,"### Describe This Problem

Currently, we use u32 to encode the string/bytes in the memtable, leading to much encoding overhead for small string or bytes.

### Proposal

Use varint for the encoding of length.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1055/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1059,https://api.github.com/repos/apache/horaedb/issues/1059,horaedb,1794661458,1059,panic on `dump_sampling_memtable`,zouxiang1993,26276281,zouxiang,,CLOSED,2023-07-08T03:18:47Z,2024-10-19T11:24:31Z,"### Describe this problem

It seems that the `Sampler` suggests an empty `TimeRange` Vec
```
2023-07-08 10:56:30.867 ERRO [common_util/src/panic.rs:42] thread 'ceres-write' panicked 'Record timestamp is not in time_ranges, timestamp:Timestamp(1688784575003), time_ranges:[]' at ""analytic_engine/src/instance/flush_compaction.rs:963""
   0: common_util::panic::set_panic_hook::{{closure}}
             at /home/root/workspace/RustProjects/ceresdb/common_util/src/panic.rs:41:18
   1: <alloc::boxed::Box<F,A> as core::ops::function::Fn<Args>>::call
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:2002:9
      std::panicking::rust_panic_with_hook
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:692:13
   2: std::panicking::begin_panic_handler::{{closure}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:579:13
   3: std::sys_common::backtrace::__rust_end_short_backtrace
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys_common/backtrace.rs:137:18
   4: rust_begin_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:575:5
   5: core::panicking::panic_fmt
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panicking.rs:64:14
   6: analytic_engine::instance::flush_compaction::split_record_batch_with_time_ranges
             at /home/root/workspace/RustProjects/ceresdb/analytic_engine/src/instance/flush_compaction.rs:963:13
   7: analytic_engine::instance::flush_compaction::FlushTask::dump_sampling_memtable::{{closure}}
             at /home/root/workspace/RustProjects/ceresdb/analytic_engine/src/instance/flush_compaction.rs:538:40
   8: analytic_engine::instance::flush_compaction::FlushTask::dump_memtables::{{closure}}
             at /home/root/workspace/RustProjects/ceresdb/analytic_engine/src/instance/flush_compaction.rs:362:17
      analytic_engine::instance::flush_compaction::FlushTask::run::{{closure}}
             at /home/root/workspace/RustProjects/ceresdb/analytic_engine/src/instance/flush_compaction.rs:268:13
      analytic_engine::instance::flush_compaction::Flusher::schedule_table_flush::{{closure}}::{{closure}}
             at /home/root/workspace/RustProjects/ceresdb/analytic_engine/src/instance/flush_compaction.rs:240:54
      analytic_engine::instance::serial_executor::TableFlushScheduler::flush_sequentially::{{closure}}::{{closure}}
             at /home/root/workspace/RustProjects/ceresdb/analytic_engine/src/instance/serial_executor.rs:206:38
   9: tokio::runtime::task::core::Core<T,S>::poll::{{closure}}
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/core.rs:223:17
      tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/loom/std/unsafe_cell.rs:14:9
      tokio::runtime::task::core::Core<T,S>::poll
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/core.rs:212:13
      tokio::runtime::task::harness::poll_future::{{closure}}
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:476:19
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271:9
      std::panicking::try::do_call
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483:40
      std::panicking::try
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447:19
      std::panic::catch_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140:14
      tokio::runtime::task::harness::poll_future
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:464:18
      tokio::runtime::task::harness::Harness<T,S>::poll_inner
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:198:27
      tokio::runtime::task::harness::Harness<T,S>::poll
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:152:15
      tokio::runtime::task::raw::poll
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/raw.rs:255:5
  10: tokio::runtime::task::raw::RawTask::poll
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/raw.rs:200:18
      tokio::runtime::task::LocalNotified<S>::run
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/mod.rs:394:9
      tokio::runtime::scheduler::multi_thread::worker::Context::run_task::{{closure}}
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/scheduler/multi_thread/worker.rs:464:13
      tokio::runtime::coop::with_budget
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/coop.rs:107:5
      tokio::runtime::coop::budget
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/coop.rs:73:5
      tokio::runtime::scheduler::multi_thread::worker::Context::run_task
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/scheduler/multi_thread/worker.rs:463:9
  11: tokio::runtime::scheduler::multi_thread::worker::Context::run
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/scheduler/multi_thread/worker.rs:426:24
      tokio::runtime::scheduler::multi_thread::worker::run::{{closure}}
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/scheduler/multi_thread/worker.rs:406:17
      tokio::macros::scoped_tls::ScopedKey<T>::set
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/macros/scoped_tls.rs:61:9
      tokio::runtime::scheduler::multi_thread::worker::run
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/scheduler/multi_thread/worker.rs:403:5
  12: tokio::runtime::scheduler::multi_thread::worker::Launch::launch::{{closure}}
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/scheduler/multi_thread/worker.rs:365:45
      <tokio::runtime::blocking::task::BlockingTask<T> as core::future::future::Future>::poll
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/blocking/task.rs:42:21
      tokio::runtime::task::core::Core<T,S>::poll::{{closure}}
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/core.rs:223:17
      tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/loom/std/unsafe_cell.rs:14:9
      tokio::runtime::task::core::Core<T,S>::poll
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/core.rs:212:13
      tokio::runtime::task::harness::poll_future::{{closure}}
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:476:19
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271:9
      std::panicking::try::do_call
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483:40
      std::panicking::try
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447:19
      std::panic::catch_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140:14
      tokio::runtime::task::harness::poll_future
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:464:18
      tokio::runtime::task::harness::Harness<T,S>::poll_inner
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:198:27
      tokio::runtime::task::harness::Harness<T,S>::poll
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/harness.rs:152:15
      tokio::runtime::task::raw::poll
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/raw.rs:255:5
  13: tokio::runtime::task::raw::RawTask::poll
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/raw.rs:200:18
      tokio::runtime::task::UnownedTask<S>::run
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/task/mod.rs:431:9
      tokio::runtime::blocking::pool::Task::run
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/blocking/pool.rs:159:9
      tokio::runtime::blocking::pool::Inner::run
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/blocking/pool.rs:513:17
      tokio::runtime::blocking::pool::Spawner::spawn_thread::{{closure}}
             at /home/root/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.27.0/src/runtime/blocking/pool.rs:471:13
      std::sys_common::backtrace::__rust_begin_short_backtrace
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys_common/backtrace.rs:121:18
  14: std::thread::Builder::spawn_unchecked_::{{closure}}::{{closure}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/thread/mod.rs:558:17
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271:9
      std::panicking::try::do_call
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483:40
      std::panicking::try
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447:19
      std::panic::catch_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140:14
      std::thread::Builder::spawn_unchecked_::{{closure}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/thread/mod.rs:557:30
      core::ops::function::FnOnce::call_once{{vtable.shim}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/ops/function.rs:250:5
  15: <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:1988:9
      <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:1988:9
      std::sys::unix::thread::Thread::new::thread_start
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys/unix/thread.rs:108:17
  16: start_thread
  17: clone

```

### Server version

CeresDB Server 
Version: 1.2.2
Git commit: c6c306d
Git branch: main
Opt level: 3
Rustc version: 1.69.0-nightly
Target: x86_64-unknown-linux-gnu
Build date: 2023-07-07T02:25:17.842532518Z

### Steps to reproduce

1. Create table without `segment_duration`
2. write some data

### Expected behavior

Don't panic

### Additional Information","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1059/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1059,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5hJ7Xx,horaedb,1629992433,1059,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-07-11T02:08:23Z,2023-07-11T02:08:23Z,"This is the first time this error arise, I'm curious what is your data range? 
does this panic always happen?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5hJ7Xx/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1063,https://api.github.com/repos/apache/horaedb/issues/1063,horaedb,1798223565,1063,Try to optimize the writing protocol to reduce the cpu overhead of pb encoding and decoding,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-07-11T06:43:10Z,2024-10-19T11:28:45Z,"### Describe This Problem

Refer to [#1044](https://github.com/CeresDB/ceresdb/issues/1044#issuecomment-1621574053).



### Proposal

There are two ideas:
1. The writing protocol directly uses the column-store encoding, such as the `arrow`.
2. Learn `prost` encoding implementation, whether there is optimization.



### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1063/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1066,https://api.github.com/repos/apache/horaedb/issues/1066,horaedb,1800044377,1066,Add /debug/shards API,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-07-12T02:22:10Z,2023-07-13T06:41:02Z,"### Describe This Problem

In distributed deployments, it's helpful to know which shards a ceresdb server has.

### Proposal

Add HTTP API `GET /debug/shards`.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1066/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1072,https://api.github.com/repos/apache/horaedb/issues/1072,horaedb,1802262031,1072,Bump datafusion to newest version,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-07-13T06:12:54Z,2023-07-20T08:21:31Z,"### Describe This Problem

+ Some exciting features such as cache encode `page` rather than `record batch` in `ParquetWriter` has been supported in new version parquet crate.
+ Panic led by page filter has been fixed in new version parquet crate.

We need to bump version of datafusion for owning them.


### Proposal

Bump version of datafusion.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1072/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1099,https://api.github.com/repos/apache/horaedb/issues/1099,horaedb,1817845549,1099,"Shard lock granted, but it's not sent to ceresmeta via heartbeat",ZuLiangWang,11689031,CooooolFrog,coolfrog@apache.org,CLOSED,2023-07-13T08:20:24Z,2023-07-26T04:23:11Z,"**Describe this problem**
In current open shard process, it will first try grant shard lock, and then do real `open_shard` logic.

But there is one corner case in implementation of cluster_impl.rs
- https://github.com/CeresDB/ceresdb/blob/035080c90f3ec8f2489254a105f6cd24e979da6a/cluster/src/cluster_impl.rs#L233

It will only heartbeat to meta after it's inserted into `shard_set`, this is not guaranteed, and if `get_tables_of_shards` operation failed, it will not insert to `shard_set`


**Steps to reproduce**
Hard to reproduce, it required both ceresdb and ceresmeta to restart at the same time.


**Expected behavior**
When open shard failed, it should release its shard lock, so meta can assign other node for it.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1099/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1075,https://api.github.com/repos/apache/horaedb/issues/1075,horaedb,1804397039,1075,Failed to create the table because of creation of the snapshot,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-07-14T07:51:15Z,2023-07-17T09:24:57Z,"### Describe this problem

Failed to create the table because of creation of the snapshot.
```
{
    ""code"": 400,
    ""message"": ""Internal err:Rpc error, code:500, message:Failed to execute plan, sql:CREATE TABLE `demo8`(\n    `name`string TAG,\n    `id` int TAG,\n    `value` double NOT NULL,\n    `t` timestamp NOT NULL,\n    TIMESTAMP KEY(t)\n    )  ENGINE = Analytic with (compaction_strategy=\""size_tiered\""), err:Internal error, msg:Failed to execute interpreter, err:Failed to execute create table, err:Failed to create table by table manipulator, err:Failed to operate table, err:Failed to operate table, msg:Some(\""failed to create table on shard, request:CreateTableRequest { catalog_name: \\\""ceresdb\\\"", schema_name: \\\""public\\\"", table_name: \\\""demo8\\\"", table_id: None, table_schema: Schema { timestamp_index: 1, tsid_index: Some(0), column_schemas: ColumnSchemas { columns: [ColumnSchema { id: 1, name: \\\""tsid\\\"", data_type: UInt64, is_nullable: false, is_tag: false, is_dictionary: false, comment: \\\""\\\"", escaped_name: \\\""tsid\\\"", default_value: None }, ColumnSchema { id: 2, name: \\\""t\\\"", data_type: Timestamp, is_nullable: false, is_tag: false, is_dictionary: false, comment: \\\""\\\"", escaped_name: \\\""t\\\"", default_value: None }, ColumnSchema { id: 3, name: \\\""name\\\"", data_type: String, is_nullable: true, is_tag: true, is_dictionary: false, comment: \\\""\\\"", escaped_name: \\\""name\\\"", default_value: None }, ColumnSchema { id: 4, name: \\\""id\\\"", data_type: Int32, is_nullable: true, is_tag: true, is_dictionary: false, comment: \\\""\\\"", escaped_name: \\\""id\\\"", default_value: None }, ColumnSchema { id: 5, name: \\\""value\\\"", data_type: Double, is_nullable: false, is_tag: false, is_dictionary: false, comment: \\\""\\\"", escaped_name: \\\""value\\\"", default_value: None }] }, version: 1, primary_key_indexes: [0, 1] }, engine: \\\""Analytic\\\"", options: {\\\""compaction_strategy\\\"": \\\""size_tiered\\\""}, state: Stable, shard_id: 0, partition_info: None }\""), err:Failed to create table, err:Failed to write meta data, err:Failed to persist meta update to manifest, space_id:2, table:demo8, table_id:2199023255561, err:Failed to build snapshot, msg:table data not exist, space_id:2, table_id:2199023255561.""
}
```

### Server version

commit:cc290f206fae75f07bbe4f876eb79f42cc1769bc


### Steps to reproduce

Set `snapshot_every_n_updates` = 4

There will be a failure when creating the fifth table.
```
CREATE TABLE `demo`(
    `name`string TAG,
    `id` int TAG,
    `value` double NOT NULL,
    `t` timestamp NOT NULL,
    TIMESTAMP KEY(t)
    )  ENGINE = Analytic
```


### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1075/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1087,https://api.github.com/repos/apache/horaedb/issues/1087,horaedb,1811878452,1087,Use more optimize rule in datafusion,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-07-19T12:49:37Z,2023-08-11T10:38:30Z,"### Describe This Problem

Datafusion has update its default optimize rules to following:  
```
let rules: Vec<Arc<dyn OptimizerRule + Sync + Send>> = vec![
      Arc::new(SimplifyExpressions::new()),
      Arc::new(UnwrapCastInComparison::new()),
      Arc::new(ReplaceDistinctWithAggregate::new()),
      Arc::new(EliminateJoin::new()),
      Arc::new(DecorrelatePredicateSubquery::new()),
      Arc::new(ScalarSubqueryToJoin::new()),
      Arc::new(ExtractEquijoinPredicate::new()),
      // simplify expressions does not simplify expressions in subqueries, so we
      // run it again after running the optimizations that potentially converted
      // subqueries to joins
      Arc::new(SimplifyExpressions::new()),
      Arc::new(MergeProjection::new()),
      Arc::new(RewriteDisjunctivePredicate::new()),
      Arc::new(EliminateDuplicatedExpr::new()),
      Arc::new(EliminateFilter::new()),
      Arc::new(EliminateCrossJoin::new()),
      Arc::new(CommonSubexprEliminate::new()),
      Arc::new(EliminateLimit::new()),
      Arc::new(PropagateEmptyRelation::new()),
      Arc::new(FilterNullJoinKeys::default()),
      Arc::new(EliminateOuterJoin::new()),
      // Filters can't be pushed down past Limits, we should do PushDownFilter after PushDownLimit
      Arc::new(PushDownLimit::new()),
      Arc::new(PushDownFilter::new()),
      Arc::new(SingleDistinctToGroupBy::new()),
      // The previous optimizations added expressions and projections,
      // that might benefit from the following rules
      Arc::new(SimplifyExpressions::new()),
      Arc::new(UnwrapCastInComparison::new()),
      Arc::new(CommonSubexprEliminate::new()),
      Arc::new(PushDownProjection::new()),
      Arc::new(EliminateProjection::new()),
      // PushDownProjection can pushdown Projections through Limits, do PushDownLimit again.
      Arc::new(PushDownLimit::new()),
  ];
```

### Proposal

Update and check the optimize rules in datafusion.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1087/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1091,https://api.github.com/repos/apache/horaedb/issues/1091,horaedb,1813386064,1091,Support Join Operation,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,CLOSED,2023-07-20T07:51:11Z,2024-10-19T11:20:45Z,"### Describe This Problem

Join is one of the foundational operators of databases, and a large portion of datafusion's current optimization is focused on join optimization. We can support join operation from the upper layer and benefit from datafusion to a greater extent.

### Proposal

as above

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1091/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1093,https://api.github.com/repos/apache/horaedb/issues/1093,horaedb,1815261068,1093,Tracking Issue: Implementing a column-based memtable,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,CLOSED,2023-07-21T06:57:24Z,2024-10-19T11:28:33Z,"### Describe This Problem

Implementing a column-based memtable

We'll be developing in the `impl-column-based-memtable` branch, which will eventually merge into the main branch using cherry-pick.
### Proposal

Implementing a column-based memtable

### Additional Context

- [ ] Designing a columnar memtable approach(There's already a first draft) @tanruixiang 
- [ ] https://github.com/CeresDB/ceresdb/issues/1094 @chunshao90 
- [ ] https://github.com/CeresDB/ceresdb/issues/1095 @tanruixiang 
- [ ] https://github.com/CeresDB/ceresdb/issues/1096 @chunshao90 @tanruixiang 
- [ ] Optimize performance in high concurrency read/write scenarios @tanruixiang ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1093/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1094,https://api.github.com/repos/apache/horaedb/issues/1094,horaedb,1815267563,1094,Passes all tests using the simplest columnar approach,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,CLOSED,2023-07-21T07:02:51Z,2024-10-19T11:28:23Z,"### Describe This Problem

Passes all tests using the simplest columnar approach. 

### Proposal

Passes all tests using the simplest columnar approach. 

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1094/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1095,https://api.github.com/repos/apache/horaedb/issues/1095,horaedb,1815278627,1095,Optimize columnar write performance,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,CLOSED,2023-07-21T07:12:26Z,2024-10-19T11:33:09Z,"### Describe This Problem

As I was looking at the [poc](https://github.com/CeresDB/ceresdb/pull/1081) code, I realized that the way it was written was very unfriendly to the cache and didn't take advantage of columnar writes. In this issue, I will optimize these areas that are currently rough.

### Proposal

Optimize columnar write performance

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1095/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1096,https://api.github.com/repos/apache/horaedb/issues/1096,horaedb,1815282037,1096,The mutable and imutable implementations are separated by design,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,CLOSED,2023-07-21T07:15:16Z,2024-10-19T11:20:35Z,"### Describe This Problem

Based on our initial discussion of design options, we need to separate mutable and imutable to achieve better performance.

### Proposal

Based on our initial discussion of design options, we need to separate mutable and imutable to achieve better performance.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1096/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1097,https://api.github.com/repos/apache/horaedb/issues/1097,horaedb,1815425594,1097,Row decode out of range,baojinri,52273009,鲍金日,baojinri@apache.org,CLOSED,2023-07-21T08:54:08Z,2023-07-25T03:08:58Z,"### Describe this problem

 Read row out of range 
`2023-07-21 14:35:36.881 ERRO [components/panic_ext/src/lib.rs:42] thread 'ceres-write' panicked 'range start index 1919247122 out of range for slice of length 150' at ""common_types/src/row/contiguous.rs:675""
   0: <unknown>
   1: std::panicking::rust_panic_with_hook
   2: <unknown>
   3: <unknown>
   4: rust_begin_unwind
   5: core::panicking::panic_fmt
   6: <unknown>
   7: core::slice::index::slice_start_index_len_fail
   8: <unknown>
   9: common_types::row::contiguous::datum_view_at
  10: <unknown>
  11: <unknown>
  12: <unknown>
  13: <unknown>
  14: <unknown>
  15: <unknown>
  16: <unknown>
  17: <unknown>
  18: <unknown>
  19: <unknown>
  20: <unknown>
  21: <unknown>
  22: tokio::runtime::scheduler::multi_thread::worker::run
  23: <unknown>
  24: <unknown>
  25: <unknown>
  26: <unknown>
  27: <unknown>
  28: <unknown>
  29: <unknown>
  30: start_thread
  31: clone`

### Server version

After commit: `4a20105bf54c429d7061bc262a0fd72a1a2f75ec`

### Steps to reproduce

Occasional bugs. May occur with a large number of writes and queries.


### Expected behavior

When wirte or query, encode and decode row are fine.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1097/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1097,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5iPiWw,horaedb,1648240048,1097,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-07-24T16:32:30Z,2023-07-24T16:32:30Z,"#1103 may fix this, but I'm not very sure.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5iPiWw/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1105,https://api.github.com/repos/apache/horaedb/issues/1105,horaedb,1819753434,1105,Avoid abuse dictionary encoding in parquet writer,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-07-25T07:49:06Z,2024-01-11T02:43:25Z,"### Describe This Problem

Currently, the default configuration is used for parquet writer to generate the sst, and dictionary encoding is used for bytes column (including string column) by default. Only the dictionary page exceeds 1MB, the parquet writer will fall back to plain encoding without the dictionary.

However, it maybe not very efficient for the column of high cardinality to be encoded by dictionary.


### Proposal

Maybe we should choose to control whether to choose dictionary encoding according to the column's cardinality. However, the threshold of cardinality to enable dictionary encoding should be benchmarked and tested.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1105/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1106,https://api.github.com/repos/apache/horaedb/issues/1106,horaedb,1821788289,1106,Failed to close shard,ZuLiangWang,11689031,CooooolFrog,coolfrog@apache.org,CLOSED,2023-07-26T07:47:14Z,2024-10-19T11:20:18Z,"### Describe this problem

CeresMeta fails to close shard when load balance scheduler scheduling.
```
event dispatch failed, cause:close shard, addr:33.147.37.33:8831, request:{54}, err:failed to close shard. Caused by: Failed to operate table, msg:Failed to close shard, shard id:54, success_count:3, close_err_count:1
```


### Server version

CeresDB Server 
Version: 1.2.2
Git commit: be2593e
Git branch: HEAD
Opt level: 3
Rustc version: 1.69.0-nightly
Target: x86_64-unknown-linux-gnu
Build date: 2023-06-14T08:45:57.442062577Z

### Steps to reproduce

1. Build a stable online cluster with topologyType set to dynamic.
2. Stop and restart some CeresDB nodes to trigger CeresMeta's load balance scheduling.
3. For some unknown reason, an error occurs when closing the shard。

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1106/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1108,https://api.github.com/repos/apache/horaedb/issues/1108,horaedb,1821983535,1108,Tracking issue: push down computation in distributed query,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-07-26T09:34:10Z,2023-09-30T08:18:02Z,"### Describe This Problem

Now, we support the rough disrtibuted sql query by hooking in table scan level, that leading actual computation such as aggregated can't be pushed down...

So, I plan to refactor it, and support distributed query in plan level for pushing down more things.

### Proposal
**1. Background**
The exist implementations can be divided into two ways:
+ Generate explicit distibuted logical plan, and generate distributed physical plan after, like [Drios](https://doris.apache.org/blog/principle-of-Doris-SQL-parsing/)
+ No explicit distributed loigcal plan(can't do it because no schema info?), and generate distributed physical plan directly, like [TiDB](https://github.com/pingcap/tidb/blob/7a653959e14ad7d1d81b902928780dc4e3fe1be7/planner/core/find_best_task.go#L209C84-L209C84) and [Datafusion](https://github.com/apache/arrow-datafusion).

As I see, they are almost same, the more clear way is to have the explicit distributed logical plan but it is the problem about code organization.

The real problem is should we depend on datafusion to do this? If we do it ourself, it may be more controllable? But it may need to design the complete physical plan generating process.

I think we should try to reuse the logic in datafusion first.

**2. General**
Works can be broken down as following:
+ Generate distributed physical plan according to the original, I think we make it refering to [TiDB](https://github.com/pingcap/tidb/blob/7a653959e14ad7d1d81b902928780dc4e3fe1be7/planner/core/find_best_task.go#L209C84-L209C84).
+ Support querying by physical plan in `RemoteEngine`.

**3. Two role of node in proposal**
My proposal is designed as folliowing:
+ Scheduler node(responsible for invoking the query, dispatching sub query to executor node, and computing the final result).
+ Executor node(where sub table in, responsible for computing the sub result).

**4. Process**
+ Scheduler node generates the initial physical plan of partitioned table. In this initial physical plan, the `TableScan` node is just a placeholder(can't execute actually) with some information for generating later executable plan, so I name it `UnresolvePartitionedScan`.
+ Scheduler node traverses the initial physical plan, finds the sub plan can be pushed down, and generate the sub plans for remote executing(using the information in `UnresolvePartitionedScan`). The sub plans are unable to execute like `UnresolvePartitionedScan` before being sent to and be rewriting in the executor nodes, so I name them `UnresolveSubScan`s.
+ Scheduler node sends the sub plans to executor nodes and wait result, and `UnresolveSubScan` is converted to `ResolvePartitionedScan` now.
+ Executor nodes receive the sub plans, and converts the `UnresolveSubScan` to `ResolveSubScan` using the carried information and catalog in local.
+ Executor nodes execute the converted sub plans and return the results.

![aaa](https://raw.githubusercontent.com/Rachelint/drawio-store/main/remote_scan.drawio.svg)

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1108/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1108,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5n0Gvv,horaedb,1741712367,1108,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-09-30T08:18:02Z,2023-09-30T08:18:02Z,"Initial version has been completed, more detailed optimizes will be tracked in new issue.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5n0Gvv/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1112,https://api.github.com/repos/apache/horaedb/issues/1112,horaedb,1825733034,1112,Support remote scan in physical plan level,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-07-28T05:32:26Z,2023-09-27T05:24:16Z,"### Describe This Problem

As the first step of #1108 , I prefer to refactor our impl for remote scanning.The refactor maybe includes following domain:
+ Make `RemoteEngine` able to execute physical plan in remote nodes.
+ Identify and rewrite the physical plan nodes to a remote executing ones(In the first step, just rewrite the table scan).


### Proposal
- [x] Impl `UnresolvedPartitionedScan`, `ResolvedPartitionedScan`, `UnresolvedSubTableScan`, `ResolvedSubTableScan`.
- [x] Generate `UnresolvedPartitionedScan` when scan partitioned table.
   - Define a dedicated `TableProviderAdapter` for `PartitionedTable`.
   - Judge and decide to generate which `TableProviderAdapter`(according to if `PartitionInfo` exists).
- [x] Define `Resolver` to convert `UnresolvedPartitionedScan` to  `ResolvedPartitionedScan`.
- [ ] Send the `UnresolvedSubTableScan` to remote and use the `SubTableScanExecutor` to run it.

### Additional Context
- Some possible optimizations:
  - Can we generate `ResolvedPartitionedScan` directly rather than generating `UnresolvedPartitionedScan`?
  - More elegant way(physical optimization rule?) to resolve the `UnresolvedPartitionedScan`?

- Communication between remote plan execution client side and server side.
  ![remote plan](https://raw.githubusercontent.com/Rachelint/drawio-store/main/remote_plan.drawio.svg)


_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1112/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1115,https://api.github.com/repos/apache/horaedb/issues/1115,horaedb,1825933288,1115,ceresdb does manifest store updates constantly,dust1,18304424,Yoke,834902408@qq.com,CLOSED,2023-07-28T08:12:34Z,2023-07-28T08:29:58Z,"### Describe this problem

when I start ceresed, and I don't have any operation. After waiting for a period of time, I guess about a minute or so, the background will refresh a lot of 'Manifest store update' related logs. The scary thing is that it keeps generating logs

### Server version

CeresDB Server 
Version: 1.2.5-alpha
Git commit: b42eb0d
Git branch: issue341
Opt level: 0
Rustc version: 1.69.0-nightly
Target: x86_64-unknown-linux-gnu
Build date: 2023-07-28T07:37:28.114729763Z

### Steps to reproduce

after startup, just wait

### Expected behavior

logs are not generated all the time

### Additional Information

2023-07-28 16:05:36.904 INFO [analytic_engine/src/instance/flush_compaction.rs:695] Try do compaction for table:sys_catalog#1099511627777, estimated input files size:0, input files number:0
2023-07-28 16:05:36.904 INFO [analytic_engine/src/manifest/details.rs:450] Manifest store update, request:MetaEditRequest { shard_info: TableShardInfo { shard_id: 0 }, meta_edit: Update(VersionEdit(VersionEditMeta { space_id: 1, table_id: TableId(1099511627777), flushed_sequence: 0, files_to_add: [], files_to_delete: [], mems_to_remove: [], max_file_id: 0 })) }
2023-07-28 16:05:36.904 INFO [analytic_engine/src/instance/flush_compaction.rs:695] Try do compaction for table:sys_catalog#1099511627777, estimated input files size:0, input files number:0
2023-07-28 16:05:36.904 INFO [analytic_engine/src/manifest/details.rs:450] Manifest store update, request:MetaEditRequest { shard_info: TableShardInfo { shard_id: 0 }, meta_edit: Update(VersionEdit(VersionEditMeta { space_id: 1, table_id: TableId(1099511627777), flushed_sequence: 0, files_to_add: [], files_to_delete: [], mems_to_remove: [], max_file_id: 0 })) }
2023-07-28 16:05:36.904 INFO [analytic_engine/src/instance/flush_compaction.rs:695] Try do compaction for table:sys_catalog#1099511627777, estimated input files size:0, input files number:0
2023-07-28 16:05:36.904 INFO [analytic_engine/src/manifest/details.rs:450] Manifest store update, request:MetaEditRequest { shard_info: TableShardInfo { shard_id: 0 }, meta_edit: Update(VersionEdit(VersionEditMeta { space_id: 1, table_id: TableId(1099511627777), flushed_sequence: 0, files_to_add: [], files_to_delete: [], mems_to_remove: [], max_file_id: 0 })) }
2023-07-28 16:05:36.905 INFO [analytic_engine/src/instance/flush_compaction.rs:695] Try do compaction for table:sys_catalog#1099511627777, estimated input files size:0, input files number:0
2023-07-28 16:05:36.905 INFO [analytic_engine/src/manifest/details.rs:450] Manifest store update, request:MetaEditRequest { shard_info: TableShardInfo { shard_id: 0 }, meta_edit: Update(VersionEdit(VersionEditMeta { space_id: 1, table_id: TableId(1099511627777), flushed_sequence: 0, files_to_add: [], files_to_delete: [], mems_to_remove: [], max_file_id: 0 })) }
2023-07-28 16:05:36.905 INFO [analytic_engine/src/instance/flush_compaction.rs:695] Try do compaction for table:sys_catalog#1099511627777, estimated input files size:0, input files number:0
2023-07-28 16:05:36.905 INFO [analytic_engine/src/manifest/details.rs:450] Manifest store update, request:MetaEditRequest { shard_info: TableShardInfo { shard_id: 0 }, meta_edit: Update(VersionEdit(VersionEditMeta { space_id: 1, table_id: TableId(1099511627777), flushed_sequence: 0, files_to_add: [], files_to_delete: [], mems_to_remove: [], max_file_id: 0 })) }
2023-07-28 16:05:36.905 INFO [analytic_engine/src/instance/flush_compaction.rs:695] Try do compaction for table:sys_catalog#1099511627777, estimated input files size:0, input files number:0
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1115/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1115,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5iqT0G,horaedb,1655258374,1115,NA,baojinri,52273009,鲍金日,baojinri@apache.org,NA,2023-07-28T08:21:15Z,2023-07-28T08:21:15Z,"hi, this pr https://github.com/CeresDB/ceresdb/pull/1113 has solved this bug","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5iqT0G/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1115,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5iqWsM,horaedb,1655270156,1115,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2023-07-28T08:29:50Z,2023-07-28T08:29:50Z,"Oops, I close it now ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5iqWsM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1136,https://api.github.com/repos/apache/horaedb/issues/1136,horaedb,1837595258,1136,The originazation of query engine crate is too messy,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-08-05T06:01:55Z,2023-08-07T05:48:29Z,"### Describe This Problem

Contrary to datafusion's design, leading to the strange impl for current datafusion based query engine. 
In fact, datafusion just expose `create_logical_plan` and `create_physical_plan` through `SessionContext`, and works inlucding `logical plan optimization`, `initial physical plan creation`, `phyiscal plan optimization` are hidden behind `create_physical_plan`. 
However, we want to expose hidden works in `create_physical_plan`, and the result is the messy design and impl now.

For example:
+ We optimize the `logical plan` twice... We firstly call the `optimize`, and when we call datafusion's `create_physical_plan`, `optimize` will be called interanlly again...
+ We use `physical optimizer` to build physical plan from logical plan,  and the input is `logical plan`??? We use `physical optimizer` to optimize `logical plan`???
+ We define internal `LogicalOptimizer` and `PhysicalOptimizer`, but related `optimize rule`s are not internal defined `OptimizeRule` and use ones from datafusion directly...

### Proposal

+ Follow the design of datafusion, remove `LogicalOptimzer` and `PhysicalOptimizer`, define new `PhysicalPlanner` trait to carry out the those works.
+ Split the query engine traits and detailed datafusion based impls.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1136/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1144,https://api.github.com/repos/apache/horaedb/issues/1144,horaedb,1842385635,1144,Infer timestamp column when possible,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-08-09T03:16:01Z,2023-10-18T02:13:50Z,"### Describe This Problem

When create table with sql below

```sql
CREATE TABLE `abc` (
    `tag` string TAG,
    `id` int TAG,
    `value` double NOT NULL,
    `t` timestamp NOT NULL,
    )
```

It will throw
```
MysqlWorker on_query failed. err:Failed to handle sql:CREATE TABL
E `abc` (                                                                                                                          `tag` string TAG,                                          
    `id` int TAG,                                                                                                                  `value` double NOT NULL,                                   
    `t` timestamp NOT NULL,                                                                                                        ), err:Rpc error, code:500, message:Failed to create plan, query:CREATE TABLE `abc` (                                          `tag` string TAG,                                                                                                          
    `id` int TAG,                                                                                                                  `value` double NOT NULL,                                                                                                       `t` timestamp NOT NULL,                                                                                                    
    ), err:Failed to create plan, err:Table must contain timestamp constraint    
```

### Proposal

In theory, if there is only one timestamp column in our SQL, we can refer this without user to set explicitly.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1144/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1144,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5pPg6W,horaedb,1765674646,1144,NA,Dennis40816,146855708,Dennis Liu,dennis48161025@gmail.com,NA,2023-10-17T05:05:43Z,2023-10-17T05:05:43Z,"Hi, @jiacai2050. Is anyone currently working on this? If not, I'd like to give it a try.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5pPg6W/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1144,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5pP57v,horaedb,1765777135,1144,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2023-10-17T06:51:17Z,2023-10-17T06:51:17Z,"> Hi, @jiacai2050. Is anyone currently working on this? If not, I'd like to give it a try.

Thanks. Feel free to try it.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5pP57v/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1149,https://api.github.com/repos/apache/horaedb/issues/1149,horaedb,1848889083,1149,Route API should allow partial fails,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-08-14T02:33:41Z,2024-10-19T11:30:44Z,"### Describe This Problem

Route API's requests contains multiple tables, when only one table cannot route successfully, the whole request will fail, this is very unfriendly, and we don't know which table failed, since the error message have no useful info.

```
2023-08-14 10:26:53.046 ERRO [proxy/src/grpc/route.rs:29] Failed to handle route, err:Rpc error, code:500, message:fail to route, err:Failure caused by others, msg:Failed to route tables by cluster, req:RouteTablesRequest { schema_name: ""public"", table_names: [""abc"", ""def""] }, err:Meta client execute failed, err:Bad response, resp code:404, msg:grpc routeTables.
```

### Proposal

Route response should tell failed tables from successful tables

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1149/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1152,https://api.github.com/repos/apache/horaedb/issues/1152,horaedb,1850849991,1152,Add pushdown tests for partition tables,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,OPEN,2023-08-15T03:22:27Z,2024-09-24T01:59:35Z,"### Describe This Problem

In #1150, a `support_pushdown` method is added, and we need to ensure analytic table and remote engine table can pushdown specific columns, analytic table tests have been added, but not for remote engine tables.

### Proposal

- Explain support remote table 
- Add pushdown check for remote table

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1152/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1152,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6M199U,horaedb,2362957652,1152,NA,OussamaSaoudi,45303303,,,NA,2024-09-20T06:49:55Z,2024-09-20T06:49:55Z,I'd like to give this issue a shot. Are there avenues where I can ask questions?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6M199U/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1152,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6NQqk3,horaedb,2369956151,1152,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-09-24T01:59:35Z,2024-09-24T01:59:35Z,"You can ask question here, also you can subscribe to the dev mail list, which can be found in our readme ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6NQqk3/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1156,https://api.github.com/repos/apache/horaedb/issues/1156,horaedb,1852719371,1156,Introduce the `sqllogictest` framework,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,CLOSED,2023-08-16T08:01:48Z,2024-10-19T11:30:45Z,"### Describe This Problem

`SQLite` have vary tests for ensuring database's corrects.But most of these tests are using `sqllogictest` framework's format, If we Introduce the `sqllogictest` framework, we can benefit from these tests.

### Proposal

We can introduce the https://github.com/risinglightdb/sqllogictest-rs (which is also used by datafusion)

### Additional Context
https://www.sqlite.org/sqllogictest/doc/trunk/about.wiki","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1156/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1157,https://api.github.com/repos/apache/horaedb/issues/1157,horaedb,1852804245,1157,Enable to get schema and catalog information from table,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-08-16T08:56:11Z,2023-11-21T05:14:38Z,"### Describe This Problem

We ofen need to know table's schema and catalog information, and we can do it just by passing from caller now...
It make the codes verbose and messy...

### Proposal

Store `schema id`,  `catalog manager` in `TableData`.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1157/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1157,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5p4oHc,horaedb,1776452060,1157,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2023-10-24T03:22:53Z,2023-10-24T03:22:53Z,"If I want to finish this issue, is it directly assigned to myself?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5p4oHc/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1157,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5p4zmv,horaedb,1776499119,1157,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2023-10-24T03:54:29Z,2023-10-24T03:54:29Z,"> If I want to finish this issue, is it directly assigned to myself?

Yes, welcome!","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5p4zmv/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1178,https://api.github.com/repos/apache/horaedb/issues/1178,horaedb,1868827315,1178,Request id will be duplicated in partitioned table situation,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-08-28T02:19:25Z,2024-01-11T02:43:24Z,"### Describe This Problem

Now, request id in ceresdb wil just be unique in a standalone node.
However, partitioned table's query/write will involve multiple nodes...

### Proposal

Maybe we can add `node ip` or `hostname` as a part of `request id`?

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1178/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1178,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5lDQwe,horaedb,1695353886,1178,NA,mdfarhananwar,91552730,Md Farhan Anwar,,NA,2023-08-28T09:27:00Z,2023-08-28T09:27:00Z,I am new to Open Source Contribitution. Can you assign me this task? ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5lDQwe/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1178,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5lH9ch,horaedb,1696585505,1178,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2023-08-29T00:06:53Z,2023-08-29T00:06:53Z,"> I am new to Open Source Contribitution. Can you assign me this task?

Sure！Feel free to pick it.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5lH9ch/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1178,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5t2Nkc,horaedb,1842927900,1178,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-12-06T13:50:29Z,2023-12-06T13:50:29Z,"Hi, since this issue has been quite for a while, I will work on this.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5t2Nkc/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1191,https://api.github.com/repos/apache/horaedb/issues/1191,horaedb,1879663518,1191,Improved test for partitioned table powered by tsbs,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-09-04T07:16:31Z,2023-09-07T02:10:51Z,"### Describe This Problem

Now we just do integration test for the partitioned table using only a few data.
I think it is not engouh for keeping the correctness.


### Proposal

Build improved test for partitioned table powered by tsbs.
- modify `tsbs`
    - [x] support setting `partition-keys`, `access-mode`.
    - [x] support outputing query results to file.
- [x] impl comparing script
- [x] make integration bin able to build env only

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1191/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1194,https://api.github.com/repos/apache/horaedb/issues/1194,horaedb,1881432538,1194,compaction panic when read disk cache,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-09-05T08:04:29Z,2023-09-11T07:01:05Z,"### Describe this problem

```bash
2023-09-05 11:59:38.852 ERRO [components/panic_ext/src/lib.rs:54] thread 'ceres-compact' panicked 'range end index 78088 out of range for slice of length 33814' at ""components/object_store/src/disk_cache.rs:689""
   0: panic_ext::set_panic_hook::{{closure}}
             at /home/db/ceresdb/components/panic_ext/src/lib.rs:53:18
   1: <alloc::boxed::Box<F,A> as core::ops::function::Fn<Args>>::call
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:2002:9
      std::panicking::rust_panic_with_hook
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:692:13
   2: std::panicking::begin_panic_handler::{{closure}}
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:579:13
   3: std::sys_common::backtrace::__rust_end_short_backtrace
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys_common/backtrace.rs:137:18
   4: rust_begin_unwind
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:575:5
   5: core::panicking::panic_fmt
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panicking.rs:64:14
   6: core::slice::index::slice_end_index_len_fail_rt
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/slice/index.rs:77:5
   7: core::slice::index::slice_end_index_len_fail
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/slice/index.rs:69:9
   8: <core::ops::range::Range<usize> as core::slice::index::SliceIndex<[T]>>::index
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/slice/index.rs:409:13
      core::slice::index::<impl core::ops::index::Index<I> for [T]>::index
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/slice/index.rs:18:9
      <object_store::disk_cache::DiskCacheStore as object_store::ObjectStore>::get_range::{{closure}}
             at /home/db/ceresdb/components/object_store/src/disk_cache.rs:689:18
   9: <core::pin::Pin<P> as core::future::future::Future>::poll
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125:9
      object_store::mem_cache::MemCacheStore::get_range_with_ro_cache::{{closure}}
             at /home/db/ceresdb/components/object_store/src/mem_cache.rs:196:57
      <object_store::mem_cache::MemCacheStore as object_store::ObjectStore>::get_range::{{closure}}
             at /home/db/ceresdb/components/object_store/src/mem_cache.rs:244:58
  10: <core::pin::Pin<P> as core::future::future::Future>::poll
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125:9
      <analytic_engine::sst::parquet::async_reader::ChunkReaderAdapter as parquet_ext::meta_data::ChunkReader>::get_bytes::{{closure}}
             at /home/db/ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:454:47
  11: <core::pin::Pin<P> as core::future::future::Future>::poll
             at /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125:9
      parquet_ext::meta_data::fetch_parquet_metadata::{{closure}}
             at /home/db/ceresdb/components/parquet_ext/src/meta_data.rs:51:9
      analytic_engine::sst::parquet::async_reader::Reader::load_meta_data_from_storage::{{closure}}
             at /home/db/ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:373:17

```

### Server version

$ ceresdb-server --version
CeresDB Server 
Version: 1.2.6-alpha
Git commit: 7f8faff79
Git branch: main
Opt level: 3
Rustc version: 1.69.0-nightly
Target: x86_64-unknown-linux-gnu
Build date: 2023-08-17T09:04:30.077953531Z

### Steps to reproduce

N/A

### Expected behavior

No panic

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1194/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1196,https://api.github.com/repos/apache/horaedb/issues/1196,horaedb,1883122728,1196,Inconsistent query results of partitioned table when loading data multiple times,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-09-06T03:39:36Z,2024-12-18T04:42:24Z,"### Describe this problem

I want to introduce tsbs to integration test of partitioned table.
Howerver, I found Inconsistent query results when loading data multiple times(will drop table after new loading).

### Server version

CeresDB Server 
Version: 1.2.6-alpha
Git commit: b0d7c2ca
Git branch: main
Opt level: 0
Rustc version: 1.74.0-nightly
Target: aarch64-apple-darwin
Build date: 2023-09-06T03:13:57.949975000Z

### Steps to reproduce
+ pull the branch in my pr #1195  
+ start cluster and load data
```
cd integration_tests 
make run-dist-query
```
+ query data using following sql:
```
SELECT
    time_bucket(timestamp, 'PT60S') as minute,
    max(usage_user) AS max_usage_user, max(usage_system) AS max_usage_system, max(usage_idle) AS max_usage_idle, max(usage_nice) AS max_usage_nice, max(usage_iowait) AS max_usage_iowait
FROM cpu
WHERE (hostname = 'host_2987' OR hostname = 'host_2302' OR hostname = 'host_3182' OR hostname = 'host_3215' OR hostname = 'host_248' OR hostname = 'host_3701' OR hostname = 'host_1103' OR hostname = 'host_3837') AND (timestamp >= '2022-09-05T00:00:00Z') AND (timestamp < '2022-09-05T01:00:00Z')
GROUP BY minute
ORDER BY minute ASC
```
+ drop table and load data again
```
cd dist_query 
NO_INIT=true ./run.sh
```
+ query data again using above sql
+ you may found the inconsistent query results ...

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1196/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1205,https://api.github.com/repos/apache/horaedb/issues/1205,horaedb,1889569573,1205,OB_ERR_PRIMARY_KEY_DUPLICATE when doing compaction,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-09-11T03:18:17Z,2024-10-19T11:19:27Z,"### Describe this problem

In our deployments with OBKV as wal implementation, we found following errors:
```bash
2023-09-11 10:24:03.322 ERRO [proxy/src/grpc/write.rs:33] Failed to handle write, err:Internal error, msg:Failed to execute interpreter, err:Failed to execute insert, err:Failed to write table, err:Failed to wri
te tables, table:adrtbcoreV2__DEFAULT__1__DEFAULT_goc_ctr_latency_us_bucket, err:Failed to flush table, table:adrtbcoreV2__DEFAULT__1__DEFAULT_goc_ctr_latency_us_bucket, err:Background flush failed, cannot write
 more data, retry_count:20, err:Failed to run flush job, msg:Some(""table:adrtbcoreV2__DEFAULT__1__DEFAULT_goc_ctr_latency_us_bucket, table_id:4449, request_id:607920880""), err:Failed to alloc file id, err:Failed
 to alloc file id, err:Failed to write update to wal, err:Failed to write log entries, err:Failed to write table unit, namespace:manifest, wal location:WalLocation { region_id: 99, table_id: 4449 }, err:Failed t
o write log to table, region_id:99, err:Failed to write to table, table:wal_manifest_permanent_000099, err:Common error, code:ObException(OB_ERR_PRIMARY_KEY_DUPLICATE), err:OBKV server return exception in batch
response: ObTableOperationResult { base: BasePayLoad { channel_id: 1262076206, version: 1, timeout: 10000, flag: 7 }, header: ObTableResult { base: BasePayLoad { channel_id: 1262076207, version: 1, timeout: 1000
0, flag: 7 }, errorno: -5024, sql_state: [0], msg: [0] }, operation_type: Insert, entity: ObTableEntity { base: BasePayLoad { channel_id: 1262076208, version: 1, timeout: 10000, flag: 7 }, row_key: ObRowKey { ke
ys: [] }, properties: {} }, affected_rows: 0 }..                                                                               
Backtrace:     
0 <snafu::backtrace_shim::Backtrace as snafu::GenerateBacktrace>::generate::h02217cd51b8458df                                                              
  /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/backtrace_shim.rs:15     
  <table_kv::obkv::WriteTable<__T0> as snafu::IntoError<table_kv::obkv::Error>>::into_error::hb2712bb104fe5bf5
  /home/db/ceresdb/components/table_kv/src/obkv.rs:35                                                  
  <core::result::Result<T,E> as snafu::ResultExt<T,E>>::context::{{closure}}::ha945fa61a9d0fdc4         
  /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/lib.rs:318               
  core::result::Result<T,E>::map_err::h2c67e337ccf5ca8b                                                                                 
  /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/result.rs:860                        
  <core::result::Result<T,E> as snafu::ResultExt<T,E>>::context::h79ba9da8f5cc9087                                 
  /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/lib.rs:318                            
1 <table_kv::obkv::ObkvImpl as table_kv::TableKv>::write::h4c7f29ac2567028a                                          
  /home/db/ceresdb/components/table_kv/src/obkv.rs:503                             
2 wal::table_kv_impl::table_unit::TableUnitWriter::write_log::{{closure}}::{{closure}}::h374597f853fb1ebd          
  /home/db/ceresdb/wal/src/table_kv_impl/table_unit.rs:972                                           
  <tokio::runtime::blocking::task::BlockingTask<T> as core::future::future::Future>::poll::hd732123b7ce277e0                                                                                                         /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/blocking/task.rs:42                
  tokio::runtime::task::core::Core<T,S>::poll::{{closure}}::h7bc8dab5bff1edd8                                                                                                                                        /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/core.rs:311                                                                                                                 tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut::h874e5b1272fa4233                                                                                                                                       
  /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/loom/std/unsafe_cell.rs:14                                                                                                               tokio::runtime::task::core::Core<T,S>::poll::h004175d454d51de2                                                                                                                                                     /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/core.rs:300                                                                                                                 tokio::runtime::task::harness::poll_future::{{closure}}::h214bee1116474500                                             
  /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/harness.rs:476      
  <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once::h8283513884f3bd25       
  /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271            
  std::panicking::try::do_call::h35e5c17abc1297f6                                                                         
  /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483                  
  std::panicking::try::h3476acde4fde0c11                                                                                           
  /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447 
  std::panic::catch_unwind::hc0a7aa6a07741dbc
  /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140
  tokio::runtime::task::harness::poll_future::h4d9ff312aa07f229
  /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/harness.rs:464
  tokio::runtime::task::harness::Harness<T,S>::poll_inner::h4a3e33931b374525
  /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/harness.rs:198
  tokio::runtime::task::harness::Harness<T,S>::poll::h33185797ff335d5d
  /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/harness.rs:152
  tokio::runtime::task::raw::poll::hee93371fc98f2eb6
  /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/raw.rs:276
3 tokio::runtime::task::raw::RawTask::poll::heabf9c70e6fd77a4
  /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/raw.rs:200
  tokio::runtime::task::UnownedTask<S>::run::h71abe9846161356c
  /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/mod.rs:437
  tokio::runtime::blocking::pool::Task::run::h447d1827b63320cd
  /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/blocking/pool.rs:159
  tokio::runtime::blocking::pool::Inner::run::h512f4d4d040e4562
  /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/blocking/pool.rs:513
  tokio::runtime::blocking::pool::Spawner::spawn_thread::{{closure}}::h136aab7e70ce0684
  /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/blocking/pool.rs:471
  std::sys_common::backtrace::__rust_begin_short_backtrace::h0fdc9691673882ce
  /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys_common/backtrace.rs:121
4 std::thread::Builder::spawn_unchecked_::{{closure}}::{{closure}}::hafa9124fedea5fb8
  /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/thread/mod.rs:558
  <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once::hd98e2f10cf2e1f38
  /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/panic/unwind_safe.rs:271
  std::panicking::try::do_call::h46b8f1ab94cefb1f
  /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:483
  std::panicking::try::h347f95a1f7845419
  /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panicking.rs:447
  std::panic::catch_unwind::h5dd7ee5b5feeb457
  /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/panic.rs:140
  std::thread::Builder::spawn_unchecked_::{{closure}}::h2561cf64a40e49e6
  /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/thread/mod.rs:557
  core::ops::function::FnOnce::call_once{{vtable.shim}}::h46fa823051c99d1e
  /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/ops/function.rs:250
5 <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once::hc8beb91c5e39b692
  /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:1988
  <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once::h20e58dce1054acc4
  /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/alloc/src/boxed.rs:1988
  std::sys::unix::thread::Thread::new::thread_start::h848946a57aa81736
  /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/std/src/sys/unix/thread.rs:108
6 start_thread
7 __clone
Backtrace:                                   
 0 <snafu::backtrace_shim::Backtrace as snafu::GenerateBacktrace>::generate::h02217cd51b8458df
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/backtrace_shim.rs:15
   <wal::manager::error::Write as snafu::IntoError<wal::manager::error::Error>>::into_error::hf5c49394fa68e61d
   /home/db/ceresdb/wal/src/manager.rs:41                                   
 1 <core::result::Result<T,E> as snafu::ResultExt<T,E>>::context::{{closure}}::h557595a434999597       
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/lib.rs:318
   core::result::Result<T,E>::map_err::h10c8702abcd0713f                                               
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/result.rs:860
   <core::result::Result<T,E> as snafu::ResultExt<T,E>>::context::h2728c3a6b3d1a399                
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/lib.rs:318
   <wal::table_kv_impl::wal::WalNamespaceImpl<T> as wal::manager::WalManager>::write::{{closure}}::hace34fa1b277d26d
   /home/db/ceresdb/wal/src/table_kv_impl/wal.rs:175          
 2 <core::pin::Pin<P> as core::future::future::Future>::poll::haffa190cb0c71e7d                    
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125
   <analytic_engine::manifest::details::WalBasedLogStore as analytic_engine::manifest::details::MetaUpdateLogStore>::append::{{closure}}::h4696a77b8bdfbd84
   /home/db/ceresdb/analytic_engine/src/manifest/details.rs:723
 3 <core::pin::Pin<P> as core::future::future::Future>::poll::h3b85d708da0a520b                         
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125
   analytic_engine::manifest::details::ManifestImpl::store_update_to_wal::{{closure}}::h75d609256d00d067
   /home/db/ceresdb/analytic_engine/src/manifest/details.rs:442              
   <analytic_engine::manifest::details::ManifestImpl as analytic_engine::manifest::Manifest>::apply_edit::{{closure}}::h0f6eb57b366fa8dc
   /home/db/ceresdb/analytic_engine/src/manifest/details.rs:502                      
 4 <core::pin::Pin<P> as core::future::future::Future>::poll::h6b95f43fd3e5a084    
   /rustc/11d96b59307b1702fffe871bfc2d0145d070881e/library/core/src/future/future.rs:125                          
   analytic_engine::table::data::TableData::persist_max_file_id::{{closure}}::h3408c2f626b3cd19
   /home/db/ceresdb/analytic_engine/src/table/data.rs:551
   analytic_engine::table::data::TableData::alloc_file_id::{{closure}}::{{closure}}::{{closure}}::h02b6a214ddc98868
   /home/db/ceresdb/analytic_engine/src/table/data.rs:520
   id_allocator::Inner::alloc_id::{{closure}}::h1964701c5b592986                  
   /home/db/ceresdb/components/id_allocator/src/lib.rs:52
   id_allocator::IdAllocator::alloc_id::{{closure}}::h22881461833e70d2        
   /home/db/ceresdb/components/id_allocator/src/lib.rs:80               
   analytic_engine::table::data::TableData::alloc_file_id::{{closure}}::h1c839670dd38ad8d
   /home/db/ceresdb/analytic_engine/src/table/data.rs:525                 
 5 analytic_engine::instance::flush_compaction::FlushTask::dump_normal_memtable::{{closure}}::h31660e44e945a85b
   /home/db/ceresdb/analytic_engine/src/instance/flush_compaction.rs:620                     
   analytic_engine::instance::flush_compaction::FlushTask::dump_memtables::{{closure}}::h897337f24183c2ab
   /home/db/ceresdb/analytic_engine/src/instance/flush_compaction.rs:384                     
   analytic_engine::instance::flush_compaction::FlushTask::run::{{closure}}::h6d1391310def4ea6
   /home/db/ceresdb/analytic_engine/src/instance/flush_compaction.rs:280
   analytic_engine::instance::flush_compaction::Flusher::schedule_table_flush::{{closure}}::{{closure}}::h0ce1ae1039065eee
   /home/db/ceresdb/analytic_engine/src/instance/flush_compaction.rs:252
   analytic_engine::instance::serial_executor::TableFlushScheduler::flush_sequentially::{{closure}}::{{closure}}::h1aae6e2506874b14
   /home/db/ceresdb/analytic_engine/src/instance/serial_executor.rs:219


```

### Server version

$ ceresdb-server --version
CeresDB Server 
Version: 1.2.6-alpha
Git commit: 7f8faff79
Git branch: main
Opt level: 3
Rustc version: 1.69.0-nightly
Target: x86_64-unknown-linux-gnu
Build date: 2023-08-17T09:04:30.077953531Z

### Steps to reproduce

After some discussion with @ShiKaiWi @Rachelint, this problem will arise when close shard, and table's compaction belonging to this shard still exists, this means there will be two nodes writing the same manifest file.

### Expected behavior

No error

### Additional Information

In theory, when a close shard request is received, it should release all resources(such as: WAL/manifest/object_store) before finish closing shard, only when those resources all are released, then we can open this shard in a new node.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1205/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1205,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5rKleF,horaedb,1797937029,1205,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-11-07T07:15:18Z,2023-11-07T07:15:18Z,This error still exist on the production environment. And I'll fix it in the Nov Iteration.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5rKleF/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1211,https://api.github.com/repos/apache/horaedb/issues/1211,horaedb,1894701571,1211,Query debugging improvement tickets,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-09-13T14:49:09Z,2023-10-08T08:14:04Z,"### Describe This Problem

I found hard to debug query performance problem now, following utils I think may be necessary to have:
- Slow queries logging file
- Hot reload for some debugging options
- Analyze for distributed query

### Proposal

- [x] Slow queries logging file
- [x] Hot reload for some debugging options
- [x] Analyze for distributed query

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1211/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1215,https://api.github.com/repos/apache/horaedb/issues/1215,horaedb,1896340780,1215,Error getting data from disk cache in concurrent situation,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,OPEN,2023-09-14T11:09:56Z,2024-04-26T09:47:49Z,"### Describe this problem
see titile

```
---- disk_cache::test::test_disk_cache_multi_thread_fetch_same_block stdout ----
thread 'disk_cache::test::test_disk_cache_multi_thread_fetch_same_block' panicked at components/object_store/src/disk_cache.rs:1107:13:
assertion `left == right` failed
  left: b""i j k l m n o p q r s t u v w x y za b c d e f g\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0 p q r s t u v w x y""
 right: b""i j k l m n o p q r s t u v w x y za b c d e f g h i j k l m n o p q r s t u v w x y""
stack backtrace:
   0: rust_begin_unwind
             at /rustc/8550f15e148407159af401e02b1d9259762b3496/library/std/src/panicking.rs:617:5
   1: core::panicking::panic_fmt
             at /rustc/8550f15e148407159af401e02b1d9259762b3496/library/core/src/panicking.rs:67:14
   2: core::panicking::assert_failed_inner
   3: core::panicking::assert_failed
             at /rustc/8550f15e148407159af401e02b1d9259762b3496/library/core/src/panicking.rs:248:5
   4: object_store::disk_cache::test::test_disk_cache_multi_thread_fetch_same_block::{{closure}}
             at ./src/disk_cache.rs:1107:13
   5: <core::pin::Pin<P> as core::future::future::Future>::poll
             at /rustc/8550f15e148407159af401e02b1d9259762b3496/library/core/src/future/future.rs:125:9
   6: <core::pin::Pin<P> as core::future::future::Future>::poll
             at /rustc/8550f15e148407159af401e02b1d9259762b3496/library/core/src/future/future.rs:125:9
   7: tokio::runtime::scheduler::current_thread::CoreGuard::block_on::{{closure}}::{{closure}}::{{closure}}
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/scheduler/current_thread.rs:651:57
   8: tokio::runtime::coop::with_budget
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/coop.rs:107:5
   9: tokio::runtime::coop::budget
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/coop.rs:73:5
  10: tokio::runtime::scheduler::current_thread::CoreGuard::block_on::{{closure}}::{{closure}}
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/scheduler/current_thread.rs:651:25
  11: tokio::runtime::scheduler::current_thread::Context::enter
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/scheduler/current_thread.rs:410:19
  12: tokio::runtime::scheduler::current_thread::CoreGuard::block_on::{{closure}}
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/scheduler/current_thread.rs:650:36
  13: tokio::runtime::scheduler::current_thread::CoreGuard::enter::{{closure}}
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/scheduler/current_thread.rs:729:68
  14: tokio::runtime::context::scoped::Scoped<T>::set
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/context/scoped.rs:40:9
  15: tokio::runtime::context::set_scheduler::{{closure}}
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/context.rs:176:26
  16: std::thread::local::LocalKey<T>::try_with
             at /rustc/8550f15e148407159af401e02b1d9259762b3496/library/std/src/thread/local.rs:270:16
  17: std::thread::local::LocalKey<T>::with
             at /rustc/8550f15e148407159af401e02b1d9259762b3496/library/std/src/thread/local.rs:246:9
  18: tokio::runtime::context::set_scheduler
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/context.rs:176:9
  19: tokio::runtime::scheduler::current_thread::CoreGuard::enter
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/scheduler/current_thread.rs:729:27
  20: tokio::runtime::scheduler::current_thread::CoreGuard::block_on
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/scheduler/current_thread.rs:638:19
  21: tokio::runtime::scheduler::current_thread::CurrentThread::block_on::{{closure}}
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/scheduler/current_thread.rs:175:28
  22: tokio::runtime::context::runtime::enter_runtime
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/context/runtime.rs:65:16
  23: tokio::runtime::scheduler::current_thread::CurrentThread::block_on
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/scheduler/current_thread.rs:167:9
  24: tokio::runtime::runtime::Runtime::block_on
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/runtime.rs:311:47
  25: object_store::disk_cache::test::test_disk_cache_multi_thread_fetch_same_block
             at ./src/disk_cache.rs:1106:9
  26: object_store::disk_cache::test::test_disk_cache_multi_thread_fetch_same_block::{{closure}}
             at ./src/disk_cache.rs:[1066](https://github.com/CeresDB/ceresdb/actions/runs/6183763154/job/16786118811?pr=1214#step:8:1067):62
  27: core::ops::function::FnOnce::call_once
             at /rustc/8550f15e148407159af401e02b1d9259762b3496/library/core/src/ops/function.rs:250:5
  28: core::ops::function::FnOnce::call_once
             at /rustc/8550f15e148407159af401e02b1d9259762b3496/library/core/src/ops/function.rs:250:5
note: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.
```
https://github.com/CeresDB/ceresdb/actions/runs/6183763154/job/16786118811?pr=1214

### Server version

a7f09dfd509f5ba770ab35301125a1a32ab5b66d

### Steps to reproduce

CI

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1215/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1215,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5761T7,horaedb,2079020283,1215,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-04-26T09:40:48Z,2024-04-26T09:40:48Z,"Sadly, this issue still exists 😅

https://github.com/apache/incubator-horaedb/actions/runs/8845827437/job/24290512689","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5761T7/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1217,https://api.github.com/repos/apache/horaedb/issues/1217,horaedb,1897617975,1217,Disk cache access range out of SST size,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-09-15T03:01:24Z,2024-10-19T11:28:05Z,"### Describe this problem


> 2023-09-15 10:41:18.673 ERRO [analytic_engine/src/compaction/scheduler.rs:537] Failed to compact table, table_name:xx, table_id:81909, request_id:379738764, err:Failed to build merge iterator, table:xx, err:Failed to build record batch from sst, err:Fail to read sst meta, err:Failed to decode sst meta data, file_path:0/81909/19008.sst, err:Parquet error: failed to get footer bytes, err:Generic DiskCacheStore error: Access is out of range, range:3979530..3979538, file_size:770151, last_modified:2023-09-12T08:03:07Z, file:0/81909/19008.sst.
 
I found above compaction error in one of our cluster, this is due to disk cache try to access SST with out-of-range requests, which is not expected.

```
            {
                ""name"": ""19008.sst"",
                ""type"": ""Multipart"",
                ""size"": 770151,
                ""storage_class"": ""Standard"",
                ""last_modified"": ""2023-09-12T16:03:07""
            }
```
This is metadata of `19008.sst`, as we can see, the `last_modified` time is equal to time in error log, which is to say file is not overwritten.

### Server version

$ ceresdb-server --version
CeresDB Server 
Version: 1.2.6-alpha
Git commit: 179c4bd7a
Git branch: refactor-skiplist-memory
Opt level: 3
Rustc version: 1.74.0-nightly
Target: x86_64-unknown-linux-gnu
Build date: 2023-09-14T08:57:00.527676870Z

### Steps to reproduce

Hard to reproduce now.

### Expected behavior

Range should less than sst file size.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1217/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1222,https://api.github.com/repos/apache/horaedb/issues/1222,horaedb,1903274260,1222,Cannot build librocksdb-sys on macOS with Apple M1 chip,tisonkun,18818196,tison,wander4096@gmail.com,CLOSED,2023-09-19T15:46:24Z,2023-09-20T03:43:18Z,"### Describe this problem

```
  [100%] Building CXX object CMakeFiles/rocksdb.dir/build_version.cc.o
  /usr/bin/c++ -DBZIP2 -DHAVE_FULLFSYNC -DLZ4 -DOS_MACOSX -DROCKSDB_LIB_IO_POSIX -DROCKSDB_NO_DYNAMIC_EXTENSION -DROCKSDB_PLATFORM_POSIX -DROCKSDB_SUPPORT_THREAD_LOCAL -DSNAPPY -DZLIB -DZSTD -I/Users/tison/Brittani/ceresdb/target/release/build/bzip2-sys-ba94b04f857441ed/out/include -I/Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb -I/Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb/include -isystem /Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb/third-party/gtest-1.8.1/fused-src -isystem /opt/homebrew/include -isystem /Users/tison/Brittani/ceresdb/target/release/build/libz-sys-17ae00708587f9ec/out/include -isystem /Users/tison/Brittani/ceresdb/target/release/build/lz4-sys-0f6dc9ee184cf579/out/include -isystem /Users/tison/Brittani/ceresdb/target/release/build/zstd-sys-755532645a75b72e/out/include -ffunction-sections -fdata-sections -fPIC -arch arm64 -W -Wextra -Wall -pthread -Wsign-compare -Wshadow -Wno-unused-parameter -Wno-unused-variable -Woverloaded-virtual -Wnon-virtual-dtor -Wno-missing-field-initializers -Wno-strict-aliasing -fno-omit-frame-pointer -momit-leaf-frame-pointer -march=armv8-a+crc+crypto -Wno-unused-function -Werror -O2 -g -DNDEBUG -std=gnu++11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX13.3.sdk -MD -MT CMakeFiles/rocksdb.dir/build_version.cc.o -MF CMakeFiles/rocksdb.dir/build_version.cc.o.d -o CMakeFiles/rocksdb.dir/build_version.cc.o -c /Users/tison/Brittani/ceresdb/target/release/build/librocksdb_sys-2139cabf29855579/out/build/build_version.cc
  [100%] Linking CXX static library librocksdb.a
  /opt/homebrew/Cellar/cmake/3.27.1/bin/cmake -P CMakeFiles/rocksdb.dir/cmake_clean_target.cmake
  /opt/homebrew/Cellar/cmake/3.27.1/bin/cmake -E cmake_link_script CMakeFiles/rocksdb.dir/link.txt --verbose=1
  /usr/bin/ar qc librocksdb.a CMakeFiles/rocksdb.dir/cache/cache.cc.o CMakeFiles/rocksdb.dir/cache/cache_entry_roles.cc.o CMakeFiles/rocksdb.dir/cache/cache_key.cc.o CMakeFiles/rocksdb.dir/cache/cache_reservation_manager.cc.o CMakeFiles/rocksdb.dir/cache/clock_cache.cc.o CMakeFiles/rocksdb.dir/cache/lru_cache.cc.o CMakeFiles/rocksdb.dir/cache/sharded_cache.cc.o CMakeFiles/rocksdb.dir/db/arena_wrapped_db_iter.cc.o CMakeFiles/rocksdb.dir/db/blob/blob_fetcher.cc.o CMakeFiles/rocksdb.dir/db/blob/blob_file_addition.cc.o CMakeFiles/rocksdb.dir/db/blob/blob_file_builder.cc.o CMakeFiles/rocksdb.dir/db/blob/blob_file_cache.cc.o CMakeFiles/rocksdb.dir/db/blob/blob_file_garbage.cc.o CMakeFiles/rocksdb.dir/db/blob/blob_file_meta.cc.o CMakeFiles/rocksdb.dir/db/blob/blob_file_reader.cc.o CMakeFiles/rocksdb.dir/db/blob/blob_garbage_meter.cc.o CMakeFiles/rocksdb.dir/db/blob/blob_log_format.cc.o CMakeFiles/rocksdb.dir/db/blob/blob_log_sequential_reader.cc.o CMakeFiles/rocksdb.dir/db/blob/blob_log_writer.cc.o CMakeFiles/rocksdb.dir/db/blob/prefetch_buffer_collection.cc.o CMakeFiles/rocksdb.dir/db/builder.cc.o CMakeFiles/rocksdb.dir/db/c.cc.o CMakeFiles/rocksdb.dir/db/column_family.cc.o CMakeFiles/rocksdb.dir/db/compaction/compaction.cc.o CMakeFiles/rocksdb.dir/db/compaction/compaction_iterator.cc.o CMakeFiles/rocksdb.dir/db/compaction/compaction_picker.cc.o CMakeFiles/rocksdb.dir/db/compaction/compaction_job.cc.o CMakeFiles/rocksdb.dir/db/compaction/compaction_picker_fifo.cc.o CMakeFiles/rocksdb.dir/db/compaction/compaction_picker_level.cc.o CMakeFiles/rocksdb.dir/db/compaction/compaction_picker_universal.cc.o CMakeFiles/rocksdb.dir/db/compaction/sst_partitioner.cc.o CMakeFiles/rocksdb.dir/db/convenience.cc.o CMakeFiles/rocksdb.dir/db/db_filesnapshot.cc.o CMakeFiles/rocksdb.dir/db/db_impl/compacted_db_impl.cc.o CMakeFiles/rocksdb.dir/db/db_impl/db_impl.cc.o CMakeFiles/rocksdb.dir/db/db_impl/db_impl_write.cc.o CMakeFiles/rocksdb.dir/db/db_impl/db_impl_compaction_flush.cc.o CMakeFiles/rocksdb.dir/db/db_impl/db_impl_files.cc.o CMakeFiles/rocksdb.dir/db/db_impl/db_impl_open.cc.o CMakeFiles/rocksdb.dir/db/db_impl/db_impl_debug.cc.o CMakeFiles/rocksdb.dir/db/db_impl/db_impl_experimental.cc.o CMakeFiles/rocksdb.dir/db/db_impl/db_impl_readonly.cc.o CMakeFiles/rocksdb.dir/db/db_impl/db_impl_secondary.cc.o CMakeFiles/rocksdb.dir/db/db_impl/db_impl_merge.cc.o CMakeFiles/rocksdb.dir/db/db_info_dumper.cc.o CMakeFiles/rocksdb.dir/db/db_iter.cc.o CMakeFiles/rocksdb.dir/db/dbformat.cc.o CMakeFiles/rocksdb.dir/db/error_handler.cc.o CMakeFiles/rocksdb.dir/db/event_helpers.cc.o CMakeFiles/rocksdb.dir/db/experimental.cc.o CMakeFiles/rocksdb.dir/db/external_sst_file_ingestion_job.cc.o CMakeFiles/rocksdb.dir/db/file_indexer.cc.o CMakeFiles/rocksdb.dir/db/flush_job.cc.o CMakeFiles/rocksdb.dir/db/flush_scheduler.cc.o CMakeFiles/rocksdb.dir/db/forward_iterator.cc.o CMakeFiles/rocksdb.dir/db/import_column_family_job.cc.o CMakeFiles/rocksdb.dir/db/internal_stats.cc.o CMakeFiles/rocksdb.dir/db/logs_with_prep_tracker.cc.o CMakeFiles/rocksdb.dir/db/log_reader.cc.o CMakeFiles/rocksdb.dir/db/log_writer.cc.o CMakeFiles/rocksdb.dir/db/malloc_stats.cc.o CMakeFiles/rocksdb.dir/db/memtable.cc.o CMakeFiles/rocksdb.dir/db/memtable_list.cc.o CMakeFiles/rocksdb.dir/db/merge_helper.cc.o CMakeFiles/rocksdb.dir/db/merge_operator.cc.o CMakeFiles/rocksdb.dir/db/output_validator.cc.o CMakeFiles/rocksdb.dir/db/periodic_work_scheduler.cc.o CMakeFiles/rocksdb.dir/db/range_del_aggregator.cc.o CMakeFiles/rocksdb.dir/db/range_tombstone_fragmenter.cc.o CMakeFiles/rocksdb.dir/db/repair.cc.o CMakeFiles/rocksdb.dir/db/snapshot_impl.cc.o CMakeFiles/rocksdb.dir/db/table_cache.cc.o CMakeFiles/rocksdb.dir/db/table_properties_collector.cc.o CMakeFiles/rocksdb.dir/db/transaction_log_impl.cc.o CMakeFiles/rocksdb.dir/db/trim_history_scheduler.cc.o CMakeFiles/rocksdb.dir/db/version_builder.cc.o CMakeFiles/rocksdb.dir/db/version_edit.cc.o CMakeFiles/rocksdb.dir/db/version_edit_handler.cc.o CMakeFiles/rocksdb.dir/db/version_set.cc.o CMakeFiles/rocksdb.dir/db/wal_edit.cc.o CMakeFiles/rocksdb.dir/db/wal_manager.cc.o CMakeFiles/rocksdb.dir/db/write_batch.cc.o CMakeFiles/rocksdb.dir/db/write_batch_base.cc.o CMakeFiles/rocksdb.dir/db/write_controller.cc.o CMakeFiles/rocksdb.dir/db/write_thread.cc.o CMakeFiles/rocksdb.dir/encryption/encryption.cc.o CMakeFiles/rocksdb.dir/env/composite_env.cc.o CMakeFiles/rocksdb.dir/env/env.cc.o CMakeFiles/rocksdb.dir/env/env_chroot.cc.o CMakeFiles/rocksdb.dir/env/env_encryption.cc.o CMakeFiles/rocksdb.dir/env/env_hdfs.cc.o CMakeFiles/rocksdb.dir/env/env_inspected.cc.o CMakeFiles/rocksdb.dir/env/file_system.cc.o CMakeFiles/rocksdb.dir/env/file_system_tracer.cc.o CMakeFiles/rocksdb.dir/env/fs_remap.cc.o CMakeFiles/rocksdb.dir/env/mock_env.cc.o CMakeFiles/rocksdb.dir/env/unique_id_gen.cc.o CMakeFiles/rocksdb.dir/file/delete_scheduler.cc.o CMakeFiles/rocksdb.dir/file/file_prefetch_buffer.cc.o CMakeFiles/rocksdb.dir/file/file_util.cc.o CMakeFiles/rocksdb.dir/file/filename.cc.o CMakeFiles/rocksdb.dir/file/line_file_reader.cc.o CMakeFiles/rocksdb.dir/file/random_access_file_reader.cc.o CMakeFiles/rocksdb.dir/file/read_write_util.cc.o CMakeFiles/rocksdb.dir/file/readahead_raf.cc.o CMakeFiles/rocksdb.dir/file/sequence_file_reader.cc.o CMakeFiles/rocksdb.dir/file/sst_file_manager_impl.cc.o CMakeFiles/rocksdb.dir/file/writable_file_writer.cc.o CMakeFiles/rocksdb.dir/logging/auto_roll_logger.cc.o CMakeFiles/rocksdb.dir/logging/event_logger.cc.o CMakeFiles/rocksdb.dir/logging/log_buffer.cc.o CMakeFiles/rocksdb.dir/memory/arena.cc.o CMakeFiles/rocksdb.dir/memory/concurrent_arena.cc.o CMakeFiles/rocksdb.dir/memory/jemalloc_nodump_allocator.cc.o CMakeFiles/rocksdb.dir/memory/memkind_kmem_allocator.cc.o CMakeFiles/rocksdb.dir/memory/memory_allocator.cc.o CMakeFiles/rocksdb.dir/memtable/alloc_tracker.cc.o CMakeFiles/rocksdb.dir/memtable/hash_linklist_rep.cc.o CMakeFiles/rocksdb.dir/memtable/hash_skiplist_rep.cc.o CMakeFiles/rocksdb.dir/memtable/skiplistrep.cc.o CMakeFiles/rocksdb.dir/memtable/vectorrep.cc.o CMakeFiles/rocksdb.dir/memtable/write_buffer_manager.cc.o CMakeFiles/rocksdb.dir/monitoring/histogram.cc.o CMakeFiles/rocksdb.dir/monitoring/histogram_windowing.cc.o CMakeFiles/rocksdb.dir/monitoring/in_memory_stats_history.cc.o CMakeFiles/rocksdb.dir/monitoring/instrumented_mutex.cc.o CMakeFiles/rocksdb.dir/monitoring/iostats_context.cc.o CMakeFiles/rocksdb.dir/monitoring/perf_context.cc.o CMakeFiles/rocksdb.dir/monitoring/perf_level.cc.o CMakeFiles/rocksdb.dir/monitoring/perf_flag.cc.o CMakeFiles/rocksdb.dir/monitoring/persistent_stats_history.cc.o CMakeFiles/rocksdb.dir/monitoring/statistics.cc.o CMakeFiles/rocksdb.dir/monitoring/thread_status_impl.cc.o CMakeFiles/rocksdb.dir/monitoring/thread_status_updater.cc.o CMakeFiles/rocksdb.dir/monitoring/thread_status_util.cc.o CMakeFiles/rocksdb.dir/monitoring/thread_status_util_debug.cc.o CMakeFiles/rocksdb.dir/options/cf_options.cc.o CMakeFiles/rocksdb.dir/options/configurable.cc.o CMakeFiles/rocksdb.dir/options/customizable.cc.o CMakeFiles/rocksdb.dir/options/db_options.cc.o CMakeFiles/rocksdb.dir/options/options.cc.o CMakeFiles/rocksdb.dir/options/options_helper.cc.o CMakeFiles/rocksdb.dir/options/options_parser.cc.o CMakeFiles/rocksdb.dir/port/stack_trace.cc.o CMakeFiles/rocksdb.dir/table/adaptive/adaptive_table_factory.cc.o CMakeFiles/rocksdb.dir/table/block_based/binary_search_index_reader.cc.o CMakeFiles/rocksdb.dir/table/block_based/block.cc.o CMakeFiles/rocksdb.dir/table/block_based/block_based_filter_block.cc.o CMakeFiles/rocksdb.dir/table/block_based/block_based_table_builder.cc.o CMakeFiles/rocksdb.dir/table/block_based/block_based_table_factory.cc.o CMakeFiles/rocksdb.dir/table/block_based/block_based_table_iterator.cc.o CMakeFiles/rocksdb.dir/table/block_based/block_based_table_reader.cc.o CMakeFiles/rocksdb.dir/table/block_based/block_builder.cc.o CMakeFiles/rocksdb.dir/table/block_based/block_prefetcher.cc.o CMakeFiles/rocksdb.dir/table/block_based/block_prefix_index.cc.o CMakeFiles/rocksdb.dir/table/block_based/data_block_hash_index.cc.o CMakeFiles/rocksdb.dir/table/block_based/data_block_footer.cc.o CMakeFiles/rocksdb.dir/table/block_based/filter_block_reader_common.cc.o CMakeFiles/rocksdb.dir/table/block_based/filter_policy.cc.o CMakeFiles/rocksdb.dir/table/block_based/flush_block_policy.cc.o CMakeFiles/rocksdb.dir/table/block_based/full_filter_block.cc.o CMakeFiles/rocksdb.dir/table/block_based/hash_index_reader.cc.o CMakeFiles/rocksdb.dir/table/block_based/index_builder.cc.o CMakeFiles/rocksdb.dir/table/block_based/index_reader_common.cc.o CMakeFiles/rocksdb.dir/table/block_based/parsed_full_filter_block.cc.o CMakeFiles/rocksdb.dir/table/block_based/partitioned_filter_block.cc.o CMakeFiles/rocksdb.dir/table/block_based/partitioned_index_iterator.cc.o CMakeFiles/rocksdb.dir/table/block_based/partitioned_index_reader.cc.o CMakeFiles/rocksdb.dir/table/block_based/reader_common.cc.o CMakeFiles/rocksdb.dir/table/block_based/uncompression_dict_reader.cc.o CMakeFiles/rocksdb.dir/table/block_fetcher.cc.o CMakeFiles/rocksdb.dir/table/cuckoo/cuckoo_table_builder.cc.o CMakeFiles/rocksdb.dir/table/cuckoo/cuckoo_table_factory.cc.o CMakeFiles/rocksdb.dir/table/cuckoo/cuckoo_table_reader.cc.o CMakeFiles/rocksdb.dir/table/format.cc.o CMakeFiles/rocksdb.dir/table/get_context.cc.o CMakeFiles/rocksdb.dir/table/iterator.cc.o CMakeFiles/rocksdb.dir/table/merging_iterator.cc.o CMakeFiles/rocksdb.dir/table/meta_blocks.cc.o CMakeFiles/rocksdb.dir/table/persistent_cache_helper.cc.o CMakeFiles/rocksdb.dir/table/plain/plain_table_bloom.cc.o CMakeFiles/rocksdb.dir/table/plain/plain_table_builder.cc.o CMakeFiles/rocksdb.dir/table/plain/plain_table_factory.cc.o CMakeFiles/rocksdb.dir/table/plain/plain_table_index.cc.o CMakeFiles/rocksdb.dir/table/plain/plain_table_key_coding.cc.o CMakeFiles/rocksdb.dir/table/plain/plain_table_reader.cc.o CMakeFiles/rocksdb.dir/table/sst_file_dumper.cc.o CMakeFiles/rocksdb.dir/table/sst_file_reader.cc.o CMakeFiles/rocksdb.dir/table/sst_file_writer.cc.o CMakeFiles/rocksdb.dir/table/table_factory.cc.o CMakeFiles/rocksdb.dir/table/table_properties.cc.o CMakeFiles/rocksdb.dir/table/two_level_iterator.cc.o CMakeFiles/rocksdb.dir/table/unique_id.cc.o CMakeFiles/rocksdb.dir/test_util/sync_point.cc.o CMakeFiles/rocksdb.dir/test_util/sync_point_impl.cc.o CMakeFiles/rocksdb.dir/test_util/testutil.cc.o CMakeFiles/rocksdb.dir/test_util/transaction_test_util.cc.o CMakeFiles/rocksdb.dir/tools/block_cache_analyzer/block_cache_trace_analyzer.cc.o CMakeFiles/rocksdb.dir/tools/dump/db_dump_tool.cc.o CMakeFiles/rocksdb.dir/tools/io_tracer_parser_tool.cc.o CMakeFiles/rocksdb.dir/tools/ldb_cmd.cc.o CMakeFiles/rocksdb.dir/tools/ldb_tool.cc.o CMakeFiles/rocksdb.dir/tools/sst_dump_tool.cc.o CMakeFiles/rocksdb.dir/tools/trace_analyzer_tool.cc.o CMakeFiles/rocksdb.dir/trace_replay/block_cache_tracer.cc.o CMakeFiles/rocksdb.dir/trace_replay/io_tracer.cc.o CMakeFiles/rocksdb.dir/trace_replay/trace_record_handler.cc.o CMakeFiles/rocksdb.dir/trace_replay/trace_record_result.cc.o CMakeFiles/rocksdb.dir/trace_replay/trace_record.cc.o CMakeFiles/rocksdb.dir/trace_replay/trace_replay.cc.o CMakeFiles/rocksdb.dir/util/coding.cc.o CMakeFiles/rocksdb.dir/util/compaction_job_stats_impl.cc.o CMakeFiles/rocksdb.dir/util/comparator.cc.o CMakeFiles/rocksdb.dir/util/compression_context_cache.cc.o CMakeFiles/rocksdb.dir/util/concurrent_task_limiter_impl.cc.o CMakeFiles/rocksdb.dir/util/crc32c.cc.o CMakeFiles/rocksdb.dir/util/dynamic_bloom.cc.o CMakeFiles/rocksdb.dir/util/hash.cc.o CMakeFiles/rocksdb.dir/util/murmurhash.cc.o CMakeFiles/rocksdb.dir/util/random.cc.o CMakeFiles/rocksdb.dir/util/rate_limiter.cc.o CMakeFiles/rocksdb.dir/util/ribbon_config.cc.o CMakeFiles/rocksdb.dir/util/regex.cc.o CMakeFiles/rocksdb.dir/util/slice.cc.o CMakeFiles/rocksdb.dir/util/file_checksum_helper.cc.o CMakeFiles/rocksdb.dir/util/status.cc.o CMakeFiles/rocksdb.dir/util/string_util.cc.o CMakeFiles/rocksdb.dir/util/thread_local.cc.o CMakeFiles/rocksdb.dir/util/threadpool_imp.cc.o CMakeFiles/rocksdb.dir/util/xxhash.cc.o CMakeFiles/rocksdb.dir/utilities/backupable/backupable_db.cc.o CMakeFiles/rocksdb.dir/utilities/blob_db/blob_compaction_filter.cc.o CMakeFiles/rocksdb.dir/utilities/blob_db/blob_db.cc.o CMakeFiles/rocksdb.dir/utilities/blob_db/blob_db_impl.cc.o CMakeFiles/rocksdb.dir/utilities/blob_db/blob_db_impl_filesnapshot.cc.o CMakeFiles/rocksdb.dir/utilities/blob_db/blob_dump_tool.cc.o CMakeFiles/rocksdb.dir/utilities/blob_db/blob_file.cc.o CMakeFiles/rocksdb.dir/utilities/cache_dump_load.cc.o CMakeFiles/rocksdb.dir/utilities/cache_dump_load_impl.cc.o CMakeFiles/rocksdb.dir/utilities/cassandra/cassandra_compaction_filter.cc.o CMakeFiles/rocksdb.dir/utilities/cassandra/format.cc.o CMakeFiles/rocksdb.dir/utilities/cassandra/merge_operator.cc.o CMakeFiles/rocksdb.dir/utilities/checkpoint/checkpoint_impl.cc.o CMakeFiles/rocksdb.dir/utilities/compaction_filters.cc.o CMakeFiles/rocksdb.dir/utilities/compaction_filters/remove_emptyvalue_compactionfilter.cc.o CMakeFiles/rocksdb.dir/utilities/debug.cc.o CMakeFiles/rocksdb.dir/utilities/env_mirror.cc.o CMakeFiles/rocksdb.dir/utilities/env_timed.cc.o CMakeFiles/rocksdb.dir/utilities/fault_injection_env.cc.o CMakeFiles/rocksdb.dir/utilities/fault_injection_fs.cc.o CMakeFiles/rocksdb.dir/utilities/fault_injection_secondary_cache.cc.o CMakeFiles/rocksdb.dir/utilities/leveldb_options/leveldb_options.cc.o CMakeFiles/rocksdb.dir/utilities/memory/memory_util.cc.o CMakeFiles/rocksdb.dir/utilities/merge_operators.cc.o CMakeFiles/rocksdb.dir/utilities/merge_operators/bytesxor.cc.o CMakeFiles/rocksdb.dir/utilities/merge_operators/max.cc.o CMakeFiles/rocksdb.dir/utilities/merge_operators/put.cc.o CMakeFiles/rocksdb.dir/utilities/merge_operators/sortlist.cc.o CMakeFiles/rocksdb.dir/utilities/merge_operators/string_append/stringappend.cc.o CMakeFiles/rocksdb.dir/utilities/merge_operators/string_append/stringappend2.cc.o CMakeFiles/rocksdb.dir/utilities/merge_operators/uint64add.cc.o CMakeFiles/rocksdb.dir/utilities/object_registry.cc.o CMakeFiles/rocksdb.dir/utilities/option_change_migration/option_change_migration.cc.o CMakeFiles/rocksdb.dir/utilities/options/options_util.cc.o CMakeFiles/rocksdb.dir/utilities/persistent_cache/block_cache_tier.cc.o CMakeFiles/rocksdb.dir/utilities/persistent_cache/block_cache_tier_file.cc.o CMakeFiles/rocksdb.dir/utilities/persistent_cache/block_cache_tier_metadata.cc.o CMakeFiles/rocksdb.dir/utilities/persistent_cache/persistent_cache_tier.cc.o CMakeFiles/rocksdb.dir/utilities/persistent_cache/volatile_tier_impl.cc.o CMakeFiles/rocksdb.dir/utilities/rate_limiters/write_amp_based_rate_limiter.cc.o CMakeFiles/rocksdb.dir/utilities/simulator_cache/cache_simulator.cc.o CMakeFiles/rocksdb.dir/utilities/simulator_cache/sim_cache.cc.o CMakeFiles/rocksdb.dir/utilities/table_properties_collectors/compact_on_deletion_collector.cc.o CMakeFiles/rocksdb.dir/utilities/trace/file_trace_reader_writer.cc.o CMakeFiles/rocksdb.dir/utilities/trace/replayer_impl.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/lock/lock_manager.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/lock/point/point_lock_tracker.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/lock/point/point_lock_manager.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/lock/range/range_tree/range_tree_lock_manager.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/lock/range/range_tree/range_tree_lock_tracker.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/optimistic_transaction_db_impl.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/optimistic_transaction.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/pessimistic_transaction.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/pessimistic_transaction_db.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/snapshot_checker.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/transaction_base.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/transaction_db_mutex_impl.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/transaction_util.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/write_prepared_txn.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/write_prepared_txn_db.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/write_unprepared_txn.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/write_unprepared_txn_db.cc.o CMakeFiles/rocksdb.dir/utilities/ttl/db_ttl_impl.cc.o CMakeFiles/rocksdb.dir/utilities/wal_filter.cc.o CMakeFiles/rocksdb.dir/utilities/write_batch_with_index/write_batch_with_index.cc.o CMakeFiles/rocksdb.dir/utilities/write_batch_with_index/write_batch_with_index_internal.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/lock/range/range_tree/lib/locktree/concurrent_tree.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/lock/range/range_tree/lib/locktree/keyrange.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/lock/range/range_tree/lib/locktree/lock_request.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/lock/range/range_tree/lib/locktree/locktree.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/lock/range/range_tree/lib/locktree/manager.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/lock/range/range_tree/lib/locktree/range_buffer.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/lock/range/range_tree/lib/locktree/treenode.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/lock/range/range_tree/lib/locktree/txnid_set.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/lock/range/range_tree/lib/locktree/wfg.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/lock/range/range_tree/lib/standalone_port.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/lock/range/range_tree/lib/util/dbt.cc.o CMakeFiles/rocksdb.dir/utilities/transactions/lock/range/range_tree/lib/util/memarena.cc.o CMakeFiles/rocksdb.dir/util/crc32c_arm64.cc.o CMakeFiles/rocksdb.dir/port/port_posix.cc.o CMakeFiles/rocksdb.dir/env/env_posix.cc.o CMakeFiles/rocksdb.dir/env/fs_posix.cc.o CMakeFiles/rocksdb.dir/env/io_posix.cc.o CMakeFiles/rocksdb.dir/build_version.cc.o
  /usr/bin/ranlib librocksdb.a
  [100%] Built target rocksdb
  /opt/homebrew/Cellar/cmake/3.27.1/bin/cmake -E cmake_progress_start /Users/tison/Brittani/ceresdb/target/release/build/librocksdb_sys-2139cabf29855579/out/build/CMakeFiles 0

  cargo:root=/Users/tison/Brittani/ceresdb/target/release/build/librocksdb_sys-2139cabf29855579/out
  cargo:rustc-link-search=native=/Users/tison/Brittani/ceresdb/target/release/build/librocksdb_sys-2139cabf29855579/out/build

  --- stderr
     Entering             /Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb/third-party/gtest-1.8.1/fused-src/gtest
     Called from: [1]	/Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb/CMakeLists.txt
     Returning to         /Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb
     Called from: [1]	/Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb/CMakeLists.txt
     Entering             /Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb/tools
     Called from: [1]	/Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb/CMakeLists.txt
     Returning to         /Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb
     Called from: [1]	/Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb/CMakeLists.txt
  thread 'main' panicked at /Users/tison/.cargo/registry/src/index.crates.io-6f17d22bba15001f/proc-macro2-1.0.66/src/fallback.rs:774:9:
  ""enum_(unnamed_at_crocksdb/crocksdb/c_h_814_1)"" is not a valid Ident
  note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
warning: build failed, waiting for other jobs to finish...
```

### Server version

main at ff6d27b320fa2677f28aac7215bb6c82e831e455

### Steps to reproduce

Clone and build with `cargo build --release`.

### Expected behavior

Successfully build.

### Additional Information

Do we depend on libtitan? If not, we can try to switch to [the upstream](https://github.com/rust-rocksdb/rust-rocksdb) which can be more active to fix portable build issues.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1222/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1222,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m38JC,horaedb,1725940290,1222,NA,tisonkun,18818196,tison,wander4096@gmail.com,NA,2023-09-19T15:55:34Z,2023-09-19T15:55:34Z,"> switch to [the upstream](https://github.com/rust-rocksdb/rust-rocksdb) which

No. At least it's not quite easy since it seems that the upstream diverges from TiKV's fork quite a lot.

Perhaps we can pick up https://github.com/CeresDB/ceresdb/issues/206 again.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m38JC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1222,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m4Qzs,horaedb,1726024940,1222,NA,tisonkun,18818196,tison,wander4096@gmail.com,NA,2023-09-19T16:18:57Z,2023-09-19T16:18:57Z,"Full stacktrace:

```
  --- stderr
     Entering             /Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb/third-party/gtest-1.8.1/fused-src/gtest
     Called from: [1]	/Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb/CMakeLists.txt
     Returning to         /Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb
     Called from: [1]	/Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb/CMakeLists.txt
     Entering             /Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb/tools
     Called from: [1]	/Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb/CMakeLists.txt
     Returning to         /Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb
     Called from: [1]	/Users/tison/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/a9fbe32/librocksdb_sys/rocksdb/CMakeLists.txt
  thread 'main' panicked at /Users/tison/.cargo/registry/src/index.crates.io-6f17d22bba15001f/proc-macro2-1.0.66/src/fallback.rs:774:9:
  ""enum_(unnamed_at_crocksdb/crocksdb/c_h_814_1)"" is not a valid Ident
  stack backtrace:
     0: rust_begin_unwind
               at /rustc/8550f15e148407159af401e02b1d9259762b3496/library/std/src/panicking.rs:617:5
     1: core::panicking::panic_fmt
               at /rustc/8550f15e148407159af401e02b1d9259762b3496/library/core/src/panicking.rs:67:14
     2: proc_macro2::fallback::validate_ident
     3: proc_macro2::fallback::Ident::_new
     4: proc_macro2::fallback::Ident::new
     5: proc_macro2::imp::Ident::new
     6: proc_macro2::Ident::new
     7: bindgen::ir::context::BindgenContext::rust_ident_raw
     8: bindgen::ir::context::BindgenContext::rust_ident
     9: <bindgen::ir::enum_ty::Enum as bindgen::codegen::CodeGenerator>::codegen
    10: <bindgen::ir::ty::Type as bindgen::codegen::CodeGenerator>::codegen
    11: <bindgen::ir::item::Item as bindgen::codegen::CodeGenerator>::codegen
    12: <bindgen::ir::module::Module as bindgen::codegen::CodeGenerator>::codegen::{{closure}}
    13: <bindgen::ir::module::Module as bindgen::codegen::CodeGenerator>::codegen
    14: <bindgen::ir::item::Item as bindgen::codegen::CodeGenerator>::codegen
    15: bindgen::codegen::codegen::{{closure}}
    16: bindgen::ir::context::BindgenContext::gen
    17: bindgen::codegen::codegen
    18: bindgen::Bindings::generate
    19: bindgen::Builder::generate
    20: build_script_build::bindgen_rocksdb
    21: build_script_build::config_binding_path
    22: build_script_build::build_rocksdb
    23: build_script_build::main
    24: core::ops::function::FnOnce::call_once
  note: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m4Qzs/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1222,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m4zn0,horaedb,1726167540,1222,NA,tisonkun,18818196,tison,wander4096@gmail.com,NA,2023-09-19T17:21:04Z,2023-09-19T17:21:04Z,"Upgrade TiKV upstream's commit help. I'll prepare a patch and see if the CI passed.

I may spend some time for opt-out the RocksDB WAL impl.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m4zn0/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1222,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m7AuQ,horaedb,1726745488,1222,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-09-20T01:22:16Z,2023-09-20T01:22:16Z,"> Upgrade TiKV upstream's commit help. I'll prepare a patch and see if the CI passed.
> 
> I may spend some time for opt-out the RocksDB WAL impl.

Thanks a lot. In some environments, `rocksb-wal` is not chosen, but it still costs too much time for compiling.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m7AuQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1222,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m7HXb,horaedb,1726772699,1222,NA,tisonkun,18818196,tison,wander4096@gmail.com,NA,2023-09-20T02:03:36Z,2023-09-20T02:03:36Z,I don't find a feature named `rocksb-wal`. May you show me the related code?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m7HXb/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1222,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m7IO6,horaedb,1726776250,1222,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-09-20T02:09:16Z,2023-09-20T02:09:16Z,"> I don't find a feature named `rocksb-wal`. May you show me the related code?

It is what we need, but doesn't exist for now. :joy:","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m7IO6/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1222,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m7IkN,horaedb,1726777613,1222,NA,tisonkun,18818196,tison,wander4096@gmail.com,NA,2023-09-20T02:11:30Z,2023-09-20T02:11:30Z,"@ShiKaiWi Yep. I have some thought but we don't have a one for all feature registry, and the rocksdb wal should be opt in both wal and analytic_engine so we may use the same feature name for these two crates?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m7IkN/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1222,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m7KnS,horaedb,1726786002,1222,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-09-20T02:25:23Z,2023-09-20T02:25:23Z,"> @ShiKaiWi Yep. I have some thought but we don't have a one for all feature registry, and the rocksdb wal should be opt in both wal and analytic_engine so we may use the same feature name for these two crates?

Here are my simple thoughts about introducing this feature gate:
- Firstly, separate the `rocks_impl` module from the wal `crate`;
- Secondly, we can introduce this feature to the crate, which takes responsibilities to setup the server (the sepecific file is `setup.rs`), to control which wal implementation will be used for the engine;
- After the two steps, the `analytic_engine` should not have any knowledge about the `rocks_impl`, but I find it that `WalStorageConfig::RocksDB` is used in the `analytic_engine::Config`. In order to remove the `rocks_impl` from the `analytic_engine`, maybe we should move `WalStorageConfig` outside the `analytic_engine::Config`, and I guess it is good to put it into `src/config.rs`;
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m7KnS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1222,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m7K8z,horaedb,1726787379,1222,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-09-20T02:27:36Z,2023-09-20T02:27:36Z,"After that, I guess only one feature gate is used for the project, and it will be used in the `src/setup.rs`.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m7K8z/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1222,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m7OmD,horaedb,1726802307,1222,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-09-20T02:51:52Z,2023-09-20T02:51:52Z,"> Do we depend on libtitan? If not, we can try to switch to [the upstream](https://github.com/rust-rocksdb/rust-rocksdb) which can be more active to fix portable build issues.

Actually, switching to [the upstream](https://github.com/rust-rocksdb/rust-rocksdb) is very attractive, which was also taken into consideration before, but there are still some issues preventing us from a direct and simple switch:
- The compatibility about the data can't be ensured, and some tests must be taken for it;
- Some apis are different, especially for the cases involving lifetime, and after switching it may have influences on the trait `WalManager`'s methods signature;","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5m7OmD/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1241,https://api.github.com/repos/apache/horaedb/issues/1241,horaedb,1931281743,1241,UT test_db_write_buffer_size_mem_wal failed sometimes,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,OPEN,2023-10-07T09:46:30Z,2023-12-07T06:17:07Z,"### Describe this problem

```
---- tests::read_write_test::test_db_write_buffer_size_mem_wal stdout ----
thread 'tests::read_write_test::test_db_write_buffer_size_mem_wal' panicked at analytic_engine/src/tests/read_write_test.rs:677:9:
assertion `left == right` failed
  left: 1
 right: 0
stack backtrace:
   0: rust_begin_unwind
             at /rustc/8550f15e148407159af401e02b1d9259762b3496/library/std/src/panicking.rs:617:5
   1: core::panicking::panic_fmt
             at /rustc/8550f15e148407159af401e02b1d9259762b3496/library/core/src/panicking.rs:67:14
   2: core::panicking::assert_failed_inner
   3: core::panicking::assert_failed
             at /rustc/8550f15e148407159af401e02b1d9259762b3496/library/core/src/panicking.rs:248:5
   4: analytic_engine::tests::read_write_test::test_write_buffer_size_overflow::{{closure}}
             at ./src/tests/read_write_test.rs:677:9
   5: tokio::runtime::park::CachedParkThread::block_on::{{closure}}
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/park.rs:283:63
   6: tokio::runtime::coop::with_budget
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/coop.rs:107:5
   7: tokio::runtime::coop::budget
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/coop.rs:73:5
   8: tokio::runtime::park::CachedParkThread::block_on
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/park.rs:283:31
   9: tokio::runtime::context::blocking::BlockingRegionGuard::block_on
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/context/blocking.rs:66:9
  10: tokio::runtime::scheduler::multi_thread::MultiThread::block_on::{{closure}}
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/scheduler/multi_thread/mod.rs:87:13
  11: tokio::runtime::context::runtime::enter_runtime
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/context/runtime.rs:65:16
  12: tokio::runtime::scheduler::multi_thread::MultiThread::block_on
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/scheduler/multi_thread/mod.rs:86:9
  13: tokio::runtime::runtime::Runtime::block_on
             at /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/runtime.rs:313:45
  14: runtime::Runtime::block_on
             at /home/runner/work/ceresdb/ceresdb/components/runtime/src/lib.rs:98:9
  15: analytic_engine::tests::util::TestEnv::block_on
             at ./src/tests/util.rs:472:9
  16: analytic_engine::tests::read_write_test::test_write_buffer_size_overflow
             at ./src/tests/read_write_test.rs:587:5
  17: analytic_engine::tests::read_write_test::test_db_write_buffer_size
             at ./src/tests/read_write_test.rs:554:5
  18: analytic_engine::tests::read_write_test::test_db_write_buffer_size_mem_wal
             at ./src/tests/read_write_test.rs:546:9
  19: analytic_engine::tests::read_write_test::test_db_write_buffer_size_mem_wal::{{closure}}
             at ./src/tests/read_write_test.rs:542:40
  20: core::ops::function::FnOnce::call_once
             at /rustc/8550f15e148407159af401e02b1d9259[762](https://github.com/CeresDB/ceresdb/actions/runs/6439577851/job/17487413595?pr=1240#step:8:763)b3496/library/core/src/ops/function.rs:250:5
  21: core::ops::function::FnOnce::call_once
             at /rustc/8550f15e148407159af401e02b1d9259762b3496/library/core/src/ops/function.rs:250:5
note: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.


failures:
    tests::read_write_test::test_db_write_buffer_size_mem_wal

test result: FAILED. 92 passed; 1 failed; 2 ignored; 0 measured; 0 filtered out; finished in 9.89s

error: test failed, to rerun pass `-p analytic_engine --lib`
make: *** [Makefile:54: test-ut] Error 101
Error: Process completed with exit code 2.
```

### Server version

https://github.com/CeresDB/ceresdb/actions/runs/6439577851/job/17487413595?pr=1240

### Steps to reproduce

Difficult to reproduce, this happens sometimes in CI.

### Expected behavior

No error

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1241/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1241,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5t85xV,horaedb,1844681813,1241,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-12-07T06:17:07Z,2023-12-07T06:17:07Z,"Sadly, this still happens frequently in CI...","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5t85xV/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1242,https://api.github.com/repos/apache/horaedb/issues/1242,horaedb,1931700934,1242,Late materialization for paquet reading,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-10-08T08:10:07Z,2024-10-19T11:19:11Z,"### Describe This Problem

We found one of the cpu bottlenecks for query is parquet's decoding in production, and `late materialization` is the effective method for optimizing this.


### Proposal

Parquet's late materialization impl is too naive now, we must be very careful when using it, so as I see filters can be pushed down for late materialization should satisfy following conditions now:
- Should contain just a single column (just need to pull one column for eval).
- Should be selective enough(such as `=`, `in`).
- We should sort the filters according to the encoded columns size in them.
However, it is too tired for us users to use this feature, maybe we need to help to improve this in parquet.


- [x] Impl first version late materialization following design above.
- [x] Define metrics to measure the effect of late materialization.
- [x] Improve late materialization impl in parquet.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1242/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1242,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5otJAU,horaedb,1756663828,1242,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-10-11T02:45:00Z,2023-10-11T02:45:00Z,Suggest giving links to parquet related information.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5otJAU/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1247,https://api.github.com/repos/apache/horaedb/issues/1247,horaedb,1932441472,1247,Concurrent control for altering schema on partition tables,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-10-09T07:13:54Z,2024-10-19T11:27:40Z,"### Describe This Problem

Currently, there is no concurrency control in place. If alter schema operations are performed simultaneously on the same partition table, it may result in schema inconsistency issue.
Refer to #1244.

### Proposal

By using centralized storage, we can achieve serial execution of alter schema operations on same partition table.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1247/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1248,https://api.github.com/repos/apache/horaedb/issues/1248,horaedb,1932444531,1248,Updating schema information for partition table on ceresdb nodes,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-10-09T07:16:11Z,2024-10-19T11:27:29Z,"### Describe This Problem

Due to the ability to open partition table on multiple ceresdb nodes, there may be inconsistencies in the schema information maintained in memory. Currently, the approach of cleaning up through errors occurring during writing is relatively rough.
Refer to #1244.


### Proposal

There are two approaches:

Passive notification:
Currently, the `partition table` is also stored in the single-node CeresDB catalog. One idea is to abstract a catalog implementation with TTL capability and store the partitioned table inside it, updating the `partition table` periodically.
Drawback: Table alteration operations are not currently frequent. If regular expiration operations for `partition table` are carried out due to alter table reasons, it can be costly.

Active notification: Introduce Ceresmeta. After the alter table is completed, notify Ceresmeta, which asynchronously notifies all CeresDB nodes of the schema change.
Drawback: Introducing Ceresmeta makes the implementation more complex. Ceresmeta procedure currently does not have persistence functionality, so implementing persistence and retry capabilities is needed.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1248/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1262,https://api.github.com/repos/apache/horaedb/issues/1262,horaedb,1943325938,1262,Distinguish the source call's api of grpc,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,CLOSED,2023-10-14T15:05:12Z,2023-10-16T08:52:22Z,"### Describe This Problem

Different api such as opentsdb are using grpc calls at the bottom, we need to use Context to distinguish the source of grpc in order to perform different operations. For example,in https://github.com/CeresDB/ceresdb/pull/1261  we need to disable the auto create table feature.

### Proposal

Distinguish the source call's api of grpc

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1262/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1262,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5pJM6g,horaedb,1764019872,1262,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-10-16T08:52:21Z,2023-10-16T08:52:21Z,No need for now.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5pJM6g/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1274,https://api.github.com/repos/apache/horaedb/issues/1274,horaedb,1954136255,1274,Golang version mismatch in CI,tisonkun,18818196,tison,wander4096@gmail.com,CLOSED,2023-10-20T11:57:41Z,2023-10-20T12:50:45Z,"### Describe this problem

https://github.com/CeresDB/ceresdb/actions/runs/6587091350/job/17896703324?pr=1272

```
$ make integration-test
...
go: errors parsing go.mod:
/tmp/ceresmeta-src/go.mod:3: invalid go version '1.21.3': must match format 1.23

$ make run-dist-query
...
go: errors parsing go.mod:
/tmp/ceresmeta-src/go.mod:3: invalid go version '1.21.3': must match format 1.23
```

### Server version

master

### Steps to reproduce

CI

### Expected behavior

_No response_

### Additional Information

We may modify Golang version or try to pin the version.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1274/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1274,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5pqC1v,horaedb,1772629359,1274,NA,tisonkun,18818196,tison,wander4096@gmail.com,NA,2023-10-20T12:13:12Z,2023-10-20T12:13:12Z,"Related to https://github.com/golang/go/issues/61888.

Perhaps update https://github.com/ceresdb/ceresmeta","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5pqC1v/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1276,https://api.github.com/repos/apache/horaedb/issues/1276,horaedb,1955288182,1276,Introducing Opendal as our new object_store dependency.,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,CLOSED,2023-10-21T03:50:10Z,2024-10-19T11:12:19Z,"### Describe This Problem

The introduction of [opendal](https://github.com/apache/incubator-opendal) as our object_store dependency has been considered [before](https://github.com/CeresDB/ceresdb/issues/608). We would very much welcome someone from the community who can continue to push this forward.

### Proposal

Replace our current `object_store` dependency with opendal.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1276/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1276,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5pt66x,horaedb,1773645489,1276,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-10-21T03:50:48Z,2023-10-21T03:50:48Z,cc @ShiKaiWi @Rachelint @tisonkun ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5pt66x/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1276,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5p7IbZ,horaedb,1777108697,1276,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-10-24T12:27:14Z,2023-10-24T12:27:14Z,"Some concerns:
1. Will opendal add too many dependencies, if we only use one or two backend?
2. How is its performance? especially s3-like backend.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5p7IbZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1276,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5p_D0a,horaedb,1778138394,1276,NA,tisonkun,18818196,tison,wander4096@gmail.com,NA,2023-10-24T22:23:43Z,2023-10-24T22:23:43Z,"> Will opendal add too many dependencies, if we only use one or two backend?

Most of services (storage backend) is optional. I build the default crate (with s3 bundled) and the result is 

```
$ cargo build --release
$ ls -l target/release/libopendal.d 
-rw-r--r--  1 tison  staff  11398 Oct 25 06:22 target/release/libopendal.d
```

> How is its performance?

I remember @Xuanwo and @Zheaoli once did a performance test. Maybe they'll have more information.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5p_D0a/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1276,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qAMEf,horaedb,1778434335,1276,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-10-25T03:09:22Z,2023-10-25T03:09:22Z,"> Most of services (storage backend) is optional.

👍 

Then we will wait to see how opendal performed compared with object-store.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qAMEf/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1276,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tBSYV,horaedb,1829053973,1276,NA,Xuanwo,5351546,Xuanwo,github@xuanwo.io,NA,2023-11-28T04:30:41Z,2023-11-28T04:30:41Z,"> Then we will wait to see how opendal performed compared with object-store.

We have added a benchmark inside opendal which compared with `aws-sdk-s3`, welcome to take a look and bench on your env: https://github.com/apache/incubator-opendal/tree/main/core/benches/vs_s3

Despite potential variations in results, the conclusion is clear: we are all built on rust and powered by hyper, exhibiting similar performance.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tBSYV/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1276,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6Qd791,horaedb,2423766901,1276,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-10-19T11:12:19Z,2024-10-19T11:12:19Z,Finish in #1557 ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6Qd791/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1279,https://api.github.com/repos/apache/horaedb/issues/1279,horaedb,1959197889,1279,Introduce a new disk-based WAL implementation for standalone deployment,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-10-24T12:58:45Z,2024-10-19T11:25:42Z,"### Describe This Problem

After #1272, we have successfully put different WAL implementation behind feature gates, this is important to reduce compile time since wal based on RocksDB is very slow.

However, we have to enable rocksdb wal by default since we have no reliable WAL implementation, message queue and table-kv introduce more complex, so a better choice is to implement another wal based on disk directly.

### Proposal

The main trait to implement is WalManager
- https://github.com/CeresDB/ceresdb/blob/affa2c65a279252d10475c6a6d201cd7f60b5689/src/wal/src/manager.rs#L320

A simple structure I think of is like this:
```bash
$ tree wal
wal
└──region-1
   ├──f1
   ├──f2
   └──....
```
WAL of different tables is saved together in a fixed-size file(such as 64M), this have the advantage of fast write.

As for deletion, since different tables are saved intertwined, we need to loop all tables to get the minimal sequence location to delete.

The layout of each file could refer prometheus design:
https://github.com/prometheus/prometheus/blob/main/tsdb/docs/format/wal.md

### Additional Context

We will finish this task with help of @dracoooooo at [OSPP](https://summer-ospp.ac.cn/org/prodetail/2444e0563?list=org&navpage=org),  TODOs are listed below:

- [ ] Implement a disk-based WAL that can pass the existing unit tests.
  - [x] write
  - [x] read
  - [x] scan
  - [ ] delete
  - [ ] multiple segments
- [x] Remove `unwarp` and handle errors.
- [x] Add unit tests for the new code.
- [ ] Test on large-scale data.
- [ ] Compare with the existing RocksDB WAL implementation and optimize
performance.
_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1279/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1279,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qBlKf,horaedb,1778799263,1279,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-10-25T08:46:49Z,2023-10-25T08:46:49Z,@jiacai2050 Should the replication of wal be taken into considerations?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qBlKf/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1279,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qIPwq,horaedb,1780546602,1279,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-10-26T07:16:49Z,2023-10-26T07:16:49Z,"Do you mean distributed WAL storage?

If it's, I don't think we need a component like this, kafka works well for this case.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5qIPwq/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1281,https://api.github.com/repos/apache/horaedb/issues/1281,horaedb,1962646610,1281,Reduce the cost of massive stale tables,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-10-26T03:47:18Z,2024-10-19T11:11:26Z,"### Describe This Problem

In the current implementation of the storage engine, one table must occupy some resources even if the table may not be used anymore. And the number of such tables grows larger, the waste of the resources grows too, e.g. slow recover, finding the max memory usage in all the tables.

### Proposal

I could come up two ways to solve this problem:
- One way is to support clear these stale tables by introducing TTL mechanism on the table meta data.
- Another way is to open all the tables lazily to avoid load the stale tables into the memory to manage.

### Additional Context
#1278 is an example caused by this issue.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1281/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1281,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5rvOT0,horaedb,1807541492,1281,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-11-13T06:26:30Z,2023-11-13T06:26:30Z,Opening all the tables lazily is a sound method.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5rvOT0/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1283,https://api.github.com/repos/apache/horaedb/issues/1283,horaedb,1962838203,1283,Better strategy to force flush when the memtable size limit reached,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-10-26T06:57:01Z,2024-10-19T11:34:01Z,"### Describe This Problem

In the current implementation, the table whose memtable consumes most memory will be forced flush if the space's memory usage limit is reached.

The problems include:
- To find the table with hugest memtable is not a trivial thing if the number of the tables is large.
- After the table is found, only this table will be forced flush, that is to say, the space memory usage may just be reduced a little.
- For every write request, the check whether the space memory usage limit is triggered, which costs too much cpu resources if massive tables exist.

### Proposal

Here is a simple proposal to address the problems mentioned above:
- There is no need to find the one table whose memtable is largest, and we can choose a bunch of tables whose memtable is large enough (exceeding half of the table's memtable size limit) to flush.
- There is no need to check the space memory usage every time the write request is executed, and it won't be executed until some requirements are met:
    - It has been a while since last check;
    - It has been a while since last space-level flush if it occurs;


### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1283/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1285,https://api.github.com/repos/apache/horaedb/issues/1285,horaedb,1964687385,1285,Update docker image for more user-friendly usage,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-10-27T03:14:20Z,2024-05-12T00:27:05Z,"### Describe This Problem

If new users try ceresdb with docker image, there is no easy way for them to explore data stored in ceresdb. 

### Proposal

Since ceresdb already support many query protocol, we can include following tools in docker images to help users explore :
- mysql client
- grafana, with ceresdb added as data source by default.

In this way, users can either explore in terminal or web UI.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1285/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1285,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59VPzq,horaedb,2102721770,1285,NA,dracoooooo,55609330,Draco,dracode01@gmail.com,NA,2024-05-09T14:00:47Z,2024-05-09T14:00:47Z,"Hi, I would like to do this task, please assign this issue to me.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59VPzq/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1285,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59VWie,horaedb,2102749342,1285,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-05-09T14:15:42Z,2024-05-09T14:15:42Z,"Thanks, assigned.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59VWie/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1285,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59Zi3p,horaedb,2103848425,1285,NA,dracoooooo,55609330,Draco,dracode01@gmail.com,NA,2024-05-10T04:38:00Z,2024-05-10T04:38:00Z,"Can HoraeDB be used directly as a datasource for Grafana, or is it necessary to install Prometheus first?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59Zi3p/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1285,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59ZlaA,horaedb,2103858816,1285,NA,dracoooooo,55609330,Draco,dracode01@gmail.com,NA,2024-05-10T04:50:45Z,2024-05-10T04:50:45Z,"> Can HoraeDB be used directly as a datasource for Grafana, or is it necessary to install Prometheus first?

Oh I found the doc in [Ecosystem/InfluxDB](https://horaedb.apache.org/en/ecosystem/influxdb.html). [Operation/Observability](https://horaedb.apache.org/en/operation/observability.html#observability) seems to be outdated with typo and broken picture. I will also update this doc later.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59ZlaA/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1288,https://api.github.com/repos/apache/horaedb/issues/1288,horaedb,1971556254,1288,The test case `test_space_write_buffer_size_rocks` often fails,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-11-01T01:57:59Z,2024-10-19T11:11:27Z,"### Describe this problem

The test case `test_space_write_buffer_size_rocks` often fails.

### Server version

46bf739adb89b6e6947f5f29df71c2e865ae532a

### Steps to reproduce

Under the source root directory, execute the command `make test` multiple times and the test mentioned in the title may fail.

### Expected behavior

All the tests always pass.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1288/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1291,https://api.github.com/repos/apache/horaedb/issues/1291,horaedb,1975527086,1291,Tracking Iteration 2023-11,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-11-03T06:25:35Z,2023-12-04T02:48:50Z,"## Ease to use
- [x] Improve the success rate for creating new table @ZuLiangWang @ShiKaiWi 
	- [x] #1286 
	- [x] https://github.com/CeresDB/ceresmeta/pull/264
- [ ] Avoid the primary key duplicate error when use obkv wal implementation @ShiKaiWi  #1205
- [ ] Supports idempotence and retry of table creation and deletion. @ZuLiangWang 

## Performance

### Write
- [ ] Better load balance according to the actual load of ceresdb-servers @ShiKaiWi 
- [x] Reduce the payload size written to the wal by support columnar encoding and compression @ShiKaiWi #1179 
- [ ] Improve the write performance when massive tables exist @ShiKaiWi  #1281 

### Query
- [ ] Separate the resources used by the queries involving different amount of data @jiacai2050 https://github.com/CeresDB/horaedb/pull/1303
- [ ] Cache the arrow payload in the memory or on the disk instead of the parquet payload to reduce the frequent decoding cost @jiacai2050 @Rachelint 
- [ ] Use a read-friendly format for the data in the immutable memtable to reduce the encoding cost during frequent queries @Rachelint 

## Features
- [ ] Support `use <database>` statement @chunshao90 
- [ ] Support analyze sql for distributed query @baojinri 
- [ ] Support query by opentsdb @baojinri https://github.com/CeresDB/horaedb/pull/1284

## Docs
- [ ] Update the docs for some new features, e.g. partition table.
- [ ] Design docs for the distributed query @Rachelint ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1291/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1291,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5q0KE9,horaedb,1792057661,1291,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-11-03T08:44:00Z,2023-11-03T08:44:00Z,"If there are any ideas, feel free to bring them up in this issue.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5q0KE9/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1291,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tieoF,horaedb,1837754885,1291,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-12-04T02:48:49Z,2023-12-04T02:48:49Z,The unfinished tasks will be transfered to the Iterration 2023-12.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tieoF/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1293,https://api.github.com/repos/apache/horaedb/issues/1293,horaedb,1975765884,1293,Refactor write/read error message,ZuLiangWang,11689031,CooooolFrog,coolfrog@apache.org,CLOSED,2023-11-03T09:22:57Z,2024-03-11T06:48:45Z,"### Describe This Problem

CeresDB currently handles user error messages unreasonably. When a shard is in the `frozen` state, the error message when reading or writing the tables in this shard is `Table not found`, which misleads users.s

### Proposal

Refactor error message, when a shard is in the `frozen` state, reading or writing the tables in this shard throw error like `shard state is invalid`

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1293/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1293,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5rVzmf,horaedb,1800878495,1293,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-11-08T01:53:22Z,2023-11-08T01:53:22Z,I plan to fix this misleading error message this week.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5rVzmf/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1293,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM52emso,horaedb,1987734312,1293,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-03-11T06:48:45Z,2024-03-11T06:48:45Z,"https://github.com/apache/incubator-horaedb/pull/1418  add shard status when table is not found, this should be enough for debugging.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM52emso/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1295,https://api.github.com/repos/apache/horaedb/issues/1295,horaedb,1978146193,1295,Memtable scan params are too deep coupling with `skiplist` impl,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-11-06T02:18:37Z,2024-10-19T11:11:27Z,"### Describe This Problem

I found `ScanRequest`, the main scan param is too deep coupling with `skiplist` impl, which exposes the low level concepts like `xxx_key`, `sequence`. As I see, it hurts the extensibility of the interface deeply...

### Proposal

I think somethings like `projection` and `filter` exprs should be placed in `ScanRequest` rather than the too low level concepts only meaning in a specific impl.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1295/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1296,https://api.github.com/repos/apache/horaedb/issues/1296,horaedb,1978925024,1296,Support `database` for namespace isolation,mrrtree,10230091,,,CLOSED,2023-11-06T11:33:57Z,2024-10-19T11:10:56Z,"### Describe This Problem

There is a business scenario where tables with the same name exist, and it is hoped that namespace isolation can be done through `database`.

### Proposal


### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1296/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1296,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5q-byw,horaedb,1794751664,1296,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-11-06T12:52:50Z,2023-11-06T12:52:50Z,"This feature is very useful for some cases, and I'll try to take this ticket.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5q-byw/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1299,https://api.github.com/repos/apache/horaedb/issues/1299,horaedb,1981261270,1299,Read runtime isolation ,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-11-07T12:42:22Z,2024-01-11T02:43:26Z,"### Describe This Problem

Currently all queries go into one shared thread pool, this works for simple case, but when workloads get diversity, one cost query could block other queries from getting executed, which cause RT of all queries become terrible.

### Proposal

Introduce a RuntimeDispatcher, which contains two inner runtime, and users can submit task to it with reference to its priority.

Ex:
```rust
use tokio::runtime::Runtime;

struct RuntimeDispatcher {
    high_priority_runtime: Runtime,
    low_priority_runtime: Runtime,
}

impl RuntimeDispatcher {
    fn new() -> Self {
        Self {
            high_priority_runtime: tokio::runtime::Builder::new_multi_thread()
                .enable_all()
                .build()
                .unwrap(),
            low_priority_runtime: tokio::runtime::Builder::new_multi_thread()
                .enable_all()
                .build()
                .unwrap(),
        }
    }

    fn dispatch(&mut self, task: impl tokio::task::JoinHandle<()>, priority: Priority) {
        match priority {
            Priority::High => self.high_priority_runtime.spawn(task),
            Priority::Low => self.low_priority_runtime.spawn(task),
        }
    }
}

enum Priority {
    High,
    Low,
}

// Example usage:

let mut runtime_dispatcher = RuntimeDispatcher::new();

// Dispatch a high-priority task.
runtime_dispatcher.dispatch(async {}, Priority::High);

// Dispatch a low-priority task.
runtime_dispatcher.dispatch(async {}, Priority::Low);

// Wait for all tasks to finish.
runtime_dispatcher.high_priority_runtime.block_on(async {}).unwrap();
runtime_dispatcher.low_priority_runtime.block_on(async {}).unwrap();

```

### Additional Context

As for how to decide query's priority, we can infer it by following strategy:
1. query time range
2. query slow log
3. Explicit rules set by developers, such as query without `timestamp` will be assigned lower priority.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1299/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1300,https://api.github.com/repos/apache/horaedb/issues/1300,horaedb,1982807677,1300,Parquet argument error: Parquet error: encountered non UTF-8 data ,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,OPEN,2023-11-08T06:00:06Z,2024-02-01T02:01:36Z,"### Describe this problem

This error arise from one of our cluster, it seems we store non utf8 string into a string column.

```
2023-11-08 13:49:55.018 ERRO [analytic_engine/src/compaction/scheduler.rs:544] Failed to compact table, table_name:xxx, table_id:9196, request_id:80914480, err:Failed to write sst, file_path:0/9196/345089.sst, source:Failed to poll record batch, err:Failed to pull record batch, error:Failed to decode record batch, err:Meet a parquet error, err:Arrow: Parquet argument error: Parquet error: encountered non UTF-8 data
Backtrace:
 0 <snafu::backtrace_shim::Backtrace as snafu::GenerateBacktrace>::generate::h54b5cf46dc653acc
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/backtrace_shim.rs:15
   <analytic_engine::sst::reader::error::ParquetError as snafu::IntoError<analytic_engine::sst::reader::error::Error>>::into_error::h26ab79ad83d94414
   /home/db/ceresdb/analytic_engine/src/sst/reader.rs:27
   <core::result::Result<T,E> as snafu::ResultExt<T,E>>::with_context::{{closure}}::h502414bc5134fa38
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/lib.rs:329
   core::result::Result<T,E>::map_err::h8b5aae79bf4eeee7
   /rustc/8550f15e148407159af401e02b1d9259762b3496/library/core/src/result.rs:829
   <core::result::Result<T,E> as snafu::ResultExt<T,E>>::with_context::h5f0d22c6cfc4c9a7
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/snafu-0.6.10/src/lib.rs:327
   analytic_engine::sst::parquet::async_reader::Reader::fetch_record_batch_streams::{{closure}}::{{closure}}::h5032bb1589d30d49
   /home/db/ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:338
   <T as futures_util::fns::FnMut1<A>>::call_mut::h4803b5271a79a10a
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.28/src/fns.rs:28
   <futures_util::stream::stream::map::Map<St,F> as futures_core::stream::Stream>::poll_next::{{closure}}::h82eb17ff05aa3317
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.28/src/stream/stream/map.rs:59
   core::option::Option<T>::map::h3aa96e39d04db90d
   /rustc/8550f15e148407159af401e02b1d9259762b3496/library/core/src/option.rs:1075
   <futures_util::stream::stream::map::Map<St,F> as futures_core::stream::Stream>::poll_next::h266d6f3961d47a67
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.28/src/stream/stream/map.rs:59
 1 <core::pin::Pin<P> as futures_core::stream::Stream>::poll_next::h08e75d419af5e6e2
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.28/src/stream.rs:120
   futures_util::stream::stream::StreamExt::poll_next_unpin::hfb1d654748c8b788
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.28/src/stream/stream/mod.rs:1632
   <analytic_engine::sst::parquet::async_reader::RecordBatchProjector as futures_core::stream::Stream>::poll_next::hd2c15bdab7f57920
   /home/db/ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:518
 2 futures_core::stream::if_alloc::<impl futures_core::stream::Stream for alloc::boxed::Box<S>>::poll_next::hac4396be82a57db3
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-core-0.3.28/src/stream.rs:209
   futures_util::stream::stream::StreamExt::poll_next_unpin::hde73d86315dca273
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.28/src/stream/stream/mod.rs:1632
   <futures_util::stream::stream::next::Next<St> as core::future::future::Future>::poll::h3d1cbbc2b1cd7585
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/futures-util-0.3.28/src/stream/stream/next.rs:32
   analytic_engine::sst::parquet::async_reader::ThreadedReader::read_record_batches_from_sub_reader::{{closure}}::h7c0d12422d5c273b
   /home/db/ceresdb/analytic_engine/src/sst/parquet/async_reader.rs:696
 3 tokio::runtime::task::core::Core<T,S>::poll::{{closure}}::h715e7d77b31c390f
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/core.rs:311
   tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut::h58626b95480bd76a
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/loom/std/unsafe_cell.rs:14
   tokio::runtime::task::core::Core<T,S>::poll::ha524851086f3a884
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/core.rs:300
   tokio::runtime::task::harness::poll_future::{{closure}}::h696b10229d0c843d
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/harness.rs:476
   <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once::h979fc5db0783c4d6
   /rustc/8550f15e148407159af401e02b1d9259762b3496/library/core/src/panic/unwind_safe.rs:271
   std::panicking::try::do_call::hf8e3ba57652ca889
   /rustc/8550f15e148407159af401e02b1d9259762b3496/library/std/src/panicking.rs:524
   std::panicking::try::hd29ae04528e7a837
   /rustc/8550f15e148407159af401e02b1d9259762b3496/library/std/src/panicking.rs:488
   std::panic::catch_unwind::hb6125781bc368ede
   /rustc/8550f15e148407159af401e02b1d9259762b3496/library/std/src/panic.rs:142
   tokio::runtime::task::harness::poll_future::h49d00d55acef36c9
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/harness.rs:464
   tokio::runtime::task::harness::Harness<T,S>::poll_inner::hc56dbf0ca33e1ca3
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/harness.rs:198
   tokio::runtime::task::harness::Harness<T,S>::poll::hc6ac7853754be6ed
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/harness.rs:152
   tokio::runtime::task::raw::poll::ha92596021a564794
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/raw.rs:276
 4 tokio::runtime::task::raw::RawTask::poll::hb45f2fe81691c337
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/raw.rs:200
   tokio::runtime::task::LocalNotified<S>::run::h0536a90178ddee16
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/mod.rs:400
   tokio::runtime::scheduler::multi_thread::worker::Context::run_task::{{closure}}::h7d241c72fbddb052
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/scheduler/multi_thread/worker.rs:576
   tokio::runtime::coop::with_budget::hec65761cece85585
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/coop.rs:107
   tokio::runtime::coop::budget::h0d1a8787eff4d994
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/coop.rs:73
   tokio::runtime::scheduler::multi_thread::worker::Context::run_task::hc185bee783ef0f6a
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/scheduler/multi_thread/worker.rs:575
 5 tokio::runtime::scheduler::multi_thread::worker::Context::run::h58a2f09f23f395c4
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/scheduler/multi_thread/worker.rs:526
   tokio::runtime::scheduler::multi_thread::worker::run::{{closure}}::{{closure}}::h517dc94dae364c92
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/scheduler/multi_thread/worker.rs:491
   tokio::runtime::context::scoped::Scoped<T>::set::h36363213b4e04c2b
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/context/scoped.rs:40
   tokio::runtime::context::set_scheduler::{{closure}}::h37825f5042431ee1
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/context.rs:176
   std::thread::local::LocalKey<T>::try_with::hec479b72c81fd313
   /rustc/8550f15e148407159af401e02b1d9259762b3496/library/std/src/thread/local.rs:270
   std::thread::local::LocalKey<T>::with::he078dc0ac2b82166
   /rustc/8550f15e148407159af401e02b1d9259762b3496/library/std/src/thread/local.rs:246
   tokio::runtime::context::set_scheduler::hc5da922222919a15
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/context.rs:176
   tokio::runtime::scheduler::multi_thread::worker::run::{{closure}}::h779fc6adc01e47ce
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/scheduler/multi_thread/worker.rs:486
   tokio::runtime::context::runtime::enter_runtime::haab5099640c5426f
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/context/runtime.rs:65
   tokio::runtime::scheduler::multi_thread::worker::run::h79384f145d6d8416
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/scheduler/multi_thread/worker.rs:478
 6 tokio::runtime::scheduler::multi_thread::worker::Launch::launch::{{closure}}::h0cbe85cdf2846e78
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/scheduler/multi_thread/worker.rs:447
   <tokio::runtime::blocking::task::BlockingTask<T> as core::future::future::Future>::poll::he547489663e6982f
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/blocking/task.rs:42
   tokio::runtime::task::core::Core<T,S>::poll::{{closure}}::h50c9ca7f694a0561
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/core.rs:311
   tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut::h06d3ff95963a525b
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/loom/std/unsafe_cell.rs:14
   tokio::runtime::task::core::Core<T,S>::poll::h4e7a6528a4c4f978
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/core.rs:300
   tokio::runtime::task::harness::poll_future::{{closure}}::h19a6006bf6f679c6
   /home/db/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.29.1/src/runtime/task/harness.rs:476
   <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once::h74e084ab7cc47154
   /rustc/8550f15e148407159af401e02b1d9259762b3496/library/core/src/panic/unwind_safe.rs:271
   std::panicking::try::do_call::hfd68c80fe0b41e09
   /rustc/8550f15e148407159af401e02b1d9259762b3496/library/std/src/panicking.rs:524
   std::panicking::try::h612ba8841f413452
   /rustc/8550f15e148407159af401e02b1d9259762b3496/library/std/src/panicking.rs:488
   std::panic::catch_unwind::hc305b19dce8f8392
   /rustc/8550f15e148407159af401e02b1d9259762b3496/library/std/src/panic.rs:142
```

### Server version

All version.

### Steps to reproduce

NA

### Expected behavior

If there are non utf8 string, we should throw error when write, not compaction.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1300/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1300,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ydWSh,horaedb,1920296097,1300,NA,dust1,18304424,Yoke,834902408@qq.com,NA,2024-02-01T01:15:49Z,2024-02-01T01:15:49Z,assign me!,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ydWSh/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1301,https://api.github.com/repos/apache/horaedb/issues/1301,horaedb,1982918188,1301,Useless to pull all primary key columns in `append` mode,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-11-08T07:20:34Z,2023-11-08T07:23:10Z,"### Describe This Problem

We pull `projected + pk` columns when querying in both `append` and `overwrite` mode.
However pulling  `pk` columns is totally unnecessary in `append` mode.

### Proposal

Just pulling the `projected` columns in query of `append` mode.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1301/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1302,https://api.github.com/repos/apache/horaedb/issues/1302,horaedb,1982919574,1302,Wasteful to pull all primary key columns in query of `append` mode,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2023-11-08T07:21:38Z,2024-01-11T02:43:25Z,"### Describe This Problem

We pull `projected + pk` columns when querying in both `append` and `overwrite` mode.
However pulling  `pk` columns is totally unnecessary in `append` mode.

### Proposal

Just pulling the `projected` columns in query of `append` mode.

The record batch reading steps will be divided to following threes:
+ Pulling the `ArrowRecordBatch`.
+ Converting it to `FetchingRecordBatch`, somethings like filling not exist column a null/default value will be done here.
`FetchingRecordBatch` will be used in `ChainIterator`/ `MergeIterator`, and it may include not only the projected columns but the primary key columns for dedupping or else.
+ Prune to `RecordBatch`, as saying above, `FetchingRecordBatch` can include not only projected columns, prune the non-projecteds here.

Main changes:
+ Don't pass the `ProjectedSchema` including too many informations to where building the inner `RecordBatchStream` in `ChainIterator`/ `MergeIterator`. Instead, refactor the `RowProjector` to just include the needed informations and pass it to(mainly `ScanRequest` and `SstReadOptions`).
+ Refactor `RecordBatchWithKey` to `FetchingRecordBatch`, the main difference is that `FetchingRecordBatch` can include `primary_keys_indexes` or not. Actually, `FetchingRecordBatch` should not include `primary_keys_indexes` anymore, but it is hard to remove it completely, so maybe we can delay it to later prs.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1302/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1308,https://api.github.com/repos/apache/horaedb/issues/1308,horaedb,1993899780,1308,Support split the encoded wal payload in columnar format,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-11-15T02:14:21Z,2024-10-19T11:26:43Z,"### Describe This Problem

Although wal payload in columnar format is supported, the feature is not offered to split the payload in columnar format to avoid large wal log. And in some cases, large wal logs may lead to bad peformance.

### Proposal

For the rowwise wal payload, the manual split on the encoded rows helps reduce the wal log's size. However, it's hard to split the columnar payload. Maybe let the wal manager implement to provide the split mechanism is better so that there is no need to manipulate the encoded payload encoded in different format.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1308/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1309,https://api.github.com/repos/apache/horaedb/issues/1309,horaedb,1994121244,1309,Faster to drop table,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-11-15T06:18:01Z,2023-11-23T08:18:58Z,"### Describe This Problem

It's slow to drop a table because flush must be done before real drop.

### Proposal

Actually, there is no need to flush memtable before dropping, and just dropping the memtable may make the dropping table faster.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1309/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1309,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5sXo24,horaedb,1818135992,1309,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-11-20T02:45:12Z,2023-11-20T02:45:12Z,"This issue has been resolved in https://github.com/CeresDB/horaedb/pull/1257, do you still see flush happens when drop table?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5sXo24/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1309,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5st3aJ,horaedb,1823962761,1309,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-11-23T08:18:58Z,2023-11-23T08:18:58Z,"> This issue has been resolved in #1257, do you still see flush happens when drop table?

Umm... I guess the deployed ceresdb-server may be outdated.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5st3aJ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1312,https://api.github.com/repos/apache/horaedb/issues/1312,horaedb,1998810017,1312,cannot access documentation,waldoweng,6841691,Zhao Weng,waldoweng@gmail.com,CLOSED,2023-11-17T10:43:36Z,2023-11-17T10:43:52Z,"### Describe this problem

cannot access [ceresdb.github.io/docs](https://ceresdb.github.io/docs), browser says http 404 not found, site migrated?

### Server version

web

### Steps to reproduce

visit  [ceresdb.github.io/docs](https://ceresdb.github.io/docs) with chrome

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1312/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1319,https://api.github.com/repos/apache/horaedb/issues/1319,horaedb,2014203854,1319,Rename all occurance of CeresDB into HoraeDB,tisonkun,18818196,tison,wander4096@gmail.com,CLOSED,2023-11-28T11:14:32Z,2024-02-27T10:49:40Z,"Let's search and replace it globally where it should be HoraeDB now.

* [x] #1329
* [x] Update all CeresDB URLs after repo transferred","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1319/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1319,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tDck1,horaedb,1829620021,1319,NA,tisonkun,18818196,tison,wander4096@gmail.com,NA,2023-11-28T11:19:02Z,2023-11-28T11:19:02Z,"We won't change the GitHub org name ""CeresDB"" now. It will be done once we transfer the repo.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tDck1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1319,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tD8Is,horaedb,1829749292,1319,NA,tisonkun,18818196,tison,wander4096@gmail.com,NA,2023-11-28T12:30:54Z,2023-11-28T12:30:54Z,Ditto for the docs site [ceresdb.github.io/docs](https://ceresdb.github.io/docs) that binds to to org name. We will change it once we transfer the repo.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tD8Is/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1319,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tKYLd,horaedb,1831437021,1319,NA,tisonkun,18818196,tison,wander4096@gmail.com,NA,2023-11-29T08:30:44Z,2023-11-29T08:30:44Z,"* [x] The first step should be rename ceresdbproto to horaedbproto as it's a root dependency. https://github.com/CeresDB/horaedbproto/pull/109
* [x] Then, rename the usage of ceresdbproto to horaedbproto in ceresdb-client-rs https://github.com/CeresDB/horaedb-client-rs/pull/47
* [x] Now, we can rename the deps in horaedb repo https://github.com/CeresDB/horaedb/pull/1324.
* [x] Let's take a look at Golang repos now. horaedb-client-go and horaemeta should rename ceres occurrences and then we can change the references in this repo.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tKYLd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1319,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tRzkK,horaedb,1833384202,1319,NA,tisonkun,18818196,tison,wander4096@gmail.com,NA,2023-11-30T09:22:17Z,2023-11-30T09:22:17Z,Also the Maven artifact's group name `io.ceresdb` will be changed once transfer happen.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tRzkK/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1319,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tkjH0,horaedb,1838297588,1319,NA,tisonkun,18818196,tison,wander4096@gmail.com,NA,2023-12-04T10:43:53Z,2023-12-04T10:43:53Z,"All ""ceres"" occurances updated. Except the TODO items in the issue description that we will handle later.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tkjH0/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1319,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5uFWQO,horaedb,1846895630,1319,NA,tematou,35472988,,,NA,2023-12-08T10:04:09Z,2023-12-08T10:04:09Z,"![image](https://github.com/CeresDB/horaedb/assets/35472988/00e17283-380f-41de-8245-9d2013edf18f)
被京东收购了？","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5uFWQO/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1319,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5uFbDX,horaedb,1846915287,1319,NA,tisonkun,18818196,tison,wander4096@gmail.com,NA,2023-12-08T10:19:23Z,2023-12-08T10:19:23Z,"@tematou This is a coincident name conflict while the post you refer to should be years ago.

You can take a look at [this proposal](https://cwiki.apache.org/confluence/display/INCUBATOR/HoraeDB+Proposal) that HoraeDB (this project) is going to enter the ASF incubator.

I'll hide your comment as Off Topic and Resolved.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5uFbDX/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1319,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vrzJF,horaedb,1873752645,1319,NA,tisonkun,18818196,tison,wander4096@gmail.com,NA,2024-01-02T08:56:00Z,2024-01-02T08:56:00Z,Most of the tasks are done. Closing...,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vrzJF/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1329,https://api.github.com/repos/apache/horaedb/issues/1329,horaedb,2021805692,1329,"Rolling update from ""CeresDB"" binaries and libraries to the new ""HoraeDB"" ones",tisonkun,18818196,tison,wander4096@gmail.com,CLOSED,2023-12-02T02:07:10Z,2024-02-27T10:49:40Z,"We should discuss about how the existing CeresDB provisions and libraries can be rolling updated to the new ""HoraeDB"" ones.

For example, we may duplicate the service `CeresmetaRpcService` with `HoraeMetaRpcService`. Release a version that contains both of these two endpoint, and a new one that only the `HoraeMetaRpcService` exists.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1329/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1329,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tim8-,horaedb,1837788990,1329,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-12-04T03:39:02Z,2023-12-04T03:39:02Z,"This indeed need a careful plan to discuss how to do the migration.

As for now, I create a dev branch as the starting point(3bf05ff7e), we have clusters running with this version, so it should be stable for use and have no breaking changes. 

We plan all features-related PR to be merged into dev branch, main no longer accept those PR, main is only used for merge PR about changing names and other breaking changes.

Later when we go into asf incubator, we merge dev into main, make a release, and document how to do the upgrade.

Hope this proess doesn't break anything.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tim8-/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1329,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tiobD,horaedb,1837795011,1329,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-12-04T03:48:20Z,2023-12-04T03:48:20Z,"# Breaking changes in main
- Etcd default root path change from `/ceresdb` to `/horaedb` https://github.com/CeresDB/horaedb/pull/1330#issuecomment-1837334176
- Default field in promql change from `__ceresdb_field` to `__horaedb_field__` https://github.com/CeresDB/horaedb/pull/1340#discussion_r1413383500","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tiobD/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1329,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5uFVkT,horaedb,1846892819,1329,NA,tematou,35472988,,,NA,2023-12-08T10:02:22Z,2023-12-08T10:02:22Z,"![image](https://github.com/CeresDB/horaedb/assets/35472988/baa37980-090f-482c-9acb-134aac07286f)
被京东收购了？","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5uFVkT/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1329,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vry-p,horaedb,1873751977,1329,NA,tisonkun,18818196,tison,wander4096@gmail.com,NA,2024-01-02T08:55:16Z,2024-01-02T08:55:16Z,@chunshao90 @jiacai2050 @tanruixiang do we have a resolution now? Maybe we can close this issue with the resolution. Or we'll have a plan to finish it.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vry-p/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1329,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vvt5K,horaedb,1874779722,1329,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2024-01-03T02:33:06Z,2024-01-03T02:33:06Z,"> @chunshao90 @jiacai2050 @tanruixiang do we have a resolution now? Maybe we can close this issue with the resolution. Or we'll have a plan to finish it.

I've been dealing with it recently and expect it to be resolved within a week.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vvt5K/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1329,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM51MwvQ,horaedb,1966279632,1329,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-02-27T10:49:40Z,2024-02-27T10:49:40Z,"This https://github.com/apache/incubator-horaedb/pull/1455 list what are required to upgrade to latest main branch.

During the update process, there will be errors between horaedb-server and horaemeta, but should be fine when all servers updated.

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM51MwvQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1333,https://api.github.com/repos/apache/horaedb/issues/1333,horaedb,2022912575,1333,Tracking Iteration 2023-12,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-12-04T02:51:06Z,2024-01-02T02:36:19Z,"## Ease to use
- [x] Avoid the primary key duplicate error when use obkv wal implementation @ShiKaiWi  #1205 (The problem has been fixed in a work around way #1362)
- [ ] Supports idempotence and retry of table creation and deletion. @ZuLiangWang 

## Performance

### Write
- [ ] Split WAL to OBKV to avoid single large log or log request @ShiKaiWi #1313 
- [ ] Support table migration @chunshao90 

### Query
- [x] Separate the resources used by the queries involving different amount of data @jiacai2050 https://github.com/CeresDB/horaedb/pull/1303
- [ ] Cache the arrow payload in the memory or on the disk instead of the parquet payload to reduce the frequent decoding cost @jiacai2050 @Rachelint 
- [x] Use a read-friendly format for the data in the immutable memtable to reduce the encoding cost during frequent queries @Rachelint 
- [x] Avoid abusing dictionary encoding @ShiKaiWi #1365

## Features
- [x] Support analyze sql for distributed query @baojinri 
- [ ] Support query by opentsdb @baojinri https://github.com/CeresDB/horaedb/pull/1284","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1333/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1339,https://api.github.com/repos/apache/horaedb/issues/1339,horaedb,2023036486,1339,Local and CI license checking and formatting use the same tools.,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,CLOSED,2023-12-04T05:05:50Z,2023-12-07T01:44:56Z,"### Describe This Problem

Now Ci  uses HawkEye to check license, but precommit use https://github.com/Lucas-C/pre-commit-hooks to format license.

can refer to the discussion in https://github.com/CeresDB/horaedb/pull/1336

### Proposal

Local and CI license checking and formatting use the same tools.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1339/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1339,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ti3Bn,horaedb,1837854823,1339,NA,ananta,12180395,Ananta Bastola,hi@anntz.com,NA,2023-12-04T05:12:08Z,2023-12-04T05:12:08Z,@tanruixiang I would love to work on this!,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ti3Bn/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1339,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ti3xh,horaedb,1837857889,1339,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-12-04T05:16:07Z,2023-12-04T05:16:07Z,"> @tanruixiang I would love to work on this!

@ananta  Thank you for your attention. Feel free to do that. Have fun! Note that dicussion in https://github.com/CeresDB/horaedb/pull/1336 is important information.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ti3xh/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1342,https://api.github.com/repos/apache/horaedb/issues/1342,horaedb,2023212955,1342,feat：align README documents in both English and Chinese,caicancai,77189278,Cancai Cai,,CLOSED,2023-12-04T07:35:46Z,2023-12-22T09:00:01Z,"### Describe This Problem

The README document and the REAME-CN content are not aligned.

### Proposal

I think we should align the README English and Chinese documents

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1342/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1342,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tkQYd,horaedb,1838220829,1342,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-12-04T10:09:16Z,2023-12-04T10:09:16Z,"Thank you for your suggestions. The English version is expected to undergo a major revision in the coming weeks, so we can wait until the English version is revised and aligned.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tkQYd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1342,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tkRKC,horaedb,1838224002,1342,NA,caicancai,77189278,Cancai Cai,,NA,2023-12-04T10:11:09Z,2023-12-04T10:11:09Z,"> Thank you for your suggestions. The English version is expected to undergo a major revision in the coming weeks, so we can wait until the English version is revised and aligned.

Thank you for your reply. If you need help, I can provide it","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tkRKC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1342,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tkSiF,horaedb,1838229637,1342,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-12-04T10:13:50Z,2023-12-04T10:13:50Z,"> Thank you for your reply. If you need help, I can provide it

Thank you for your help. I have assigned that task to you. I will call you in this ISSUE when the English version is revised.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tkSiF/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1342,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tkdUp,horaedb,1838273833,1342,NA,caicancai,77189278,Cancai Cai,,NA,2023-12-04T10:40:35Z,2023-12-04T10:40:35Z,"@tanruixiang Hello，I found that README's docker link is wrong because horaedb-server doesn't exist in docker hub yet, and it's still ceresdb-server before, and I'm not sure if I want to fix it.
https://hub.docker.com/r/ceresdb/ceresdb-server","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tkdUp/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1342,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tkdde,horaedb,1838274398,1342,NA,caicancai,77189278,Cancai Cai,,NA,2023-12-04T10:40:56Z,2023-12-04T10:40:56Z,cc @tisonkun ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tkdde/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1342,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tkmK4,horaedb,1838310072,1342,NA,tisonkun,18818196,tison,wander4096@gmail.com,NA,2023-12-04T10:44:58Z,2023-12-04T10:44:58Z,"> because horaedb-server doesn't exist in docker hub yet

Let's hold a bit after repo transfer happened and new release happened.

You can go back here then.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tkmK4/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1342,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tkpFL,horaedb,1838321995,1342,NA,caicancai,77189278,Cancai Cai,,NA,2023-12-04T10:46:01Z,2023-12-04T10:46:01Z,"> > because horaedb-server doesn't exist in docker hub yet
> 
> Let's hold a bit after repo transfer happened and new release happened.
> 
> You can go back here then.

thank you.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tkpFL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1342,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tkrx_,horaedb,1838333055,1342,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-12-04T10:46:57Z,2023-12-04T10:46:57Z,"> @tanruixiang Hello，I found that README's docker link is wrong because horaedb-server doesn't exist in docker hub yet, and it's still ceresdb-server before, and I'm not sure if I want to fix it. https://hub.docker.com/r/ceresdb/ceresdb-server

Related with https://github.com/CeresDB/horaedb/issues/1319#issuecomment-1829620021. You can check out that issue for more context.
Anything related to the org name should need to wait for the org transfer.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tkrx_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1342,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tk4Uz,horaedb,1838384435,1342,NA,caicancai,77189278,Cancai Cai,,NA,2023-12-04T10:54:29Z,2023-12-04T10:54:29Z,"> > @tanruixiang Hello，I found that README's docker link is wrong because horaedb-server doesn't exist in docker hub yet, and it's still ceresdb-server before, and I'm not sure if I want to fix it. https://hub.docker.com/r/ceresdb/ceresdb-server
> 
> Related with [#1319 (comment)](https://github.com/CeresDB/horaedb/issues/1319#issuecomment-1829620021). You can check out that issue for more context. Anything related to the org name should need to wait for the org transfer.

Thank you. I'll go look at the code first.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5tk4Uz/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1342,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vN3F5,horaedb,1865904505,1342,NA,caicancai,77189278,Cancai Cai,,NA,2023-12-21T09:08:09Z,2023-12-21T09:08:09Z,@tanruixiang @tisonkun  I noticed that there was a pr who did this part of my work,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vN3F5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1350,https://api.github.com/repos/apache/horaedb/issues/1350,horaedb,2029815249,1350,;;,tematou,35472988,,,CLOSED,2023-12-07T03:32:57Z,2023-12-08T10:16:06Z,,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1350/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1350,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5t7FiS,horaedb,1844205714,1350,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-12-07T03:40:43Z,2023-12-07T03:40:43Z,"It seems you haven't installed protobuf-compiler? You can install dependencies with `make dev-setup` if you are on macOS or ubuntu.
- https://github.com/CeresDB/horaedb/blob/da7c5a126949fc385fd78e57e4f2ce3fb3d19c2f/Makefile#L107","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5t7FiS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1350,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5t8btd,horaedb,1844558685,1350,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-12-07T05:45:33Z,2023-12-07T05:45:33Z,"Closing, feel free to open new issue if you still have questions.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5t8btd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1353,https://api.github.com/repos/apache/horaedb/issues/1353,horaedb,2030037451,1353,;;,tematou,35472988,,,CLOSED,2023-12-07T06:58:19Z,2023-12-08T10:14:58Z,,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1353/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1356,https://api.github.com/repos/apache/horaedb/issues/1356,horaedb,2030204726,1356,;;,tematou,35472988,,,CLOSED,2023-12-07T08:46:42Z,2023-12-08T10:15:15Z,,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1356/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1359,https://api.github.com/repos/apache/horaedb/issues/1359,horaedb,2032331106,1359,;;,tematou,35472988,,,CLOSED,2023-12-08T09:48:02Z,2023-12-08T10:16:22Z,,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1359/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1360,https://api.github.com/repos/apache/horaedb/issues/1360,horaedb,2032331722,1360,;;,tematou,35472988,,,CLOSED,2023-12-08T09:48:27Z,2023-12-08T10:16:50Z,,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1360/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1361,https://api.github.com/repos/apache/horaedb/issues/1361,horaedb,2032339985,1361,;;,tematou,35472988,,,CLOSED,2023-12-08T09:52:53Z,2023-12-08T10:17:03Z,,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1361/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1361,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5uFXO1,horaedb,1846899637,1361,NA,ZuLiangWang,11689031,CooooolFrog,coolfrog@apache.org,NA,2023-12-08T10:07:03Z,2023-12-08T10:07:03Z,"Please refer to the document link in Github. The official website document below is not the document of HoraeDB, it is the document of the CeresDB commercial version.

请参照 Github 中的文档链接，下面的官网文档并非 HoraeDB 的文档，它是 CeresDB 商业版的文档。","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5uFXO1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1373,https://api.github.com/repos/apache/horaedb/issues/1373,horaedb,2047848896,1373,[DISCUSS][VOTE]: Some breaking changes resulting from the renaming of `CeresDB` to `HoraeDB`,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-12-19T03:27:35Z,2023-12-26T11:55:16Z,"### Describe This Problem

Due to the renaming of `CeresDB` to `Horaedb`, some breaking changes have occurred.
Refer to #1319



### Proposal

#### Plan 1: Compatibility Upgrade
Support for separate upgrades of Horaedb and Horaemeta, with version compatibility.
For instance, if there are incompatible service interfaces in `horaedbproto`, one could define two service names, `HoraeMeta` and `HoraeDB`, and implement two sets of services for compatibility.

Disadvantages: Compatibility upgrades are quite costly and would result in a lot of temporary code.

#### Plan 2: Incompatible Upgrade
`HoraeDB` releases a new major version that is not compatible with historical versions, users need to upgrade `HoraeDB` and `HoraeMeta` at same time. As a result, the service will be unavailable for longer time than plan 1.


### Additional Context

If you're using CeresDB in production or interested in this project, please share your ideas about which plan you prefer.

You can use :+1: in comments below to express your choice. 

> Note: The vote will be lasting for one week(2023-12-25).","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1373/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1373,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5u_LDl,horaedb,1862054117,1373,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-12-19T03:35:17Z,2023-12-19T03:35:17Z,1,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5u_LDl/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1373,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5u_LEK,horaedb,1862054154,1373,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2023-12-19T03:35:21Z,2023-12-19T03:35:21Z,2,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5u_LEK/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1373,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5u_aPl,horaedb,1862116325,1373,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-12-19T04:54:31Z,2023-12-19T04:54:31Z,"Once the mailbox and transfer are set up, it is recommended to use either the `dev@horaedb.apache.org` or `github.com/apache/incubator-horaedb/discussions` for discussion or voting.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5u_aPl/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1373,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vaf4W,horaedb,1869217302,1373,NA,tisonkun,18818196,tison,wander4096@gmail.com,NA,2023-12-26T03:03:23Z,2023-12-26T03:03:23Z,Shall we conclude now?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vaf4W/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1373,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vbh6w,horaedb,1869487792,1373,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2023-12-26T11:54:35Z,2023-12-26T11:54:35Z,Conclusion: we will adopt `Plan 2: Incompatible Upgrade`.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vbh6w/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1374,https://api.github.com/repos/apache/horaedb/issues/1374,horaedb,2048982175,1374,Modify the repo about's docs website,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,CLOSED,2023-12-19T16:12:15Z,2023-12-20T12:49:43Z,"### Describe This Problem

I'don't have the right to modify the repo about, I'm not sure if it's because it's just been migrated and therefore the permissions aren't in effect. It seems to be a bit of a nuisance for users.
https://github.com/apache/incubator-horaedb-docs/issues/111

### Proposal

Modify the repo about

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1374/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1379,https://api.github.com/repos/apache/horaedb/issues/1379,horaedb,2050210313,1379,Flush multiple memtables into one sst,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-12-20T09:52:10Z,2024-10-19T11:26:09Z,"### Describe This Problem

Currently, one memtable is flushed as one sst, and a batch of small sst files will be generated if multiple memtables are chosen to flush.

### Proposal

Flush multiple memtables into one sst file.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1379/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1380,https://api.github.com/repos/apache/horaedb/issues/1380,horaedb,2050273379,1380,feat: update license,caicancai,77189278,Cancai Cai,,CLOSED,2023-12-20T10:29:45Z,2023-12-20T16:19:21Z,"### Describe This Problem

Should we update the license for all rust files？


### Proposal

```rust
// Copyright 2023 The HoraeDB Authors
//
// Licensed under the Apache License, Version 2.0 (the ""License"");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an ""AS IS"" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//! Build script
```
to

```rust
// Licensed under the Apache License, Version 2.0 (the ""License"");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an ""AS IS"" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//! Build script
```

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1380/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1380,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vJTZI,horaedb,1864709704,1380,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-12-20T15:43:43Z,2023-12-20T15:43:43Z,"Thanks to attention, we need the asf's license, it's different from the version before.
```
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vJTZI/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1381,https://api.github.com/repos/apache/horaedb/issues/1381,horaedb,2050404870,1381,Missing metrics for object store `put_multipart`,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-12-20T11:54:54Z,2024-10-19T11:10:57Z,"### Describe This Problem

The metric for the write throughput of object `put_multipart` is missing.

### Proposal

Any `AsyncWrite` trait object is returned by the object store's `put_multipart`, based on which the metric about the throughput is not a trival thing to add. A new adapter for counting the number of put bytes implementing `AsyncWrite` is needed.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1381/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1386,https://api.github.com/repos/apache/horaedb/issues/1386,horaedb,2051747064,1386,Improve the readability of error messages,ZuLiangWang,11689031,CooooolFrog,coolfrog@apache.org,CLOSED,2023-12-21T05:57:21Z,2024-01-12T01:50:14Z,"### Describe This Problem

Currently, the error information in HoraeDB is unclear (for example, In many cases, users will see the error `table not found`.). When an error occurs, it confuses users and increases the difficulty of troubleshooting and locating problems.

### Proposal

Make error messages clearer, classify error messages, and even consider designing error codes to classify error messages in the form of error codes.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1386/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1386,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vOLD6,horaedb,1865986298,1386,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-12-21T10:07:39Z,2023-12-21T10:07:39Z,May be we can make a example and split tasks into modules and track them. cc @ShiKaiWi ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vOLD6/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1386,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5wjPwu,horaedb,1888287790,1386,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-01-12T01:50:13Z,2024-01-12T01:50:13Z,Fixed in #1418,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5wjPwu/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1392,https://api.github.com/repos/apache/horaedb/issues/1392,horaedb,2052096790,1392,Release all clients' version,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,CLOSED,2023-12-21T09:53:26Z,2024-12-18T04:40:21Z,"### Describe This Problem

We need new version to replace something like `import "" github.com/CeresDB/horaedb-client-go/horaedb""`

### Proposal

- [ ] https://github.com/apache/incubator-horaedb-client-java
- [x] https://github.com/apache/incubator-horaedb-client-rs
- [ ] https://github.com/apache/incubator-horaedb-client-go
- [x] https://github.com/apache/incubator-horaedb-client-py


### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1392/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1392,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vOGmO,horaedb,1865968014,1392,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-12-21T09:54:41Z,2023-12-21T09:54:41Z,related https://github.com/apache/incubator-horaedb/pull/1387,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vOGmO/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1392,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6YAtGk,horaedb,2550321572,1392,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-12-18T04:40:21Z,2024-12-18T04:40:21Z,"Python/Rust client are released, the others are in maintaining status, so have no plan to release them.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6YAtGk/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1393,https://api.github.com/repos/apache/horaedb/issues/1393,horaedb,2052458902,1393,Docker images publish,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,CLOSED,2023-12-21T13:49:04Z,2023-12-22T04:38:13Z,"### Describe This Problem

publish docker images to ghcr

### Proposal

see title

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1393/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1393,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vPTwR,horaedb,1866284049,1393,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2023-12-21T13:49:23Z,2023-12-21T13:49:23Z,https://github.com/apache/incubator-horaedb/actions/runs/7288666957/job/19861668939 seems have no right. cc @chunshao90 ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vPTwR/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1397,https://api.github.com/repos/apache/horaedb/issues/1397,horaedb,2053307684,1397,Missing soft limit on the space's mem table usage,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2023-12-22T03:02:29Z,2024-10-19T11:10:57Z,"### Describe This Problem

There is no soft limit over the space/instance's memtable usage in the current write procedure, and the following write requests must be blocked/rejected once the hard limit is reached. In conclusion, write errors will occur immediately when the hard limit of the instance/space's memtable is reached.

### Proposal

Introduce the soft limit over the space/instance's memtable usage, and once it is reached, the flush of the largest memtables in the space/instance should be triggered, and the flush should not block the write procedure. Only the hard limit is reached, the following write requests should be rejected.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1397/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1397,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vZBrr,horaedb,1868831467,1397,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2023-12-25T07:52:57Z,2023-12-25T07:52:57Z,I'll take this ticket.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vZBrr/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1405,https://api.github.com/repos/apache/horaedb/issues/1405,horaedb,2056218478,1405,APPROX_PERCENTILE_CONT is not supported in distributed table,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2023-12-26T09:23:19Z,2024-10-19T11:10:57Z,"### Describe this problem

Query contains APPROX_PERCENTILE_CONT function will throw errors.

### Server version

main

### Steps to reproduce

```sql
CREATE TABLE `demo` (
    `name` string TAG,
    `value` double NOT NULL,
    `t` timestamp NOT NULL,
    timestamp KEY (t))
PARTITION BY RANDOM PARTITIONS 4
ENGINE=Analytic
  with
(enable_ttl=""false"");

INSERT INTO demo (t, name, value)
    VALUES (1651737067000, ""ceresdb"", 100)
    ,(1651737067000, ""ceresdb1"", 110)
    ,(1651737067000, ""ceresdb2"", 120);
;
 analyze select time_bucket(t, ""PT1M"") as a,  approx_percentile_cont(value, 0.9) as b from   demo
where name = ""ceresdb""
and t >= 1651737067000 
group by time_bucket(t, ""PT1M"")
;
```

### Expected behavior

No error.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1405/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1411,https://api.github.com/repos/apache/horaedb/issues/1411,horaedb,2059025754,1411,[DISCUSS]: Roadmap 2024,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2023-12-29T03:19:28Z,2024-11-22T07:07:38Z,"Here are some of the features I have planned for 2024. Everyone is welcome to discuss.

### Distributed Solution
- Meta Procedure Module 
	- Refine operations such as retries and rollbacks for different procedures.
- Automated Load Balancing Based on Real Load 
	- Implement dynamic load information collection mechanisms, with horaedb uploading real load data. 
	- Incorporate advanced scheduling algorithms to balance load distribution based on actual load conditions.
- Increase chaos testing to validate system robustness.

### Query Optimization
- Optimize PQL by supporting operator pushdown to reduce data retrieval costs.
- Add SST-level sorted keys to improve data-fetching efficiency during queries.
- Pre-Aggregation and Pre-Downsample.

### Storage Optimization
- Improve query performance for both high and low cardinality by enhancing storage formats
	- Introduce inverted indexes. 
	- Organize data along timelines. 
	- Implement a columnar storage Memtable.
- Compaction Offload
	- Offload compaction tasks to dedicated nodes or services to minimize the impact on real-time query nodes.
- Import opendal.

### Other
- horaecontrol cluster command-line tool, supporting functions such as automated deployment, monitoring integration, and fault diagnosis.
- Optimize the code directory structure.

### Need to discuss
~~- Evaluate the feasibility of reimplementing horaemeta with Rust to unify the development language and improve development efficiency.~~
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1411/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1411,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vrH3q,horaedb,1873575402,1411,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2024-01-02T02:32:06Z,2024-01-02T02:32:06Z,I guess Pre-Aggregation and Pre-Downsample is also an attractive feature to help improve the performance of the queries involving massive data points. ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vrH3q/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1411,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vw160,horaedb,1875074740,1411,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2024-01-03T09:32:38Z,2024-01-03T09:32:38Z,"- import opendal
- file structure simplify","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5vw160/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1411,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM56-e1p,horaedb,2063199593,1411,NA,kcrazy,1505797,,,NA,2024-04-18T07:25:09Z,2024-04-18T07:25:09Z,Support Windows system.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM56-e1p/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1411,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59S6xz,horaedb,2102111347,1411,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-05-09T07:28:35Z,2024-05-09T07:28:35Z,"> Support Windows system.

Hi, would you give us more background about your user case? 

We haven't test horaedb on Windows since there is no one asking for this feature. 
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59S6xz/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1411,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59TGpM,horaedb,2102159948,1411,NA,kcrazy,1505797,,,NA,2024-05-09T08:03:25Z,2024-05-09T08:03:25Z,"> > Support Windows system.
> 
> Hi, would you give us more background about your user case?
> 
> We haven't test horaedb on Windows since there is no one asking for this feature.

Although my production environment is Linux, my development and testing environments are on Windows. 
Sometimes I also perform tasks on Windows. 

If there is a native support version for Windows, it would be more convenient to use, and this is also a factor I consider in whether to use horaedb.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59TGpM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1411,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59TigO,horaedb,2102274062,1411,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-05-09T09:13:37Z,2024-05-09T09:13:37Z,"Thanks, that's sounds reasonable for me.

In theory, IMO horaedb don't use any platform specific features, so it may work on Windows, but I can't ensure that, I think we need someone to try build/run on windows, would you like to give it a try? I create a tracking issue [here](https://github.com/apache/incubator-horaedb/issues/1527).

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59TigO/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1411,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59UtwS,horaedb,2102582290,1411,NA,kcrazy,1505797,,,NA,2024-05-09T12:37:44Z,2024-05-09T12:37:44Z,"> Thanks, that's sounds reasonable for me.
> 
> In theory, IMO horaedb don't use any platform specific features, so it may work on Windows, but I can't ensure that, I think we need someone to try build/run on windows, would you like to give it a try? I create a tracking issue [here](https://github.com/apache/incubator-horaedb/issues/1527).

ok","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59UtwS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1411,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6UmHND,horaedb,2493018947,1411,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-11-22T07:07:38Z,2024-11-22T07:07:38Z,Closing since it's stale.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6UmHND/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1413,https://api.github.com/repos/apache/horaedb/issues/1413,horaedb,2060927544,1413,Who is using HoraeDB,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,OPEN,2023-12-31T03:42:22Z,2024-01-12T01:49:49Z,"### Who is using HoraeDB?
Thank you for using HoraeDB, we will try our best to make HoraeDB better and the community more active.
### Motivation of this issue
- We hope to attract more developers to the HoraeDB community.
- We want more voices from the community to make HoraeDB better!
- We want to polish HoraeDB in different scenarios.
- We want to understand the community's requests for new features.


### What we expect from you

Please submit a comment under this issue or just submit a pr. The following information needs to be included:
- Company name and logo (for presentation purposes)
- (Optional) Your personal information, such as e-mail address, city, etc.
- (Optional) What you would like to see happen next in the community, such as new features（also can discuss in #1411）.

Thank you very much for your participation. Your participation will make HoraeDB community a better place.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1413/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1424,https://api.github.com/repos/apache/horaedb/issues/1424,horaedb,2064910023,1424,Tracking Issue: Automated load balancing based on real load,ZuLiangWang,11689031,CooooolFrog,coolfrog@apache.org,CLOSED,2024-01-04T02:49:38Z,2024-10-19T09:40:36Z,"### Describe This Problem

In the roadmap https://github.com/apache/incubator-horaedb/issues/1411 we discussed recently, we need to implement load balancing based on real load this year, rather than simply scheduling based on the number of shards and tables.

Currently, all our balancing strategies are based on the number of shards and tables. However, the number of shards and tables cannot reflect the actual load of the node. In the presence of hotspot tables, our current load balancing schedule cannot avoid excessive load on some nodes.

### Proposal

Design a load balancing strategy based on real load scheduling.

### Collection and reporting of real loads.
- Define the indicators that HoraeDB reports to HoraeMeta.
  - These indicators must be able to reflect the actual load of the current shard and node. 
  - Collect this indicators, the cost of collecting these indicators needs to be small enough so that it does not affect the performance of the service.
 

### Scheduling based on real load
- Design a scheduling strategy based on real load in HoreaMeta
  - The scheduling strategy must be compatible and coexist with the existing scheduling based on the number of shards and tables.
  - The automatic scheduling strategy should be simple and safe enough, this is even more important than the effectiveness of scheduling.


### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1424/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1433,https://api.github.com/repos/apache/horaedb/issues/1433,horaedb,2071892602,1433,Introduce a compatibility test in the workflows before merging,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2024-01-09T08:44:30Z,2024-12-18T04:41:54Z,"### Describe This Problem

After #1271 is merged, compiled binary is not compactible with the data used by old version's one. And the reason is that the storage format is updated, but there is no test for the compatibility.

### Proposal

Introduce a compatibility test which follows such steps:
1. Compile the binary from the latest main branch and run a batch of write and query requests;
2. Apply the change set of the PR and compile a new binary, run the new one over the old data and do some write/query tests

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1433/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1434,https://api.github.com/repos/apache/horaedb/issues/1434,horaedb,2071918456,1434,Tracking Iteration 2024-01,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2024-01-09T09:00:25Z,2024-11-25T09:35:50Z,"## Ease to use
- [x] Supports idempotence and retry of table creation and deletion. @ZuLiangWang 
- [ ] #1424 @ZuLiangWang @ShiKaiWi 
- [x] horaecontrol cluster command-line tool @chunshao90 

## Performance

### Query
- [ ] Cache the arrow payload in the memory or on the disk instead of the parquet payload to reduce the frequent decoding cost @jiacai2050 @Rachelint 
- [ ] Memory cache for object store with new eviction strategy to improve the performance of the query involving large data @Rachelint 
- [ ] Upgrade datafusion, arrow, parquet to latest version @jiacai2050  @tanruixiang 
- [x] #1441 

## Features
- [x] Preparations for compaction offloading @Rachelint 

## Others
- [x] Merge the `dev` branch into the `main` branch and resolve all the compatibility problems for next version release @ShiKaiWi ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1434/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1438,https://api.github.com/repos/apache/horaedb/issues/1438,horaedb,2081172983,1438,Statistics about the shards' loads and instance's load,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2024-01-15T03:20:11Z,2024-01-26T07:29:31Z,"### Describe This Problem

The statistics about the shards' loads and instance's load is necessary to address #1424.

### Proposal

- Instance load statistics
	- cpu usage
	- memory usage
- Shards' load statistics:
	- written bytes
	- read bytes

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1438/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1441,https://api.github.com/repos/apache/horaedb/issues/1441,horaedb,2081586173,1441,Query improvement about the query partition table,ShiKaiWi,8605990,WEI Xikai,,CLOSED,2024-01-15T09:15:51Z,2024-11-25T09:35:49Z,"### Describe This Problem

Considering the query targeting at a partition table whose hash partition key is called `partition_col`:
`select * from partition_table where partition_col in (""a"", ""b"", ""c"", ...)`.

And all the sub query plans share the same predicate, and if the inlist is large, the `min-max` and `bloom-filter` index may exhibit a very bad performance. However, actually, most of the values in the inlist don't exist at one specific partition, that is to say, the predicate in the sub query plan can be simplified into a more simple one.

### Proposal

Introduce an optimization procedure to remove the unnecessary values in the in-list predicate of the distributed sub query plan.

More implementation details are necessary before coding.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1441/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1441,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6SH5wy,horaedb,2451545138,1441,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-11-01T08:59:42Z,2024-11-01T08:59:42Z,@zealchen Are you interested in this?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6SH5wy/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1441,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6SY2Ag,horaedb,2455986208,1441,NA,zealchen,20197724,NeoChen,neochen428@gmail.com,NA,2024-11-05T00:38:03Z,2024-11-05T00:38:03Z,"> @zealchen Are you interested in this?

Yes. Let me handle it.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6SY2Ag/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1447,https://api.github.com/repos/apache/horaedb/issues/1447,horaedb,2089775686,1447,assertion failed: stats.cpu_usage > 0.0,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2024-01-19T07:01:30Z,2024-01-22T02:09:42Z,"### Describe this problem

CI failed
```
---- tests::test_normal_case stdout ----
thread 'tests::test_normal_case' panicked at src/components/system_stats/src/lib.rs:132:9:
assertion failed: stats.cpu_usage > 0.0
stack backtrace:
   0: rust_begin_unwind
             at /rustc/8550f15e148407159af401e02b1d9259762b3496/library/std/src/panicking.rs:617:5
   1: core::panicking::panic_fmt
             at /rustc/8550f15e148407159af401e02b1d9259762b3496/library/core/src/panicking.rs:67:14
   2: core::panicking::panic
             at /rustc/8550f15e148407159af401e02b1d9259762b3496/library/core/src/panicking.rs:117:5
   3: system_stats::tests::check_system_stats
             at ./src/lib.rs:132:9
   4: system_stats::tests::test_normal_case::{{closure}}
             at ./src/lib.rs:144:9
```


### Server version

https://github.com/apache/incubator-horaedb/actions/runs/7580286355/job/20645860686?pr=1446

### Steps to reproduce

Run CI

### Expected behavior

Pass CI

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1447/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1447,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5xa8c3,horaedb,1902888759,1447,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2024-01-22T02:06:03Z,2024-01-22T02:06:03Z,I'll remove it.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5xa8c3/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1447,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5xa-lR,horaedb,1902897489,1447,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2024-01-22T02:09:42Z,2024-01-22T02:09:42Z,It seems fixed already.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5xa-lR/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1449,https://api.github.com/repos/apache/horaedb/issues/1449,horaedb,2092344983,1449,repo still has CLAassistant from pre-transfer legacy,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,CLOSED,2024-01-21T02:17:42Z,2024-11-12T08:13:56Z,"### Describe this problem
New contributor still need to sign CLA.
https://github.com/apache/incubator-horaedb/pull/1448

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1449/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1449,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5xZYzR,horaedb,1902480593,1449,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2024-01-21T02:19:45Z,2024-01-21T02:19:45Z,"@chunshao90 please check it, if it can't be solved  by ppmc, we need to create a new issue in infra.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5xZYzR/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1449,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5x8I5d,horaedb,1911590493,1449,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2024-01-26T07:12:21Z,2024-01-26T07:12:21Z,https://issues.apache.org/jira/browse/INFRA-25431?filter=-2,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5x8I5d/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1449,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6TNw9F,horaedb,2469859141,1449,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-11-12T08:13:56Z,2024-11-12T08:13:56Z,New PR show CLA is gone.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6TNw9F/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1461,https://api.github.com/repos/apache/horaedb/issues/1461,horaedb,2099487963,1461,thread 'read-high' panicked 'assertion failed: max.total_cmp(&min).is_ge()',jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2024-01-25T03:21:33Z,2024-02-23T07:06:47Z,"### Describe this problem

Found this panic in one of our deployment
```
2024-01-25 11:05:29.726 ERRO [src/components/panic_ext/src/lib.rs:57] thread 'read-high' panicked 'assertion failed: max.total_cmp(&min).is_ge()' at ""/home/db/.cargo/git/checkouts/arrow-datafusion-b9eb4f789f8bda1f/9c3a537/datafusion/physical-expr/src/aggregate/tdigest.rs:613""
```



### Server version

```
HoraeDB Server 
Version: 1.2.6-alpha
Git commit: 1a452f2c4
Git branch: main
Opt level: 3
Rustc version: 1.74.0-nightly
Target: x86_64-unknown-linux-gnu
Build date: 2024-01-23T07:20:04.728479053Z
```

### Steps to reproduce

I guess this error is caused by  `approx_percentile_cont` function, and depend on data distribution. 

I will try to find a demo to reproduce this soon.

### Expected behavior

No panic.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1461/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1466,https://api.github.com/repos/apache/horaedb/issues/1466,horaedb,2101510163,1466,Tracking issue for fix TODO comments,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2024-01-26T03:16:22Z,2024-03-12T09:06:54Z,"### Describe This Problem

Hi community, 

Our codebase contains numerous 200+ TODOs now! This indicates significant technical debt. Ignoring this will inevitably cause severe code rot.

As a community, we must acknowledge this problem affects our collective interests. While specific individuals may own portions of the code, we share responsibility for the overall health of our ecosystem.

The time to act is now. Let's work together to strengthen the foundation of our codebase before decay spreads further. Our future depends on it.

### Proposal

I will begin this task by review, list the most easy TODOs first, anyone interested are welcome to help us fixing them.


### Additional Context

1. [x] https://github.com/apache/incubator-horaedb/blob/63de16701c9f8614b1c7a52b15b9d2b8cbc3be59/src/benchmarks/src/merge_sst_bench.rs#L170
2. [x] https://github.com/apache/incubator-horaedb/blob/63de16701c9f8614b1c7a52b15b9d2b8cbc3be59/src/components/future_ext/src/retry.rs#L24
3. [x] https://github.com/apache/incubator-horaedb/blob/63de16701c9f8614b1c7a52b15b9d2b8cbc3be59/src/components/tracing_util/src/logging.rs#L129
4. [x] https://github.com/apache/incubator-horaedb/blob/63de16701c9f8614b1c7a52b15b9d2b8cbc3be59/src/table_engine/src/partition/rule/key.rs#L240
5. [x] https://github.com/apache/incubator-horaedb/blob/63de16701c9f8614b1c7a52b15b9d2b8cbc3be59/src/components/parquet_ext/src/prune/min_max.rs#L91
6. [x] https://github.com/apache/incubator-horaedb/blob/63de16701c9f8614b1c7a52b15b9d2b8cbc3be59/src/analytic_engine/src/instance/wal_replayer.rs#L377
7. [x] https://github.com/apache/incubator-horaedb/blob/63de16701c9f8614b1c7a52b15b9d2b8cbc3be59/src/analytic_engine/src/manifest/details.rs#L379","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1466/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1466,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5x7hDx,horaedb,1911427313,1466,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-01-26T03:40:51Z,2024-01-26T03:40:51Z,"To avoid conflicts with others, please lease leave an comment below to tell us which TODO you're going to fix.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5x7hDx/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1466,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ywCZI,horaedb,1925195336,1466,NA,Lethannn,49628648,Lethannn,,NA,2024-02-03T07:30:31Z,2024-02-03T07:30:31Z,"Working on ""TODO: Replay logs of different tables in parallel. ""","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5ywCZI/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1466,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM50mgPd,horaedb,1956250589,1466,NA,zealchen,20197724,NeoChen,neochen428@gmail.com,NA,2024-02-21T09:39:51Z,2024-02-21T09:39:51Z,"Working on ""TODO: make level configurable ""
Working on ""TODO: add backoff ""","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM50mgPd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1466,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM51lkOs,horaedb,1972781996,1466,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-03-01T08:59:03Z,2024-03-01T08:59:03Z,"@Lethannn  It's has been quiet for a while, I wonder if there are any problems when you implement this task?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM51lkOs/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1466,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM51l1y7,horaedb,1972853947,1466,NA,Lethannn,49628648,Lethannn,,NA,2024-03-01T09:44:27Z,2024-03-01T09:44:27Z,"> @Lethannn It's has been quiet for a while, I wonder if there are any problems when you implement this task?

Apologies for the silence lately. I've been dealing with some health issues over the past few weeks :( . Planning to tackle the task this weekend","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM51l1y7/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1466,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM51weVq,horaedb,1975641450,1466,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-03-04T03:39:23Z,2024-03-04T03:39:23Z,"> I've been dealing with some health issues over the past few weeks

Sorry to hear that, let me know if you need any help.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM51weVq/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1466,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM52Zztv,horaedb,1986476911,1466,NA,zealchen,20197724,NeoChen,neochen428@gmail.com,NA,2024-03-08T21:56:54Z,2024-03-08T21:56:54Z,"Working on ""TODO: props should be an argument ""  and ""use NonZeroUsize ""","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM52Zztv/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1466,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM52ra56,horaedb,1991093882,1466,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-03-12T09:06:35Z,2024-03-12T09:06:35Z,"Tasks are all finished, thanks everyone involved.

Will open an new issue to track more TODO.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM52ra56/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1468,https://api.github.com/repos/apache/horaedb/issues/1468,horaedb,2103470119,1468,Build error: failed to run custom build command for `librocksdb_sys v0.1.0 ,YxYL6125,91076160,YxYL,,CLOSED,2024-01-27T09:23:37Z,2024-02-27T10:43:06Z,"
when I clone the repo and run `cargo c` it occured :
error: failed to run custom build command for `librocksdb_sys v0.1.0 (https://github.com/tikv/rust-rocksdb.git?rev=f04f4dd8eacc30e67c24bc2529a6d9c6edb85f8f#f04f4dd8)`

---
### env
OS: EndeavourOS Linux x86_64
Kernel: 6.7.1-arch1-1

By the way I'v installed clang cmake llvm","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1468/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1468,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5yEtIp,horaedb,1913836073,1468,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2024-01-29T01:58:13Z,2024-01-29T01:58:13Z,@YxYL6125 Could you provide more details about the error message?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5yEtIp/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1468,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5yE8ZF,horaedb,1913898565,1468,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-01-29T03:26:28Z,2024-01-29T03:26:28Z,"Also please tell us which llvm, cmake version you are using.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5yE8ZF/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1468,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5yIWdy,horaedb,1914791794,1468,NA,YxYL6125,91076160,YxYL,,NA,2024-01-29T14:20:42Z,2024-01-29T14:20:42Z,"> Also please tell us which llvm, cmake version you are using.

cmake version 3.28.1
llvm version 16.0.6","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5yIWdy/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1468,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5yIbC7,horaedb,1914810555,1468,NA,YxYL6125,91076160,YxYL,,NA,2024-01-29T14:30:16Z,2024-01-29T14:30:16Z,"> @YxYL6125 Could you provide more details about the error message?
```
  --- stderr
     Entering             /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/third-party/gtest-1.8.1/fused-src/gtest
     Called from: [1]   /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/CMakeLists.txt
     Returning to         /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb
     Called from: [1]   /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/CMakeLists.txt
     Entering             /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/tools
     Called from: [1]   /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/CMakeLists.txt
     Returning to         /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb
     Called from: [1]   /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/CMakeLists.txt
  make[3]: 警告：文件“CMakeFiles/rocksdb.dir/build.make”的修改时间在未来 28524 秒后
  make[3]: 警告：检测到时钟错误。您的构建版本可能是不完整的。
  make[3]: 警告：文件“CMakeFiles/rocksdb.dir/build.make”的修改时间在未来 28524 秒后
  In file included from /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/monitoring/perf_flag.cc:1:
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/include/rocksdb/perf_flag.h:11:15: 错误：found ‘:’ in nested-name-specifier, expected ‘::’
     11 | enum PerfFlag : uint32_t {
        |               ^
        |               ::
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/include/rocksdb/perf_flag.h:11:6: 错误：‘PerfFlag’未声明
     11 | enum PerfFlag : uint32_t {
        |      ^~~~~~~~
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/include/rocksdb/perf_flag.h:11:26: 错误：expected unqualified-id before ‘{’ token
     11 | enum PerfFlag : uint32_t {
        |                          ^
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/include/rocksdb/perf_flag.h:118:31: 错误：‘PerfFlag’在此作用域中尚未声明
    118 | using PerfFlags = std::bitset<PerfFlag::COUNT>;
        |                               ^~~~~~~~
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/include/rocksdb/perf_flag.h:118:46: 错误：模板第 1 个参数无效
    118 | using PerfFlags = std::bitset<PerfFlag::COUNT>;
        |                                              ^
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/include/rocksdb/perf_flag.h:120:1: 错误：‘PerfFlags’不是一个类型名
    120 | PerfFlags NewPerfFlags(std::initializer_list<PerfFlag> l);
        | ^~~~~~~~~
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/include/rocksdb/perf_flag.h:121:20: 错误：‘PerfFlag’ was not declared in this scope; did you mean ‘ckPerfFlag’?
    121 | bool CheckPerfFlag(PerfFlag flag);
        |                    ^~~~~~~~
        |                    CheckPerfFlag
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/include/rocksdb/perf_flag.h:122:1: 错误：‘PerfFlags’不是一个类型名
    122 | PerfFlags GetPerfFlags();
        | ^~~~~~~~~
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/include/rocksdb/perf_flag.h:123:6: 错误：变量或字段‘SetPerfFlags’声明为 void
    123 | void SetPerfFlags(PerfFlags flags);
        |      ^~~~~~~~~~~~
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/include/rocksdb/perf_flag.h:123:19: 错误：‘PerfFlags’在此作用域中尚未声明
    123 | void SetPerfFlags(PerfFlags flags);
        |                   ^~~~~~~~~
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/monitoring/perf_flag.cc:6:10: 错误：‘PerfFlags’不是一个类型名
      6 | __thread PerfFlags perf_flags = {};
        |          ^~~~~~~~~
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/monitoring/perf_flag.cc:11:1: 错误：‘PerfFlags’不是一个类型名
     11 | PerfFlags NewPerfFlags(std::initializer_list<PerfFlag> l) {
        | ^~~~~~~~~
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/monitoring/perf_flag.cc:19:6: 错误：‘bool rocksdb::CheckPerfFlag’ 重定义
     19 | bool CheckPerfFlag(PerfFlag flag) { return perf_flags.test(flag); }
        |      ^~~~~~~~~~~~~
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/include/rocksdb/perf_flag.h:121:6: 附注：‘bool rocksdb::CheckPerfFlag’ previously defined here
    121 | bool CheckPerfFlag(PerfFlag flag);
        |      ^~~~~~~~~~~~~
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/monitoring/perf_flag.cc:19:20: 错误：‘PerfFlag’ was not declared in this scope; did you mean ‘CheckfFlag’?
     19 | bool CheckPerfFlag(PerfFlag flag) { return perf_flags.test(flag); }
        |                    ^~~~~~~~
        |                    CheckPerfFlag
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/monitoring/perf_flag.cc:21:1: 错误：‘PerfFlags’不是一个类型名
     21 | PerfFlags GetPerfFlags() { return perf_flags; }
        | ^~~~~~~~~
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/monitoring/perf_flag.cc:23:6: 错误：变量或字段‘SetPerfFlags’声明为 void
     23 | void SetPerfFlags(PerfFlags flags) { perf_flags = flags; }
        |      ^~~~~~~~~~~~
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/monitoring/perf_flag.cc:23:19: 错误：‘PerfFlags’在此作用域中尚未声明
     23 | void SetPerfFlags(PerfFlags flags) { perf_flags = flags; }
        |                   ^~~~~~~~~
  make[3]: *** [CMakeFiles/rocksdb.dir/build.make:1885：CMakeFiles/rocksdb.dir/monitoring/perf_flag.cc.o] 错误 1
  make[3]: *** 正在等待未完成的任务....
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/db/db_impl/db_impl.cc: In member function ‘virtual rocksdb::Status rocksdb::DBImpl::FlushWAL(bool)’:
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/db/db_impl/db_impl.cc:1349:23: 错误：redundant move in return statement [-Werror=redundant-move]
   1349 |       return std::move(io_s);
        |              ~~~~~~~~~^~~~~~
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/db/db_impl/db_impl.cc:1349:23: 附注：remove ‘std::move’ call
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/db/db_impl/db_impl.cc:1353:23: 错误：redundant move in return statement [-Werror=redundant-move]
   1353 |       return std::move(io_s);
        |              ~~~~~~~~~^~~~~~
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/db/db_impl/db_impl.cc:1353:23: 附注：remove ‘std::move’ call
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/db/db_impl/db_impl.cc: In member function ‘virtual rocksdb::Status rocksdb::DBImpl::LockWAL()’:
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/db/db_impl/db_impl.cc:1470:19: 错误：redundant move in return statement [-Werror=redundant-move]
   1470 |   return std::move(status);
        |          ~~~~~~~~~^~~~~~~~
  /home/yxyl/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/db/db_impl/db_impl.cc:1470:19: 附注：remove ‘std::move’ call
  cc1plus：所有的警告都被当作是错误
  make[3]: *** [CMakeFiles/rocksdb.dir/build.make:555：CMakeFiles/rocksdb.dir/db/db_impl/db_impl.cc.o] 错误 1
  make[2]: *** [CMakeFiles/Makefile2:142：CMakeFiles/rocksdb.dir/all] 错误 2
  make[1]: *** [CMakeFiles/Makefile2:149：CMakeFiles/rocksdb.dir/rule] 错误 2
  make: *** [Makefile:172：rocksdb] 错误 2
  thread 'main' panicked at /home/yxyl/.cargo/registry/src/rsproxy.cn-0dccff568467c15b/cmake-0.1.50/src/lib.rs:1098:5:

  command did not execute successfully, got: exit status: 2

  build script failed, must exit now
  note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
warning: build failed, waiting for other jobs to finish...
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5yIbC7/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1468,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5yNB3U,horaedb,1916018132,1468,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-01-30T03:41:23Z,2024-01-30T03:41:23Z,"Hi, it's seems you are using Linux, I usually use gcc8 to compile, would you mind try this?

Or you can disable rocksdb with `make build-wal-message-queue`, this will use kafka as WAL implementation, this should be enough for local development.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5yNB3U/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1468,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5yNE5C,horaedb,1916030530,1468,NA,YxYL6125,91076160,YxYL,,NA,2024-01-30T03:59:06Z,2024-01-30T03:59:06Z,"> Hi, it's seems you are using Linux, I usually use gcc8 to compile, would you mind try this?
> 
> Or you can disable rocksdb with `make build-wal-message-queue`, this will use kafka as WAL implementation, this should be enough for local development.
> 

tks, I'll try it later❤","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5yNE5C/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1468,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM51MuGT,horaedb,1966268819,1468,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-02-27T10:43:05Z,2024-02-27T10:43:05Z,Closed since there is nothing we need to fix in this issue.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM51MuGT/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1469,https://api.github.com/repos/apache/horaedb/issues/1469,horaedb,2104549375,1469,EnforceSorting caused by Internal error: Children cannot be replaced in Scan Table,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2024-01-29T03:10:42Z,2024-10-19T11:09:02Z,"### Describe this problem

This error arise when bump datafusion

### Server version

```
HoraeDB Server 
Version: 1.2.6-alpha
Git commit: b09ae7ef9
Git branch: bump_datafusion49
Opt level: 0
Rustc version: 1.74.0-nightly
Target: aarch64-apple-darwin
Build date: 2024-01-29T03:02:11.755552000Z
```

### Steps to reproduce

Run integration tests.

### Expected behavior

No error

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1469/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1473,https://api.github.com/repos/apache/horaedb/issues/1473,horaedb,2109210059,1473,Object store get range support `0..` syntax,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2024-01-31T03:27:24Z,2024-10-19T09:40:37Z,"### Describe This Problem

S3 support set range without explicitly end like `start..`, but object_store crate don't support, we should support it in upstream first.


### Proposal

- Implement this in upstream first, then in horaedb.

### Additional Context

- https://github.com/aws/aws-sdk-java/issues/1551
- https://github.com/apache/incubator-horaedb/pull/1440#issuecomment-1903576439


","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1473/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1479,https://api.github.com/repos/apache/horaedb/issues/1479,horaedb,2112038275,1479,`-h` causes horaectl to be unavailable,baojinri,52273009,鲍金日,baojinri@apache.org,CLOSED,2024-02-01T09:45:18Z,2024-04-02T11:52:41Z,"### Describe this problem

When we use horaectl, once the -h command is used, the original command will be executed as the -h command.
![3B73581F-D460-4AE7-BA6F-55507BB2CD5F](https://github.com/apache/incubator-horaedb/assets/52273009/b0547d5c-d08a-449a-a445-11b9d13ca7d7)


### Server version

Horaectl

### Steps to reproduce

See above

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1479/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1479,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM55G2s2,horaedb,2031840054,1479,NA,baojinri,52273009,鲍金日,baojinri@apache.org,NA,2024-04-02T11:52:40Z,2024-04-02T11:52:40Z,"Since horaectl is developed using the rust language, this problem no longer exists","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM55G2s2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1480,https://api.github.com/repos/apache/horaedb/issues/1480,horaedb,2114189154,1480,Tracking issue for compaction offload,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2024-02-02T06:30:27Z,2024-07-17T16:22:59Z,"### Describe This Problem

We found in production that the speed of sst compaction is unable to keep up with the speed of sst generation, leading to poor query performance... However we are unable give more resource to compaction to solve the problem because query/write is more important than compaction in the same node.
It is really hard to do a trade-off about resource allocation among query, write and compaction in lsm model... We want to compact the generated small ssts as fast as possible, but we can't tolerate its influence to query/write. And finally I think offload the compaction to the seperated nodes may be the key for it.


### Proposal

For supporting compaction offload, we need:

- Special node supporting remote compaction service

  - [ ] Impl compaction service

- Horaedb node supports submitting the real compaction node to remote

  - [x] Refactor the compaction process and define necessary traits
  - [ ] Impl remote mode compactor based on traits above

- Horaemeta supports managing the special compaction nodes
  
  - [ ] Impl the ability to manage the compaction nodes
  - [ ] Expose the api for horaedb node to get the proper remote compaction ndoe

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1480/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1498,https://api.github.com/repos/apache/horaedb/issues/1498,horaedb,2181080473,1498,Replay WAL of different tables concurrently for TableBasedReplay,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2024-03-12T09:04:12Z,2024-03-25T11:52:07Z,"### Describe This Problem

After #1492, ShardBasedReplay already support concurrent replay, it's better to add this for TableBasedReplay.

### Proposal

See above.

### Additional Context

https://github.com/apache/incubator-horaedb/blob/3064cb3d20077189050c14eb448660cfb7111536/src/analytic_engine/src/instance/wal_replayer.rs#L189","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1498/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1498,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM52rmJf,horaedb,1991139935,1498,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-03-12T09:23:09Z,2024-03-12T09:23:09Z,@zealchen ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM52rmJf/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1501,https://api.github.com/repos/apache/horaedb/issues/1501,horaedb,2187593355,1501,Improve log readability for horaemeta,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2024-03-15T02:21:23Z,2024-12-18T04:41:40Z,"### Describe This Problem

This is what we get when meta log err message:


> 2024-03-15T10:16:16.439+0800    error   procedure/util.go:35    open shard      {""shardID"": 231, ""newLeaderNode"": ""11.69.60.125:8831"", ""error"": ""(#500)event dispatch failed, cause:open shard, addr:11.69.60.125:8831, request:{{231 1 10 0}}, err:fail to open shard, id:231. Caused by: Fail to open shard, msg:Shard is already in opening.\ngithub.com/apache/incubator-horaedb-meta/pkg/coderr.(*codeError).WithCausef\n\t/horaedb/horaemeta/pkg/coderr/error.go:91\ngithub.com/apache/incubator-horaedb-meta/server/coordinator/eventdispatch.(*DispatchImpl).OpenShard\n\t/horaedb/horaemeta/server/coordinator/eventdispatch/dispatch_impl.go:58\ngithub.com/apache/incubator-horaedb-meta/server/coordinator/procedure/operation/transferleader.openNewShardCallback\n\t/horaedb/horaemeta/server/coordinator/procedure/operation/transferleader/transfer_leader.go:272\ngithub.com/looplab/fsm.(*FSM).afterEventCallbacks\n\t/root/go/pkg/mod/github.com/looplab/fsm@v0.3.0/fsm.go:435\ngithub.com/looplab/fsm.(*FSM).Event.func1\n\t/root/go/pkg/mod/github.com/looplab/fsm@v0.3.0/fsm.go:330\ngithub.com/looplab/fsm.transitionerStruct.transition\n\t/root/go/pkg/mod/github.com/looplab/fsm@v0.3.0/fsm.go:375\ngithub.com/looplab/fsm.(*FSM).doTransition\n\t/root/go/pkg/mod/github.com/looplab/fsm@v0.3.0/fsm.go:360\ngithub.com/looplab/fsm.(*FSM).Event\n\t/root/go/pkg/mod/github.com/looplab/fsm@v0.3.0/fsm.go:343\ngithub.com/apache/incubator-horaedb-meta/server/coordinator/procedure/operation/transferleader.(*Procedure).Start\n\t/horaedb/horaemeta/server/coordinator/procedure/operation/transferleader/transfer_leader.go:194\ngithub.com/apache/incubator-horaedb-meta/server/coordinator/procedure/operation/transferleader.(*BatchTransferLeaderProcedure).Start.func1\n\t/horaedb/horaemeta/server/coordinator/procedure/operation/transferleader/batch_transfer_leader.go:115\ngolang.org/x/sync/errgroup.(*Group).Go.func1\n\t/root/go/pkg/mod/golang.org/x/sync@v0.0.0-20210220032951-036812b2e83c/errgroup/errgroup.go:57\nruntime.goexit\n\t/usr/local/go/src/runtime/asm_amd64.s:1650\ngithub.com/apache/incubator-horaedb-meta/pkg/coderr.(*codeError).WithCausef\n\t/horaedb/horaemeta/pkg/coderr/error.go:91\ngithub.com/apache/incubator-horaedb-meta/server/coordinator/eventdispatch.(*DispatchImpl).OpenShard\n\t/horaedb/horaemeta/server/coordinator/eventdispatch/dispatch_impl.go:58\ngithub.com/apache/incubator-horaedb-meta/server/coordinator/procedure/operation/transferleader.openNewShardCallback\n\t/horaedb/horaemeta/server/coordinator/procedure/operation/transferleader/transfer_leader.go:272\ngithub.com/looplab/fsm.(*FSM).afterEventCallbacks\n\t/root/go/pkg/mod/github.com/looplab/fsm@v0.3.0/fsm.go:435\ngithub.com/looplab/fsm.(*FSM).Event.func1\n\t/root/go/pkg/mod/github.com/looplab/fsm@v0.3.0/fsm.go:330\ngithub.com/looplab/fsm.transitionerStruct.transition\n\t/root/go/pkg/mod/github.com/looplab/fsm@v0.3.0/fsm.go:375\ngithub.com/looplab/fsm.(*FSM).doTransition\n\t/root/go/pkg/mod/github.com/looplab/fsm@v0.3.0/fsm.go:360\ngithub.com/looplab/fsm.(*FSM).Event\n\t/root/go/pkg/mod/github.com/looplab/fsm@v0.3.0/fsm.go:343\ngithub.com/apache/incubator-horaedb-meta/server/coordinator/procedure/operation/transferleader.(*Procedure).Start\n\t/horaedb/horaemeta/server/coordinator/procedure/operation/transferleader/transfer_leader.go:194\ngithub.com/apache/incubator-horaedb-meta/server/coordinator/procedure/operation/transferleader.(*BatchTransferLeaderProcedure).Start.func1\n\t/horaedb/horaemeta/server/coordinator/procedure/operation/transferleader/batch_transfer_leader.go:115\ngolang.org/x/sync/errgroup.(*Group).Go.func1\n\t/root/go/pkg/mod/golang.org/x/sync@v0.0.0-20210220032951-036812b2e83c/errgroup/errgroup.go:57\nruntime.goexit\n\t/usr/local/go/src/runtime/asm_amd64.s:1650""}
github.com/apache/incubator-horaedb-meta/server/coordinator/procedure.CancelEventWithLog
        /horaedb/horaemeta/server/coordinator/procedure/util.go:35
github.com/apache/incubator-horaedb-meta/server/coordinator/procedure/operation/transferleader.openNewShardCallback
        /horaedb/horaemeta/server/coordinator/procedure/operation/transferleader/transfer_leader.go:273
github.com/looplab/fsm.(*FSM).afterEventCallbacks
        /root/go/pkg/mod/github.com/looplab/fsm@v0.3.0/fsm.go:435
github.com/looplab/fsm.(*FSM).Event.func1
        /root/go/pkg/mod/github.com/looplab/fsm@v0.3.0/fsm.go:330
github.com/looplab/fsm.transitionerStruct.transition
        /root/go/pkg/mod/github.com/looplab/fsm@v0.3.0/fsm.go:375
github.com/looplab/fsm.(*FSM).doTransition
        /root/go/pkg/mod/github.com/looplab/fsm@v0.3.0/fsm.go:360
github.com/looplab/fsm.(*FSM).Event
        /root/go/pkg/mod/github.com/looplab/fsm@v0.3.0/fsm.go:343


### Proposal

Break new line at `\n`

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1501/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1501,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM53JD8P,horaedb,1998864143,1501,NA,ShiKaiWi,8605990,WEI Xikai,,NA,2024-03-15T03:13:56Z,2024-03-15T03:13:56Z,"Here are my some ideas about this issue:
- There is no need to include the call stack information in the error message
- Format the call stack when logging","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM53JD8P/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1502,https://api.github.com/repos/apache/horaedb/issues/1502,horaedb,2191108913,1502,Tracking issue for horaectl features,Rachelint,34352236,kamille,kamille@apache.org,CLOSED,2024-03-18T02:45:25Z,2024-11-12T08:13:17Z,"### Describe This Problem

Collect needed features for horaectl here.
## Horaemeta side

- [ ] Support listing all stale table metas(generated from table creation failure)

## Horaedb side

- [ ] Support cleaning flush failure status

### Proposal

Build them into horaectl.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1502/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1502,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM544jEZ,horaedb,2028089625,1502,NA,irenjj,122664327,,renj.jiang@gmail.com,NA,2024-03-30T14:34:50Z,2024-03-30T14:34:50Z,can i take Horaedb side task?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM544jEZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1502,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM546bji,horaedb,2028583138,1502,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2024-03-31T07:16:14Z,2024-03-31T07:16:14Z,"> can i take Horaedb side task?

Sure, feel free to take it.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM546bji/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1502,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6TNwsA,horaedb,2469858048,1502,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-11-12T08:13:16Z,2024-11-12T08:13:16Z,"It's has been silent for a while, closing.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6TNwsA/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1506,https://api.github.com/repos/apache/horaedb/issues/1506,horaedb,2208253814,1506,Build failed ,Shylock-Hg,33566796,shylock,,CLOSED,2024-03-26T13:28:13Z,2024-10-19T09:39:20Z,"### Describe this problem

```
 /home/shylock/.cargo/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/f04f4dd/librocksdb_sys/rocksdb/db/db_impl/db_impl.cc:1353:23: error: redundant move in return statement [-Werror=redundant-move]
   1353 |       return std::move(io_s);
```

### Server version

44caa997e37991e570741c3da6c961563569da42

### Steps to reproduce

1. Build with 

```
c (SUSE Linux) 13.2.1 20240206 [revision 67ac78caf31f7cb3202177e6428a46d829b70f23]
Copyright (C) 2023 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```
Or maybe any new c++ compiler.

### Expected behavior

Build success.

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1506/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1506,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM54gejM,horaedb,2021779660,1506,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-03-27T01:55:22Z,2024-03-27T01:55:22Z,"> Or maybe any new c++ compiler.

Yes, rocksdb require a latest C++ compiler, gcc 8 or clang 16 should work.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM54gejM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1506,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM54gguJ,horaedb,2021788553,1506,NA,Shylock-Hg,33566796,shylock,,NA,2024-03-27T02:06:39Z,2024-03-27T02:06:39Z,"> > Or maybe any new c++ compiler.
> 
> Yes, rocksdb require a latest C++ compiler, gcc 8 or clang 16 should work.

My compiler is `gcc-13.2.1`","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM54gguJ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1506,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM54giFE,horaedb,2021794116,1506,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-03-27T02:13:39Z,2024-03-27T02:13:39Z,"Your version seems too new, try downgrade, or you can try build rust-rocksdb directly
https://github.com/tikv/rust-rocksdb

if it works against latest commit, we can bump our dependency.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM54giFE/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1506,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6QdxIt,horaedb,2423722541,1506,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-10-19T09:38:39Z,2024-10-19T09:38:39Z,"Rocksdb is not need any more, we switch to the in-house WAL based on local disk already, so this issue also disappear.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6QdxIt/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1512,https://api.github.com/repos/apache/horaedb/issues/1512,horaedb,2236508938,1512,benchmark failed,zealchen,20197724,NeoChen,neochen428@gmail.com,CLOSED,2024-04-10T22:05:42Z,2024-10-19T09:37:38Z,"### Describe this problem

Run benchmark with default config failed. Err msg is like:

1. Missing field.
Failed to parse config.: ParseToml { path: ""/Users/chenmian.cm/Projects/incubator-horaedb/src/benchmarks/config/bench.toml"", source: Error { inner: Error { inner: TomlError { message: ""missing field `num_rows_per_row_group`"", original: Some(""# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \""License\""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n[sst_bench]\nstore_path = \""/path/to/data/1/1\""\nsst_file_name = \""37.sst\""\nruntime_thread_num = 1\nbench_measurement_time = \""30s\""\nbench_sample_size = 30\nmax_projections = 5\nread_batch_row_num = 8192\nis_async = false\nreverse = false\n\n[sst_bench.predicate]\n# start_time_ms = 0\nstart_time_ms = 1632985200000\n# end_time_ms = 0\nend_time_ms = 1632985800000\n\n[merge_sst_bench]\nstore_path = \""/path/to/data\""\nspace_id = 1\ntable_id = 1\nsst_file_ids = [ 34, 37 ]\nruntime_thread_num = 1\nbench_measurement_time = \""30s\""\nbench_sample_size = 30\nmax_projections = 5\nread_batch_row_num = 500\nsst_level = 0\n\n[merge_sst_bench.predicate]\nstart_time_ms = 0\n# start_time_ms = 1632985200000\nend_time_ms = 0\n# end_time_ms = 1632985800000\n\n[scan_memtable_bench]\nstore_path = \""/path/to/data/1/1\""\nsst_file_name = \""37.sst\""\nruntime_thread_num = 1\nmax_projections = 5\narena_block_size = \""64M\""\n\n[merge_memtable_bench]\nstore_path = \""/path/to/data\""\nspace_id = 1\ntable_id = 1\nsst_file_ids = [ 37 ]\nruntime_thread_num = 1\nmax_projections = 5\narena_block_size = \""64M\""\n\n[wal_write_bench]\nbench_measurement_time = \""60s\""\nbench_sample_size = 60\nbatch_size = 512\nvalue_size = 1024\n\n[replay_bench]\nbench_measurement_time = \""3s\""\nbench_sample_size = 10""), keys: [""sst_bench""], span: Some(786..1011) } } }, backtrace: Backtrace(   0: backtrace::backtrace::libunwind::trace

2. Invalid config.
thread 'main' panicked at src/benchmarks/src/parquet_bench.rs:49:83:
called `Result::unwrap()` on an `Err` value: Generic { store: ""LocalFileSystem"", source: UnableToCanonicalize { path: ""/path/to/data/1/1"", source: Os { code: 2, kind: NotFound, message: ""No such file or directory"" } } }

### Server version

HoraeDB Server 
Version: 2.0.0
Git commit: cfbbe3d3
Git branch: feat_replay_benchmark
Opt level: 0
Rustc version: 1.77.0-nightly
Target: x86_64-apple-darwin
Build date: 2024-04-10T07:38:04.708239000Z

### Steps to reproduce

ANALYTIC_BENCH_CONFIG_PATH=/path/to/incubator-horaedb/src/benchmarks/config/bench.toml cargo bench --bench bench -p benchmarks -- bench_parquet

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1512/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1512,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM569bsa,horaedb,2062924570,1512,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-04-18T03:26:56Z,2024-04-18T03:26:56Z,"```
UnableToCanonicalize { path: ""/path/to/data/1/1"", s
```

It seems you need to update config to set it to a real path.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM569bsa/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1513,https://api.github.com/repos/apache/horaedb/issues/1513,horaedb,2239316985,1513,Explore new error define pattern,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,OPEN,2024-04-12T07:44:25Z,2024-04-18T03:15:01Z,"### Describe This Problem

Currently, we rely on snafu to define/handle errors, there are some issues with current usage.

First, let me explain how error is defined within horaedb.
- Every module has an an `Enum`-style error, which contains detailed error information
- When convert error from one module to another, we use `context` to do the heavy work.

This sounds like a good design pattern, but it become very verbose in practice. Some example:
- Every module force an `Enum`-style error, which contains too many unnecessary item.
- When consume error from module A in module B, we usually box them, since we don't want to define too many error item in module B. This make the error opaque again...


All in all, enum based error design should be limited in some way.

### Proposal

Error details should be opaque, if caller want to program based on it, the error provide a `pub fn kind() -> ErrorKind`.

In this way, we can better to encapsulate error details, and make error small.

### Additional Context

- https://matklad.github.io/2020/10/15/study-of-std-io-error.html
- https://www.unwoundstack.com/blog/rust-error-handling.html","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1513/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1515,https://api.github.com/repos/apache/horaedb/issues/1515,horaedb,2249655426,1515,Explore new serialization library used for communication between horaedb-server,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2024-04-18T03:22:23Z,2024-08-28T09:00:22Z,"### Describe This Problem

Our [RemoteEngineService](https://github.com/apache/incubator-horaedb/blob/55bbe273ab4a09e2f3428582ca9b4cdb5ee88763/src/server/src/grpc/remote_engine_service/mod.rs#L924) use gRPC to communicate between different servers, and adopt protobuf as encode/decode methods, protobuf is not a very high performance library, and we observe high CPU usage in our production env.

### Proposal

Explore new serialization library, which support following features:
- Zero copy, which means its wire format is the same as memory layout.
- Native rust binding
- Support integration with [tonic](https://github.com/hyperium/tonic)

### Additional Context

Some choices:
- https://flatbuffers.dev/
- https://fury.apache.org/docs/start/usage#rust","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1515/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1515,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM569bQ8,horaedb,2062922812,1515,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-04-18T03:24:22Z,2024-04-18T03:24:22Z,@zealchen ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM569bQ8/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1515,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM569m4k,horaedb,2062970404,1515,NA,zealchen,20197724,NeoChen,neochen428@gmail.com,NA,2024-04-18T04:31:01Z,2024-04-18T04:31:01Z,"> @zealchen

Okay. Those libs look super awesome, let me build a benchmark to demonstrate performance first.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM569m4k/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1515,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59JZJm,horaedb,2099614310,1515,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-05-08T02:24:20Z,2024-05-08T02:24:20Z,"[Our benchmarks](https://github.com/CeresDB/serialization-benchmark-rs) show flatbuffers has the best perf, and it can be integrated with tonic, so let's refactor our remote service.

https://github.com/apache/incubator-horaedb/blob/6baf6df0445f085dba4c330e12d019104483f2ee/src/server/src/grpc/remote_engine_service/mod.rs#L950

Currently we can only refactor the `write` API, since the CPU cost of it is the most obvious.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59JZJm/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1519,https://api.github.com/repos/apache/horaedb/issues/1519,horaedb,2250173901,1519,Adapt to a variety of object storage.,ZuLiangWang,11689031,CooooolFrog,coolfrog@apache.org,CLOSED,2024-04-18T09:08:38Z,2024-10-19T09:37:14Z,"### Describe This Problem

Currently, HoraeDB only supports Aliyun OSS and AWS S3 as object storage implementations. We hope to support more object stores to facilitate deployment in various environments. We found that some object storage implementations do not support multipartupload, and currently HoraeDB uses For this interface implementation, we need to replace it with put.

### Proposal

Use object storage interface `put` to simulate `multipartupload`.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1519/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1519,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6CpZsW,horaedb,2191891222,1519,NA,poelzi,66107,Daniel Poelzleithner,,NA,2024-06-26T14:43:29Z,2024-06-26T14:43:29Z,"I suggest using the [object-store](https://crates.io/crates/object-store) crate.
TiKV support would be awesome","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6CpZsW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1519,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6Qdw76,horaedb,2423721722,1519,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-10-19T09:37:14Z,2024-10-19T09:37:14Z,"We already switch opendal for object store access, so this is not required.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6Qdw76/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1525,https://api.github.com/repos/apache/horaedb/issues/1525,horaedb,2265118571,1525,UT of meta sometimes fails,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2024-04-26T07:15:13Z,2024-10-19T11:26:56Z,"### Describe this problem

```bash
--- FAIL: TestWatch (0.82s)
    watch_test.go:68: 
        	Error Trace:	/home/runner/work/incubator-horaedb/incubator-horaedb/horaemeta/server/coordinator/watch/watch_test.go:68
        	Error:      	Not equal: 
        	            	expected: 2
        	            	actual  : 0
        	Test:       	TestWatch
FAIL
coverage: 30.2% of statements
```

### Server version

commit dc38488ff9e540087069feea7d26727a1c942ee8

### Steps to reproduce

Just run go test in meta, this issue is not 100% reproducible.

### Expected behavior

No error.

### Additional Information

https://github.com/apache/incubator-horaedb/actions/runs/8844259737/job/24285819263?pr=1524","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1525/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1527,https://api.github.com/repos/apache/horaedb/issues/1527,horaedb,2287257481,1527,Support running on Windows,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,OPEN,2024-05-09T09:12:56Z,2024-05-16T03:36:09Z,"### Describe This Problem

Background: https://github.com/apache/incubator-horaedb/issues/1411#issuecomment-2102159948

Although Windows is not recommended as production env, it's convenient for testing and develop.

### Proposal

We have two choices:
- Build horaedb in Linux with static link(musl libc), and cross compile to Windows
- Build horaedb in Windows natively

Method 1 maybe simpler than 2, but I little experience here, so welcome any voltenner to tackle this issue in either ways.

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1527/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1527,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59UthA,horaedb,2102581312,1527,NA,kcrazy,1505797,,,NA,2024-05-09T12:37:01Z,2024-05-09T12:37:01Z,"I tried to compile it on Windows, but the compilation failed when it reached [pprof](https://crates.io/crates/pprof) because [pprof ](https://crates.io/crates/pprof)uses the crate nix and libc, which are not supported on Windows.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59UthA/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1527,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59VABU,horaedb,2102657108,1527,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-05-09T13:23:49Z,2024-05-09T13:23:49Z,"Thanks for trying.

I think you can add a feature gate on that crate, and see what happens next.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59VABU/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1527,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59VygX,horaedb,2102863895,1527,NA,kcrazy,1505797,,,NA,2024-05-09T15:15:45Z,2024-05-09T15:15:45Z,"The repository at [rust-rocksdb](https://github.com/tikv/rust-rocksdb) also does not support Windows.

","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59VygX/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1527,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59Y5DA,horaedb,2103677120,1527,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-05-10T01:14:46Z,2024-05-10T01:14:46Z,"> The repository at [rust-rocksdb](https://github.com/tikv/rust-rocksdb) also does not support Windows.

Try build with `make build-wal-message-queue`, this will disable rocksdb and use kafka as wal.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59Y5DA/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1527,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59Y7wv,horaedb,2103688239,1527,NA,kcrazy,1505797,,,NA,2024-05-10T01:28:46Z,2024-05-10T01:28:46Z,"> > The repository at [rust-rocksdb](https://github.com/tikv/rust-rocksdb) also does not support Windows.
> 
> Try build with `make build-wal-message-queue`, this will disable rocksdb and use kafka as wal.

Can we directly use cargo to replace make?
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59Y7wv/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1527,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59ZLVR,horaedb,2103752017,1527,NA,tanruixiang,34568378,Ruixiang Tan,tanruixiang0104@gmail.com,NA,2024-05-10T02:33:44Z,2024-05-10T02:33:44Z,"> > > The repository at [rust-rocksdb](https://github.com/tikv/rust-rocksdb) also does not support Windows.
> > 
> > 
> > Try build with `make build-wal-message-queue`, this will disable rocksdb and use kafka as wal.
> 
> Can we directly use cargo to replace make?

Of course you can, our makefile is very simple, you can refer to the https://github.com/apache/incubator-horaedb/blob/main/Makefile#L40","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59ZLVR/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1527,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59ZUuJ,horaedb,2103790473,1527,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-05-10T03:26:47Z,2024-05-10T03:26:47Z,"
If you're interested in contributing more, welcome to join [our community](https://github.com/apache/incubator-horaedb?tab=readme-ov-file#community).","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59ZUuJ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1527,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59_KGi,horaedb,2113708450,1527,NA,kcrazy,1505797,,,NA,2024-05-16T00:48:19Z,2024-05-16T00:48:19Z,"> If you're interested in contributing more, welcome to join [our community](https://github.com/apache/incubator-horaedb?tab=readme-ov-file#community).

I don't have much experience in developing databases and I'm not particularly interested at the moment. However, I'm open to occasionally testing builds on Windows.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59_KGi/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1527,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59_Km7,horaedb,2113710523,1527,NA,kcrazy,1505797,,,NA,2024-05-16T00:51:16Z,2024-05-16T00:51:16Z,"> > > > The repository at [rust-rocksdb](https://github.com/tikv/rust-rocksdb) also does not support Windows.
> > > 
> > > 
> > > Try build with `make build-wal-message-queue`, this will disable rocksdb and use kafka as wal.
> > 
> > 
> > Can we directly use cargo to replace make?
> 
> Of course you can, our makefile is very simple, you can refer to the https://github.com/apache/incubator-horaedb/blob/main/Makefile#L40

I tried using the command 
```
cargo build --no-default-features --features wal-message-queue
```
but encountered some issues with jemalloc: jemalloc-sys@0.3.2 states that 'jemalloc support for x86_64-pc-windows-msvc is untested'.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM59_Km7/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1527,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5-AIds,horaedb,2113963884,1527,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-05-16T03:34:47Z,2024-05-16T03:34:47Z,"Thanks for testing, I will add a feature gate for jemalloc and pprof this week, so you can disable them  on Windows.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM5-AIds/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1538,https://api.github.com/repos/apache/horaedb/issues/1538,horaedb,2354887700,1538,Queries involving multiple tables will fail in a cluster environment,dracoooooo,55609330,Draco,dracode01@gmail.com,CLOSED,2024-06-15T14:21:27Z,2024-11-25T07:19:52Z,"### Describe this problem

Queries involving multiple tables fail to find table in a cluster environment.

### Server version

commit [0970b03](https://github.com/apache/horaedb/commit/0970b0386662bb477f4d96f1655e726c27f7d3eb)

### Steps to reproduce

```sql
CREATE TABLE `t1` (`timestamp` timestamp NOT NULL, `value` int, `name` string, timestamp KEY (timestamp)) ENGINE=Analytic WITH(  enable_ttl='false' );
Query OK, 0 rows affected (0.05 sec)

CREATE TABLE `t2` (`timestamp` timestamp NOT NULL, `value` int, `name` string, timestamp KEY (timestamp)) ENGINE=Analytic WITH(  enable_ttl='false' );
Query OK, 0 rows affected (0.04 sec)

CREATE TABLE `t3` ( `timestamp` timestamp NOT NULL, `value` int, `name` string, timestamp KEY (timestamp)) ENGINE=Analytic WITH(  enable_ttl='false' );
Query OK, 0 rows affected (0.06 sec)

SELECT * FROM `t1`, `t2`, `t3`;
ERROR 1105 (HY000): Failed to handle sql:SELECT * FROM `t1`, `t2`, `t3`, err:Rpc error, code:500, message:Failed to create plan, err:Failed to create plan, err:Failed to generate datafusion plan, err:Execution error: Table is not found, ""table:t3""
```

### Expected behavior

No error

### Additional Information

Use the same config as the integration test.

Cannot be 100% replicated, possibly due to the randomness of shard allocation?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1538/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1538,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6EI_Nd,horaedb,2216948573,1538,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-07-09T08:33:09Z,2024-07-09T08:33:09Z,"> Cannot be 100% replicated, possibly due to the randomness of shard allocation?

Yes, currently tables included in one query should be on same instance(an instance may have many shards).","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6EI_Nd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1542,https://api.github.com/repos/apache/horaedb/issues/1542,horaedb,2408223124,1542, INSERT INTO SELECT in a streaming way,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2024-07-15T09:01:57Z,2024-07-18T06:40:32Z,"### Describe This Problem

Followup for https://github.com/apache/horaedb/pull/1536, now we will collect all rows to vec before do insert, it's best if we can do insert select in a streaming way.

### Proposal

Support insert select like
```rs
while let Some(rows) = query.next_batch().await? {
  insert(rows);
}
```

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1542/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1543,https://api.github.com/repos/apache/horaedb/issues/1543,horaedb,2408345897,1543,Drop table panicked using kafka,linanh,61613477,,,CLOSED,2024-07-15T10:03:40Z,2024-08-13T03:28:27Z,"### Describe this problem

Drop table panicked when using kafka, then process exit abnormal.
[src/components/panic_ext/src/lib.rs:57](B thread 'horaedb-meta' panicked 'attempt to add with overflow' at ""src/wal/src/message_queue_impl/wal.rs:74""

### Server version

Version: 2.0.0
Git commit: a1869dc
Git branch: main
Opt level: 3
Rustc version: 1.77.0-nightly
Target: x86_64-unknown-linux-gnu
Build date: 2024-06-24T20:26:38.246766624Z

### Steps to reproduce

1. create table demo
2. drop table demo

### Expected behavior

_No response_

### Additional Information
```
2024-07-15T09:53:10.261198847Z (B2024-07-15 09:53:10.260(B ERRO(B [src/components/panic_ext/src/lib.rs:57](B thread 'horaedb-meta' panicked 'attempt to add with overflow' at ""src/wal/src/message_queue_impl/wal.rs:74""
   0: panic_ext::set_panic_hook::{{closure}}
2024-07-15T09:53:10.261297023Z              at horaedb/src/components/panic_ext/src/lib.rs:56:18
   1: <alloc::boxed::Box<F,A> as core::ops::function::Fn<Args>>::call
             at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/alloc/src/boxed.rs:2029:9
      std::panicking::rust_panic_with_hook
2024-07-15T09:53:10.261349283Z              at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/panicking.rs:785:13
   2: std::panicking::begin_panic_handler::{{closure}}
2024-07-15T09:53:10.261366843Z              at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/panicking.rs:651:13
   3: std::sys_common::backtrace::__rust_end_short_backtrace
2024-07-15T09:53:10.261384733Z              at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/sys_common/backtrace.rs:171:18
2024-07-15T09:53:10.261394490Z    4: rust_begin_unwind
             at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/panicking.rs:647:5
2024-07-15T09:53:10.261412093Z    5: core::panicking::panic_fmt
2024-07-15T09:53:10.261421168Z              at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/core/src/panicking.rs:72:14
2024-07-15T09:53:10.261430290Z    6: core::panicking::panic
             at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/core/src/panicking.rs:144:5
   7: <wal::message_queue_impl::wal::MessageQueueImpl<M> as wal::manager::WalManager>::mark_delete_entries_up_to::{{closure}}
2024-07-15T09:53:10.261477869Z              at horaedb/src/wal/src/message_queue_impl/wal.rs:74:39
2024-07-15T09:53:10.261486948Z    8: <core::pin::Pin<P> as core::future::future::Future>::poll
             at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/core/src/future/future.rs:124:9
2024-07-15T09:53:10.261522465Z       analytic_engine::instance::drop::Dropper::drop::{{closure}}
             at horaedb/src/analytic_engine/src/instance/drop.rs:75:14
      analytic_engine::instance::engine::<impl analytic_engine::instance::Instance>::drop_table::{{closure}}
2024-07-15T09:53:10.261549821Z              at horaedb/src/analytic_engine/src/instance/engine.rs:392:31
      <analytic_engine::engine::TableEngineImpl as table_engine::engine::TableEngine>::drop_table::{{closure}}
2024-07-15T09:53:10.261580185Z              at horaedb/src/analytic_engine/src/engine.rs:143:67
   9: <core::pin::Pin<P> as core::future::future::Future>::poll
             at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/core/src/future/future.rs:124:9
2024-07-15T09:53:10.261633371Z       <table_engine::proxy::TableEngineProxy as table_engine::engine::TableEngine>::drop_table::{{closure}}
2024-07-15T09:53:10.261644227Z              at horaedb/src/table_engine/src/proxy.rs:73:71
  10: <core::pin::Pin<P> as core::future::future::Future>::poll
2024-07-15T09:53:10.261661803Z              at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/core/src/future/future.rs:124:9
      <catalog_impls::volatile::SchemaImpl as catalog::schema::Schema>::drop_table::{{closure}}
             at horaedb/src/catalog_impls/src/volatile.rs:404:14
2024-07-15T09:53:10.261689160Z   11: <core::pin::Pin<P> as core::future::future::Future>::poll
             at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/core/src/future/future.rs:124:9
      <catalog_impls::cluster_based::SchemaWithCluster as catalog::schema::Schema>::drop_table::{{closure}}
             at horaedb/src/catalog_impls/src/cluster_based.rs:104:49
  12: <core::pin::Pin<P> as core::future::future::Future>::poll
             at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/core/src/future/future.rs:124:9
      catalog::table_operator::TableOperator::drop_table_on_shard::{{closure}}
             at horaedb/src/catalog/src/table_operator.rs:253:14
2024-07-15T09:53:10.261761369Z       cluster::shard_operator::ShardOperator::drop_table::{{closure}}
             at horaedb/src/cluster/src/shard_operator.rs:318:14
2024-07-15T09:53:10.261793079Z       cluster::shard_set::Shard::drop_table::{{closure}}
             at horaedb/src/cluster/src/shard_set.rs:165:34
2024-07-15T09:53:10.261810271Z       server::grpc::meta_event_service::handle_drop_table_on_shard::{{closure}}
             at horaedb/src/server/src/grpc/meta_event_service/mod.rs:561:10
      server::grpc::meta_event_service::MetaServiceImpl::drop_table_on_shard_internal::{{closure}}::{{closure}}
             at horaedb/src/server/src/grpc/meta_event_service/mod.rs:213:57
2024-07-15T09:53:10.261846429Z   13: tokio::runtime::task::core::Core<T,S>::poll::{{closure}}
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/task/core.rs:311:17
      tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut
2024-07-15T09:53:10.261872797Z              at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/loom/std/unsafe_cell.rs:14:9
      tokio::runtime::task::core::Core<T,S>::poll
2024-07-15T09:53:10.261890079Z              at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/task/core.rs:300:30
2024-07-15T09:53:10.261914775Z       tokio::runtime::task::harness::poll_future::{{closure}}
2024-07-15T09:53:10.261923022Z              at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/task/harness.rs:476:19
2024-07-15T09:53:10.261931757Z       <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/core/src/panic/unwind_safe.rs:272:9
      std::panicking::try::do_call
             at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/panicking.rs:554:40
      std::panicking::try
             at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/panicking.rs:518:19
      std::panic::catch_unwind
             at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/panic.rs:142:14
2024-07-15T09:53:10.261999635Z       tokio::runtime::task::harness::poll_future
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/task/harness.rs:464:18
2024-07-15T09:53:10.262016676Z       tokio::runtime::task::harness::Harness<T,S>::poll_inner
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/task/harness.rs:198:27
      tokio::runtime::task::harness::Harness<T,S>::poll
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/task/harness.rs:152:15
2024-07-15T09:53:10.262109800Z       tokio::runtime::task::raw::poll
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/task/raw.rs:276:5
  14: tokio::runtime::task::raw::RawTask::poll
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/task/raw.rs:200:18
      tokio::runtime::task::LocalNotified<S>::run
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/task/mod.rs:400:9
2024-07-15T09:53:10.262164737Z       tokio::runtime::scheduler::multi_thread::worker::Context::run_task::{{closure}}
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/scheduler/multi_thread/worker.rs:576:18
2024-07-15T09:53:10.262182747Z       tokio::runtime::coop::with_budget
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/coop.rs:107:5
      tokio::runtime::coop::budget
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/coop.rs:73:5
2024-07-15T09:53:10.262217114Z       tokio::runtime::scheduler::multi_thread::worker::Context::run_task
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/scheduler/multi_thread/worker.rs:575:9
2024-07-15T09:53:10.262234767Z   15: tokio::runtime::scheduler::multi_thread::worker::Context::run
2024-07-15T09:53:10.262243415Z              at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/scheduler/multi_thread/worker.rs:526:24
      tokio::runtime::scheduler::multi_thread::worker::run::{{closure}}::{{closure}}
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/scheduler/multi_thread/worker.rs:491:21
      tokio::runtime::context::scoped::Scoped<T>::set
2024-07-15T09:53:10.262279188Z              at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/context/scoped.rs:40:9
2024-07-15T09:53:10.262288320Z       tokio::runtime::context::set_scheduler::{{closure}}
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/context.rs:176:26
2024-07-15T09:53:10.262312118Z       std::thread::local::LocalKey<T>::try_with
2024-07-15T09:53:10.262323192Z              at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/thread/local.rs:286:16
      std::thread::local::LocalKey<T>::with
2024-07-15T09:53:10.262339545Z              at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/thread/local.rs:262:9
      tokio::runtime::context::set_scheduler
2024-07-15T09:53:10.262356079Z              at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/context.rs:176:17
      tokio::runtime::scheduler::multi_thread::worker::run::{{closure}}
2024-07-15T09:53:10.262386506Z              at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/scheduler/multi_thread/worker.rs:486:9
      tokio::runtime::context::runtime::enter_runtime
2024-07-15T09:53:10.262406000Z              at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/context/runtime.rs:65:16
2024-07-15T09:53:10.262414668Z       tokio::runtime::scheduler::multi_thread::worker::run
2024-07-15T09:53:10.262422387Z              at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/scheduler/multi_thread/worker.rs:478:5
  16: tokio::runtime::scheduler::multi_thread::worker::Launch::launch::{{closure}}
2024-07-15T09:53:10.262441369Z              at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/scheduler/multi_thread/worker.rs:447:45
      <tokio::runtime::blocking::task::BlockingTask<T> as core::future::future::Future>::poll
2024-07-15T09:53:10.262458585Z              at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/blocking/task.rs:42:21
2024-07-15T09:53:10.262467055Z       tokio::runtime::task::core::Core<T,S>::poll::{{closure}}
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/task/core.rs:311:17
2024-07-15T09:53:10.262485995Z       tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/loom/std/unsafe_cell.rs:14:9
2024-07-15T09:53:10.262504065Z       tokio::runtime::task::core::Core<T,S>::poll
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/task/core.rs:300:30
2024-07-15T09:53:10.262523081Z       tokio::runtime::task::harness::poll_future::{{closure}}
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/task/harness.rs:476:19
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
2024-07-15T09:53:10.262560526Z              at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/core/src/panic/unwind_safe.rs:272:9
      std::panicking::try::do_call
             at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/panicking.rs:554:40
2024-07-15T09:53:10.262583184Z       std::panicking::try
2024-07-15T09:53:10.262588688Z              at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/panicking.rs:518:19
      std::panic::catch_unwind
             at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/panic.rs:142:14
      tokio::runtime::task::harness::poll_future
2024-07-15T09:53:10.262611593Z              at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/task/harness.rs:464:18
2024-07-15T09:53:10.262617665Z       tokio::runtime::task::harness::Harness<T,S>::poll_inner
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/task/harness.rs:198:27
      tokio::runtime::task::harness::Harness<T,S>::poll
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/task/harness.rs:152:15
2024-07-15T09:53:10.262641383Z       tokio::runtime::task::raw::poll
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/task/raw.rs:276:5
2024-07-15T09:53:10.262653428Z   17: tokio::runtime::task::raw::RawTask::poll
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/task/raw.rs:200:18
2024-07-15T09:53:10.262664997Z       tokio::runtime::task::UnownedTask<S>::run
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/task/mod.rs:437:9
2024-07-15T09:53:10.262676809Z       tokio::runtime::blocking::pool::Task::run
             at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/blocking/pool.rs:159:9
      tokio::runtime::blocking::pool::Inner::run
2024-07-15T09:53:10.262694973Z              at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/blocking/pool.rs:513:17
      tokio::runtime::blocking::pool::Spawner::spawn_thread::{{closure}}
2024-07-15T09:53:10.262706902Z              at usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokio-1.29.1/src/runtime/blocking/pool.rs:471:13
      std::sys_common::backtrace::__rust_begin_short_backtrace
2024-07-15T09:53:10.262722948Z              at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/sys_common/backtrace.rs:155:18
  18: std::thread::Builder::spawn_unchecked_::{{closure}}::{{closure}}
             at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/thread/mod.rs:529:17
      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once
             at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/core/src/panic/unwind_safe.rs:272:9
2024-07-15T09:53:10.262752844Z       std::panicking::try::do_call
2024-07-15T09:53:10.262761324Z              at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/panicking.rs:554:40
2024-07-15T09:53:10.262771182Z       std::panicking::try
2024-07-15T09:53:10.262779980Z              at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/panicking.rs:518:19
2024-07-15T09:53:10.262788784Z       std::panic::catch_unwind
             at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/panic.rs:142:14
2024-07-15T09:53:10.262809251Z       std::thread::Builder::spawn_unchecked_::{{closure}}
2024-07-15T09:53:10.262818841Z              at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/thread/mod.rs:528:30
      core::ops::function::FnOnce::call_once{{vtable.shim}}
             at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/core/src/ops/function.rs:250:5
  19: <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once
2024-07-15T09:53:10.262857926Z              at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/alloc/src/boxed.rs:2015:9
2024-07-15T09:53:10.262873992Z       <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once
             at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/alloc/src/boxed.rs:2015:9
      std::sys::pal::unix::thread::Thread::new::thread_start
2024-07-15T09:53:10.262913882Z              at rustc/6b4f1c5e782c72a047a23e922decd33e7d462345/library/std/src/sys/pal/unix/thread.rs:108:17
2024-07-15T09:53:10.262923813Z   20: start_thread
2024-07-15T09:53:10.262935140Z              at build/glibc-LcI20x/glibc-2.31/nptl/pthread_create.c:477:8
2024-07-15T09:53:10.262945997Z   21: clone
             at build/glibc-LcI20x/glibc-2.31/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:95
2024-07-15T09:53:10.262963620Z (B
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1543/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1543,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6FEl0h,horaedb,2232573217,1543,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2024-07-17T07:00:30Z,2024-07-17T07:00:30Z,"Thanks for reply, debugging about it.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6FEl0h/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1543,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6INn5N,horaedb,2285272653,1543,NA,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,NA,2024-08-13T03:28:26Z,2024-08-13T03:28:26Z,"#1550 has fixed this bug, you can recheck it. @linanh ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6INn5N/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1545,https://api.github.com/repos/apache/horaedb/issues/1545,horaedb,2413974542,1545,Tracking issue for compaction offloading,LeslieKid,119803231,Leslie Su,,OPEN,2024-07-17T15:51:17Z,2024-11-01T12:40:05Z,"## Describe This Problem

We found in production that the speed of sst compaction is unable to keep up with the speed of sst generation, leading to poor query performance. However we are unable give more resource to compaction to solve the problem because query/write is more important than compaction in the same node.

It is really hard to do a trade-off about resource allocation among query, write and compaction in lsm model. We want to compact the generated small ssts as fast as possible, but we can't tolerate its influence to query/write. And finally I think offload the compaction to the seperated nodes may be the key for it.

## Proposal

**The following is the architecture for compaction offloading.**

![Architecture.png](https://s2.loli.net/2024/07/17/ivH4E2MJq8pZbnQ.png)

To support compaction offloading, we need:

- [ ] **Horaedb node supports submitting the compaction task to remote** #1563 
  - Introduce runnable compaction node and expose the api  for horaedb node to ask for remote compaction service (Execute Plane)
  - Impl remote mode compactor supporting compaction offload
- [ ] **Horaemeta supports managing the compaction nodes (Control Plane)**
  - Impl the ability to monitor the compaction servers (Monitor) 
  - Impl scheduling algorithm for load balance and expose the api for horaedb node to get the proper remote compaction node (Scheduler)
- [ ] **Integration tests for compaction** #1573 
    - See #1571 for more details


## Additional Context

This issue replaces issue #1480. Please close issue #1480 as it is outdated.

[incubator-horaedb-proto#133](https://github.com/apache/incubator-horaedb-proto/issues/133) is highly related to this issue.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1545/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1545,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6FI8Ep,horaedb,2233712937,1545,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2024-07-17T16:24:29Z,2024-07-17T16:24:29Z,looks great!,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6FI8Ep/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1545,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6Gd11n,horaedb,2255969639,1545,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2024-07-29T13:35:22Z,2024-07-29T13:35:22Z,"I think in the first stage, we can treat the `compaction cluster` as same as the `horaedb cluster`.
And we just reuse the ability about monitoring and scheduling for `horaedb cluster` in `horaemeta`.

And we can start to desigin the specific `schedule` strategy  for `compaction cluster` after it can work.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6Gd11n/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1545,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6Gsx7B,horaedb,2259885761,1545,NA,LeslieKid,119803231,Leslie Su,,NA,2024-07-31T07:48:07Z,2024-07-31T07:48:07Z,"> I think in the first stage, we can treat the `compaction cluster` as same as the `horaedb cluster`. And we just reuse the ability about monitoring and scheduling for `horaedb cluster` in `horaemeta`.
> 
> And we can start to desigin the specific `schedule` strategy for `compaction cluster` after it can work.

Thanks for your suggestion! I think it's a good idea to first implement a simple working version.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6Gsx7B/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1545,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6K6aF_,horaedb,2330567039,1545,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2024-09-05T04:21:44Z,2024-09-05T04:21:44Z,"I think `Horaedb node supports submitting the compaction task to remote` may be similar as `Compaction node supporting remote compaction service `.

I guess the third step can be integration test, test is very important actually.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6K6aF_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1545,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6LS2xC,horaedb,2336975938,1545,NA,Rachelint,34352236,kamille,kamille@apache.org,NA,2024-09-09T02:10:24Z,2024-09-09T02:10:24Z,"Maybe you will be insterested when writing test
https://github.com/apache/horaedb/tree/main/integration_tests/dist_query","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6LS2xC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1549,https://api.github.com/repos/apache/horaedb/issues/1549,horaedb,2443900276,1549,SDK supports writing `Date` type,chunshao90,15178480,chunshao.rcs,chunshao@apache.org,CLOSED,2024-08-02T03:04:39Z,2024-12-18T04:44:42Z,"### Describe This Problem

The document supports the Date type, but the SDK does not support writing the Date type.
Ref: https://horaedb.apache.org/docs/user-guide/sql/model/data_types/

### Proposal

SDK supports writing `Date` type.

- [ ] Java
- [ ] Pthon
- [ ] Golang
- [ ] Rust

### Additional Context

This is what server defines:

https://github.com/apache/horaedb/blob/a57770679aa7482faeed232567d34c96b5bc387b/src/common_types/src/datum.rs#L441-L449
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1549/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1549,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6OLMLZ,horaedb,2385298137,1549,NA,adi-kmt,11575549,Adithya Kamath,kamathis4@gmail.com,NA,2024-10-01T09:37:14Z,2024-10-01T09:37:14Z,Hey @chunshao90 can I pick this up?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6OLMLZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1549,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6QoHqy,horaedb,2426436274,1549,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-10-21T11:42:50Z,2024-10-21T11:42:50Z,"@adi-kmt Just go ahead, which sdk are you interested at?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6QoHqy/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1549,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6QoxI3,horaedb,2426606135,1549,NA,adi-kmt,11575549,Adithya Kamath,kamathis4@gmail.com,NA,2024-10-21T12:58:52Z,2024-10-21T12:58:52Z,Can start with Go and once I'm done with that can pick up rust and so on.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6QoxI3/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1549,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6Qpic0,horaedb,2426808116,1549,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-10-21T14:12:59Z,2024-10-21T14:12:59Z,"Sounds good, let me know if you have any problems.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6Qpic0/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1549,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6TNj7y,horaedb,2469805810,1549,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-11-12T07:42:08Z,2024-11-12T07:42:08Z,@adi-kmt Any progress?,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6TNj7y/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1549,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6Tbl7a,horaedb,2473483994,1549,NA,adi-kmt,11575549,Adithya Kamath,kamathis4@gmail.com,NA,2024-11-13T12:32:37Z,2024-11-13T12:32:37Z,"Sorry, got a bit busy with work
Just confirming @jiacai2050 , is the sdk the same inside the `integration_test/sdk`? Or is there some other path. Asking since i could only find one `*.java` file in the code, but no `*.py` file in that integration test","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6Tbl7a/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1549,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6TcI6p,horaedb,2473627305,1549,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-11-13T13:29:28Z,2024-11-13T13:29:28Z,"They are in different repository, please see https://horaedb.apache.org/docs/user-guide/sdk/","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6TcI6p/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1553,https://api.github.com/repos/apache/horaedb/issues/1553,horaedb,2457384758,1553,Manifest file for disk-based WAL implementation,dracoooooo,55609330,Draco,dracode01@gmail.com,CLOSED,2024-08-09T08:20:28Z,2024-09-14T02:02:44Z,"### Describe This Problem

To implement a WAL based on the local disk, in addition to using segment files to record logs, it is also necessary to use another file to record the metadata of the WAL. This is because the current WAL `Delete` interface includes a tableId as a parameter, and logs for multiple tables are recorded in the same segment file. This means that it is not possible to simply mark all logs before a certain sequence number as deletable. Therefore, a manifast file is needed to maintain this information.

### Proposal

### Format

Using protobuf as the file format for WAL manifest:

```proto
syntax = ""proto3"";

message Manifest {
  map<string, uint64> latest_mark_deleted = 1;
}
```

The key in the map is `<regionId>:<tableId>`, and the value is the highest sequance number marked as deleted for this table in the WAL.

Not using the manifest file to record more information is to avoid updating this file during appending logs, thereby reducing I/O overhead.

### Append Logs

Do not update the manifest file.

### Read Logs

Use the manifest file to skip logs that have already been deleted.

### Delete Logs

Update the values in the map, create a new manifest file, and overwrite the old file.

Record the maximum sequence number of all tables in each segment file in memory. When all the tables mark the deleted sequence number as greater than the maximum sequence number in the segment, delete this segment.

When an old segment is deleted, if a table’s log exists only in this old segment, then remove this table from the manifest’s map.

### Potential Risks

1. If the number of tables is very large, the overhead of overwriting this manifest file each time could be significant.


### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1553/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1553,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6H7jkZ,horaedb,2280536345,1553,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-08-10T09:17:16Z,2024-08-10T09:17:16Z,"> The key in the map is `<regionId>:<tableId>`,

I think we can encode regionId in wal directory path, so the key could only contains `tableId`.

> Use the manifest file to skip logs that have already been deleted.

How will you skip WAL files? Which strategy will you use?

> Update the values in the map, create a new manifest file, and overwrite the old file.

This is the normal case, what if there are some partials error, such as overwrite failed, you need to document more details, pseudo code or sequence diagram may help.


> Record the maximum sequence number of all tables in each segment file in memory

How will you recovery this info when server start up? do we need to iterate the whole WAL files?



","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6H7jkZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1553,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6IESLW,horaedb,2282824406,1553,NA,dracoooooo,55609330,Draco,dracode01@gmail.com,NA,2024-08-11T16:57:55Z,2024-08-11T16:57:55Z,"
> > The key in the map is `<regionId>:<tableId>`,
>
> I think we can encode regionId in wal directory path, so the key could only contains `tableId`.

Indeed.

> > Use the manifest file to skip logs that have already been deleted.
>
> How will you skip WAL files? Which strategy will you use?

This manifest exists both in the file system and in memory. In memory, it is represented as a map. Since we record the min and max sequence numbers of each table in segments in memory, we can skip the segments that are not needed. While iterating through the necessary segments, we might encounter logs that have already been deleted. In such cases, we can skip them based on the information in the map.

> > Update the values in the map, create a new manifest file, and overwrite the old file.
>
> This is the normal case, what if there are some partials error, such as overwrite failed, you need to document more details, pseudo code or sequence diagram may help.

The general steps for overwriting are to acquire the write lock for the manifest, create a new temporary file, write to this temporary file, use `fsync` to ensure the content has been written to the disk, and then use rename to replace the original file.

If an error occurs in the steps above, I don’t think it can be handled, and we would have to panic.

> > Record the maximum sequence number of all tables in each segment file in memory
>
> How will you recovery this info when server start up? do we need to iterate the whole WAL files?

Yes. I think this is a trade-off to avoid writing manifest file during the WAL write operation.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6IESLW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1553,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6MHheM,horaedb,2350782348,1553,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-09-14T02:02:43Z,2024-09-14T02:02:43Z,"After some discussion, the manifest isn't a must, and introduce more burden. 

The idea is that if we can delete unused segments in time, then when server restarts, we can reconstruct table seq from segment one after one.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6MHheM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1564,https://api.github.com/repos/apache/horaedb/issues/1564,horaedb,2506456828,1564,"The ""Development Guide"" link in the README.md is incorrect",jimmiejams,7391874,James Whitwell,,CLOSED,2024-09-04T23:10:10Z,2024-09-05T04:57:31Z,"### Describe this problem

The link in the README.md to ""Development Guide"" should point to [https://horaedb.apache.org/docs/dev/compile_run/]

### Server version

n/a

### Steps to reproduce

n/a

### Expected behavior

_No response_

### Additional Information

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1564/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1567,https://api.github.com/repos/apache/horaedb/issues/1567,horaedb,2525934697,1567,Cli tools to decode wal segment,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2024-09-14T02:13:30Z,2024-11-02T13:15:03Z,"### Describe This Problem

I'm excited to share that we've successfully implemented a local Write-Ahead Log (WAL) following the completion of #1566. While we haven't conducted extensive performance benchmarking yet, our initial tests suggest that our implementation is performing comparably to the one based on RocksDB. This marks a significant milestone in our project's development, and we're looking forward to further optimizing and testing this new feature.



### Proposal


In order to trouble shoot WAL issues, we need a cli tool to decode wal segment, to include at least following infos:
- segment version
- min/max seq of each table


### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1567/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1567,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6RaXGy,horaedb,2439606706,1567,NA,zealchen,20197724,NeoChen,neochen428@gmail.com,NA,2024-10-26T14:44:42Z,2024-10-26T14:44:42Z,Nice. Let me handle this one.,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6RaXGy/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1567,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6Rdbxj,horaedb,2440412259,1567,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-10-28T02:31:12Z,2024-10-28T02:31:12Z,"Thanks, you can add another file here https://github.com/apache/horaedb/tree/main/src/tools/src/bin","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6Rdbxj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1571,https://api.github.com/repos/apache/horaedb/issues/1571,horaedb,2537874978,1571,Integration test for compaction offload,LeslieKid,119803231,Leslie Su,,CLOSED,2024-09-20T04:56:59Z,2024-10-30T01:20:38Z,"### Describe This Problem

The subproblem for #1545 . 
Currently, the compaction offload lacks integration tests. Impl these tests is crucial for ensuring the reliability and correctness of the compaction offload.
Secondly, adding support for a manual compact operation in SQL would provide more flexibility to users, allowing them to trigger table compaction on demand. 

### Proposal

The procedure to test the compaction module (in sql):
1. Create a table.
2. Insert data into table.
3. Flush in-memory data into sst.
4. Update data to simulate conditions requiring compaction.
5. Flush.
6. Trigger compaction. (manual compaction)
7. ~~Show compaction status.(optional)~~ (delete for easiness)
8. Verify the result.

The problem is that horaedb has not support manual compaction in sqlness yet. I want to impl `compact` as a pre-command, similar to `flush` command. The reason is: 
+ **Consistency with existing command.** `flush` forces in-memory data to be written to SST files. Similarly, `compact` could be triggered to merge and optimize these SST files, aligning the two operations under the same command paradigm.


### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1571/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1575,https://api.github.com/repos/apache/horaedb/issues/1575,horaedb,2563291137,1575,Type `test` is not supported in pr title,LeslieKid,119803231,Leslie Su,,CLOSED,2024-10-03T07:08:24Z,2024-10-25T02:53:54Z,"### Describe This Problem

I encountered an error in pr #1573 when run Checks `check-pr-title`:
`PR title does not follow the convention, the pattern: ^(feat|fix|docs|refactor|chore)((.+))?!?: .+$`
because the pr title is **test: add integration test for compaction offload**, not match patter above.

But I found that `test` type is supported when reading [convential commit guide in official document](https://horaedb.apache.org/docs/dev/conventional_commit/). Similar situation include `style` and `build` type.

So I think these types should be supported in `check-pr-title`.

### Proposal
If `test`, `style` and `build` can be supported, we can add these types in the `check-pr-title` part in `.github/workflows/check-pr-title.yml`.
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1575/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1575,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6QDYTu,horaedb,2416805102,1575,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-10-16T13:12:49Z,2024-10-16T13:12:49Z,"Good suggestion, I think we can update the pattern to include test, style and build.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6QDYTu/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1577,https://api.github.com/repos/apache/horaedb/issues/1577,horaedb,2593432384,1577,Add integration with rust-jemalloc-pprof for easy mem perf,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,OPEN,2024-10-17T02:27:50Z,2024-11-12T01:48:18Z,"### Describe This Problem

[rust-jemalloc-pprof](https://github.com/polarsignals/rust-jemalloc-pprof) is a cool project to do memory perf.

### Proposal

Add a new HTTP endpoint `/debug/pprof/heap` to serve the profiling.

### Additional Context

We can remove old `/debug/profile/heap/{seconds}` endpoint when this is ready.
https://github.com/apache/horaedb/blob/79627995477acdaeaf43bcd128fdc902f5fbeaad/src/server/src/http.rs#L607","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1577/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1577,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6SQQvP,horaedb,2453736399,1577,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-11-04T02:49:08Z,2024-11-04T02:49:08Z,"@zuston Thanks, assigned.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6SQQvP/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1577,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6S6_dE,horaedb,2464937796,1577,NA,zuston,8609142,Junfan Zhang,zuston@apache.org,NA,2024-11-08T14:45:04Z,2024-11-08T14:45:04Z,"> @zuston Thanks, assigned.

From this crate doc, I found this way is not better than the vaillina jeprof way. It just generates the pprof format, if you want to dig it with human format, you have to execute in the host machine instead of any remote machine to call api directly. Because it still depends on the execute binary. ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6S6_dE/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1577,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6TGBrt,horaedb,2467830509,1577,NA,zuston,8609142,Junfan Zhang,zuston@apache.org,NA,2024-11-11T10:38:59Z,2024-11-11T10:38:59Z,PTAL @jiacai2050 ,"{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6TGBrt/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1577,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6TMHgt,horaedb,2469427245,1577,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-11-12T01:48:17Z,2024-11-12T01:48:17Z,"> you have to execute in the host machine instead of any remote machine to call api directly.

What do you mean by call api directly?

With pprof, we can explore the perf data in various format, such as [flamegraph](https://github.com/google/pprof/issues/401#issuecomment-1088744271)
- https://github.com/google/pprof/blob/main/doc/README.md#graph 
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6TMHgt/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1583,https://api.github.com/repos/apache/horaedb/issues/1583,horaedb,2623011653,1583,Support Merge operator for TimeMergeStorage,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2024-10-30T06:05:34Z,2024-12-13T02:12:28Z,"### Describe This Problem

For now, [TimeMergeStorage](https://github.com/apache/incubator-horaedb/blob/63c4e9bb1c546aad89350c56987fbb4204147622/horaedb/metric_engine/src/storage.rs#L55) speak RecordBatch, for rows with same primary key, how to choose their values? 

In initial design only `overwrite` mode is supported, which means row with max sequence will win, this may works for index/series, but not for data. 

For data, we usually accumulate points of 30 min into one row, this means we need to update this row incrementally.

### Proposal

 TimeMergeStorage should support a mode param, which is 
```rs
enum UpdateMode {
  Overwrite,
  Append,
}
```
When query and compaction, use different strategy to choose value.

### Additional Context

My proposal has one fatal drawback: we can't have different mode for different rows for the same storage instance.

This should be fine for tables in our design here: https://github.com/apache/horaedb/blob/main/docs/rfcs/20240827-metric-engine.md 


If we want to support this feature, a more complex design is required, following is how rocksdb does:

```cpp
// 在写入 memtable 时，每个 entry 都会带上类型标记
enum ValueType : unsigned char {
  kTypeDeletion = 0x0,
  kTypeValue = 0x1,
  kTypeMerge = 0x2,
  // ...
};

// 内部 key 格式：user_key + sequence + type
// 比如对于同一个 key 的操作序列：
key1 | seq5 | kTypeValue   // Put 操作
key1 | seq4 | kTypeMerge   // Merge 操作
key1 | seq3 | kTypeMerge   // Merge 操作

Status DBImpl::Get(const ReadOptions& options, const Slice& key,
                  std::string* value) {
  // 1. 查找最新版本的 entry
  // 2. 如果遇到 kTypeValue，直接返回值
  // 3. 如果遇到 kTypeMerge，需要：
  //    - 继续向前查找直到找到 kTypeValue 或到达开头
  //    - 收集所有遇到的 Merge 操作
  //    - 按从旧到新的顺序应用 Merge 操作
  //    - 使用用户定义的 MergeOperator 执行实际合并
}

void CompactionBuilder::ProcessKV() {
  // 合并同一个 key 的多个版本时：
  if (type == kTypeValue) {
    // 对于 Put，直接保留最新的值
    AddToBuilder(key, value);
  } else if (type == kTypeMerge) {
    // 对于 Merge，需要：
    // 1. 收集连续的 Merge 操作
    // 2. 找到最近的一个 Put 值(如果存在)
    // 3. 使用 MergeOperator 执行合并
    // 4. 将合并结果作为一个新的 Put 值写入
    MergeHelper::TimedFullMerge(...);
  }
}
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1583/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1589,https://api.github.com/repos/apache/horaedb/issues/1589,horaedb,2650807796,1589,Skip build latest docker tag for RC version,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2024-11-12T02:15:31Z,2024-11-12T07:36:28Z,"ASF require this

> The latest tag should not point to an artifact containing unapproved code e.g. to master or dev branches or to a RC or snapshot.

https://incubator.apache.org/guides/distribution.html#docker

Related file:
https://github.com/apache/horaedb/blob/0d8d43d4f434e098353ee0940382a8f00e144e8b/.github/workflows/publish-image.yml#L45","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1589/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1592,https://api.github.com/repos/apache/horaedb/issues/1592,horaedb,2651383934,1592,Support more efficient way to persist manifest of TimeMergeStorage,baojinri,52273009,鲍金日,baojinri@apache.org,CLOSED,2024-11-12T08:08:14Z,2024-11-22T03:12:51Z,"### Describe This Problem

Now, when meta information of TimeMergeStorage changes, the efficiency of persisting manifest sst files is very low. Each time, the new sst file will be added to the manifest and then persisted it whole.

### Proposal

Each time, the new sst file will be persisted instantly, then add the new sst into snapshot background.
The manifest file is organized as follows:
```
/root/manifest/snapshot
/root/manifest/new_sst_0
/root/manifest/new_sst_1
/root/manifest/new_sst_2
...
```

### Additional Context
Process manifest files：
- When initializing the manifest, we will list all the sst files in the manifest directory and then add them into the snapshot
- Every time a new file is added, we will count the new files. When the number of new files reaches 50, we will list all the sst files in the manifest directory and then add them into the snapshot","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1592/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1597,https://api.github.com/repos/apache/horaedb/issues/1597,horaedb,2676303575,1597,support either nanosecond timestamp precision or composite primary keys,grenade,111819,rob thijssen,trob@pm.me,OPEN,2024-11-20T15:38:05Z,2024-11-25T09:54:30Z,"### Describe This Problem

my use case is trade data analytics for signal or strategy indicators. a trade has:
- timestamp: the time (in nanosecond precision) of the execution of the trade
- base: the tag/symbol of the commodity or security that was bought or sold
- quote: the tag/symbol of the currency, commodity or security that the price of `base` is given in
- exchange: the tag/symbol of the exchange where the trade was executed
- id: the (uint64) id used by the exchange to uniquely identify the trade. due to the volume of trades, most exchanges use a composite key of (id, base, quote), so there are always collisions if `base`, `quote`, `exchange` and `id` are not considered together
- price: the price (in `quote`) for 1 unit of `base`
- size: the quantity of `base` in the trade. a positive `size` indicates a buy. a negative `size` indicates a sell

i am struggling to document a table definition in horaedb that does not lose trades. since there are many trades which occur within the same millisecond. nanosecond precision appears to be unavailable.

i have attempted to work around the millisecond precision limitation by using a composite primary key (time, exchange, quote, id). each base gets it's own table which allows for multi-exchange series and combining quotes where their underlying values are equivalent (ie: usdt == usdc). however it appears that horaedb accepts the definition but ignores any value in the composite key that is not the timestamp. this results in all but the first trade in a given millisecond being discarded as a duplicate.

### Proposal

i would appreciate ideas about how to utilise nanosecond timestamp precision or how to correctly define a composite key that will actually work.

### Additional Context

here's the table definition i have tried:
```sql
CREATE TABLE IF NOT EXISTS {base} (
    time timestamp NOT NULL,
    id uint64,
    exchange string,
    quote string,
    price double,
    size double,
    TIMESTAMP KEY(time),
    PRIMARY KEY(time, id, exchange, quote)
) ENGINE=Analytic with (enable_ttl='false')
```
here's an example of source data where `base` is _btc_, `quote` is _usd_ and `exchange` is _coinbase_:
https://api.exchange.coinbase.com/products/BTC-USD/trades?limit=10&after=1000","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1597/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1597,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6UekTl,horaedb,2491040997,1597,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-11-21T12:43:32Z,2024-11-21T12:43:32Z,"Hi, nanoseconds is not supported in horaedb now.

For your case, I think you could add another column to bypass this limit,

``` sql
CREATE TABLE IF NOT EXISTS {base} (
    time timestamp NOT NULL,
    time2 uint64 NOT NULL,
    id uint64,
    exchange string,
    quote string,
    price double,
    size double,
    TIMESTAMP KEY(time),
    PRIMARY KEY(time, time2, id, exchange, quote)
) ENGINE=Analytic with (enable_ttl='false')
```

Here we add another column `time2` to primary keys, and it's defined as `time(nanoseconds) % 1e6`, so rows with same `time` value won't be overwritten.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6UekTl/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1600,https://api.github.com/repos/apache/horaedb/issues/1600,horaedb,2689619309,1600,Explore another format for manifest,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,CLOSED,2024-11-25T07:46:28Z,2024-12-18T16:08:40Z,"### Describe This Problem

Currently our manifest is defined using protobuf, that's:
https://github.com/apache/horaedb/blob/9e81c4ed5df1998cbd210dc48fc67b6b7405a553/horaedb/pb_types/protos/sst.proto#L32

Protobuf is useful for schema evolution, but not very efficient in our case:
- For `Vec<struct>` field, protobuf will serialize metadata of every struct, which is a waste of space.
- Serialize manifest body using protobuf in a whole make it hard to update incrementally.

### Proposal

We can serialize manifest all by ourselves, a proposed format:

```
| version(u8) |  Record(N) ... |

# Record is a self-descriptive message
| id(u64) | time_range(i64*2)| size(u32) | 
```
When update incrementally, we can just append new record in the end.

### Additional Context

This is where manifest get merged:
- https://github.com/apache/horaedb/blob/e2970b1171523b182b36dc67e642641c47db078f/horaedb/metric_engine/src/manifest.rs#L269","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1600/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1600,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6VRO9C,horaedb,2504322882,1600,NA,zealchen,20197724,NeoChen,neochen428@gmail.com,NA,2024-11-27T16:37:09Z,2024-11-27T16:37:09Z,"Interesting, I'll get into it. ","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6VRO9C/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1600,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6VYzCM,horaedb,2506305676,1600,NA,zealchen,20197724,NeoChen,neochen428@gmail.com,NA,2024-11-28T14:55:48Z,2024-11-28T14:55:48Z,"Here are some key design considerations to clarify in advance:

1. Since the sstmeta schema may evolve over time, we need to ensure backward compatibility for each of the self-descriptive record. If so, I'm wondering how and when these manifest files will be utilized.
2. The object storage crate (e.g., LocalFileSystem) does not appear to have an append interface. This implies that during the do_merge operation, we would need to load the entire file at snapshot_path into memory, append the new data, and then write the entire file back.
3. What is the expected order of magnitude for the number of sstmeta files? For example, are we dealing with millions?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6VYzCM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1600,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6VbEP5,horaedb,2506900473,1600,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-11-29T00:56:37Z,2024-11-29T00:56:37Z,"Those are all great questions, 
1. We use `version` field to deal with schema  evolution, if we want to add some fields to manifest, a new version could be added, and when merge, to convert old manifest to new one.
2. Object store don't have the append interface, so append here means download the old one into memory, then merge with new delta manifests, then upload it again to overwrite the old one.
3. There shouldn't many manifest files, we have a hard limit on how many delta files a manifest can have, if there are more than that, the write process will fail, only after the merge process is finished, we can allow creating new manifest deltas.

As for the third question, that why we need to keep metadata of each sst small, so we can hold millions of sst files in one manifest snapshot, whose size is less than 1GB.

``` shell
1024*1024*1024 / 28 (size of each sst's metadata) = 38347922
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6VbEP5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1600,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6V76C1,horaedb,2515509429,1600,NA,zealchen,20197724,NeoChen,neochen428@gmail.com,NA,2024-12-03T20:41:07Z,2024-12-03T20:41:07Z,"Agreed, your proposal works for me. Specifically, I can implement a MetaSerializer to convert Vec<SstFile> into bytes and merge them with the delta metadata bytes. Additionally, we'll need a deserialization function to parse the bytes back into Vec<SstFile>. To account for future evolution, we'll include version verification and handle data migration when the versions differ during startup. However the upgrade process can be tackled separately in another issue.

```text
Old data flow in do_merge:
                                      delta_sstmetas
                                             | (extend vec)
                                             V                                
object_store -> org_bytes -> org_pb -> Vec<sstmeta> -> dst_pb -> dst_bytes -> object_store

New data flow in do_merge:
               delta_sstmetas -> bytes
                                  | (append)
                                  V                                
object_store -> org_bytes -> dst_bytes -> object_store
````","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6V76C1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1600,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6V_CAh,horaedb,2516328481,1600,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-12-04T06:44:25Z,2024-12-04T06:44:25Z,"The new flow looks GOOD for me, before you get started, you need to design the format manifest will be using.

Or using the format I proposed in description?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6V_CAh/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1600,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6WFUpM,horaedb,2517977676,1600,NA,zealchen,20197724,NeoChen,neochen428@gmail.com,NA,2024-12-04T16:38:16Z,2024-12-04T16:38:16Z,"> The new flow looks GOOD for me, before you get started, you need to design the format manifest will be using.
> 
> Or using the format I proposed in description?

The manifest format of my design is as below:
```text
| magic(u32) | version(u8) |  flags(u8) | length(u64) | Record(N) ... |

The Magic field (u32) is used to ensure the validity of the data source.
The Flags field (u8) is reserved for future extensibility, such as enabling compression or supporting additional features.
The length field (u64) represents the total length of the subsequent records and serves as a straightforward method for verifying their integrity. (length = record_length * record_count)

# Record is a self-descriptive message
| id(u64) | time_range(i64*2)| size(u32) |  num_rows(u32)|
```
I add num_rows field as the struct FileMeta has that field. 
```rust
pub struct FileMeta {
    pub max_sequence: u64,
    pub num_rows: u32,
    pub size: u32,
    pub time_range: TimeRange,
}
```
","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6WFUpM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1600,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6WJHKh,horaedb,2518971041,1600,NA,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,NA,2024-12-05T02:47:17Z,2024-12-05T02:47:17Z,"The general ideas looks GOOD for me, but I wonder if you put length in the middle the manifest, how would you update it when append new record?","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6WJHKh/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1600,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6WKPAF,horaedb,2519265285,1600,NA,zealchen,20197724,NeoChen,neochen428@gmail.com,NA,2024-12-05T06:10:03Z,2024-12-05T06:10:03Z,"> The general ideas looks GOOD for me, but I wonder if you put length in the middle the manifest, how would you update it when append new record?

Since the Bytes obtained from object storage is a fixed-length object, each time we merge SST files, we need to create a new Vec`<u8>`  with a capacity equal to the original bytes length plus the number of records multiplied by the record length.

Following that, We make Vec`<u8>` as_mut_slice [u8] and then:
1.  create the new header struct and write it to slice[0..header_length].
2. copy original records bytes to slice[header_length..header_length+original_record_length].
3. for each sstfiles, write record to the rest of the slice one by one.

Another way is to:
1. copy original bytes to slice [0..header_length+original_record_length]
2. update the slice at length field position with the new length.
3. for each sstfiles, write record to the rest of the slice one by one.


I prefer the first way, looks more ergonomics.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6WKPAF/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/horaedb/issues/1600,https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6YGD8b,horaedb,2551725851,1600,NA,zealchen,20197724,NeoChen,neochen428@gmail.com,NA,2024-12-18T16:08:39Z,2024-12-18T16:08:39Z,"Append 100 new delta sstfiles to snapshot with 1000 records.
```sh
Benchmarking bench_encoding/new_format_encoding/0: Collecting 10 samples in estimated 5.0005 s (494k iterations)
bench_encoding/new_format_encoding/0
                        time:   [10.232 µs 10.611 µs 10.900 µs]
                        change: [-1.5176% +2.1745% +5.9315%] (p = 0.30 > 0.05)
                        No change in performance detected.
```","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/comments/IC_kwDOHZgSUM6YGD8b/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1608,https://api.github.com/repos/apache/horaedb/issues/1608,horaedb,2739229371,1608,Redesigning the manifest deduplication mechanism.,zealchen,20197724,NeoChen,neochen428@gmail.com,OPEN,2024-12-13T21:21:38Z,2024-12-13T21:41:10Z,"### Describe This Problem

The current manifest snapshot archiving process requires deduplication during every merge operation involving a single snapshot file and multiple SST files.  Let's first look at the snapshot archiving procedure:
<img width=""1011"" alt=""image"" src=""https://github.com/user-attachments/assets/96595179-b4d3-4222-8ced-c72d9704d33b"" />

The duplication scenario can only exist when the manifest merger reads an old metafile that has already gone through the merging procedure once.  This inconsistent problem is due to the failure to delete the meta files after the persistence of the snapshot file. The failure itself can lie in many reasons like network problem or crashing of the running process.

This issue is an idempotency problem, or in other words, the merge, write, and delete operations must be performed within a single transaction.

### Proposal

We could design a batch operation transaction mechanism to ensure that:  
1. If the last merge procedure was successfully completed, there is no need to perform deduplication.  
2. If the last merge procedure was partially completed, deduplication is required.  
3. If it is unclear whether the last merge procedure was successful or not, deduplication should be performed.

To determine which scenario applies, we need to record the states of the merge operations and store them locally. This allows us to handle the scenarios as follows:
a. If the state file does not exist, proceed to scenario 3.
b. If the state file indicates that the delete operation is complete, proceed to scenario 1.
c. If the state file indicates that the snapshot was successfully stored but the delete operation is incomplete, proceed to scenario 2.

The pseudo code is like below:
```rust
fn do_merge() {
      let sstfiles = ...;
      let snapshot = ...;
      let ops = MergeOps::new(snapshot, sstfiles);
     {
          let ops_guard = BatchOpsGuard::transaction(ops);
          ops_guard.execute(|| batch_ops.merge_sst_meta());
          ops_guard.execute(|| batch_ops.store_snapshot());
          ops_guard.execute(|| batch_ops.delete_sst_meta());
      }
}

trait BatchOps {
    type State;
    fn need_redo(state: State) -> bool;
    fn redo(state: State) -> Result<()>;
    fn start() -> State;
    fn finish() -> State;
}

impl BatchOps for MergeOps {
    type State = MergeState;

    fn need_redo(state: State) -> bool
    {
         // check the states to determine if to redo
    }
    fn redo(state: State) -> Result<()>
    {
        // just do dedup
    }
    fn start() -> State;
    fn finish() -> State;

     fn merge_sst_meta(..) -> State;
     fn store_snapshot(..) -> State;
     fn delete_sst_meta(..) -> State;
}

impl BatchOpsGuard {
   fn transaction(ops) -> Self
   where ops: BatchOps;
   {
       let state = read_file(..);
        if ops.need_redo(state) {
            ops.redo(state);
        }
        let state = ops.start();
        save_file(state);
        Self { ops }
   }

    fn execute(&self, fn: F) 
    where F: Fn -> BatchOps::State
    {
        let state = fn();
        save_file(state);
    }
}

impl Drop for BatchOpsGuard {
    fn drop(&mut self) {
        let state = self.ops.finish();
        save_file(state);
    }
}
```

### Additional Context

_No response_","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1608/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/horaedb/issues/1623,https://api.github.com/repos/apache/horaedb/issues/1623,horaedb,2788931142,1623,Implement MetricManager,jiacai2050,3848910,Jiacai Liu,dev@liujiacai.net,OPEN,2025-01-15T06:15:02Z,2025-01-15T06:17:23Z,"The main task of MetricManager is to manage metric name of a sample, it will:
1. Calculate the id for metric name, using seahash. Since there is no field concept in prometheus, we could use a hard-coded one, such as `value`, and its type is f64.
2. Persist new name to storage. For existing metric name, we should avoid this step, since it will involve IO operation.
3. In order to support 2, we need to add a cache, and when server start up, we load all existing metric name into cache.","{""url"": ""https://api.github.com/repos/apache/horaedb/issues/1623/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
