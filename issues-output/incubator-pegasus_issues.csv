type,issue_url,comment_url,repo_name,id,issue_num,title,user_login,user_id,user_name,user_email,issue_state,created_at,updated_at,body,reactions
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1,https://api.github.com/repos/apache/incubator-pegasus/issues/1,incubator-pegasus,266725089,1,当一个节点挂了比较长的时间或者向一个replicat set里面新增一个节点，数据是如何同步的,superwood,4394639,superwood,,CLOSED,2017-10-19T05:33:31Z,2017-10-25T01:58:59Z,当一个节点挂了比较长的时间，重新复活后数据是如何同步的？,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzNzgxNTgzMg==,incubator-pegasus,337815832,1,NA,shengofsun,1041832,Vincent,,NA,2017-10-19T06:51:43Z,2017-10-19T06:51:43Z,"在pegasus里这一过程叫做learn。
简单概括下过程：replica set中每个副本都会保存自己commit到WAL的哪一条了，然后开始向leader汇报。leader根据副本所在的点，来决定是要拷贝数据库checkpoint，还是要拷贝增量log，还是要把内存缓存拷贝过去。这几个阶段也不是完全互斥的，有可能是接力的过程(先checkpoint, 再log, 再内存缓存)。

几个比较关键的点是：
1. 拷贝log时怎样做到高效。这个在[设计与实现](https://github.com/XiaoMi/pegasus/wiki/%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0#sharedlog%E5%92%8Cprivatelog)里强调过，每个副本都有自己的private log。
2. 副本在向leader追数据的时候，leader可能也在接受写。所以在同步到某个阶段时，leader会开始把自己最新收到的数据一起发送给副本。
3. 同步的过程需要限流别把集群搞崩

整个过程中，为了把数据拷完整，细节还是比较微妙的。这个就只能看代码了：[replica_learn.cpp](https://github.com/XiaoMi/rdsn/blob/3c3541f48dee15936422b5533f9be556de05edf8/src/dist/replication/lib/replica_learn.cpp)

如果真对pegasus感兴趣，强烈建议先看PacificA的论文。欢迎PR :-)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzNzgxNTgzMg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzOTE4OTg1OA==,incubator-pegasus,339189858,1,NA,Allianzcortex,7290780,,,NA,2017-10-25T01:58:59Z,2017-10-25T01:58:59Z,@shengofsun 有机会的时候看一下，好像蛮有趣的，用 PacificA 做的还是少。第一眼的时候以为在用 zk 。,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzOTE4OTg1OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2,https://api.github.com/repos/apache/incubator-pegasus/issues/2,incubator-pegasus,266725532,2,基于Hash的partition的调度,xieyilun,5448978,解轶伦,xylnasos@qq.com,CLOSED,2017-10-19T05:36:49Z,2017-10-19T13:55:28Z,"我看到你们在微信上的文章，partition的调度使用了网络流算法，正好我以前针对这个问题做过一个费用流的算法，目标里包括了分机架和总迁移量尽量小，感兴趣的话可以参考一下。

http://www.jianshu.com/p/686b55e2f96b","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzNzgxMzY4OA==,incubator-pegasus,337813688,2,NA,shengofsun,1041832,Vincent,,NA,2017-10-19T06:39:15Z,2017-10-19T06:39:15Z,"多谢提建议，partition的调度我们后期会继续优化，我也想过费用流的模型，你的算法我也会仔细学习:-) 
后面假如真用你算法了，是不是直接在代码里附上链接就好?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzNzgxMzY4OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzNzgzNTc3MA==,incubator-pegasus,337835770,2,NA,xieyilun,5448978,解轶伦,xylnasos@qq.com,NA,2017-10-19T08:24:24Z,2017-10-19T08:24:24Z,可以,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzNzgzNTc3MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzNzkxNTM4OA==,incubator-pegasus,337915388,2,NA,shengofsun,1041832,Vincent,,NA,2017-10-19T13:55:28Z,2017-10-19T13:55:28Z,resolved,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzNzkxNTM4OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/3,https://api.github.com/repos/apache/incubator-pegasus/issues/3,incubator-pegasus,267946475,3,小米的开源不够热闹啊，我来消灭一下0回复,liudf0716,1182593,staylightblow8,liudf0716@gmail.com,CLOSED,2017-10-24T08:46:26Z,2017-10-29T07:53:27Z,如题,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/3/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/3,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzODk1MTIxNA==,incubator-pegasus,338951214,3,NA,shengofsun,1041832,Vincent,,NA,2017-10-24T10:49:49Z,2017-10-24T10:49:49Z,3q,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzODk1MTIxNA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/3,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0MDI0NDA3Nw==,incubator-pegasus,340244077,3,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2017-10-29T07:53:24Z,2017-10-29T07:53:24Z,多谢@liudf0716 捧场哈,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0MDI0NDA3Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/4,https://api.github.com/repos/apache/incubator-pegasus/issues/4,incubator-pegasus,267948545,4,有怎么编译的说明吗？,liudf0716,1182593,staylightblow8,liudf0716@gmail.com,CLOSED,2017-10-24T08:52:56Z,2018-07-19T03:03:49Z,"run.sh build 不行
cd src
cmake ..

![image](https://user-images.githubusercontent.com/1182593/31933479-ce8687d2-b8db-11e7-8966-44d116e598d4.png)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/4/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/4,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzODk1MTA0OA==,incubator-pegasus,338951048,4,NA,shengofsun,1041832,Vincent,,NA,2017-10-24T10:49:00Z,2017-10-24T10:49:00Z,"目前直接调用用cmake是不能编译的，随着后面的重构这一点可能改进下。

是按[编译](https://github.com/XiaoMi/pegasus/wiki/%E7%BC%96%E8%AF%91%E6%9E%84%E5%BB%BA)的做法吗？出什么问题了？可以给个截图么？

注意拖代码的时候要把rdsn这个submodule也拖下来的。
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzODk1MTA0OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/4,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzOTI2ODU1OA==,incubator-pegasus,339268558,4,NA,liudf0716,1182593,staylightblow8,liudf0716@gmail.com,NA,2017-10-25T09:19:41Z,2017-10-25T09:19:41Z,编译文档写的挺详细的，可惜是github下载的时候下不动。,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzOTI2ODU1OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/4,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzOTI5Mzg1NQ==,incubator-pegasus,339293855,4,NA,shengofsun,1041832,Vincent,,NA,2017-10-25T11:02:40Z,2017-10-25T11:02:40Z,主要是从github上拖第三方库不太友好，后面我们会把第三方库的下载和编译过程抽出来，到时候也可以考虑换个下载起来快的镜像。,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzOTI5Mzg1NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/4,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzOTU0Mjg3NA==,incubator-pegasus,339542874,4,NA,Allianzcortex,7290780,,,NA,2017-10-26T03:46:36Z,2017-10-26T03:46:36Z,@shengofsun  build standalone cluster on laptop ubuntu 14.04 LTS successfully~~~,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzOTU0Mjg3NA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/4,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0MDI0NDIwMA==,incubator-pegasus,340244200,4,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2017-10-29T07:56:16Z,2017-10-29T07:56:16Z,"@liudf0716 ，yes you are right, the download speed from github is badly slow. We are considering some solution to make it faster and more friendly.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0MDI0NDIwMA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/4,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NDgxNjU3MQ==,incubator-pegasus,344816571,4,NA,shengofsun,1041832,Vincent,,NA,2017-11-16T05:01:52Z,2017-11-16T05:01:52Z,"@liudf0716 @Allianzcortex 
现在第三方依赖的下载在单独的脚本[rdsn/thirdparty/download-thirdparty.sh](https://github.com/XiaoMi/rdsn/blob/master/thirdparty/download-thirdparty.sh)中进行。
在编译的时候不用指定-g选项了。
依赖下载现在也只下载压缩包，而不用clone整个依赖的repo，下载速度应该有很大的提升。
另外欢迎加入我们的[slack讨论群](https://join.slack.com/t/pegasus-kv/shared_invite/enQtMjcyMjQzOTk4Njk1LWVkMjlkMGE5Mzg1Y2M3MDc0NGYyYzQ5YzYyMGE0ZjlhMDMyNjU1ZGViYzdjZmUwNjVmNGE0ZDdkMWJiN2Q1MDY)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NDgxNjU3MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/4,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0Nzc4NTg1Ng==,incubator-pegasus,347785856,4,NA,31737951,17918309,,,NA,2017-11-29T08:21:21Z,2017-11-29T08:21:21Z,"In file included from /home/x/pegasus/src/server/info_collector.cpp:5:0:
/home/x/pegasus/src/server/info_collector.h:15:20: fatal error: evhttp.h: 没有那个文件或目录
 #include <evhttp.h>
                    ^
compilation terminated.
server/CMakeFiles/pegasus_server.dir/build.make:86: recipe for target 'server/CMakeFiles/pegasus_server.dir/info_collector.cpp.o' failed
make[2]: *** [server/CMakeFiles/pegasus_server.dir/info_collector.cpp.o] Error 1
CMakeFiles/Makefile2:308: recipe for target 'server/CMakeFiles/pegasus_server.dir/all' failed
make[1]: *** [server/CMakeFiles/pegasus_server.dir/all] Error 2
make[1]: *** 正在等待未完成的任务....
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0Nzc4NTg1Ng==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/4,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0Nzc4NjU4OQ==,incubator-pegasus,347786589,4,NA,31737951,17918309,,,NA,2017-11-29T08:24:56Z,2017-11-29T08:24:56Z,"编译的时候出了个错误,环境 ububtu ,按照编译说明来的

还有 ,哥们,能不能出个编译好的,建个编译好的环境实在是麻烦

出个预览版本也不错啊,大家可以先使用上有个初步认识

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0Nzc4NjU4OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/4,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0Nzc4NzYxNw==,incubator-pegasus,347787617,4,NA,31737951,17918309,,,NA,2017-11-29T08:29:34Z,2017-11-29T08:29:34Z,"gcc (Ubuntu 6.3.0-12ubuntu2) 6.3.0 20170406
cmake version 3.7.2
/usr/lib/x86_64-linux-gnu/libboost_random.so.1.62.0

其他的安装是
sudo apt-get install build-essential cmake libboost-all-dev libaio-dev libsnappy-dev libbz2-dev libreadline-dev libgflags-dev zlib1g zlib1g.dev
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0Nzc4NzYxNw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/4,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0Nzc5Mjg0Nw==,incubator-pegasus,347792847,4,NA,shengofsun,1041832,Vincent,,NA,2017-11-29T08:52:10Z,2017-11-29T08:52:10Z,@31737951 看上去像是第三方库没有下载安装完成，建议先到rdsn/thirdparty下，清除build src output目录， 执行download-thirdparty.sh和build-thirdparty.sh，再回根目录编译。,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0Nzc5Mjg0Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/4,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NzgyOTA1MA==,incubator-pegasus,347829050,4,NA,31737951,17918309,,,NA,2017-11-29T11:13:10Z,2017-11-29T11:13:10Z,"发现两个问题
1.编译protobuf 时候生成的脚本有问题
2. https://codeload.github.com/libevent/libevent/tar.gz/release-2.0.22-stab 已经失效了","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NzgyOTA1MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/4,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NzgzNDAwMg==,incubator-pegasus,347834002,4,NA,31737951,17918309,,,NA,2017-11-29T11:34:27Z,2017-11-29T11:34:27Z,"cmake_install.cmake:650 
file INSTALL cannot find
  ""/home/x/pegasusmast/rdsn/thirdparty/build/protobuf//home/x/pegasusmast/rdsn/thirdparty/output/lib/cmake/protobuf"".

这个地方的路径生成错误了,修改正确后就可以编译过了
make -j8 && make install

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NzgzNDAwMg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/4,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0ODA3NTIwMA==,incubator-pegasus,348075200,4,NA,31737951,17918309,,,NA,2017-11-30T03:52:25Z,2017-11-30T03:52:25Z,"哥们,编译完毕了想吧部署包剔除来,请问下主要是那些文件或者目录","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0ODA3NTIwMA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/4,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0ODM4Mzg2MQ==,incubator-pegasus,348383861,4,NA,shengofsun,1041832,Vincent,,NA,2017-12-01T02:29:27Z,2017-12-01T02:29:27Z,@31737951 可以参考./run.sh start_onebox的命令是怎么起集群的，也可以参考scripts/pack_server.sh,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0ODM4Mzg2MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/4,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0ODM4NDA5Mg==,incubator-pegasus,348384092,4,NA,shengofsun,1041832,Vincent,,NA,2017-12-01T02:31:08Z,2017-12-01T02:31:08Z,@31737951 欢迎加到我们的[slack讨论群](https://join.slack.com/t/pegasus-kv/shared_invite/enQtMjcyMjQzOTk4Njk1LWVkMjlkMGE5Mzg1Y2M3MDc0NGYyYzQ5YzYyMGE0ZjlhMDMyNjU1ZGViYzdjZmUwNjVmNGE0ZDdkMWJiN2Q1MDY)里面,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0ODM4NDA5Mg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/4,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0OTk0MTUwNw==,incubator-pegasus,349941507,4,NA,31737951,17918309,,,NA,2017-12-07T11:32:19Z,2017-12-07T11:32:19Z,"给后面的人补充下 centos下编译
* 编译说明上有点问题 GCC 4.8.2是编译不过的，目前7下面update下可以升级到 4.8.5 是可以编译过的

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0OTk0MTUwNw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/5,https://api.github.com/repos/apache/incubator-pegasus/issues/5,incubator-pegasus,268977948,5,Pegasus编译出错,mago960806,18606660,,mago960806@hotmail.com,CLOSED,2017-10-27T03:31:53Z,2017-10-29T07:10:21Z,"![image](https://user-images.githubusercontent.com/18606660/32086773-0e422f36-bb0a-11e7-975a-94709618b3f8.png)

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/5,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzOTg4NjI5MA==,incubator-pegasus,339886290,5,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2017-10-27T06:38:49Z,2017-10-27T06:38:49Z,你的cmake版本是？,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzOTg4NjI5MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/5,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzOTk2MjY3MQ==,incubator-pegasus,339962671,5,NA,mago960806,18606660,,mago960806@hotmail.com,NA,2017-10-27T12:51:47Z,2017-10-27T12:51:47Z,cmake version 2.8.12.2,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDMzOTk2MjY3MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/5,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0MDAwNzE1Mg==,incubator-pegasus,340007152,5,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2017-10-27T15:39:20Z,2017-10-27T15:39:20Z,"重新用这个命令试试？
 ./run.sh build -g github -c

On Fri, Oct 27, 2017 at 8:52 PM, mago960806 <notifications@github.com>
wrote:

> cmake version 2.8.12.2
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/XiaoMi/pegasus/issues/5#issuecomment-339962671>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AAgQPXbyet4S_-dPAU4a7z4m_DeqKd-Aks5swdH3gaJpZM4QIgIc>
> .
>
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0MDAwNzE1Mg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/5,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0MDI0MjI3Ng==,incubator-pegasus,340242276,5,NA,mago960806,18606660,,mago960806@hotmail.com,NA,2017-10-29T07:10:18Z,2017-10-29T07:10:18Z,"![image](https://user-images.githubusercontent.com/18606660/32141435-36c392bc-bcbb-11e7-8195-90056b024cf9.png)
感谢！加-c参数之后就编译成功了！","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0MDI0MjI3Ng==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/7,https://api.github.com/repos/apache/incubator-pegasus/issues/7,incubator-pegasus,272479969,7,"unexpected low qps on ssd machine, any thing wrong?",secwang,5615339,Sec Wang,monadic.io@gmail.com,CLOSED,2017-11-09T08:39:57Z,2017-11-09T11:51:46Z,"Init pegasus succeed
LevelDB:    version 4.0
Date:       Thu Nov  9 08:13:07 2017
CPU:        2 * Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz
CPUCache:   46080 KB
Keys:       16 bytes each
Values:     100 bytes each (100 bytes after compression)
Entries:    100000
Prefix:    0 bytes
Keys per prefix:    0
RawSize:    11.1 MB (estimated)
FileSize:   11.1 MB (estimated)
Writes per second: 0
Compression: NoCompression
Memtablerep: skip_list
Perf Level: 0
WARNING: Optimization is disabled: benchmarks unnecessarily slow
WARNING: Assertions are enabled; benchmarks unnecessarily slow


Thread  Count   Runtime QPS     AvgLat  P99Lat
1       10000   11.763  850     1176    2425
2       20000   20.256  987     2018    4953
3       30000   29.534  1015    2938    7817
4       40000   38.809  1030    3765    12794","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/7/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/7,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0MzA5NTUwOQ==,incubator-pegasus,343095509,7,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2017-11-09T09:22:59Z,2017-11-09T09:22:59Z,"Is it read or write? The result shows that the average latency is in range of [1,4] milliseconds, depending on the thread count.
These aspects may improve performance:
- build with -t release option
- use SSD instead of HDD","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0MzA5NTUwOQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/7,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0MzEzMjIzMg==,incubator-pegasus,343132232,7,NA,secwang,5615339,Sec Wang,monadic.io@gmail.com,NA,2017-11-09T11:51:46Z,2017-11-09T11:51:46Z,"build with -t release, Thanks.
Thread Count Runtime QPS AvgLat P99Lat
1 10000 4.9163 2033 491 1377
2 20000 8.1208 2462 808 2443
3 30000 11.623 2580 1153 3607
4 40000 15.369 2602 1517 5742
5 50000 19.670 2541 1937 7728","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0MzEzMjIzMg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/8,https://api.github.com/repos/apache/incubator-pegasus/issues/8,incubator-pegasus,273319997,8,build时出现Not a git repository: ../.git/modules/rdsn 错误,qinhongsheng,948144,,,CLOSED,2017-11-13T06:08:56Z,2017-11-14T08:07:05Z,"环境
CentOS 7.3.1611
kernel 3.10.0-514.el7.x86_64
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)
cmake 2.8.12.2
boost 1.53.0 Release 27.el7

参考：
https://github.com/XiaoMi/pegasus/blob/master/docs/installation.md

1、安装开发包
yum -y install cmake boost-devel libaio-devel snappy-devel bzip2-devel 
readline-devel
2、clone
3、build

日志如下：
ln: failed to create symbolic link ‘/root/pegasus/DSN_ROOT’: File exists
INFO: start build rdsn...
CLEAR=NO
BUILD_TYPE=debug
SERIALIZE_TYPE=
GIT_SOURCE=github
ONLY_BUILD=YES
RUN_VERBOSE=NO
WARNING_ALL=NO
ENABLE_GCOV=NO
Use system boost
CMAKE_OPTIONS= -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ 
-DCMAKE_BUILD_TYPE=Debug -DDSN_GIT_SOURCE=github
MAKE_OPTIONS= -j8
#############################################################################
fatal: Not a git repository: ../.git/modules/rdsn
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/8/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/8,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0MzgyNDA0OQ==,incubator-pegasus,343824049,8,NA,shengofsun,1041832,Vincent,,NA,2017-11-13T06:15:19Z,2017-11-13T06:15:19Z,稍等，我看一下，可能是最近提交的时候子项目的索引没注意,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0MzgyNDA0OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/8,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0MzgyNzI4Ng==,incubator-pegasus,343827286,8,NA,shengofsun,1041832,Vincent,,NA,2017-11-13T06:37:30Z,2017-11-13T06:37:30Z,@qinhongsheng clone的时候是不是没有指定--recursive参数？,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0MzgyNzI4Ng==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/8,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0MzgzNjIxNg==,incubator-pegasus,343836216,8,NA,shengofsun,1041832,Vincent,,NA,2017-11-13T07:33:52Z,2017-11-13T07:33:52Z,@qinhongsheng 我建了一个[slack讨论群](https://join.slack.com/t/pegasus-kv/shared_invite/enQtMjcyMjQzOTk4Njk1LWVkMjlkMGE5Mzg1Y2M3MDc0NGYyYzQ5YzYyMGE0ZjlhMDMyNjU1ZGViYzdjZmUwNjVmNGE0ZDdkMWJiN2Q1MDY)，欢迎进行进一步交流,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0MzgzNjIxNg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/8,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NDE3MDM4Mg==,incubator-pegasus,344170382,8,NA,quanzl,33650959,,,NA,2017-11-14T07:33:40Z,2017-11-14T07:33:40Z,"已确认是 git clone 后的文件夹不完整导致，可能是网络原因引起。

删除整个 pegasus，重新尝试，目前已经正常。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NDE3MDM4Mg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/8,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NDE3Njc4NA==,incubator-pegasus,344176784,8,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2017-11-14T08:06:53Z,2017-11-14T08:06:53Z,"OK, Thanks.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NDE3Njc4NA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/9,https://api.github.com/repos/apache/incubator-pegasus/issues/9,incubator-pegasus,273693089,9,文档未提及CentOS7平台的一点补充,quanzl,33650959,,,CLOSED,2017-11-14T07:31:15Z,2017-11-14T09:00:30Z,"需要两个文档未提及的包
1、zlib-devel
编译时需要，否则出错
2、nmap-ncat
没有这个会在 ./run.sh start_onebox 这一步骤遇到：
./scripts/start_zk.sh: line 62: nc: command not found

目前编译完成，还在继续作新手尝试。

感谢各位开发者的贡献。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/9/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/9,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NDE3NjU3Nw==,incubator-pegasus,344176577,9,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2017-11-14T08:05:53Z,2017-11-14T08:05:53Z,Thanks for your contribution.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NDE3NjU3Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/9,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NDE4MDUzMg==,incubator-pegasus,344180532,9,NA,quanzl,33650959,,,NA,2017-11-14T08:24:18Z,2017-11-14T08:24:18Z," 继续补充，编译过程对内存还是有一点要求的，因为用的是macOS，编译环境用的是1G内存的虚拟机。编译过程很慢，硬盘狂闪，可能有大量的页交换，最后终于 outofmemory。
增加虚拟机内存至4G，编译可以顺利进行。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NDE4MDUzMg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/9,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NDE4MTk1NQ==,incubator-pegasus,344181955,9,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2017-11-14T08:30:11Z,2017-11-14T08:30:11Z,是的，因为C++使用了大量的模板、lambda函数等特性，编译本来就比较耗资源，同时又默认开启并行编译，所以对资源的要求还挺高的。,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NDE4MTk1NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/9,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NDE4Mjk3NA==,incubator-pegasus,344182974,9,NA,quanzl,33650959,,,NA,2017-11-14T08:34:23Z,2017-11-14T08:34:23Z,"目录下有一个 java，这里边并不包含 jvm，不要被它骗了。
如果没有安装 java，zookeeper启动仍然会提示：
Starting zookeeper ... STARTED
其实并没有

随后 nc 会报错，但是启动过程也没有停止，虽然最后看起来启动成功，仍然是连不上的，安装
yum install java-1.8.0-openjdk
这一关就通过了，zkServer.sh start-foreground 会成功，nc 测试通过。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NDE4Mjk3NA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/9,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NDE4NDcwOQ==,incubator-pegasus,344184709,9,NA,shengofsun,1041832,Vincent,,NA,2017-11-14T08:41:37Z,2017-11-14T08:41:37Z,"@quanzl 感谢反馈，我们后续会把这些加入到文档中
java目录下是java客户端，可以参见[Java客户端文档](https://github.com/XiaoMi/pegasus/wiki/Java%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%96%87%E6%A1%A3)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NDE4NDcwOQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/9,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NDE4ODk4Ng==,incubator-pegasus,344188986,9,NA,quanzl,33650959,,,NA,2017-11-14T08:59:19Z,2017-11-14T08:59:19Z,"@shengofsun 做点微薄贡献，希望对别人有点帮助。

最后补充：
1、启动时：
Ncat: Connection refused.
ERROR: start zookeeper failed
这个没有影响，猜测是启动需要时间，nc等待间隔过短引起，物理机上可能没有这个问题。
2、命令行工具
 ./run.sh shell 启动过程有点慢，也许是虚拟机的原因，多等待一会儿就能看到命令提示符。

感谢支持。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0NDE4ODk4Ng==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/10,https://api.github.com/repos/apache/incubator-pegasus/issues/10,incubator-pegasus,279614718,10,能简要的描述下数据分片分裂、动态扩容实现的思路吗？,scottzzq,1217132,scottzzq,zhai_303458@163.com,CLOSED,2017-12-06T03:33:48Z,2018-07-19T03:04:29Z,如题，感觉整个过程还是比较复杂的。多谢！,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/10/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/10,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0OTUyNjY0MA==,incubator-pegasus,349526640,10,NA,shengofsun,1041832,Vincent,,NA,2017-12-06T04:07:14Z,2017-12-06T04:07:14Z,下周就写，写好加到文档里，敬请期待,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0OTUyNjY0MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/10,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0OTU0MDAyNQ==,incubator-pegasus,349540025,10,NA,shengofsun,1041832,Vincent,,NA,2017-12-06T05:37:38Z,2017-12-06T05:37:38Z,@scottzzq 可以加入到我们的[slack讨论群](https://join.slack.com/t/pegasus-kv/shared_invite/enQtMjcyMjQzOTk4Njk1LWVkMjlkMGE5Mzg1Y2M3MDc0NGYyYzQ5YzYyMGE0ZjlhMDMyNjU1ZGViYzdjZmUwNjVmNGE0ZDdkMWJiN2Q1MDY),"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM0OTU0MDAyNQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/10,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5MzkwOTYzNg==,incubator-pegasus,393909636,10,NA,sunwsh,4278126,,,NA,2018-06-01T15:07:01Z,2018-06-01T15:07:01Z,同样期待，这周开始看这个pegasus系统，也想了解这个，这两天就看代码，还没找到。,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5MzkwOTYzNg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/10,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5Mzk2MTYxOA==,incubator-pegasus,393961618,10,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-06-01T17:57:37Z,2018-06-01T17:57:37Z,https://github.com/XiaoMi/rdsn/issues/69,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5Mzk2MTYxOA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/11,https://api.github.com/repos/apache/incubator-pegasus/issues/11,incubator-pegasus,281272819,11,Did pegasus support distributed transaction?,kewei-wang,11596924,Wang kewei,,CLOSED,2017-12-12T06:33:12Z,2019-04-24T05:32:24Z,"just like the issue title,did it support distributed transaction so I can use it like the following code?
```java
pegasus.beginTransaction();
pegasus.put(""key"",value);
v = pegasus.get(""key"");
v++;
pegasus.put(""key"",v);
pegasus.commit();
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/11/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/11,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM1MTI2ODczNA==,incubator-pegasus,351268734,11,NA,shengofsun,1041832,Vincent,,NA,2017-12-13T03:09:21Z,2017-12-13T03:09:21Z,"@kowayvwang currently we don't support distributed transaction. the only guarantee we can give is that multiple put operations of sortkeys **on the same hashkey** are atomic. 
that is to say, if you write code like this: 
```
pegasus.multi_set(""aa"", [(""bb"", ""cc""), (""dd"", ee), (""ff"" -> ""gg"") ...])
```
the operations of ""aa:bb -> cc"" & ""aa:dd -> ee"" & ""aa:ff"" -> ""gg"" will all success or all fail.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM1MTI2ODczNA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/13,https://api.github.com/repos/apache/incubator-pegasus/issues/13,incubator-pegasus,305463581,13,core in aio_task::~aio_task(),qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-03-15T08:56:54Z,2018-05-25T10:19:02Z,"# 时间
2018/03/15 15:24
# 版本
Pegasus Server 1.7.0 (9a7a06716a52cab85a239b3c8b8ca3c037a8dfdf) Release
# 平台
CentOS release 6.3 (Final)
# 现场
work@c3-hadoop-ssd-tst-st04
/home/work/coresave/issue-13
# 栈信息
```
#0  0x000000376e4328a5 in raise () from /lib64/libc.so.6
#1  0x000000376e434085 in abort () from /lib64/libc.so.6
#2  0x000000376e46ffe7 in __libc_message () from /lib64/libc.so.6
#3  0x000000376e475916 in malloc_printerr () from /lib64/libc.so.6
#4  0x00007f59df3ebe24 in deallocate (this=<optimized out>, __p=<optimized out>) at /home/work/qinzuoyan/Pegasus/toolchain/output/include/c++/4.8.2/ext/new_allocator.h:110
#5  _M_deallocate (this=<optimized out>, __n=<optimized out>, __p=<optimized out>) at /home/work/qinzuoyan/Pegasus/toolchain/output/include/c++/4.8.2/bits/stl_vector.h:174
#6  ~_Vector_base (this=0x7f5665200abc, __in_chrg=<optimized out>) at /home/work/qinzuoyan/Pegasus/toolchain/output/include/c++/4.8.2/bits/stl_vector.h:160
#7  ~vector (this=0x7f5665200abc, __in_chrg=<optimized out>) at /home/work/qinzuoyan/Pegasus/toolchain/output/include/c++/4.8.2/bits/stl_vector.h:416
#8  dsn::aio_task::~aio_task (this=0x7f56652009e4, __in_chrg=<optimized out>) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/task.cpp:682
#9  0x00007f59df3ebe89 in dsn::aio_task::~aio_task (this=0x7f56652009e4, __in_chrg=<optimized out>) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/task.cpp:682
#10 0x00007f59df3ed6ea in release_ref (this=0x7f56652009e4) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/include/dsn/utility/autoref_ptr.h:76
#11 dsn::task::exec_internal (this=this@entry=0x7f56652009e4) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/task.cpp:242
#12 0x00007f59df47e3fd in dsn::task_worker::loop (this=0x12926f0) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/task_worker.cpp:323
#13 0x00007f59df47e5c9 in dsn::task_worker::run_internal (this=0x12926f0) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/task_worker.cpp:302
#14 0x00007f59dd528600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#15 0x000000376e807851 in start_thread () from /lib64/libpthread.so.0
#16 0x000000376e4e811d in clone () from /lib64/libc.so.6
```
# 分析
- 在aio_task的析构函数出core，具体是在析构_unmerged_write_buffers变量时：
```
std::vector<dsn_file_buffer_t> _unmerged_write_buffers;
```
- task code为LPC_WRITE_REPLICATION_LOG_SHARED，是写shared log的回调task
- aio_task的ref_counter显示正常：
```
    <dsn::ref_counter> = {
      _vptr.ref_counter = 0x7f59df77b1f0 <vtable for dsn::aio_task+16>, 
      _magic = 3735928559, 
      _counter = {
        <std::__atomic_base<long>> = {
          _M_i = 0
        }, <No data fields>}
    }, 
```
- _unmerged_write_buffers的栈信息显示正常：
```
  _unmerged_write_buffers = {
    <std::_Vector_base<dsn_file_buffer_t, std::allocator<dsn_file_buffer_t> >> = {
      _M_impl = {
        <std::allocator<dsn_file_buffer_t>> = {
          <__gnu_cxx::new_allocator<dsn_file_buffer_t>> = {<No data fields>}, <No data fields>}, 
        members of std::_Vector_base<dsn_file_buffer_t, std::allocator<dsn_file_buffer_t> >::_Vector_impl: 
        _M_start = 0x7f56ff82fe20, 
        _M_finish = 0x7f56ff82fe50, 
        _M_end_of_storage = 0x7f56ff82fe60
      }
    }, <No data fields>}, 
```
- 但是_unmerged_write_buffers的内容显示不正常：
```
(gdb) pvector this._unmerged_write_buffers
elem[0]: $2 = {
  buffer = 0x0, 
  size = 0
}
elem[1]: $3 = {
  buffer = 0x0, 
  size = 0
}
elem[2]: $4 = {
  buffer = 0x7f56642154b0, 
  size = 0
}
Vector size = 3
Vector capacity = 4
Element type = std::_Vector_base<dsn_file_buffer_t, std::allocator<dsn_file_buffer_t> >::pointer
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/13/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/13,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5MjAwOTQwNg==,incubator-pegasus,392009406,13,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-05-25T10:18:39Z,2018-05-25T10:18:39Z,fixed in #49 ,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5MjAwOTQwNg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/14,https://api.github.com/repos/apache/incubator-pegasus/issues/14,incubator-pegasus,309695463,14,编译thrift失败,zhanglistar,985418,zhanglistar,zhanglinuxstar@gmail.com,CLOSED,2018-03-29T10:05:56Z,2018-03-29T10:51:37Z,"系统：ubuntu16.04

➜  thirdparty git:(dc3a3ee) ✗ ./build-thirdparty.sh 
+++ dirname ./build-thirdparty.sh
++ cd .
++ pwd
+ TP_DIR=/home/listar/Code/pegasus/rdsn/thirdparty
+ TP_SRC=/home/listar/Code/pegasus/rdsn/thirdparty/src
+ TP_BUILD=/home/listar/Code/pegasus/rdsn/thirdparty/build
+ TP_OUTPUT=/home/listar/Code/pegasus/rdsn/thirdparty/output
+ export CC=gcc
+ CC=gcc
+ export CXX=g++
+ CXX=g++
+ CLEAR_OLD_BUILD=NO
+ BOOST_ROOT=
+ [[ 0 > 0 ]]
+ '[' NO = YES ']'
+ mkdir -p /home/listar/Code/pegasus/rdsn/thirdparty/output/include
+ mkdir -p /home/listar/Code/pegasus/rdsn/thirdparty/output/lib
+ mkdir -p /home/listar/Code/pegasus/rdsn/thirdparty/output/bin
+ '[' '!' -d /home/listar/Code/pegasus/rdsn/thirdparty/output/include/concurrentqueue ']'
+ echo 'skip build concurrentqueue'
skip build concurrentqueue
+ '[' '!' -d /home/listar/Code/pegasus/rdsn/thirdparty/output/include/gtest ']'
+ echo 'skip build gtest'
skip build gtest
+ '[' '!' -d /home/listar/Code/pegasus/rdsn/thirdparty/output/include/rapidjson ']'
+ '[' '!' -d /home/listar/Code/pegasus/rdsn/thirdparty/output/include/thrift ']'
+ mkdir -p /home/listar/Code/pegasus/rdsn/thirdparty/build/thrift-0.9.3
+ cd /home/listar/Code/pegasus/rdsn/thirdparty/build/thrift-0.9.3
+ CMAKE_FLAGS='-DCMAKE_BUILD_TYPE=release        -DWITH_JAVA=OFF        -DWITH_PYTHON=OFF        -DWITH_C_GLIB=OFF        -DWITH_CPP=ON        -DBUILD_TESTING=OFF        -DBUILD_EXAMPLES=OFF        -DWITH_QT5=OFF        -DWITH_QT4=OFF        -DWITH_OPENSSL=OFF        -DBUILD_COMPILER=OFF        -DBUILD_TUTORIALS=OFF        -DWITH_LIBEVENT=OFF        -DCMAKE_INSTALL_PREFIX=/home/listar/Code/pegasus/rdsn/thirdparty/output        -DCMAKE_POSITION_INDEPENDENT_CODE=ON        -DWITH_SHARED_LIB=OFF'
+ '[' x '!=' x ']'
+ echo -DCMAKE_BUILD_TYPE=release -DWITH_JAVA=OFF -DWITH_PYTHON=OFF -DWITH_C_GLIB=OFF -DWITH_CPP=ON -DBUILD_TESTING=OFF -DBUILD_EXAMPLES=OFF -DWITH_QT5=OFF -DWITH_QT4=OFF -DWITH_OPENSSL=OFF -DBUILD_COMPILER=OFF -DBUILD_TUTORIALS=OFF -DWITH_LIBEVENT=OFF -DCMAKE_INSTALL_PREFIX=/home/listar/Code/pegasus/rdsn/thirdparty/output -DCMAKE_POSITION_INDEPENDENT_CODE=ON -DWITH_SHARED_LIB=OFF
-DCMAKE_BUILD_TYPE=release -DWITH_JAVA=OFF -DWITH_PYTHON=OFF -DWITH_C_GLIB=OFF -DWITH_CPP=ON -DBUILD_TESTING=OFF -DBUILD_EXAMPLES=OFF -DWITH_QT5=OFF -DWITH_QT4=OFF -DWITH_OPENSSL=OFF -DBUILD_COMPILER=OFF -DBUILD_TUTORIALS=OFF -DWITH_LIBEVENT=OFF -DCMAKE_INSTALL_PREFIX=/home/listar/Code/pegasus/rdsn/thirdparty/output -DCMAKE_POSITION_INDEPENDENT_CODE=ON -DWITH_SHARED_LIB=OFF
+ cmake /home/listar/Code/pegasus/rdsn/thirdparty/src/thrift-0.9.3 -DCMAKE_BUILD_TYPE=release -DWITH_JAVA=OFF -DWITH_PYTHON=OFF -DWITH_C_GLIB=OFF -DWITH_CPP=ON -DBUILD_TESTING=OFF -DBUILD_EXAMPLES=OFF -DWITH_QT5=OFF -DWITH_QT4=OFF -DWITH_OPENSSL=OFF -DBUILD_COMPILER=OFF -DBUILD_TUTORIALS=OFF -DWITH_LIBEVENT=OFF -DCMAKE_INSTALL_PREFIX=/home/listar/Code/pegasus/rdsn/thirdparty/output -DCMAKE_POSITION_INDEPENDENT_CODE=ON -DWITH_SHARED_LIB=OFF
-- Parsed Thrift package version: 0.9.3
-- Parsed Thrift version: 0.9.3 (0.9.3)
-- Building without tests
-- ----------------------------------------------------------
-- Thrift version:                       0.9.3 (0.9.3)
-- Thrift package version:               0.9.3
-- Build configuration Summary
--   Build Thrift compiler:              OFF
--   Build with unit tests:              OFF
--   Build examples:                     OFF
--   Build Thrift libraries:             ON
--  Language libraries:
--   Build C++ library:                  OFF
--    - Boost headers missing
--   Build C (GLib) library:             OFF
--    - Disabled by via WITH_C_GLIB=OFF
--   Build Java library:                 OFF
--    - Disabled by via WITH_JAVA=OFF
--    - Ant missing
--   Build Python library:               OFF
--    - Disabled by via WITH_PYTHON=OFF
--  Library features:
--   Build shared libraries:             OFF
--   Build static libraries:             ON
--   Build with ZLIB support:            ON
--   Build with libevent support:        OFF
--   Build with Qt4 support:             OFF
--   Build with Qt5 support:             OFF
--   Build with OpenSSL support:         OFF
--   Build with Boost thread support:    OFF
--   Build with C++ std::thread support: OFF
-- ----------------------------------------------------------
-- Configuring done
-- Generating done
-- Build files have been written to: /home/listar/Code/pegasus/rdsn/thirdparty/build/thrift-0.9.3
+ make -j8
+ make install
make: *** No rule to make target 'install'。 停止。
+ res=2
+ cd /home/listar/Code/pegasus/rdsn/thirdparty
+ exit_if_fail thrift 2
+ '[' 2 -ne 0 ']'
+ echo 'build thrift failed'
build thrift failed
+ exit 2
➜  thirdparty

## 分析
找到原因了，是boost版本太老导致，系统中并存了2个版本，1个版本太老是1.44版本，删除了就ok了。
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/14/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/14,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM3NzE5MTIxNA==,incubator-pegasus,377191214,14,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-03-29T10:19:17Z,2018-03-29T10:19:17Z,"gdb和cmake的版本号都是多少？
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM3NzE5MTIxNA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/14,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM3NzE5ODI4MA==,incubator-pegasus,377198280,14,NA,zhanglistar,985418,zhanglistar,zhanglinuxstar@gmail.com,NA,2018-03-29T10:51:32Z,2018-03-29T10:51:32Z,"找到原因了，是boost版本太老导致，系统中并存了2个版本，1个版本太老是1.44版本，删除了就ok了。

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM3NzE5ODI4MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/21,https://api.github.com/repos/apache/incubator-pegasus/issues/21,incubator-pegasus,316125067,21,Question about reconfiguration mechanism in Pegasus,lorneli,24292047,,lorneli@163.com,CLOSED,2018-04-20T04:36:13Z,2018-04-20T06:59:24Z,"Hi I'm getting started with Pegasus by reading PacificA consensus algorithm.

In the paper, primary/secondary data node could suspect its peer has become faulty,
then it reports to configuration manager. 

After the configuration manager removes the faulty one, the replica count in this 
replication group has been reduced.

In Pegasus's implementation, does the configuration manager pick up a new node and 
add it to this replication group automatically? Or do this via some tools?

Thanks in advance.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/21/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/21,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM4Mjk4ODkwMg==,incubator-pegasus,382988902,21,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2018-04-20T06:07:05Z,2018-04-20T06:07:05Z,"yes. meta server (aka configuration manager) will handle this.
See this https://github.com/XiaoMi/pegasus/wiki/%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0#replicaserver%E7%9A%84%E7%AE%A1%E7%90%86.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM4Mjk4ODkwMg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/21,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM4Mjk5OTczNg==,incubator-pegasus,382999736,21,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-04-20T06:56:39Z,2018-04-20T06:56:39Z,"Yes, automatically.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM4Mjk5OTczNg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/21,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM4MzAwMDM1Mw==,incubator-pegasus,383000353,21,NA,lorneli,24292047,,lorneli@163.com,NA,2018-04-20T06:59:12Z,2018-04-20T06:59:12Z,@neverchanje @qinzuoyan  Thanks. Haven't noticed the wiki page:see_no_evil:,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM4MzAwMDM1Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/26,https://api.github.com/repos/apache/incubator-pegasus/issues/26,incubator-pegasus,317046031,26,add kill_partition for kill_test,shengofsun,1041832,Vincent,,CLOSED,2018-04-24T02:15:14Z,2018-05-30T15:40:27Z,"currently if we want to test the learning in the kill test, we should only kill the replica server process, which is not friendly for memory leak test. perhaps we'd better add a command of killing one partition but not the whole replica server process.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/26/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/26,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5MzIxMDQ2MA==,incubator-pegasus,393210460,26,NA,shengofsun,1041832,Vincent,,NA,2018-05-30T15:40:27Z,2018-05-30T15:40:27Z,resolved in #50 ,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5MzIxMDQ2MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/41,https://api.github.com/repos/apache/incubator-pegasus/issues/41,incubator-pegasus,321851787,41,"it's needed to judge system load before heavy load work start, eg. manual compact",acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2018-05-10T08:30:18Z,2018-07-19T03:02:37Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/41/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/58,https://api.github.com/repos/apache/incubator-pegasus/issues/58,incubator-pegasus,326433806,58,Hive Pegasus Integration,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,OPEN,2018-05-25T08:47:26Z,2018-09-13T12:36:08Z,"Just like [Hive HBase Integration](https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration) and [Hive Mongo Integration](https://github.com/mongodb/mongo-hadoop/wiki/Hive-Usage), we can integrate Pegasus into hive, then users can query SQL on Pegasus, like [Using Hive to interact with HBase](https://hortonworks.com/blog/hbase-via-hive-part-1/).

Sounds cool, isn't it?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/58/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/59,https://api.github.com/repos/apache/incubator-pegasus/issues/59,incubator-pegasus,326436124,59,Geo Support,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-05-25T08:54:29Z,2018-09-13T08:59:42Z,"# User demand
- Near query
- With​in query

# Related work
- ​RocksDB
https://github.com/facebook/rocksdb/blob/master/include/rocksdb/utilities/geo_db.h
https://github.com/facebook/rocksdb/tree/master/utilities/geodb
- Spatial indexing in RocksDB​
https://rocksdb.org/blog/2015/07/17/spatial-indexing-in-rocksdb.html
https://code.facebook.com/videos/632343316893163/geo-spatial-features-in-rocksdb-scale-presentation/
- GeoHash
http://www.cnblogs.com/LBSer/p/3310455.html
- Spatial Keys: Memory Efficient Geohashes
https://karussell.wordpress.com/2012/05/23/spatial-keys-memory-efficient-geohashes/
- Google S2
http://s2geometry.io/​
https://www.jianshu.com/p/7332dcb978b2
https://s2.sidewalklabs.com/regioncoverer/
- Mongodb Geo2d
https://cloud.tencent.com/developer/article/1004794","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/59/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/59,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NTk0NzkyOQ==,incubator-pegasus,395947929,59,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2018-06-09T07:34:05Z,2018-06-09T07:34:05Z,code preview: https://github.com/XiaoMi/pegasus/pull/74,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NTk0NzkyOQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/59,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNjEzNzI0Nw==,incubator-pegasus,406137247,59,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-07-19T02:46:20Z,2018-07-19T02:46:20Z,"测试环境：
tjwqtst-staging
5个Replica-Server节点
机型：```金山云I2.16C | 1TB SSD```

测试数据：
LBS的线上真实数据

测试接口：
geo库的search_radial​接口：
```c++
int search_radial(double lat_degrees,
                            double lng_degrees,
                            double radius_m,
                            int count,
                            SortType sort_type,
                            int timeout_milliseconds,
                            std::list<SearchResult> &result);
```

参数：
* lat_degrees、lng_degrees：每次都选取北京五环内的随机点
* radius_m：如下表第一列，单位米
* count：-1，表示不限定结果数量
* sort_type：不排序

测试结果：
接口耗时，单位ms

搜索半径(m) | P50​ | P75 | P99 | P99.9
-- | -- | -- | -- | --
50 | 5.74500703 | 6.61061453 | 16.88 | 29.33333333
100 | 6.755 | 9.255 | 23.57142857 | 160
200 | 11.16315789 | 17.24050633 | 46.33333333 | 58.407519
300 | 17.82186235 | 28.5245098 | 71.33333333 | 106.7272727
500 | 35.95890411 | 57.41176471 | 131.4285714 | 157.1428571
1000 | 102.6530612 | 159.6453901 | 368.8571429 | 510.877916
2000 | 277.5409836 | 437.406639 | 838.4659091 | 921.878034
5000 | 1166.537468 | 1677.319588 | 2416.807227 | 2416.807227
10000 | 4015.98878 | 5372.027972 | 7470.800489 | 7470.800489

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNjEzNzI0Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/59,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNjEzODYzMw==,incubator-pegasus,406138633,59,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-07-19T02:55:40Z,2018-07-19T02:55:40Z,"使用线上环境重新测试：
c4tst-geo
5个Replica-Server节点
机型：```i2 | E5-2620v3 *2 | 8* 480G SSD(RAID 50) | 8* 16G```

测试结果：

  | P50 | P75 | P99 | P99.9
-- | -- | -- | -- | --
50 | 3.64939416 | 4.14832502 | 8.27692308 | 14
100 | 3.82015066 | 4.85101289 | 9.43780488 | 14
200 | 5.15465686 | 6.50816832 | 16.57142857 | 21.71428571
300 | 6.57623762 | 9.2990099 | 21.83333333 | 33
500 | 11.13965517 | 17.17241379 | 39.85714286 | 47.094414
1000 | 30.9528536 | 47.34599156 | 105.4482759 | 137.157581
2000 | 90.95535714 | 138.5869565 | 260.9090909 | 293.528327
5000 | 420.0909091 | 607.0136519 | 844.556314 | 977.725444
10000 | 1497.988827 | 1889.050279 | 2771.933472 | 2872.54099

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNjEzODYzMw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/59,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNjEzOTQzOA==,incubator-pegasus,406139438,59,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-07-19T03:00:45Z,2018-07-19T03:00:45Z,"测试数据和环境与上面c4tst-geo一样。
LBS的线上真实数据，请求北京五环内的随机点，半径50~2000米。

经过优化，目前的测试数据如下：

半径(m)\metric | P50(ms) | P75(ms) | P99(ms) | P99.9(ms)​ | 平均结果条数 | 单节点QPS
-- | -- | -- | -- | -- | -- | --
50 | 1.63071622 | 1.84607433 | 4.04545455 | 6.28 | 9.4608 | 740.287
100 | 1.76 | 2.33614794 | 5.4 | 6.45319149 | 38.0296 | 656.66
200 | 2.41017042 | 3.31062092 | 6.41781609 | 9.60588235 | 154.3682 | 536.624
300 | 3.30833333 | 4.21979167 | 9.4310559 | 18 | 350.9676 | 434.491
500 | 5.07763975 | 6.84964682 | 16.84931507 | 21.78082192 | 986.0826 | 347.231
1000 | 12.28164727 | 18.70972532 | 43.18181818 | 57.049698 | 3947.5294 | 204.23
2000 | 35.78666667 | 54.7300885 | 108.7331378 | 148.616578 | 15674.1198 | 98.7633

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNjEzOTQzOA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/60,https://api.github.com/repos/apache/incubator-pegasus/issues/60,incubator-pegasus,326463458,60,Datax Support,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-05-25T10:14:08Z,2018-06-11T09:34:31Z,"[Alibaba DataX](https://github.com/alibaba/DataX)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/60/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/60,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NDA5OTQzOA==,incubator-pegasus,394099438,60,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-06-02T16:28:59Z,2018-06-02T16:28:59Z,"实现了一个简单的PegasusWriter，可以实现从其他数据源向Pegasus导数据：
https://github.com/xiaomi/pegasus-datax

使用方法参见文档：
https://github.com/xiaomi/pegasus-datax/blob/master/pegasuswriter/doc/pegasuswriter.md","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NDA5OTQzOA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/65,https://api.github.com/repos/apache/incubator-pegasus/issues/65,incubator-pegasus,328460633,65,Scan only hash_key,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-06-01T10:12:51Z,2019-06-09T14:45:24Z,Only scan hash_key when do full scan.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/65/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/65,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMDIxNzQ3Nw==,incubator-pegasus,500217477,65,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-06-09T14:45:24Z,2019-06-09T14:45:24Z,Maybe we can reopen this issue when needed.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMDIxNzQ3Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/67,https://api.github.com/repos/apache/incubator-pegasus/issues/67,incubator-pegasus,329336368,67,编译失败,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-06-05T07:40:10Z,2018-06-05T11:21:49Z,"编译环境：
机器： Linux version 3.2.0-61-generic (buildd@roseapple) (gcc version 4.6.3 (Ubuntu/Linaro 4.6.3-1ubuntu5) ) #93-Ubuntu SMP Fri May 2 21:31:50 UTC 2014
GCC：4.8.4
CMake：2.8.12.2
错误信息：
```
skip build fmtlib
skip build Poco
-DPOCO_INCLUDE=/data/bigdata/pegasus/pegasus/rdsn/thirdparty/output/include -DPOCO_LIB=/data/bigdata/pegasus/pegasus/rdsn/thirdparty/output/lib -DGTEST_INCLUDE=/data/bigdata/pegasus/pegasus/rdsn/thirdparty/output/include -DGTEST_LIB=/data/bigdata/pegasus/pegasus/rdsn/thirdparty/output/lib -DCMAKE_POSITION_INDEPENDENT_CODE=ON
-- Configuring done
-- Generating done
-- Build files have been written to: /data/bigdata/pegasus/pegasus/rdsn/thirdparty/build/fds
[ 78%] Built target galaxy-fds-sdk-cpp
[ 84%] Built target sample
Linking CXX executable testrunner
/usr/bin/ld: cannot find -lgtest
/usr/bin/ld: cannot find -lgtest_main
collect2: error: ld returned 1 exit status
make[2]: *** [test/testrunner] Error 1
make[1]: *** [test/CMakeFiles/testrunner.dir/all] Error 2
make: *** [all] Error 2
build fds failed
ERROR: build rdsn failed
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/67/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/67,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NDYyNjA4MA==,incubator-pegasus,394626080,67,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-06-05T08:28:38Z,2018-06-05T08:28:38Z,"有可能是用户在编译third-party到一半的时候，Ctrl+C终止编译，这时候留下了一个不完整的third-party。重新编译的时候，就会出问题。
建议：增加 --clear_thirddparty 选项，自动清理thirdparty相关文件夹。
如果要完全清理环境，可以这样：
```
./run.sh build -c --clear_thirdparty 
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NDYyNjA4MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/67,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NDY1OTk0MA==,incubator-pegasus,394659940,67,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-06-05T10:25:56Z,2018-06-05T10:25:56Z,相关修复：https://github.com/XiaoMi/rdsn/pull/84,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NDY1OTk0MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/67,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NDY2MjI2NA==,incubator-pegasus,394662264,67,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-06-05T10:35:17Z,2018-06-05T10:35:17Z,相关修复：https://github.com/XiaoMi/pegasus/pull/68,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NDY2MjI2NA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/76,https://api.github.com/repos/apache/incubator-pegasus/issues/76,incubator-pegasus,330976385,76,Backup unit test fails occasionally,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-06-10T15:09:09Z,2019-06-09T14:39:54Z,"Pegasus travis test fails occasionally:
```
D2018-06-10 13:30:02.390 (1528637402390718359 3e4e)  mimic.io-thrd.15950: client session created, remote_server = 127.0.0.1:34601, current_count = 1
sleep 1 second to wait complete...
D2018-06-10 13:30:02.390 (1528637402390939619 3e59)  mimic.io-thrd.15961: client session connected, remote_server = 127.0.0.1:34601, current_count = 1
	new app_id = 2
sleep 10s to wait app become healthy...
partition[0] is unhealthy, coz primary is invalid...
sleep 10s to wait app become healthy...
partition[0] is unhealthy, coz primary is invalid...
sleep 10s to wait app become healthy...
partition[0] is unhealthy, coz primary is invalid...
sleep 10s to wait app become healthy...
partition[0] is unhealthy, coz primary is invalid...
sleep 10s to wait app become healthy...
partition[0] is unhealthy, coz primary is invalid...
sleep 10s to wait app become healthy...
partition[0] is unhealthy, coz primary is invalid...
sleep 10s to wait app become healthy...
partition[0] is unhealthy, coz primary is invalid...
sleep 10s to wait app become healthy...
partition[0] is unhealthy, coz primary is invalid...
sleep 10s to wait app become healthy...
partition[0] is unhealthy, coz primary is invalid...
sleep 10s to wait app become healthy...
partition[0] is unhealthy, coz primary is invalid...
sleep 10s to wait app become healthy...
partition[0] is unhealthy, coz primary is invalid...
sleep 10s to wait app become healthy...
partition[0] is unhealthy, coz primary is invalid...
/home/travis/build/XiaoMi/pegasus/src/test/function_test/test_restore.cpp:318: Failure
Value of: restore()
  Actual: false
Expected: true
```

But if you rebuild it, it probably successes.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/76/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/77,https://api.github.com/repos/apache/incubator-pegasus/issues/77,incubator-pegasus,331070460,77,shell config files conflict when exec shell concurrently under the same tool,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-06-11T07:17:31Z,2018-06-11T09:33:00Z,"# 背景
广告CTR的AB方案中，需要使用./script/pegasus_set_usage_scenario.sh来设置表的使用场景。

现在c3和c4机房分别有两个集群c3srv-adb和c4srv-adb，需要灌数据的时候，工作流会同时启动两个任务，分别设置这两个集群的usage_scenario。

由于这两个任务在同一个机器上执行，且使用的同一个pegasus tools工具目录下的shell，结果造成总是只有一个集群被设置成功。

原因是：./run.sh shell --cluster=xxx 在执行的时候，会在当前目录下生成config-shell.ini。如果同时有多个shell运行，都会生成config-shell.ini，会出现覆盖问题。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/77/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/77,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NjE1MDk4OQ==,incubator-pegasus,396150989,77,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-06-11T07:40:15Z,2018-06-11T07:40:15Z,"# 解决办法

将shell的临时config文件尾部加上进程号PID，以避免冲突问题。

https://github.com/XiaoMi/pegasus/pull/75
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NjE1MDk4OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/79,https://api.github.com/repos/apache/incubator-pegasus/issues/79,incubator-pegasus,331476109,79,multi_set数字字符串bug,sunwsh,4278126,,,CLOSED,2018-06-12T07:52:24Z,2018-06-25T02:59:37Z,"// multi set.   value=""99"" ,set 后再读取出来，value 值就错了。
用run.sh shell 看到的内容是：
>>>multi_get_range test_key sortkey sortkez
hash_key: ""test_key""
start_sort_key: ""sortkey""
start_inclusive: true
stop_sort_key: ""sortkez""
stop_inclusive: false
sort_key_filter_type: no_filter
max_count: -1
no_value: false
reverse: false
""test_key"" : ""sortkey_0"" => ""V\x7F""
""test_key"" : ""sortkey_1"" => ""V\x7F""
""test_key"" : ""sortkey_2"" => ""V\x7F""
""test_key"" : ""sortkey_3"" => ""V\x7F""
""test_key"" : ""sortkey_4"" => ""V\x7F""
""test_key"" : ""sortkey_5"" => ""V\x7F""
""test_key"" : ""sortkey_6"" => ""V\x7F""


下面是 multi_set 的代码：
```cpp
int main(int argc, const char *argv[])
{
    if (!pegasus_client_factory::initialize(""config.ini"")) {
        fprintf(stderr, ""ERROR: init pegasus failed\n"");
        return -1;
    }

    if (argc < 3) {
        fprintf(stderr, ""USAGE: %s <cluster-name> <app-name>\n"", argv[0]);
        return -1;
    }

    int  run_key_count = 2;
    if (argc == 4) {
        run_key_count = atoi(argv[3]);
    }

    // set
    pegasus_client *client = pegasus_client_factory::get_client(argv[1], argv[2]);

    std::string hashKey = ""test_key"";
    std::map<std::string, std::string>  kvs;
    for(int j =0; j < 7; ++j) {
        std::string sortKey = ""sortkey_"" + std::to_string(j);
        kvs[sortKey ] =  ""99"";
        printf(""test:key:%s,value:%s\n"", sortKey.c_str(), kvs[sortKey].c_str());
    }
    int ret = client->multi_set(hashKey, kvs);
    if (ret != PERR_OK) {
        return -1;
    }

    struct pegasus_client::multi_get_options optA;

    std::map<std::string, std::string>  values;
    ret = client->multi_get(hashKey, ""sortkey"", ""sortkez"", optA, values);
    if (ret != PERR_OK && ret != PERR_INCOMPLETE ) {
        return -1;
    }

    for ( std::map<std::string, std::string>::iterator it = values.begin(); it != values.end(); ++it ) {
        std::string newValue = ""99"";
        if (0 != strcmp(newValue.c_str(), it->second.c_str())) {
            fprintf(stdout, ""ERROR: multi_get value headKey:%s, sortKey:%s, value:%s != value:%s\n""
                , hashKey.c_str(), it->first.c_str()
                , it->second.c_str(), newValue.c_str());
            return -1;
        }
        fprintf(stdout, ""hashkey:%s, sortkey:%s, value:%s\n"", hashKey.c_str(), it->first.c_str(), it->second.c_str());
        // del
        ret = client->del(hashKey, it->first);
        if (ret != PERR_OK) {
            fprintf(stderr, ""ERROR: del failed, error=%s\n"", client->get_error_string(ret));
            return -1;
        }
    }

    return 0;
}

```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/79/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/79,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5OTgxNzE0MQ==,incubator-pegasus,399817141,79,NA,shengofsun,1041832,Vincent,,NA,2018-06-25T02:59:37Z,2018-06-25T02:59:37Z,close this as it is resolved,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5OTgxNzE0MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/84,https://api.github.com/repos/apache/incubator-pegasus/issues/84,incubator-pegasus,332236284,84,Speed up Travis CI build,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-06-14T03:06:47Z,2020-06-19T08:06:46Z,"now building pegasus takes more than 30 minutes (travis ci timeout is 50 minutes), we need to speed up it.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/84/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/84,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NzE1NjE4NA==,incubator-pegasus,397156184,84,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-06-14T03:08:36Z,2018-06-14T03:08:36Z,"some efforts: https://github.com/XiaoMi/pegasus/pull/83
but not enough","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NzE1NjE4NA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/84,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NzE1NjkyNg==,incubator-pegasus,397156926,84,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-06-14T03:14:01Z,2018-06-14T03:14:01Z,"Refer to: https://eng.localytics.com/best-practices-and-common-mistakes-with-travis-ci/
""By default, Travis will build twice for each pull request. One of the builds will be the build for the branch itself, and one of the builds will be for the potential future merge commit against the target of the pull request. For us, because we merge to master quickly, it’s rare that the merge commit build will find something that the branch build doesn’t. Because of that, we’ve disabled the second build for a lot of our less fundamental services.""

We choose to build only pushed pull requests.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NzE1NjkyNg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/84,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NzI5Njc3Mg==,incubator-pegasus,397296772,84,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-06-14T13:31:13Z,2018-06-14T13:31:13Z,more efforts: #85 ,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NzI5Njc3Mg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/86,https://api.github.com/repos/apache/incubator-pegasus/issues/86,incubator-pegasus,332396956,86,build problem when using toolchain,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-06-14T13:12:08Z,2021-05-28T22:25:27Z,"在build机器上使用toolchain进行编译时，我发现：
如果环境变量设置了：
```
export LIBRARY_PATH=""$DSN_THIRDPARTY_ROOT/lib""
``` 
那么CMakeLists.txt中的以下语句不会生效：
```
link_directories(${DSN_THIRDPARTY_ROOT}/lib)
```
造成的后果就是```${DSN_THIRDPARTY_ROOT}/lib```不在链接的-L路径里面，链接时因为找不到库或者找到错误的库，链接失败。

搜到一些可能相关的链接： https://public.kitware.com/Bug/view.php?id=16074","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/86/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/86,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NzI5MzU1Mg==,incubator-pegasus,397293552,86,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-06-14T13:20:31Z,2018-06-14T13:20:31Z,"其实与是否使用toolchain没有关系，在普通环境下也可重现：
```
export LIBRARY_PATH=`pwd`/rdsn/thirdparty/output/lib
./run.sh build -c -v
```

会发现```rdsn/thirdparty/output/lib```不包含在链接路径列表中。譬如在我的机器上：
```
Linking CXX executable ../bin/pegasus_server/pegasus_server
cd /home/mi/git.xiaomi/Pegasus/pegasus/src/builder/server && /usr/bin/cmake -E cmake_link_script CMakeFiles/pegasus_server.dir/link.txt --verbose=1
ccache /usr/bin/g++    -D__FILENAME__='""$(notdir $(abspath $<))""' -std=c++1y    -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free CMakeFiles/pegasus_server.dir/pegasus_perf_counter.cpp.o CMakeFiles/pegasus_server.dir/pegasus_write_service.cpp.o CMakeFiles/pegasus_server.dir/main.cpp.o CMakeFiles/pegasus_server.dir/pegasus_counter_updater.cpp.o CMakeFiles/pegasus_server.dir/pegasus_server_impl.cpp.o CMakeFiles/pegasus_server.dir/available_detector.cpp.o CMakeFiles/pegasus_server.dir/pegasus_server_write.cpp.o CMakeFiles/pegasus_server.dir/info_collector.cpp.o CMakeFiles/pegasus_server.dir/pegasus_manual_compact_service.cpp.o CMakeFiles/pegasus_server.dir/pegasus_event_listener.cpp.o CMakeFiles/pegasus_server.dir/info_collector_app.cpp.o  -o ../bin/pegasus_server/pegasus_server  -L/home/mi/git.xiaomi/Pegasus/pegasus/src/builder/lib  -L/home/mi/git.xiaomi/Pegasus/pegasus/rdsn/builder/output/lib  -L/home/mi/git.xiaomi/Pegasus/pegasus/src/server/../../rocksdb -rdynamic -lrocksdb -ldsn_replica_server -ldsn_meta_server -ldsn.replication.clientlib -ldsn.replication.ddlclient -ldsn.block_service.local -ldsn.block_service.fds -ldsn.failure_detector -ldsn.failure_detector.multimaster -ldsn.replication.zookeeper_provider ../lib/libpegasus_client_static.a -lzookeeper_mt -levent -lgalaxy-fds-sdk-cpp -lPocoNet -lPocoFoundation -lPocoNetSSL -lPocoJSON -lcrypto -lfmt -lz -lbz2 -lsnappy -lrt -laio -lpthread -ldsn_runtime -lboost_system -lboost_filesystem -ltcmalloc -lrt -laio -ldl -lcrypto -lpthread -lthrift -Wl,-rpath,. 
```
后果就是，可能会因为找不到```rdsn/thirdparty/output/lib```里面的库而链接失败。

由于pegasus_server依赖```event2```的libevent库，原本期望链接```rdsn/thirdparty/output/lib/libevent.a```，但是由于上面的原因：
* 在我自己的开发机上，实际链接的是系统路径下的库```/usr/lib/x86_64-linux-gnu/libevent.a```，而不是```rdsn/thirdparty/output/lib/libevent.a```，正好也匹配，编译能成功；
* 但是在build机器上，实际链接的是系统路径下的库```/usr/lib64/libevent.a```，而且还不匹配，编译就失败了。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDM5NzI5MzU1Mg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/86,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ3MjcxODEyMQ==,incubator-pegasus,472718121,86,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2019-03-14T06:14:49Z,2019-03-14T06:14:49Z,"我也测试了一下，现在是gflags库链接错误，不过问题本质相同。
原因应该是：export LIBRARY_PATH使得thirdparty的路径变成了gcc认为的“系统路径”，link_directories(${DSN_THIRDPARTY_ROOT}/lib)就是在重复link“系统路径”，于是cmake出来的link就不会-L thirdparty。
用下面的CMakeLists.txt测试
```
cmake_minimum_required(VERSION 3.11)

set(MY_PROJ_NAME ""test"")
project(${MY_PROJ_NAME} C CXX)

set(MY_PROJ_SRC main.cpp)
set(MY_SRC_SEARCH_MODE ""GLOB"")

link_directories(/home/huangwei5/export/pegasus/rdsn/thirdparty/output/lib)

add_executable(${MY_PROJ_NAME} ${MY_PROJ_SRC})

target_link_libraries(${MY_PROJ_NAME} gflags)
```
生成的link.txt为：
`/usr/bin/c++     CMakeFiles/test.dir/main.cpp.o  -o test -lgflags `

而实际链接的是系统路径下的库/usr/lib/xxx这个的原因，是因为export的LIBRARY_PATH，在gcc那里是被处理过的，通过gcc/g++ --print-search-dirs可以得到：
libraries: =**/home/huangwei5/export/pegasus/rdsn/thirdparty/output/lib**/x86_64-redhat-linux/4.8.5/:**/home/huangwei5/export/pegasus/rdsn/thirdparty/output/lib/**../lib64/:/usr/lib/gcc/x86_64-redhat-linux/4.8.5/:/usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../../x86_64-redhat-linux/lib/x86_64-redhat-linux/4.8.5/:/usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../../x86_64-redhat-linux/lib/../lib64/:/usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../x86_64-redhat-linux/4.8.5/:/usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../../lib64/:/lib/x86_64-redhat-linux/4.8.5/:/lib/../lib64/:/usr/lib/x86_64-redhat-linux/4.8.5/:/usr/lib/../lib64/:**/home/huangwei5/export/pegasus/rdsn/thirdparty/output/lib/**:/usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../../x86_64-redhat-linux/lib/:/usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../:/lib/:/usr/lib/

可以看出，/home/huangwei5/export/pegasus/rdsn/thirdparty/output/lib/这个正确的路径优先级很低，低于/usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../../lib64，其实也就是/usr/lib64这个路径。
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ3MjcxODEyMQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/86,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ3MjcxODk1Mw==,incubator-pegasus,472718953,86,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2019-03-14T06:19:05Z,2019-03-14T06:19:05Z,"这种问题本质是不指定库的绝对路径导致的，cmake在高版本里也非常提倡使用绝对路径。头文件和库的匹配还是通过find_package来解决比较好，也简单明了。
所以我打算修改下编译脚本，不再使用link_directories()，顺便可以把高版本gcc编译时的语法问题也修一下。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ3MjcxODk1Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/90,https://api.github.com/repos/apache/incubator-pegasus/issues/90,incubator-pegasus,333945033,90,shell core if compiled with clang,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-06-20T06:50:16Z,2021-05-28T22:26:12Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/90/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/91,https://api.github.com/repos/apache/incubator-pegasus/issues/91,incubator-pegasus,333945380,91,shell: hide hint options when provided by user,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-06-20T06:51:43Z,2021-05-28T22:26:45Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/91/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/92,https://api.github.com/repos/apache/incubator-pegasus/issues/92,incubator-pegasus,333946643,92,add read/write throughput statistics counter,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-06-20T06:56:49Z,2019-06-09T14:38:34Z,"Now we have read/write QPS counter, but is not enough, because the size of each request is not take into account.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/92/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/92,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMDIxNjk1Nw==,incubator-pegasus,500216957,92,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-06-09T14:38:34Z,2019-06-09T14:38:34Z,"Same issue as https://github.com/XiaoMi/pegasus/issues/235.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMDIxNjk1Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/93,https://api.github.com/repos/apache/incubator-pegasus/issues/93,incubator-pegasus,333947642,93,improve priority_queue to prevent starvation,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,OPEN,2018-06-20T07:00:39Z,2018-06-20T07:00:39Z,"now the priority queue's dequeue() is:
```cpp
    T dequeue_impl(/*out*/ long &ct, bool pop = true)
    {
        if (_count == 0) {
            ct = 0;
            return nullptr;
        }

        ct = --_count;

        int index = priority_count - 1;
        for (; index >= 0; index--) {
            if (_items[index].size() > 0) {
                break;
            }
        }

        assert(index >= 0); // ""must find something"");
        auto c = _items[index].front();
        _items[index].pop();
        return c;
    }
```
if the HIGH priority queue is always not empty, the task in COMMON/LOW queue may be starved.

we can refer to the implementation of nfs_client_impl.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/93/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/95,https://api.github.com/repos/apache/incubator-pegasus/issues/95,incubator-pegasus,335162939,95,fix counter of collector*app.pegasus*app.stat.storage_mb#_all_,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-06-24T08:50:32Z,2018-06-26T13:26:30Z,and collector*app.pegasus*app.stat.storage_count#_all_,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/95/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/96,https://api.github.com/repos/apache/incubator-pegasus/issues/96,incubator-pegasus,335244800,96,"when new meta server is added, show it in shell command of ""cluster_info""",shengofsun,1041832,Vincent,,OPEN,2018-06-25T03:07:31Z,2018-07-19T02:41:20Z,"when we run ""cluster_info"" in shell command, the ""meta_servers"" list is a static value which shows the configured meta servers when cluster is initialized: 
```
>>> cluster_info
meta_servers        : 10.112.3.11:30601,10.112.3.10:30601
primary_meta_server : 10.112.3.11:30601
zookeeper_hosts     : 10.112.3.11:2181,10.112.3.10:2181,10.112.2.33:2181
zookeeper_root      : /pegasus/c3tst-sample
meta_function_level : freezed
```

when a new meta server is added to cluster dynamically, the ""meta_servers"" list won't change.

shoud resolve this.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/96/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/98,https://api.github.com/repos/apache/incubator-pegasus/issues/98,incubator-pegasus,335650435,98,filter not take effect when scan cross different batch,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-06-26T04:01:07Z,2018-06-26T11:01:20Z,"```
>>> use temp
OK
>>> full_scan
partition: all
hash_key_filter_type: no_filter
sort_key_filter_type: no_filter
batch_size: 100
max_count: 2147483647
timout_ms: 5000
detailed: false
no_value: false

""a"" : ""m_1"" => ""a""
""a"" : ""m_2"" => ""a""
""a"" : ""m_3"" => ""a""
""a"" : ""m_4"" => ""a""
""a"" : ""m_5"" => ""a""
""a"" : ""n_1"" => ""b""
""a"" : ""n_2"" => ""b""
""a"" : ""n_3"" => ""b""

8 key-value pairs got.
>>> full_scan --batch_size 10 -s prefix -y m
partition: all
hash_key_filter_type: no_filter
sort_key_filter_type: prefix
sort_key_filter_pattern: ""m""
batch_size: 10
max_count: 2147483647
timout_ms: 5000
detailed: false
no_value: false

""a"" : ""m_1"" => ""a""
""a"" : ""m_2"" => ""a""
""a"" : ""m_3"" => ""a""
""a"" : ""m_4"" => ""a""
""a"" : ""m_5"" => ""a""

5 key-value pairs got.
>>> full_scan --batch_size 3 -s prefix -y m
partition: all
hash_key_filter_type: no_filter
sort_key_filter_type: prefix
sort_key_filter_pattern: ""m""
batch_size: 3
max_count: 2147483647
timout_ms: 5000
detailed: false
no_value: false

""a"" : ""m_1"" => ""a""
""a"" : ""m_2"" => ""a""
""a"" : ""m_3"" => ""a""
""a"" : ""m_4"" => ""a""
""a"" : ""m_5"" => ""a""
""a"" : ""n_1"" => ""b""
""a"" : ""n_2"" => ""b""
""a"" : ""n_3"" => ""b""

8 key-value pairs got.
>>> 
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/98/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/98,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwMDI1MzA5NA==,incubator-pegasus,400253094,98,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-06-26T10:03:02Z,2018-06-26T10:03:02Z,"问题已经找到，是pegasus_server_impl.cpp中的一处笔误：
https://github.com/XiaoMi/pegasus/pull/99/files#diff-3111b4bc6ec81c9104fc7716d27db4fa","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwMDI1MzA5NA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/102,https://api.github.com/repos/apache/incubator-pegasus/issues/102,incubator-pegasus,336897387,102,add NDEBUG macro when compile pegasus in release mode,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,OPEN,2018-06-29T07:45:00Z,2018-06-29T07:45:00Z,"now we use lots of assert() in our code, and do not add NDEBUG macro even when compile in release mode.

to improve:
* add NDEBUG macro when compile pegasus in release mode
* change necessary assert() to dassert()
* test and compare performance with NDEBUG and without NDEBUG","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/102/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/106,https://api.github.com/repos/apache/incubator-pegasus/issues/106,incubator-pegasus,338188433,106,github上发布的版本和小米内部使用的版本是一致的不？,zhouquan03,9213327,周权,zhouquan03@gmail.com,CLOSED,2018-07-04T08:58:34Z,2018-07-09T02:26:26Z,github上发布的版本和小米内部使用的版本是一致的不？,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/106/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/106,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwMjQ0MjY5NQ==,incubator-pegasus,402442695,106,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-07-04T10:49:37Z,2018-07-04T10:49:37Z,是完全一致的，我们现在内部线上就是用的 [v1.9.2](https://github.com/XiaoMi/pegasus/releases/tag/v1.9.2) 。,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwMjQ0MjY5NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/109,https://api.github.com/repos/apache/incubator-pegasus/issues/109,incubator-pegasus,338870132,109,Fault injection on write path,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2018-07-06T09:30:33Z,2018-07-13T02:31:58Z,"We can add random fault injection on `pegasus_write_service::impl::db_write` to test the condition where rocksdb is failed.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/109/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/109,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwMzAwMTU1OQ==,incubator-pegasus,403001559,109,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-07-06T10:57:46Z,2018-07-06T10:57:46Z,also consider adding fail_qps counters,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwMzAwMTU1OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/109,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNDA1NTIwOQ==,incubator-pegasus,404055209,109,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2018-07-11T06:05:20Z,2018-07-11T06:05:20Z,The fail point lib is added in rDSN by [util: add fail point implementation (#120)](https://github.com/XiaoMi/rdsn/commit/87d5bc2e6ab59510f069c6f3e2febc36d7eced3c),"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNDA1NTIwOQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/111,https://api.github.com/repos/apache/incubator-pegasus/issues/111,incubator-pegasus,339285266,111,support truncate table,shengofsun,1041832,Vincent,,OPEN,2018-07-09T02:29:31Z,2018-09-13T08:48:41Z,"currently the only way to quickly clean the data of a table is drop the table then create a new one. perhaps we can support a ""truncate table"" command to let users to clean the table quickly.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/111/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/113,https://api.github.com/repos/apache/incubator-pegasus/issues/113,incubator-pegasus,339367563,113,"fix ""minos_client_dir"" path in several script and make these scripts support minos2.0",shengofsun,1041832,Vincent,,CLOSED,2018-07-09T09:15:18Z,2020-02-11T05:14:20Z,"currently, several scripts assume the minos client is in dir ""/home/work/pegasus/infra/minos/client"", like:

* pegasus_rolling_update.sh
* pegasus_offline_node_list.sh
* pegasus_migrate_zookeeper.sh
* pgasus_offline_node.sh

we'd better to fix this by making the variable ""minos_client_dir"" changable.

besides, minos2.0 should also be supported in these scripts.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/113/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/119,https://api.github.com/repos/apache/incubator-pegasus/issues/119,incubator-pegasus,340561598,119,“#pragma once“ usage is not recommended ,mcgrady-forever,7893783,,,CLOSED,2018-07-12T09:36:03Z,2018-07-19T06:15:28Z,"I saw ""#pragma once"" in header file, it is better use ""#ifndef XXX #define XXX ... # endif""
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/119/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/119,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNDcwODcyMA==,incubator-pegasus,404708720,119,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2018-07-13T02:42:35Z,2018-07-13T02:42:35Z,"I'm not sure if there's any place documents about the downside of `pragma once`. The google-code-style of C++ warns only for the windows developer to prohibit the use of it (https://google.github.io/styleguide/cppguide.html#Windows_Code), not for linux. Linux is the platform pegasus only supports.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNDcwODcyMA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/119,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNTAwNzc1MA==,incubator-pegasus,405007750,119,NA,mcgrady-forever,7893783,,,NA,2018-07-14T08:12:33Z,2018-07-14T08:12:33Z,"The latter usage is more compatible, some platform not support ""#pragma once"", most open source projects do not use that either","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNTAwNzc1MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/119,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNTAzMDk1OA==,incubator-pegasus,405030958,119,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2018-07-14T15:35:37Z,2018-07-14T15:35:37Z,"I'm pretty sure `#pragma once`  is well supported by both gcc(after 4.8) and clang, and rocksdb uses it too.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNTAzMDk1OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/119,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNTA2NDkwNQ==,incubator-pegasus,405064905,119,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-07-15T03:36:50Z,2018-07-15T03:36:50Z,Thanks for discussion. Now we will keep using ```#pragma once``` unless there is a solid reason to change it.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNTA2NDkwNQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/119,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNjE2NjM4NQ==,incubator-pegasus,406166385,119,NA,mcgrady-forever,7893783,,,NA,2018-07-19T06:15:28Z,2018-07-19T06:15:28Z,Thanks for reply,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNjE2NjM4NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/124,https://api.github.com/repos/apache/incubator-pegasus/issues/124,incubator-pegasus,341321902,124,coredump when rpc_code is defined but not handled,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-07-15T13:40:19Z,2019-06-09T14:46:19Z,"when pegasus not support INCR operator, we also define RPC_RRDB_RRDB_INCR (refer to https://github.com/XiaoMi/pegasus/blob/v1.9.2/src/server/pegasus_server_impl.cpp#L30 )to keep compatibility with v1.4.x.

if we send RPC_RRDB_RRDB_INCR rpc code using Pegasus Java Client 1.9.0 to Pegasus Server <=1.9.2，then pegasus server will coredump:
```
D2018-07-15 21:27:23.725 (1531661243725307352 0295) replica.io-thrd.00661: network.cpp:619:on_server_session_accepted(): server session accepted, remote_client = x.x.x.x:xxxxx, current_count = 5
F2018-07-15 21:27:23.727 (1531661243727366961 02aa) replica.default3.030002940001000e: pegasus_server_impl.cpp:86:handle_request(): assertion expression: false
F2018-07-15 21:27:23.727 (1531661243727404626 02aa) replica.default3.030002940001000e: pegasus_server_impl.cpp:86:handle_request(): recv message with unhandled rpc name RPC_RRDB_RRDB_INCR from x.x.x.x:xxxxx, trace_id = 0000000000000000
```

That is: If some rpc code is defined, but not handled, the server will core. It it not robust enough.

refer to [storage_serverlet.h:82](https://github.com/XiaoMi/rdsn/blob/9bd165b312a648c4a0f4eb6620d87a7409e4d60c/include/dsn/dist/replication/storage_serverlet.h#L82)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/124/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/129,https://api.github.com/repos/apache/incubator-pegasus/issues/129,incubator-pegasus,342153753,129,"fix ""io_getevents returns -4"" problem",qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,OPEN,2018-07-18T02:33:28Z,2020-08-18T14:45:52Z,"Appears for many times in unit test:
```
W2018-07-18 10:16:05.669 (1531880165669313326 617f)  mimic.io-thrd.24959: io_getevents returns -4, you probably want to try on another machine:-(
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/129/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/131,https://api.github.com/repos/apache/incubator-pegasus/issues/131,incubator-pegasus,342205332,131,"'GetScanner' not support ""reverse"" order",litian33,9813294,,,CLOSED,2018-07-18T07:17:23Z,2019-06-09T14:43:48Z,"Pegasus supports only  asc order while scanning datas in default.
In some case, we want to scan the datas in reverse order (or desc order). ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/131/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/131,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMDIxNzM1Mw==,incubator-pegasus,500217353,131,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-06-09T14:43:47Z,2019-06-09T14:43:47Z,fixed in https://github.com/XiaoMi/pegasus/commit/a8c70b1ee8d69693b2d76b83fe99fe6aed5c9c57.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMDIxNzM1Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/136,https://api.github.com/repos/apache/incubator-pegasus/issues/136,incubator-pegasus,342585382,136,cold back support fuse,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,OPEN,2018-07-19T05:38:27Z,2018-09-13T08:47:42Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/136/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/139,https://api.github.com/repos/apache/incubator-pegasus/issues/139,incubator-pegasus,342788214,139,support bulk load by creating and ingesting SST files,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-07-19T15:51:29Z,2021-05-31T09:45:26Z,"Though pegasus already provides `buld_load` usage scenario on table for faster write speed, it still uses `set` or `multiSet` inteface to insert data one by one. Definitely it is not fast enough to load very quite a lot of data (typically billions of rows) into Pegasus in a short time.

Maybe we can seek for a better way, considering:
- Rocksdb support [creating and ingesting SST files](https://github.com/facebook/rocksdb/wiki/Creating-and-Ingesting-SST-files)
- Pegasus support recovering table from external file system (such as HDFS) by loading RocksDB snapshots of each partition, through the [cold backup](https://github.com/XiaoMi/pegasus/wiki/%E5%86%B7%E5%A4%87%E4%BB%BD) feature. 

Then the idea is:
- construct RocksDB snapshots for each partition by offline computing (such as MapReduce), and store them on HDFS.
- recover table from HDFS by cold backup recovery.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/139/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/139,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ1NjcxNTk5OQ==,incubator-pegasus,456715999,139,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-01-23T08:39:45Z,2019-01-23T08:39:45Z,可以参考TiDB的做法： https://zhuanlan.zhihu.com/p/55397024 ,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ1NjcxNTk5OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/139,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMDIxNDA2MQ==,incubator-pegasus,500214061,139,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-06-09T13:54:17Z,2019-06-09T13:54:17Z,@hycdong 把设计文档初稿记录在这里,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMDIxNDA2MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/139,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUyMDY4MDgxNQ==,incubator-pegasus,520680815,139,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2019-08-13T04:07:04Z,2019-08-13T04:07:04Z,"The basic idea is just as zuoyan's comment. We firstly create rocksdb SST files on remote storage such as HDFS, and ingest those files into pegasus. Rocksdb support [Creating-and-Ingesting-SST-files](https://github.com/facebook/rocksdb/wiki/Creating-and-Ingesting-SST-files). 

### Basic design
1. **Creating SST files on HDFS**
    -  Using RocksDBJava and Spark or MapReduce, write key-value pairs into SST files on HDFS
> Tips:
We plan to support converting files in specific format or HBase HFile into pegasus key structure, more details about this are already in discussion.

2. **Replica servers download SST files from HDFS**
    - Client send start bulk load request(including app and remote file storage informations) to meta server
    - Meta server send bulk load request to primary replica of each partition
    - Primary replica receive request from meta server and download SST files and notify secondaries download files and report its download progress
    - Secondary replica download SST files and report download progress through `group_check`
    - Primary replica report download progress to meta servers
    - Meta server will wait all partitions including primary and secondaries finish downloading
> Tips:
> 1. Primary and secondaries all download files from HDFS, so that we can avoid learn and data transfer through replicas which may affect normal read, write operations.
> 2. Server will restrict how many replica can download files at the same time
> 3. Download files task code has lower priority than normal write operations
> 
> In one word, we should guarantee downloading staging has minimal affect to normal user operations.

3. **Replica servers ingestion SST files**
    - Meta server send ingestion request to primary replica of each partition.
    - Primary replica will consider ingestion request as a special write request, execute 2pc staging
    - Write an empty request after primary commit to ensure secondaries commit ingestion request
> Tips:
> Rocksdb will **prevent write** operation during ingestion files, as a result, replica during ingestion staging will also reject all user operation. We should guarantee such time as short as possible.

### Relation will other functions
- Write operations: ingestion staging will prevent user write operations
- Partition split: can not execute at the same time, because split will change partition count which affect SST files generation
- Access control: user should have write permission of target cluster
- Duplication, cold backup and cu calculation: WIP","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUyMDY4MDgxNQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/140,https://api.github.com/repos/apache/incubator-pegasus/issues/140,incubator-pegasus,342793215,140,provide docker config or image,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-07-19T16:04:13Z,2020-06-19T08:09:25Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/140/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/140,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQzOTk0MTg4Nw==,incubator-pegasus,439941887,140,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2018-11-19T15:54:33Z,2018-11-19T15:54:33Z,"#214 : support for ubuntu 16.04
#138 : support for centos
we may provide a more compact image for docker build, currently it sizes 1GB at least.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQzOTk0MTg4Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/140,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY0NjUwMjg5Mg==,incubator-pegasus,646502892,140,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2020-06-19T08:09:25Z,2020-06-19T08:09:25Z,https://github.com/XiaoMi/pegasus/pull/335,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY0NjUwMjg5Mg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/141,https://api.github.com/repos/apache/incubator-pegasus/issues/141,incubator-pegasus,342796646,141,enable and test batch feature in 2pc write,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,OPEN,2018-07-19T16:13:34Z,2018-07-19T16:14:05Z,to improve write performance.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/141/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/142,https://api.github.com/repos/apache/incubator-pegasus/issues/142,incubator-pegasus,343303695,142,support incr related command in redis proxy,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-07-21T06:46:03Z,2018-07-30T01:37:51Z,and update the wiki doc.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/142/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/142,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNzMyMTQ3OQ==,incubator-pegasus,407321479,142,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2018-07-24T08:14:57Z,2018-07-24T08:14:57Z,pr: https://github.com/XiaoMi/pegasus/pull/146,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNzMyMTQ3OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/142,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwODcyMzM5Ng==,incubator-pegasus,408723396,142,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2018-07-30T01:37:51Z,2018-07-30T01:37:51Z,solved,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwODcyMzM5Ng==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/144,https://api.github.com/repos/apache/incubator-pegasus/issues/144,incubator-pegasus,343866168,144,develop ETL tools,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,OPEN,2018-07-24T03:00:40Z,2018-07-24T03:08:18Z,"including:
* DataX (https://github.com/XiaoMi/pegasus-datax/issues/1)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/144/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/145,https://api.github.com/repos/apache/incubator-pegasus/issues/145,incubator-pegasus,343871923,145,fix pack scripts to pack all dependent library,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-07-24T03:34:32Z,2020-01-19T08:19:11Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/145/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/145,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNzI4MTQwNQ==,incubator-pegasus,407281405,145,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-07-24T04:58:39Z,2018-07-24T04:58:39Z,sample: https://github.com/XiaoMi/pegasus/commit/db94f1111360cfa616e0021537b5c872d089484f,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNzI4MTQwNQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/147,https://api.github.com/repos/apache/incubator-pegasus/issues/147,incubator-pegasus,343930069,147,simplify clearing data of dropped table,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,OPEN,2018-07-24T08:12:14Z,2018-07-24T08:14:10Z,"现在过期表的物理删除逻辑太复杂，依赖的点太多，需要简化。
参见[过期表数据的物理删除](https://github.com/XiaoMi/pegasus/wiki/Table%E8%BD%AF%E5%88%A0%E9%99%A4#%E8%BF%87%E6%9C%9F%E8%A1%A8%E6%95%B0%E6%8D%AE%E7%9A%84%E7%89%A9%E7%90%86%E5%88%A0%E9%99%A4)。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/147/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/147,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNzMyMTI4MA==,incubator-pegasus,407321280,147,NA,shengofsun,1041832,Vincent,,NA,2018-07-24T08:14:10Z,2018-07-24T08:14:10Z,删了之后同时修改文档,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwNzMyMTI4MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/148,https://api.github.com/repos/apache/incubator-pegasus/issues/148,incubator-pegasus,343954027,148,benchmark test on v1.10.0,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-07-24T09:21:58Z,2020-08-18T14:45:52Z,"just like: [Benchmark on v1.8.0](https://github.com/XiaoMi/pegasus/wiki/Benchmark#benchmark-on-v180)

增加一项：
(5)数据只读: 12客户端*100线程","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/148/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/148,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwODMyNDczMw==,incubator-pegasus,408324733,148,NA,zhangyifan27,19500115,Zhang Yifan,chinazhangyifan@163.com,NA,2018-07-27T06:29:44Z,2018-07-27T06:29:44Z,[Benchmark on v1.10.0](https://github.com/XiaoMi/pegasus/wiki/Benchmark#benchmark-on-v1100),"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwODMyNDczMw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/149,https://api.github.com/repos/apache/incubator-pegasus/issues/149,incubator-pegasus,343972158,149,get cluster name in shell on start,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-07-24T10:12:36Z,2018-08-01T06:11:32Z,"when use shell as `./run.sh shell --cluster xxxx`, the cluster name shown on start is `unknown`, which user may complain with. we can use `cluster_info` to get the correct cluster name.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/149/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/151,https://api.github.com/repos/apache/incubator-pegasus/issues/151,incubator-pegasus,345459051,151,redis proxy test,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2018-07-28T15:59:44Z,2018-07-28T15:59:44Z,需要完善redis proxy的单元测试和接口测试,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/151/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/156,https://api.github.com/repos/apache/incubator-pegasus/issues/156,incubator-pegasus,345702910,156,scan may have bug when primary switches,shengofsun,1041832,Vincent,,CLOSED,2018-07-30T10:37:00Z,2018-08-06T04:25:55Z,"1. say a partition: [A(primary), B(secondary), C(secondary)]
2. Client X starts a scan operation, then an context id **C1** is generated and assigned to X
3. then the primary switch to B
4. Client Y starts another scan operation, accidenty the same context id **C1** is generated and assigned to Y
5. Client X continues its scanning with **C1**, but now the message is sent to B. However, ""C1 of B"" has assigned to another scan context. So the scan operation from X will get wrong data.

A possible fix may encode service_node's rpc_address to the ""scan_context"", so as to ensure every ""context"" is unique.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/156/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/156,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwOTA3NTc3OA==,incubator-pegasus,409075778,156,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-07-31T02:42:48Z,2018-07-31T02:42:48Z,"now that `context_id` is int64, we can add some mask in it, for example, use `server_start_time` or `random_int32` as the high 32 bits.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQwOTA3NTc3OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/156,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQxMDU4NTI2Ng==,incubator-pegasus,410585266,156,NA,shengofsun,1041832,Vincent,,NA,2018-08-06T04:25:55Z,2018-08-06T04:25:55Z,resolved ,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQxMDU4NTI2Ng==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/159,https://api.github.com/repos/apache/incubator-pegasus/issues/159,incubator-pegasus,346417155,159,support checkAndMutate() interface,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-08-01T01:43:33Z,2020-08-18T14:45:53Z,just like hbase [checkAndMutate](https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/Table.html#checkAndMutate-byte:A-byte:A-).,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/159/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/162,https://api.github.com/repos/apache/incubator-pegasus/issues/162,incubator-pegasus,348144409,162,return ERR_HANDLER_NOT_FOUND if rpc code not exist on server,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-08-07T02:35:21Z,2018-08-20T01:34:46Z,"refer to code at [rpc_engine.cpp:618](https://github.com/XiaoMi/rdsn/blob/9bd165b312a648c4a0f4eb6620d87a7409e4d60c/src/core/core/rpc_engine.cpp#L618):
```c++
    if (code != ::dsn::TASK_CODE_INVALID) {
        ... ...
    } else {
        dwarn(""recv message with unknown rpc name %s from %s, trace_id = %016"" PRIx64,
              msg->header->rpc_name,
              msg->header->from_address.to_string(),
              msg->header->trace_id);

        dassert(msg->get_count() == 0, ""request should not be referenced by anybody so far"");
        delete msg;
    }
```

if rpc code is not exist on server, then server will ignore the request, which make client timeout.

I think `timeout` is not proper, it's better to get `ERR_HANDLER_NOT_FOUND` error.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/162/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/162,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQxNDE3NDIxNg==,incubator-pegasus,414174216,162,NA,shengofsun,1041832,Vincent,,NA,2018-08-20T01:34:46Z,2018-08-20T01:34:46Z,closed as resolved,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQxNDE3NDIxNg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/165,https://api.github.com/repos/apache/incubator-pegasus/issues/165,incubator-pegasus,350776161,165,learn_app_concurrent_count inconsistent bug,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-08-15T11:29:51Z,2018-08-20T01:34:22Z,"learn_app_concurrent_count's increment and decrement may be not matched, which causes learn_app_concurrent_count inconsistent, and makes learning blocked:
```
W2018-08-15 16:42:44.772 (1534322564772409072 206e2) replica.replica6.0405000600000122: replica_learn.cpp:676:on_learn_reply(): 43.1@10.38.166.24:31801: on_learn_reply[0000004700000002]: learnee = 10.38.162.236:31801, exceed learn_app_max_concurrent_count(5) limit, skip this round
```

The reason is:
1. some replica is learning app after increment learn_app_concurrent_count
2. meta server disconnected
3. the replica status changes to INACTIVE by replica::on_meta_server_disconnected()
4. but the learn_app_concurrent_count is not decrement","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/165/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/165,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQxNDE3NDE3NQ==,incubator-pegasus,414174175,165,NA,shengofsun,1041832,Vincent,,NA,2018-08-20T01:34:22Z,2018-08-20T01:34:22Z,closed as resolved,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQxNDE3NDE3NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/166,https://api.github.com/repos/apache/incubator-pegasus/issues/166,incubator-pegasus,351959580,166,rpc支持身份认证,shengofsun,1041832,Vincent,,CLOSED,2018-08-20T01:58:02Z,2022-08-15T08:09:33Z,"## 总体过程

身份认证可以用流行的Kerberos来实现。需要引用的第三方项目有：
* [Mit Kerberos](http://web.mit.edu/kerberos/)
* [cyrus sasl](https://github.com/cyrusimap/cyrus-sasl)： 虽然真正认证用的是kerberos的方案，但流程应该采用更通用的SASL。这样代码可以更通用，也更方便兼容其他的认证方案。

关于Kerberos和sasl的更详细介绍可以参考[这里](https://www.jianshu.com/p/fc2d2dbd510b)

为了嵌入身份认证，需要修改rDSN的rpc_engine模块，从而使得socket的发起方和接收方在身份认证成功后才能将rpc_session建立成功。

## 身份认证的过程

使用sasl+kerberos的身份认证的流程如下：
```
                      client               server
                         | --- SASL_MECH --> |
                         | <-- SASL_MECH --- |
                         |                   |
                         | - SASL_SEL_MECH ->|
                         | <- SASL_SEL_OK ---|
                         |                   |
                         | --- SASL_INIT --> |
                         |                   |
                         | <-- SASL_CHAL --- |
                         | --- SASL_RESP --> |
                         |                   |
                         |      .....        |
                         |                   |
                         | <-- SASL_CHAL --- |
                         | --- SASL_RESP --> |
                         |                   | (authentication will succeed
                         |                   |  if all chanllenges passed)
                         | <-- SASL_SUCC --- |
(client won't response   |                   |
if servers says ok)      |                   |
                         | --- RPC_CALL ---> |
                         | <-- RPC_RESP ---- |

```
上述流程可以简要概括为：
1. client和server通过两轮rpc来交换身份认证的方式
2. client首先携带一些信息(initialize response)发起认证
3. server端在收到初始信息后，开始发起challenge；然后client响应server的challenge。这样的过程会进行数次。
4. 如果client能接收server的所有challenge, server会率先认为认证通过，并通知client。然后client收到通知后，标记会话认证成功，并不做任何响应。
5. 然后client就可以发送正常的rpc消息给server了。

## 如何实现
在实现身份认证的时候，我们遵循了rdsn中""client和server一问一答""的rpc模型，这样就可以复用rpc_engine的代码了。

具体来看，我们定义了几个类型和结构体：
```
DEFINE_TASK_CODE_RPC(RPC_NEGOTIATION,...)

enum negotiation_status {
    INVALID = 0,
    SASL_LIST_MECHANISMS,
    SASL_LIST_MECHANISMS_RESP,
    SASL_SELECT_MECHANISMS,
    SASL_SELECT_MECHANISMS_OK,
    SASL_INITIATE,
    SASL_CHALLENGE,
    SASL_RESPONSE,
    SASL_SUCC,
    SASL_AUTH_FAIL
}

struct negotiation_message {
    1: negotiation_status status;
    2: dsn.blob msg;
}
```
client和server端就用定义好的rpc code和IDL结构体进行交互。具体要点包括：
1. 具体的认证消息都是negotiation_message。所有的message都封装在rpc_message中。
2. client发到server端的全是rpc_request, server发到client端的都是rpc_response。注意这里一定要和SASL的(challenge, response)序对给区分开。在SASL中，server发送**CHALLENGE**，client响应**RESPONSE**。这和我们的rpc_message的类型是相反的。

## 细节要点
在设计并实现身份认证的时候，有些细节要点是必须得注意的：
1. 支持认证方式的协商, 这样可以兼容其他的认证方案。尤其是公司内部可能有一些内部的认证方式。
2. 在把认证过程集成到现有的server中时，需要考虑server的平滑升级，以及客户端的兼容性问题。
3. kerberos为身份认证所颁发的ticket存在期限。如果过期该怎么处理。
4. 为了应对kerberos的证书颁发模块出问题，应该允许临时关闭认证。

接下来我们会逐个介绍这些问题。

## 认证方式的协商
认证方式的协商是通过两轮RPC来进行的：
* client向server询问server端支持的认证方式；server端回复(当前仅支持GSSAPI)。
* client向server发送自己选择的认证方式，从而双方达成一致。

## 平滑升级
平滑升级server端时候需要考虑的问题包括：
1. 新版本的客户端访问旧版本的server。 即客户端要求认证，但server端完全不知道什么是认证。
2. 旧版本的客户端访问新版本的server。即客户端完全不知道什么是认证，但server要求认证。
3. server端升级后，其他语言的SDK没有升级(java, python, nodejs....)。同样是客户端完全不知道什么是认证，但server要求认证。

为了保证这些情况能够被处理，我们需要引入一个额外的配置项**mandatory_authentication**来表示**是不是强制认证**。如果认证是强制选项，那么对于不处理认证消息的对端，要强制关闭会话；反之，就需要跳过所有的认证过程，直接允许会话可以直接交换上层的RPC数据。

##  使用Kerberos时候的若干问题

### kerberos相关文件的管理
在使用Kerberos上，有几个文件/数据是需要管理好的：
*  kerberos的配置文件，需要记录KDC的URL，以及ticket的相关参数。
*  keytab文件。客户端需要这个文件来获取授权，访问server；server端需要这个文件验证客户端
*  credential cache, 客户端获取到相关的授权后，是存在一个叫做credential cache(ccache)的地方的

默认情况下，这些数据全部以文件的形式保存在固定的路径下。但当我们部署自己的应用时，一定是希望这些路径是可配置的。kerberos库允许我们以设置环境变量的方式来指定这些文件的存取路径。在pegasus身份认证的实现中，kerberos配置和keytab是通过配置文件来指定的。而credential cache是保存在进程的虚拟内存中的。

### TGT的更新
kerberos给客户端的授权就存放在上文所说的credential cache中(因为TGT就是credential的一种)；并且TGT是存在有效期的。我们需要使用Kerberos的相关API来获取TGT的有效期，并且要在到期前使用相关API来更新它。

### Kerberos接口的线程安全

MIT kerberos提供的接口都是线程安全的，放心使用。

## 认证的临时关闭
在当前的实现中，server端有个启动参数来控制”是否开启身份认证“。如果需要关闭这一feature，只要重启server就可以了。

一个可能的需求是：通过RPC命令来关闭认证。这样的需求可以实现，但是有些奇怪。原因在于：
* 对于一个开启了认证的SERVER，任何的RPC命令应该都要求认证
* 但当需要关闭认证时，说明认证系统已经出问题了。那么这个RPC命令可以说是无法发送到server端的。

所以通过重启的方式来关闭认证是比较合理的，这也使得整个安全过程的边界很清楚：你应该在身份认证层的**外部**来打破保护本身。从实用的角度来看，当认证系统出问题时，可用性一定会出问题。那么你在恢复可用性时所采取的操作，究竟是重启、还是发起另一个命令，对于整个系统不可用时长的影响，应该是可以忽略不计的。

另一个需求可能是：如果不能继续更新Ticket，就自动降级到不需要认证。这也涉及到了分布式系统的一个基本问题：从进程自己的角度来看，它永远无从得知，不能更新Ticket，**是自己不行了，还是网络不行了，还是认证系统不行了**。贸然的关闭掉认证系统，相当于直接关掉了整个安全层，是不明智的。所以这个动作最好还是交给管理员进行评估。

最后，如果真的需要为了可用性添加这些功能，还是要多借鉴别的项目在使用Kerberos的经验。

## 有了认证后，怎么方便的做ACL

在身份认证成功后，rpc session可以得知认证方的用户名。在每收到一条消息后，session会把用户名存入到每一条message中。这样就可以得知每个message的用户名了：
```
msg->user_name
```

假如kerberos的用户名是""sample_user/pegasus@EXAMPLE.COM""，那么user_name即为""sample_user/pegasus""，即没有realm的部分。

## 认证的进一步讨论

对于一个完备的认证过程而言，每条普通的rpc_message都需要携带身份相关的凭据。之所以这么做，是为了防止**重放攻击**：即一个攻击者在session认证成功后再劫持会话。

但我们在实现上并没有这么做。一方面而言，这么做会导致rpc_message的格式不兼容，给升级带来比较大的困扰；另一方面，身份认证的主要目的是为了防止内网用户的误操作，而非防止公网用户的攻击。

另一个值得注意的是，如果在建立会话时使用的ticket超时了，那么这个会话要不要关闭掉？当前也是从够用的角度出发，没有处理这个问题。

## 实现
当前的实现在[shengofsun/master](https://github.com/shengofsun/rdsn/tree/master)的branch上

## 如何配置
```
[security]
;; keytab文件，密钥要和krb5_principal相对应
krb5_keytab = /home/weijiesun/source/github/pegasus/local-kdc/krb5.keytab
;; kerberos的配置文件，client和server都需要
krb5_config = /home/weijiesun/source/github/pegasus/local-kdc/krb5.conf
;; 进程启动时候自己的principal
krb5_principal = test-server/myhost@EXAMPLE.COM
;; 要访问的server的用户名
service_name = test-server
;; 要访问哦server的fqdn
service_fqdn = myhost
;; sasl的路径
sasl_plugin_path = /home/weijiesun/source/github/pegasus/rdsn/thirdparty/output/lib/sasl2
;; 是否开启认证
open_auth = true
;; 是否强制认证
mandatory_auth = true
```

## 测试要点

1. 可以写个基本的rpc_session建立连接的单元测试，但前提是必须有KDC。可以调研下有没有轻量级的KDC，实在不行，就用我们编译好的KDC搭一个。
2. 要测试下集群的升级和降级，这个应该只能手动测试。
3. 要测试下ticket expire，并且本地还没有更新ticket的情况。这个可能需要加个参数来让schedule时间长一些。测试结果是观测到各种报错、新连接建不上、但进程不崩溃。
4. 要测试一下KDC不可用的情况，把我们搭的KDC关了就行。测试的结果也是观测到各种报错，新连接建立不上。
5. 写个脚本不停的建立、关闭连接(用shell就可以了)，让他跑个一两周，看看内存有没有问题。
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/166/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/166,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5CQvjC,incubator-pegasus,1111685314,166,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-04-28T02:50:59Z,2022-04-28T02:50:59Z,"Completed by commits in https://github.com/XiaoMi/rdsn prefixed with 'feat(security): ', such as https://github.com/XiaoMi/rdsn/pull/575","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5CQvjC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/166,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5IZ2Kf,incubator-pegasus,1214735007,166,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-08-15T08:09:33Z,2022-08-15T08:09:33Z,https://github.com/apache/incubator-pegasus/issues/170,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5IZ2Kf/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/170,https://api.github.com/repos/apache/incubator-pegasus/issues/170,incubator-pegasus,352833628,170,ACL方案的调研和设计,shengofsun,1041832,Vincent,,CLOSED,2018-08-22T07:27:43Z,2022-08-15T08:10:29Z,"## 背景

有了身份认证(参见 #166)之后，我们就可以给系统添加ACL控制了。系统需要ACL方案的主要出发点在于：

*  为了杜绝用户拿到shell客户端对集群私自做不合时宜的操作：如自己去做负载均衡，或者私自建表。
*  为了防止用户在做某些危险操作时候出现失误：比如删表时候敲错了表名
*  对数据的安全性提供更高级别的保护：如防止没有权限的用户去读、写某些数据
*  对某些操作提供审计的功能：如可以查询到表是谁建的，谁删除的。

此外，在设计ACL方案的时候，应该多参考[HBase的设计](http://hbase.apache.org/book.html#appendix_acl_matrix)

## 有哪些操作需要做权限规范

1. 所有meta_server的纯查询rpc
2. 所有meta_server的纯查询的remote_command
3. 所有replica_server的纯查询的remote_command
4. 集群的运维调整：主要就是set_meta_level, propose命令和balance命令
5. 涉及到某个表的操作：
     * 创建表
     * 删除表
     * 召回表
     * 表的冷备份、热备份、split、从冷备中恢复数据、设置表的环境变量
     * 表的读、写(包括又读又写的inc、check_and_mutate等)
6. 其他集群级别的操作：
     * 冷备份相关：创建冷备份，禁用、启用冷备份，修改冷备份的相关参数。
     * 元数据恢复

## 权限设计

参考HBase的设计，数据库系统的权限设计是可以从两个维度来考虑的：

1. 所授予权限起作用的数据粒度。具体到Pegasus里，权限可以在两个粒度里起作用：
     * 集群粒度
     * 表粒度
2. 权限的分类，HBase一共划分了如下几个类别：
     * Admin(A)：某个粒度内的Admin权限，意味着对这个数据集有着很高的控制权限。
     * Create(C)：Create权限，意味着可以创建一些资源。
     * Write(W)：某个粒度中的写权限
     * Read(R)：某个粒度中的读权限
     * Execute(X)：某个粒度中的执行权限，可以执行一些coprocessor函数。

如果把上面的权限类别对应到Pegasus中，不一定所有的都适用。哪怕仅仅是部分的采用，这也是一个产品设计层面的问题：

* 比如我可能只希望用户进行读写操作就足够了，那么所有的其他的表操作就可以打包为admin权限，统一不开放给用户。而如果换个角度，manual compaction要开放给用户；那么可能就需要加入一个execute的权限位，这个权限意味着用户可以设置表的环境变量来对表进行影响。
* 再比如从集群的角度来看，如果所有的集群操作都要交由系统的维护人员进行，那么可能建表、删表等一系列操作都打包为admin权限即可；否则，可能要把建表等一些操作拆分一个Create权限出来。

下文给出一个建议的权限方案。

### 表级别的权限建议

对于表级别的一个建议权限方案为：

1. read权限：可以执行表的读操作
2. write权限：可以执行表的写操作
3. execute权限：可以修改表的环境变量，进而引发某个表的某些特定行为。
4. create权限：可以给某个表创建热备，可以把某个表加到某个冷备policy中或者从某个policy中移除，可以对表执行split。
5. admin权限：可以删表、召回表；同时具有R、W、E、C四个权限中的所有操作；同时也可以把表中的R、Ｗ、Ｅ、Ｃ、Ａ权限中的一个或多个授予给某个用户。

注意：
* 如果想执行非幂等的写操作(都是先读后写)，需要同时具有R和W的权限。
* R、W、E、C四个权限是相互独立的。一个具有create权限的人，不需要可以读写表中的数据。之所以这么做，是因为做冷备、热备这些操作的一般是运维人员，他们可能更关心表的稳定性和数据的安全性，对数据的内容不一定在意。所以有必要对权限做互斥性的拆分。
* 热备份是一个比较奇怪的操作：发起热备份的人并不关心数据内容，但热备份在执行的时候，是确确实实需要数据的写权限的。考虑到热备时候，数据是由某个replica-server发起写的，而replica-server一定会分配一个特殊的帐号(因为它不可能有任何其他人的kerberos密钥)，我们只要赋予这个帐号相应的权限即可。

### 集群级别的权限建议
对于集群级别的建议权限方案为：

1. read权限：对集群中的每一个表，有read权限。
2. write权限：对集群中的每一个表，有write权限。
3. execute权限：对集群中的每个一个表，有execute权限。
4. create权限：对集群中的每个表有create权限；并且可以建表，可以从冷备中恢复表；也可以创建冷备policy，可以disable或者enable某个policy，可以修改policy的所有参数。
5. admin权限：拥有1-4规定的所有权限；可以向replica-server或者meta-server发送任意的rpc；同时也可以把集群或者表级别的任何一种权限分配给任何人。

注意：
* 在启动pegasus-server时，我们需要为每个进程本身分配一个kerberos帐号；我们要为这个帐号分配集群的admin权限。从另一个角度来看，这个帐号就是系统的superuser或者root。
* 集群级别的read和write，对于测试集群非常方便。可以让他们共用一个帐号，而不用给他们太细粒度的处理权限。
* 一个具有集群级别create权限的用户，虽然可以创建表，但不能删表(因为他没有对应表的admin权限)。但直觉告诉我们：
     * 我自己建的表，我好歹能删了它
     * 我自己建的表，我应该能对它有全部控制。
  所以我们这里可以对建表的行为稍作调整：建表的时候可以指定一个表的owner，owner对表有admin权限。如果不指定owner，建表人就是owner。

### 其他权限

所有纯查询meta server或者replica-server的命令和rpc，可以不做ACL限制，这样会简单很多。

## 可能的实现方案

需要记录的信息有：

* table的owner
* 每个table需要有五个数组: R, W, E, C, A，来分别记录各个权限级别下的用户列表。
* 集群要有五个数据：R, W, E, C, A，来分别记录各个权限级别下的用户列表。

最简单的做法：
1. table的owner和权限表全放到app_info中，这样同步到zk、冷备都变得非常容易。
2. 1带来的问题是，config_sync会重复的同步很多数据到replica-server上，因为一张table只需要有一份权限table即可。可以稍微修改下config_sync的协议，增加一个字段来记录权限表；而在config_sync时，将configuration_update_request.app_info的相关结构都清空。
3. 集群级别的权限表不做冷备份，一旦机房宕机，冷备恢复就丢失这部分信息，因为我们本来就没有集群粒度的冷备恢复。且这部分的用户量是可控的。

如果担心把权限表全放到一个zk znode下会开销较大，可以考虑稍微优化下：
* 在zk上存放多个znode，直接存到某个app_id/acl的目录下即可。这种情况下，可以维持app_info不变。但修改zk元数据的代码可能就会稍微复杂一些了。
* 权限信息从app_info中抽出来后，元数据恢复的功能可能会受影响。这里得稍微修改下代码，最好让每个replica-server在收到ACL信息的时候记录到本地，这样方便恢复。
* 对于用户名长度，以及某个ACL数组中用户名的个数，可以做适当的限制。

## 其他建议

开发和使用的时候可能会有很多的问题，请修改、补充、完善或者废弃这个文档。

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/170/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/170,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5CQtYy,incubator-pegasus,1111676466,170,NA,kirbyzhou,3401630,,,NA,2022-04-28T02:34:08Z,2022-04-28T02:34:08Z,做ACL的话，应该考虑acl能接入ranger统一管理。,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5CQtYy/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/170,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5CQv2Y,incubator-pegasus,1111686552,170,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-04-28T02:53:42Z,2022-04-28T02:53:42Z,"Completed by commits in https://github.com/XiaoMi/rdsn prefixed with 'feat(security): ', such as https://github.com/XiaoMi/rdsn/pull/655","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5CQv2Y/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/173,https://api.github.com/repos/apache/incubator-pegasus/issues/173,incubator-pegasus,354618323,173,求php版本客户端,youjiawang,6357712,,,CLOSED,2018-08-28T08:11:20Z,2019-06-09T14:32:17Z,客户端没有php的版本么？,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/173/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/173,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQxNjgzOTIxMw==,incubator-pegasus,416839213,173,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2018-08-29T06:26:11Z,2018-08-29T06:26:11Z,"Sorry, we don't have a php client now, because in XiaoMi we don't have such requirement.
But it's in our plan to rewrite the current C++ driver into a cleaner C driver, which your php application can direcly call.

It's actually not a strong requirement, you can post a pull request for this and we are glad to accept it.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQxNjgzOTIxMw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/173,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQxNjk1ODMwMg==,incubator-pegasus,416958302,173,NA,shengofsun,1041832,Vincent,,NA,2018-08-29T13:43:27Z,2018-08-29T13:43:27Z,"这边人手不足，近期支持php客户端可能有些困难。如果有兴趣的话，你可以帮我们开发一个 :-)
可以参考的资料有：
* [rpc协议](https://github.com/XiaoMi/pegasus/blob/master/docs/client-development.md)
* [java客户端的代码](https://github.com/XiaoMi/pegasus-java-client)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQxNjk1ODMwMg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/180,https://api.github.com/repos/apache/incubator-pegasus/issues/180,incubator-pegasus,360709214,180,add build option to skip building all tests,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-09-17T02:54:26Z,2020-08-18T14:45:53Z,including sub-module rdsn and rocksdb,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/180/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/184,https://api.github.com/repos/apache/incubator-pegasus/issues/184,incubator-pegasus,364402033,184,add rocksdb memory usage metric,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2018-09-27T10:15:36Z,2018-12-02T11:29:45Z,reference: https://github.com/facebook/rocksdb/wiki/Memory-usage-in-RocksDB,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/184/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/184,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyNTM0MjcyMQ==,incubator-pegasus,425342721,184,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-09-28T07:06:17Z,2018-09-28T07:06:17Z,I think we can show rocksdb memory usage metrics in built in web server monitor: https://github.com/XiaoMi/rdsn/issues/130,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyNTM0MjcyMQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/184,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyNTY0ODk2Mw==,incubator-pegasus,425648963,184,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2018-09-29T14:23:06Z,2018-09-29T14:23:06Z,@qinzuoyan  im afraid we can't because we need to store historical metrics. falcon is our only choice.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyNTY0ODk2Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/186,https://api.github.com/repos/apache/incubator-pegasus/issues/186,incubator-pegasus,367734909,186,pegasus server core dumped after scanned by nmap ,sunwsh,4278126,,,CLOSED,2018-10-08T10:52:26Z,2018-10-11T09:18:38Z,"我们在几台新的服务器上准备部署pegasus，运行一天多就会有个进程core ， 而且是没有业务请求情况下。  replica 进程和 meta 进程都会core dump。
core 文件名称： core-replica.asio.3-6001-1538262177 
                             core-meta.asio.1-29423-1538372576
![image](https://user-images.githubusercontent.com/4278126/46605187-3ea78b80-cb2b-11e8-9d53-f58d58c92f10.png)

备注：
1.  部署了3台服务器，每台一个meta进程，一个replica进程，一个zookeeper进程。
2. 程序core dump 时log 文件无异常。
3. gcc --version    gcc (Ubuntu 5.4.0-6ubuntu1~16.04.2) 5.4.0 20160609
    cat /etc/issue     Ubuntu 16.04 LTS \n \l
4. pegasus 版本：v1.11.1

这个是一个meta server 进程core dump时的记录，这个程序一直在slave，没有抢到zk 锁。
![image](https://user-images.githubusercontent.com/4278126/46642439-d055de00-cba9-11e8-912a-0c5f654c4b9d.png)

5. 用端口扫描工具nmap测试
   用端口扫描脚本测试，必现：  nmap -sT -p 1-65535 ip","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/186/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/186,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyODE0OTk2MQ==,incubator-pegasus,428149961,186,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2018-10-09T11:00:10Z,2018-10-09T11:00:10Z,"Reproducing scenario:

```
➜  pegasus git:(dup) ✗ ./run.sh list_onebox
mi       24506  1438  0 18:53 pts/28   00:00:00 /home/mi/git/pegasus/onebox/meta1/pegasus_server config.ini -app_list meta
mi       24517  1438  0 18:53 pts/28   00:00:00 /home/mi/git/pegasus/onebox/replica1/pegasus_server config.ini -app_list replica
mi       24595  1438  0 18:53 pts/28   00:00:00 /home/mi/git/pegasus/onebox/replica2/pegasus_server config.ini -app_list replica
mi       24701  1438  0 18:53 pts/28   00:00:00 /home/mi/git/pegasus/onebox/replica3/pegasus_server config.ini -app_list replica
➜  pegasus git:(dup) ✗ nmap -sT -p 34601 127.0.0.1

Starting Nmap 6.40 ( http://nmap.org ) at 2018-10-09 18:55 CST
Nmap scan report for localhost (127.0.0.1)
Host is up (0.000033s latency).
PORT      STATE SERVICE
34601/tcp open  unknown

Nmap done: 1 IP address (1 host up) scanned in 0.03 seconds
➜  pegasus git:(dup) ✗ ./run.sh list_onebox
mi       24517  1438  0 18:53 pts/28   00:00:00 /home/mi/git/pegasus/onebox/replica1/pegasus_server config.ini -app_list replica
mi       24595  1438  0 18:53 pts/28   00:00:00 /home/mi/git/pegasus/onebox/replica2/pegasus_server config.ini -app_list replica
mi       24701  1438  0 18:53 pts/28   00:00:00 /home/mi/git/pegasus/onebox/replica3/pegasus_server config.ini -app_list replica
➜  pegasus git:(dup) ✗ cd onebox/meta1                
➜  meta1 git:(dup) ✗ ls
config.ini  core  data  pegasus_server  result  zoo.log
```

gdb coredump:

```
(gdb) bt
#0  0x00007efdd3480c37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007efdd3484028 in __GI_abort () at abort.c:89
#2  0x00007efdd3d97e63 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#3  0x00007efdd3d9cfa6 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#4  0x00007efdd3d9cfe1 in std::terminate() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00007efdd3d9d213 in __cxa_throw () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x0000000000925310 in boost::throw_exception<boost::system::system_error> (e=...) at /usr/include/boost/throw_exception.hpp:67
#7  0x00000000009253e2 in boost::asio::detail::do_throw_error (err=..., location=<optimized out>) at /usr/include/boost/asio/detail/impl/throw_error.ipp:38
#8  0x00007efdd606f40f in throw_error (location=0x7efdd60daf4c ""remote_endpoint"", err=...) at /usr/include/boost/asio/detail/throw_error.hpp:42
#9  remote_endpoint (this=<optimized out>) at /usr/include/boost/asio/basic_socket.hpp:1426
#10 operator() (__closure=0x7efdbfd8e0d0, ec=...) at /home/mi/git/pegasus/rdsn/src/core/tools/common/asio_net_provider.cpp:130
#11 operator() (this=0x7efdbfd8e0d0) at /usr/include/boost/asio/detail/bind_handler.hpp:47
#12 asio_handler_invoke<boost::asio::detail::binder1<dsn::tools::asio_network_provider::do_accept()::__lambda3, boost::system::error_code> > (
    function=<error reading variable: access outside bounds of object referenced via synthetic pointer>) at /usr/include/boost/asio/handler_invoke_hook.hpp:64
#13 invoke<boost::asio::detail::binder1<dsn::tools::asio_network_provider::do_accept()::__lambda3, boost::system::error_code>, dsn::tools::asio_network_provider::do_accept()::__lambda3> (context=..., 
    function=...) at /usr/include/boost/asio/detail/handler_invoke_helpers.hpp:37
#14 boost::asio::detail::reactive_socket_accept_op<boost::asio::basic_socket<boost::asio::ip::tcp, boost::asio::stream_socket_service<boost::asio::ip::tcp> >, boost::asio::ip::tcp, dsn::tools::asio_network_provider::do_accept()::__lambda3>::do_complete(boost::asio::detail::io_service_impl *, boost::asio::detail::operation *, const boost::system::error_code &, std::size_t) (owner=<optimized out>, 
    base=<optimized out>) at /usr/include/boost/asio/detail/reactive_socket_accept_op.hpp:123
#15 0x0000000000925bf4 in complete (bytes_transferred=<optimized out>, ec=..., owner=..., this=0x1d4b180) at /usr/include/boost/asio/detail/task_io_service_operation.hpp:37
#16 do_run_one (ec=..., this_thread=..., lock=..., this=0x21c06e0) at /usr/include/boost/asio/detail/impl/task_io_service.ipp:384
#17 boost::asio::detail::task_io_service::run (this=0x21c06e0, ec=...) at /usr/include/boost/asio/detail/impl/task_io_service.ipp:153
#18 0x00007efdd606d329 in run (this=0x1d71348) at /usr/include/boost/asio/impl/io_service.ipp:59
#19 operator() (__closure=<optimized out>) at /home/mi/git/pegasus/rdsn/src/core/tools/common/asio_net_provider.cpp:70
#20 _M_invoke<> (this=<optimized out>) at /usr/include/c++/4.8/functional:1732
#21 operator() (this=<optimized out>) at /usr/include/c++/4.8/functional:1720
#22 std::thread::_Impl<std::_Bind_simple<dsn::tools::asio_network_provider::start(dsn::rpc_channel, int, bool)::__lambda2()> >::_M_run(void) (this=<optimized out>) at /usr/include/c++/4.8/thread:115
#23 0x00007efdd3dc2c10 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#24 0x00007efdd485e184 in start_thread (arg=0x7efdbfd92700) at pthread_create.c:312
#25 0x00007efdd354803d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
```

the thrown exception:

```
terminate called after throwing an instance of 'boost::exception_detail::clone_impl<boost::exception_detail::error_info_injector<boost::system::system_error> >'
  what():  remote_endpoint: Transport endpoint is not connected
task_spec.cpp:92:register_task_code():overwrite default thread pool for task RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX from THREAD_POOL_META_SERVER to THREAD_POOL_DEFAULT
task_spec.cpp:92:register_task_code():overwrite default thread pool for task RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX_ACK from THREAD_POOL_META_SERVER to THREAD_POOL_DEFAULT
main.cpp:92:main():pegasus server starting, pid(24506), version($Version: Pegasus Server 1.12.SNAPSHOT (df1169d7161a442291a6ac6673966fc0376c0ad9) Release, built with rDSN 1.0.0 (3997c34720cd1e82e838b95f389545c0af8463b7), built by gcc 4.8.5, built on mi-OptiPlex-7040, built at Oct  9 2018 18:50:43 $)
service_api_c.cpp:526:run():dsn_runtime_init_time_ms = 1539082385832 (2018-10-09 18:53:05.832)
```


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyODE0OTk2MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/186,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyODE1MzczNg==,incubator-pegasus,428153736,186,NA,shengofsun,1041832,Vincent,,NA,2018-10-09T11:15:08Z,2018-10-09T11:15:08Z,看样子是抛了个异常没有catch?,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyODE1MzczNg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/186,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyODE2MjExOQ==,incubator-pegasus,428162119,186,NA,shengofsun,1041832,Vincent,,NA,2018-10-09T11:49:09Z,2018-10-09T11:49:09Z,@neverchanje 另外你看下，为啥那边的coredump堆栈信息很少，你这边就这么详细？,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyODE2MjExOQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/186,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyODIzMzMxMw==,incubator-pegasus,428233313,186,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2018-10-09T15:16:20Z,2018-10-09T15:16:20Z,"And I found this issue (https://github.com/cpp-netlib/cpp-netlib/issues/677) from cpp-netlib having the same problem with us, they also use boost::asio, and the exception is triggered by nmap too.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyODIzMzMxMw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/186,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyODI0OTIyOQ==,incubator-pegasus,428249229,186,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2018-10-09T15:57:55Z,2018-10-09T15:57:55Z,@sunwsh you can cherry-pick this commit after it being accepted.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyODI0OTIyOQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/186,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyODQyNTY5OA==,incubator-pegasus,428425698,186,NA,sunwsh,4278126,,,NA,2018-10-10T03:27:40Z,2018-10-10T03:27:40Z,"我试过你的修改，确实修复了，发nmap 时确实在哪里打印log ，
tcpdump -i eth0 host IP and port 34600 -vvv
我用tcpdump 对 nmap 的行为分析了一下，先 tcp连接，然后发包，然后马上发rst 断开连接。  所以确实可能出现，accept 后，处理时又断开了。
你现在 的修改只是减少概率了，感觉还不能彻底避免，感觉还是应该在这里增加try catch","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyODQyNTY5OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/186,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyODQyNzIzOA==,incubator-pegasus,428427238,186,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2018-10-10T03:38:24Z,2018-10-10T03:38:24Z,:+1:能修复问题就好说,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyODQyNzIzOA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/187,https://api.github.com/repos/apache/incubator-pegasus/issues/187,incubator-pegasus,368470881,187,multi-tenant support ,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,OPEN,2018-10-10T02:38:21Z,2019-04-24T05:31:43Z,"We can consider to assign a ""cache quota"" to app when we support multi-tenant.
For example given with a specific SLA requirement, we can assign the memory size of block cache to each replica according to the following algorithm:

```
                                    lower qps require smaller cache
                                                    |
(data_size / partition_count / 5) *  (read_qps / partition_count / 100)
                |
   single partition data size, 1/5 is the memory/disk ratio
```

For a app that has 2000 read qps, 200 GB estimate data size, with 128 partitions,
the cache size is:
```
(200GB / 128 / 5) * (2000 / 128 / 100)  = 50MB
```

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/187/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/187,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyOTc0MDk1MQ==,incubator-pegasus,429740951,187,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2018-10-15T07:44:18Z,2018-10-15T07:44:18Z,docs of hbase multi-tenancy: https://github.com/apache/hbase/blob/master/src/main/asciidoc/_chapters/ops_mgt.adoc#71-quotas,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQyOTc0MDk1MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/189,https://api.github.com/repos/apache/incubator-pegasus/issues/189,incubator-pegasus,370950444,189,异常add_learner,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-10-17T08:14:38Z,2020-01-10T03:31:18Z,"# 背景
2018/09/29，c3srv-xiaomi集群多个节点重启。
server版本：Pegasus Server 1.10.0 (3ad6fe7f434569220b90d1dd7272667771747ae0) Release

# Coredump
```
(gdb) bt
#0  0x0000003847a328a5 in raise () from /lib64/libc.so.6
#1  0x0000003847a34085 in abort () from /lib64/libc.so.6
#2  0x00007f5748db61ae in dsn_coredump() () at /home/sunweijie/source/pegasus/rdsn/src/core/core/service_api_c.cpp:99
#3  0x00007f5748c47610 in dsn::replication::replica::update_local_configuration(dsn::replication::replica_configuration const&, bool) ()
    at /home/sunweijie/source/pegasus/rdsn/src/dist/replication/lib/replica_config.cpp:758
#4  0x00007f5748ceb28a in dsn::replication::replica::on_add_learner(dsn::replication::group_check_request const&) () at /home/sunweijie/source/pegasus/rdsn/src/dist/replication/lib/replica_learn.cpp:1374
#5  0x00007f5748cc6ca3 in dsn::replication::replica_stub::on_add_learner(dsn::replication::group_check_request const&) ()
    at /home/sunweijie/source/pegasus/rdsn/src/dist/replication/lib/replica_stub.cpp:959
#6  0x00007f5748cdd67e in std::_Function_handler<void ()(void*), bool dsn::serverlet<dsn::replication::replica_stub>::register_rpc_handler<dsn::replication::group_check_request>(dsn::task_code, char const*, void (dsn::replication::replica_stub::*)(dsn::replication::group_check_request const&))::{lambda(void*)#1}>::_M_invoke(std::_Any_data const&, void*) ()
    at /home/sunweijie/source/pegasus/rdsn/include/dsn/cpp/serverlet.h:170
#7  0x00007f5748db1941 in dsn::task::exec_internal() () at /home/sunweijie/source/pegasus/rdsn/src/core/core/task.cpp:177
#8  0x00007f5748e03bcd in dsn::task_worker::loop() () at /home/sunweijie/source/pegasus/rdsn/src/core/core/task_worker.cpp:323
#9  0x00007f5748e03d99 in dsn::task_worker::run_internal() () at /home/sunweijie/source/pegasus/rdsn/src/core/core/task_worker.cpp:302
#10 0x00007f5746b1c600 in execute_native_thread_routine () at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#11 0x0000003848207851 in start_thread () from /lib64/libpthread.so.0
#12 0x0000003847ae811d in clone () from /lib64/libc.so.6
(gdb)
```

# log
```
D2018-09-29 15:38:57.202 (1538206737202143690 27b5c) replica.rep_long7.04010000000002f9: replica_stub.cpp:1434:on_gc(): gc_shared: gc condition for 23.23@x.x.x.x:xxxxx, status = replication::partition_status::PS_PRIMARY, garbage_max_decree = 80530851, last_durable_decree= 80530851, plog_max_commit_on_disk = 80530874
D2018-09-29 15:39:07.622 (1538206747622587975 27b23) replica.replica0.04010001180428f1: replica_config.cpp:953:on_config_sync(): 23.23@x.x.x.x:xxxxx: configuration sync
... ...
D2018-09-29 15:39:26.236 (1538206766236494687 27b23) replica.replica0.04007afd458ee1de: replica_stub.cpp:955:on_add_learner(): 23.23@x.x.x.x:xxxxx: received add learner, primary = 10.136.133.8:31801, ballot = 63, status = replication::partition_status::PS_POTENTIAL_SECONDARY, last_committed_decree = 80530876
D2018-09-29 15:39:26.236 (1538206766236506724 27b23) replica.replica0.04007afd458ee1de: replica_learn.cpp:1365:on_add_learner(): 23.23@x.x.x.x:xxxxx: process add learner, primary = x.x.x.x:xxxxx, ballot = 63, status = replication::partition_status::PS_POTENTIAL_SECONDARY, last_committed_decree = 80530876
D2018-09-29 15:39:26.236 (1538206766236801383 27b23) replica.replica0.04007afd458ee1de: replication_app_base.cpp:95:store(): store replica_init_info to /home/work/ssd7/pegasus/c3srv-xiaomi/replica/reps/23.23.pegasus/.init-info succeed, time_used_ns = 288575: init_ballot = 63, init_durable_decree = 80530851, init_offset_in_shared_log = 127917214287498, init_offset_in_private_log = 24974930792
D2018-09-29 15:39:26.236 (1538206766236813483 27b23) replica.replica0.04007afd458ee1de: replica_config.cpp:700:update_local_configuration(): 23.23@x.x.x.x:xxxxx: update ballot to init file from 61 to 63 OK
F2018-09-29 15:39:26.236 (1538206766236816962 27b23) replica.replica0.04007afd458ee1de: replica_config.cpp:758:update_local_configuration(): assertion expression: false
F2018-09-29 15:39:26.236 (1538206766236841282 27b23) replica.replica0.04007afd458ee1de: replica_config.cpp:758:update_local_configuration(): invalid execution path
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/189/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/189,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMjczMDQ3Mg==,incubator-pegasus,502730472,189,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-06-17T15:25:24Z,2019-06-17T15:25:24Z,"## Background

c4srv-store 1.11.3 (6/17)

## Core Dump

```
(gdb) bt
#0  0x00007f238aabc1d7 in raise () from /lib64/libc.so.6
#1  0x00007f238aabd8c8 in abort () from /lib64/libc.so.6
#2  0x00007f238e5883ee in dsn_coredump () at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/service_api_c.cpp:73
#3  0x00007f238e4892b7 in dsn::replication::replica::update_local_configuration (this=this@entry=0x29cad80, config=..., same_ballot=same_ballot@entry=true)
    at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/dist/replication/lib/replica_config.cpp:807
#4  0x00007f238e4ee1cb in dsn::replication::replica::on_add_learner (this=0x29cad80, request=...) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/dist/replication/lib/replica_learn.cpp:1377
#5  0x00007f238e44fce2 in dsn::replication::replica_stub::on_add_learner (this=0x227e580, request=...) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/dist/replication/lib/replica_stub.cpp:1004
#6  0x00007f238e46a9e0 in operator() (request=<optimized out>, __closure=0x9349840) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/include/dsn/cpp/serverlet.h:169
#7  std::_Function_handler<void (dsn::message_ex*), bool dsn::serverlet<dsn::replication::replica_stub>::register_rpc_handler<dsn::replication::group_check_request>(dsn::task_code, char const*, void (dsn::replication::replica_stub::*)(dsn::replication::group_check_request const&))::{lambda(dsn::message_ex*)#1}>::_M_invoke(std::_Any_data const&, dsn::message_ex*) (__functor=..., 
    __args#0=<optimized out>) at /home/work/qinzuoyan/Pegasus/toolchain/output/include/c++/4.8.2/functional:2071
#8  0x00007f238e5d9ce9 in dsn::task::exec_internal (this=this@entry=0x13dbbc28) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/task.cpp:180
#9  0x00007f238e65a42d in dsn::task_worker::loop (this=0x26f4580) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/task_worker.cpp:211
#10 0x00007f238e65a5f9 in dsn::task_worker::run_internal (this=0x26f4580) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/task_worker.cpp:191
#11 0x00007f238b414600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#12 0x00007f238c081dc5 in start_thread () from /lib64/libpthread.so.0
#13 0x00007f238ab7e73d in clone () from /lib64/libc.so.6
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMjczMDQ3Mg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/189,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUzMTcxMjQ5MA==,incubator-pegasus,531712490,189,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-09-16T09:57:58Z,2019-09-16T09:57:58Z,"This bug is fixed in release 1.11.6 https://github.com/XiaoMi/pegasus/releases/tag/v1.11.6.
Close it now.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUzMTcxMjQ5MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/192,https://api.github.com/repos/apache/incubator-pegasus/issues/192,incubator-pegasus,371907204,192,集群缩容中下线节点数过多导致meta freezed,vagetablechicken,24697960,HuangWei,huangwei@apache.org,CLOSED,2018-10-19T10:54:54Z,2019-03-08T02:04:25Z,改进方案：提供功能，可以向meta声明节点是主动下线的，将节点不计算进_dead_set。,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/192/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/192,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ3MDc3NTczMw==,incubator-pegasus,470775733,192,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2019-03-08T02:04:25Z,2019-03-08T02:04:25Z,最终方案：#300 ,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ3MDc3NTczMw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/198,https://api.github.com/repos/apache/incubator-pegasus/issues/198,incubator-pegasus,374844150,198,support table level TTL,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-10-29T03:24:57Z,2020-08-18T14:45:53Z,"user can specify TTL time for one table, then, if user sets data without TTL, then use the table level TTL by default.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/198/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/198,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQzMzc4MDg2NA==,incubator-pegasus,433780864,198,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-10-29T03:52:36Z,2018-10-29T03:52:36Z,"业务需求：
* 希望提供表级TTL，对于用户写入的数据，如果没有设置TTL时间，则采用表级TTL；如果用户自己设置了TTL，则使用用户自己的TTL。
* 表中数据在以前写入的时候没有设置TTL，但是希望从现在开始增加TTL，以清理老数据。TTL时间可以采用上面的表级TTL。

初步方案：
* 在表的envs里面增加`default_ttl`环境变量，值为TTL秒数，>0才合法。
* 在set等操作写入数据时，检查请求中是否设置了TTL时间，如果没有设置(request.expire_ts_seconds == 0)，且设置了表级TTL，则对数据应用表级TTL。
* 在`KeyWithTTLCompactionFilter::Filter`中，需要检查是否设置了表级TTL，如果设置了，且数据原来没有设置TTL，则需要修改数据，应用表级TTL。
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQzMzc4MDg2NA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/199,https://api.github.com/repos/apache/incubator-pegasus/issues/199,incubator-pegasus,375304143,199,lack of table name validation when restore table from cold backup provider,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2018-10-30T03:25:29Z,2018-10-30T08:45:08Z,"When we restore a table from cold backup provider, current code does not validate new table name, so we can create a table whose name contain invalid character such as ‘-’","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/199/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/204,https://api.github.com/repos/apache/incubator-pegasus/issues/204,incubator-pegasus,376245638,204,coredump when pegasus_bench exit,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-11-01T04:02:44Z,2020-08-18T14:45:53Z,"```
(gdb) bt
#0  0x00000000006f566c in increase_count (count=1, this=0x307d320) at /home/mi/git.xiaomi/Pegasus/pegasus/rdsn/include/dsn/tool-api/task_queue.h:85
#1  dsn::task_queue::enqueue_internal (this=0x307d320, task=0x7f3b342c4b5c) at /home/mi/git.xiaomi/Pegasus/pegasus/rdsn/src/core/core/task_queue.cpp:110
#2  0x000000000076f221 in operator() (__closure=<optimized out>, ec=...) at /home/mi/git.xiaomi/Pegasus/pegasus/rdsn/src/core/tools/common/simple_task_queue.cpp:71
#3  operator() (this=<optimized out>) at /usr/include/boost/asio/detail/bind_handler.hpp:47
#4  asio_handler_invoke<boost::asio::detail::binder1<dsn::tools::simple_timer_service::add_timer(dsn::task*)::__lambda3, boost::system::error_code> > (function=...)
    at /usr/include/boost/asio/handler_invoke_hook.hpp:64
#5  invoke<boost::asio::detail::binder1<dsn::tools::simple_timer_service::add_timer(dsn::task*)::__lambda3, boost::system::error_code>, dsn::tools::simple_timer_service::add_timer(dsn::task*)::__lambda3>
    (context=..., function=...) at /usr/include/boost/asio/detail/handler_invoke_helpers.hpp:37
#6  boost::asio::detail::wait_handler<dsn::tools::simple_timer_service::add_timer(dsn::task*)::__lambda3>::do_complete(boost::asio::detail::io_service_impl *, boost::asio::detail::operation *, const boost::system::error_code &, std::size_t) (owner=<optimized out>, base=<optimized out>) at /usr/include/boost/asio/detail/wait_handler.hpp:70
#7  0x000000000074f9cf in complete (bytes_transferred=<optimized out>, ec=..., owner=..., this=0x7f3b3405d670) at /usr/include/boost/asio/detail/task_io_service_operation.hpp:37
#8  do_run_one (ec=..., this_thread=..., lock=..., this=<optimized out>) at /usr/include/boost/asio/detail/impl/task_io_service.ipp:384
#9  boost::asio::detail::task_io_service::run (this=0x307e220, ec=...) at /usr/include/boost/asio/detail/impl/task_io_service.ipp:153
#10 0x000000000076eb7b in run (this=<optimized out>, ec=...) at /usr/include/boost/asio/impl/io_service.ipp:66
#11 operator() (__closure=0x30aa480) at /home/mi/git.xiaomi/Pegasus/pegasus/rdsn/src/core/tools/common/simple_task_queue.cpp:50
#12 _M_invoke<> (this=0x30aa480) at /usr/include/c++/4.8/functional:1732
#13 operator() (this=0x30aa480) at /usr/include/c++/4.8/functional:1720
#14 std::thread::_Impl<std::_Bind_simple<dsn::tools::simple_timer_service::start()::__lambda2()> >::_M_run(void) (this=0x30aa468) at /usr/include/c++/4.8/thread:115
#15 0x00007f3be2531a60 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#16 0x00007f3be278c184 in start_thread (arg=0x7f3b7b7fe700) at pthread_create.c:312
#17 0x00007f3be1c98ffd in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
(gdb)
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/204/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/204,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQzNDkyNDA2NQ==,incubator-pegasus,434924065,204,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-11-01T04:04:32Z,2018-11-01T04:04:32Z,可能是退出时对象析构的顺序问题导致的。之前为了修复内存泄露问题，对很多全局对象在退出时增加了析构操作，可以从这方面追查问题。,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQzNDkyNDA2NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/207,https://api.github.com/repos/apache/incubator-pegasus/issues/207,incubator-pegasus,376288551,207,测试两副本配置（一主一备）的写吞吐性能提升,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-11-01T07:48:11Z,2018-11-19T15:48:53Z,"Pegasus目前推荐是三副本配置（一主两备），吞吐不太理想。
一种可选方案是降级为一主一备，能够降低机器负载，减少RPC交互次数，提升吞吐。
需要对这两种配置进行性能对比。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/207/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/207,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQzNzc2MjUzNA==,incubator-pegasus,437762534,207,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2018-11-12T05:38:18Z,2018-11-12T05:38:18Z,"集群包含5个ReplicaServer节点(使用1.11.1版本)，表分为64个Partition

## ​2 备份，单条数据 20KB

​测试Case | ​读写比(读 : 写) | ​运行时间(小时) | ​读QPS(条/秒​) | ​读平均延迟(微​秒) | ​读P99延迟(微秒) | ​写QPS​(条/秒​) | ​写平均延迟(微秒) | ​写P99延迟(微秒)
-- | -- | -- | -- | -- | -- | -- | -- | --
​(1) 数据加载 - 3客户端，每个10线程 | ​​0 : 1 | 0.98 | ​- | ​- | ​- | ​8454 | 3543 | ​30527
​(2) ​数据读写 - 3客户端，每个15线程 | ​1 : 3 | 0.82 | 2553​ | 4109 | 36991 | 7664 | 4480 | ​66367
​(3) 数据读写 - 3客户端，每个30线程 | ​30 : 1​ | 1.31 | 61561​ | 1398 | 17215 | 2051 | 1667 | 6955
​(4) 数据只读 - 6客户端，每个100线程 | 1 : 0​​ | ​0.86 | 966​80 | ​3098 | ​13247 | ​- | -​ | ​-

## ​3 备份，单条数据 20KB


​测试Case | ​读写比(读 : 写) | ​运行时间(小时) | ​读QPS(条/秒​) | ​读平​均延迟(微​秒) | ​读P99延迟(微秒) | ​写QPS​(条/秒​) | ​写平均延迟(微秒) | ​写P99延迟(微秒)
-- | -- | -- | -- | -- | -- | -- | -- | --
​(1) 数据加载 - 3客户端，每个10线程 | ​​0 : 1 | 1.40​​ | ​- | ​- | ​- | ​5919​ | 5063 | ​40639
​(2) ​数据读写 - 3客户端，每个15线程 | ​1 : 3 | ​1.11 | 1876 | 6927 | 44639 | 5632 | 5612 | ​76095
​(3) 数据读写 - 3客户端，每个30线程 | ​30 : 1​ | 1.63 | 49341 | 1751 | 21615 | 1644 | 1935 | 11159
​(4) 数据只读 - 6客户端，每个100线程 | 1 : 0​​ | 1.09​ | 76368 | ​3923 | ​15679 | ​- | -​ | ​-

**RocsDB配置**

```
    rocksdb_abnormal_get_time_threshold_ns = 10000000
    rocksdb_abnormal_get_size_threshold = 1000000
    rocksdb_abnormal_multi_get_time_threshold_ns = 100000000
    rocksdb_abnormal_multi_get_size_threshold = 10000000
    rocksdb_abnormal_multi_get_iterate_count_threshold = 1000
    rocksdb_write_buffer_size = 67108864
    rocksdb_max_write_buffer_number = 6
    rocksdb_max_background_flushes = 4
    rocksdb_max_background_compactions = 12
    rocksdb_num_levels = 6
    rocksdb_target_file_size_base = 67108864
    rocksdb_target_file_size_multiplier = 1
    rocksdb_max_bytes_for_level_base = 671088640
    rocksdb_max_bytes_for_level_multiplier = 10
    rocksdb_level0_file_num_compaction_trigger = 4
    rocksdb_level0_slowdown_writes_trigger = 30
    rocksdb_level0_stop_writes_trigger = 60
    rocksdb_disable_table_block_cache = false
    rocksdb_compression_type = none
```

**replication 配置**
```
    prepare_timeout_ms_for_secondaries = 3000 10000 
    prepare_timeout_ms_for_potential_secondaries = 5000 10000
    log_shared_file_count_limit = 100 200
```

**Usage Scenario**
```  
    prefer_write
```

**机器配置**
- 机型：拆掉了RAID模块
- CPU：E5-2620v3 *24
- 内存​：128GB
- 存储：480G SSD *8
- 网卡：1Gb
​","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQzNzc2MjUzNA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/207,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQzNzc2OTcwMw==,incubator-pegasus,437769703,207,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-11-12T06:21:44Z,2018-11-12T06:21:44Z,看起来，写性能大约有30%左右的提升,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQzNzc2OTcwMw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/219,https://api.github.com/repos/apache/incubator-pegasus/issues/219,incubator-pegasus,383400103,219,BlockingQueue优化,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-11-22T06:44:14Z,2021-08-27T03:29:41Z,"BlockingQueue有多种实现：
* 基于Lock
* 基于FreeLock

选择合适的BlockingQueue能提升系统性能。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/219/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/219,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ0MDkzMDA4NQ==,incubator-pegasus,440930085,219,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2018-11-22T06:50:24Z,2018-11-22T06:50:24Z,"调研 hpc task queue 对性能的影响 。

集群包含5个i2机型的ReplicaServer节点(使用1.11.1版本)，表分为64个Partition。

RocsDB配置：
```
    rocksdb_abnormal_get_time_threshold_ns = 10000000
    rocksdb_abnormal_get_size_threshold = 1000000
    rocksdb_abnormal_multi_get_time_threshold_ns = 100000000
    rocksdb_abnormal_multi_get_size_threshold = 10000000
    rocksdb_abnormal_multi_get_iterate_count_threshold = 1000

    rocksdb_write_buffer_size = 67108864
    rocksdb_max_write_buffer_number = 6
    rocksdb_max_background_flushes = 4
    rocksdb_max_background_compactions = 12
    rocksdb_num_levels = 6
    rocksdb_target_file_size_base = 67108864
    rocksdb_target_file_size_multiplier = 1
    rocksdb_max_bytes_for_level_base = 671088640
    rocksdb_max_bytes_for_level_multiplier = 10
    rocksdb_level0_file_num_compaction_trigger = 4
    rocksdb_level0_slowdown_writes_trigger = 30
    rocksdb_level0_stop_writes_trigger = 60
    rocksdb_disable_table_block_cache = false
```

【2 备份，单条数据 320B，使用 hpc task queue】

​测试Case | ​​读写比​(​读 : 写) | ​运行时间(小时) | ​读QPS(条/秒​) | ​读平均延迟(微​秒) | ​读P99延迟(微秒) | ​写QPS​(条/秒​) | ​写平均延迟(微秒) | ​写P99延迟(微秒)
-- | -- | -- | -- | -- | -- | -- | -- | --
​(1) 数据加载 - 3客户端，每个10线程 | ​​0 : 1 | 0.74​ | ​- | ​- | ​- | ​56121 | 532​ | ​ 959
​(2) ​数据读写 - 3客户端，每个15线程 | ​1 : 3 |   |   |   |   |   |   |  
​(3) 数据读写 - 3客户端，每个30线程 | ​30 : 1​ |   |   |   |   |   |   |  
​(4) 数据只读 - 6客户端，每个100线程 | 1 : 0​​ | 0.87​ | 379608​ | 788 | ​3057 | ​- | -​ | ​-

【2 备份，单条数据 320B，使用 simple task queue】

​​测试Case | ​​读写比​(读 : 写) | ​运行时间(小时) | ​读QPS(条/秒​) | ​读平均延迟(微​秒) | ​读P99延迟(微秒) | ​写QPS​(条/秒​) | ​写平均延迟(微秒) | ​写P99延迟(微秒)
-- | -- | -- | -- | -- | -- | -- | -- | --
​(1) 数据加载 - 3客户端，每个10线程 | ​​0 : 1 | ​ | ​- | ​- | ​- | ​56627​ | 526 | ​1608
​(2) ​数据读写 - 3客户端，每个15线程 | ​1 : 3 | ​ |   |   |   |   |   |  
​(3) 数据读写 - 3客户端，每个30线程 | ​30 : 1​ |   |   |   |   |   |   |  
​(4) 数据只​读 - 6客户端，每个100线程 | 1 : 0​​ | 0.98 | 336864 | 888 | ​ 2927​ | ​- | -​ | ​-
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ0MDkzMDA4NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/219,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM42Dig_,incubator-pegasus,906897471,219,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2021-08-27T03:29:38Z,2021-08-27T03:29:38Z,"[无锁队列性能测试.pdf](https://github.com/apache/incubator-pegasus/files/7063960/default.pdf)
后来我重新测试了一下，发现性能提升不明显，还是继续保持原来 blockqueue 的设计，此处应该不是性能瓶颈","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM42Dig_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/220,https://api.github.com/repos/apache/incubator-pegasus/issues/220,incubator-pegasus,383710440,220,Shell count_data support getting count of different HashKey,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-11-23T05:57:22Z,2018-12-19T08:53:03Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/220/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/221,https://api.github.com/repos/apache/incubator-pegasus/issues/221,incubator-pegasus,383778378,221,Import shell's ls command to statistics total counts,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-11-23T10:50:09Z,2018-12-19T08:52:52Z,"example:
```
total_app_count           : 100
fully_healthy_app_count   : 38
unhealthy_app_count       : 26
write_unhealthy_app_count : 5
read_unhealthy_app_count  : 0
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/221/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/235,https://api.github.com/repos/apache/incubator-pegasus/issues/235,incubator-pegasus,392495541,235,Support Capacity Unit Read/Write Statistics,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-12-19T08:45:54Z,2019-10-25T09:45:09Z,"Now we already supported read/write QPS statistics, but it is not enough for pricing.

Like [Aliyun Table Store](https://www.aliyun.com/price/product?spm=5176.8030368.1058477.28.54a57e0ejFElk6#/ots/detail) and [AWS DynamoDB](https://amazonaws-china.com/cn/dynamodb/pricing/), we should support **Capacity Unit (CU)** read/write statistics.

Comparation:

Service | Read CU Size | Write CU Size
-- | -- | --
Aliyun Table Store | 4KB | 4KB
AWS DynamoDB | 4KB | 1KB
HBase | 1KB | 1KB
Pegasus | 1KB | 1KB

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/235/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/235,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ3MDg1MDE3Nw==,incubator-pegasus,470850177,235,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-03-08T08:40:36Z,2019-03-08T08:40:36Z,"需求：
* 持久化：统计数据持久化到Pegasus中，便于查询和统计
* 准确：CU统计要准确，不要重复、不要漏
* 方便查询：每月统计账单

CU计算：
* CU粒度可配置（譬如4KB还是1KB）
* 类似recent_expire_count，ReplicaServer统计每个replica的recent_read_cu和recent_write_cu

持久化：
* collector负责收集数据，并持久化到Pegasus的stat表中（表名也是可配置的）
* collector定期从各个ReplicaServer拉取最近的counter数据，将read_cu和write_cu写入到stat表中
* 如何避免漏写：ReplicaServer的counter数据是定期生成的（参见perf_counter::take_snapshot方法），假设每10秒生成一次，collector就可以每8秒拉一次数据，这样肯定不会漏数据
* 如何避免重复写：由于collector拉数据的时间间隔小于ReplicaServer生成counter的时间间隔，collector就有可能从ReplicaServer拉到重复的数据，这时可以根据perf_counter_info.timestamp来进行排重

stat表设计（仅供参考）：
* hash_key: {timestamp}_{node}。其中timestamp是perf_counter_info.timestamp；node是ReplicaServer的地址。
* sort_key: {app_id}.{partition_id}.read_cu
* value: cu值

另外一种设计（更压缩）：
* hash_key: {timestamp}_{node}
* sort_key: read_cu或者write_cu
* value: json串，譬如`{""1.1"":100,""2.2"":30}`

统计月度账单：
* 统计整个集群的月度账单：对stat表进行full_scan，对hashKey进行前缀过滤（限定时间范围），统计read_cu和write_cu；
* 统计单个表的月度账单：对stat表进行full_scan，对hashKey进行前缀过滤（限定时间范围），对sortKey进行前缀过滤（限定app_id），计算整体的read_cu和write_cu；

优化点：
* ReplicaServer可以对counter查询的regex和result增加cache，这样避免重复创建regex或者重复生成结果","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ3MDg1MDE3Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/235,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ3Mzg3OTM3NQ==,incubator-pegasus,473879375,235,NA,zhangyifan27,19500115,Zhang Yifan,chinazhangyifan@163.com,NA,2019-03-18T11:53:03Z,2019-03-18T11:53:03Z,"#### 需要实现的主要功能：

1. ReplicaServer计算CU
- 增加两个VOLATILE_NUMBER类型的counters:  _pfc_recent_read_units 和 _pfc_recent_write_units
- `[pegasus.server]`增加配置项:perf_counter_capacity_unit_size
- 特殊计算细节
读相关操作: NotFound计1个read_cu。
原子操作: read_cu计为1，write_cu按数据size计算。

2. Collector将CU数据持久化
- 定期获取每个ReplicaServer的read_cu和write_cu（具体通过call_remote_command实现,可参考get_app_stat）
- 将收集到的数据进行**去重处理**:根据perf_counter_info的timestamp进行去重

- 将去重后的数据写入到pegasus表中（可参考available_detector的实现）
- [pegasus.collector]增加配置项
capacity_unit_stat_app (记录CU统计信息的表名)
capacity_unit_stat_fetch_interval_seconds(获取CU统计信息的时间间隔，必须小于perf_counter_update_interval_seconds才能不漏统计信息)

3. 统计账单

stat表设计：
- hash_key:{node.address}_{timestamp}
- sort_key: recent.read.cu / recent.write.cu
- value: {app_name},{partition_index},{cu_value}

stat表数据量估算：
假设每10s写一条数据，一条数据1KB，一个节点一天的数据量是：2* 3600 * 24/10=17280KB约等于17M，那么1000天（不到三年）的数据量就是17G，如果集群有10个节点就是170G。
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ3Mzg3OTM3NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/235,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ3NDMzMTIzMw==,incubator-pegasus,474331233,235,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-03-19T11:53:49Z,2019-03-19T11:53:49Z,数据存储如果使用json太浪费，可以考虑使用thrift + tcompact + zstd,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ3NDMzMTIzMw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/235,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ3NDY2MTIzNw==,incubator-pegasus,474661237,235,NA,zhangyifan27,19500115,Zhang Yifan,chinazhangyifan@163.com,NA,2019-03-20T02:38:48Z,2019-03-20T02:38:48Z,为了更准确的数据去重，在perf_counters::take_snapshot() 记录更新counter数据的timestamp，list_snapshot_by_regexp中得到的perf_counter_info.timestamp也应该是最近更新的timestamp而不是当前时间。,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ3NDY2MTIzNw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/235,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ3NTUxNTUwNg==,incubator-pegasus,475515506,235,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-03-22T07:06:21Z,2019-03-22T07:06:21Z,"上面整体看起来基本问题不大了，就是有这么几点：
* 考虑到以后要scan，所以HashKey用{timestamp}_{node}不太合适，改成这样更好：
  * HashKey: {timestamp}，pref_counter_info中的timestamp，精确到秒。这样HashKey就是固定长度的。
  * SortKey: {node}。因为每次更新数据都是节点级别的。
  * Value: {""r"":{1:{1:100,2:90},2:{0:30,2:40}}, ""w"":{1:{1:100,2:90}}}。read/write可以放在一个value里面，这样一次更新的数据写到一个value里面。
* 另外value是直接用json串，还是用binary格式（zstd压缩）以节省存储空间，也需要再考虑考虑。

不过存储到Pegasus的数据格式是怎样，对现在收集数据的过程影响不大，是解耦合的。所以可以先把数据收集这块尽快实现了。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ3NTUxNTUwNg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/235,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ4MjQ2NTc3OQ==,incubator-pegasus,482465779,235,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-04-12T07:16:23Z,2019-04-12T07:16:23Z,"更新：不使用压缩，同一个表的QPS先汇总起来，减少体积，且只输出QPS不为0的数据。
* Value: {1:[100,0],2:[0,30]} ，其实[]中为[read,write]","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ4MjQ2NTc3OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/235,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5NDY5MTc1OA==,incubator-pegasus,494691758,235,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-05-22T07:44:52Z,2019-05-22T07:44:52Z,"# 计量规则

## 读吞吐量：

### Get 单行读操作：
- 返回Value数据大小<=4KB，则消耗读CU = 1。
- 返回Value数据大小>4KB，则消耗读CU = 实际Value数据大小按4KB整除向上取整。
- 返回NotFound，则消耗读CU = 1。
- 返回rocksdb error，则消耗读CU = 0。

### BatchGet多行读操作：
- 消耗的读CU为所有单行读操作消耗的CU之和。
- 每单行读操作消耗的读CU按规则1进行计算。

### MultiGet多行读操作：
- 返回数据行数>0，则消耗读CU = 读取到的所有行的SortKey+Value数据大小之和除以4KB向上取整。
- 返回数据行数=0，则消耗读CU = 1。
- 返回InvalidArgument/NotFound，则消耗读CU=1。
- 返回rocksdb error，则消耗读CU = 0。

### SortKeyCount操作：
- 成功返回个数，则消耗读CU = 1。
- 返回rocksdb error，则消耗读CU = 0。

### TTL操作：
- 成功返回TTL，则消耗读CU = 1。
- 返回NotFound，则消耗读CU = 1。
- 返回rocksdb error，则消耗读CU = 0。

### Scan操作：
- Scan操作分多批从服务端获取数据，消耗的读CU为所有批次读操作消耗的CU之和。
- 单批返回数据行数>0，则消耗读CU = 读取到的所有行的HashKey+SortKey+Value数据大小之和除以4KB向上取整。
- 单批返回数据行数=0，则消耗读CU = 1。
- 返回InvalidArgument/NotFound，则消耗读CU=1。
- 单批返回rocksdb error，则消耗读CU = 0。

## 写吞吐量：

### 单行Set操作：
a. 写成功，则消耗写CU = 单行HashKey+SortKey+Value数据大小之和除以4KB向上取整。
b. 出现异常，则消耗写CU = 0。

### 单行Del操作：
a. 写成功，则消耗写CU = 单行HashKey+SortKey数据大小之和除以4KB向上取整。
b. 出现异常，则消耗写CU = 0。

### BatchSet/BatchDel操作：
a. 消耗的读写CU为所有单行操作消耗的CU之和。
b. 每个单行操作消耗的读写CU按规则1和规则2计算。

### MultiSet操作：
a. 写成功，则消耗写CU = 所有行的SortKey+Value数据大小之和除以4KB向上取整。
b. 出现异常，则消耗写CU = 0。

### MultiDel操作：
a. 写成功，则消耗写CU = 所有行的SortKey数据大小之和除以4KB向上取整。
b. 出现异常，则消耗写CU = 0。

### incr操作：
a. 返回操作成功后的新值，则消耗读CU = 1，写CU = 1。
b. 出现InvalidArgument异常，则消耗读CU = 1，写CU = 0。
c. 出现其它异常，则消耗读CU = 0，写CU = 0。

### check_and_set操作：
a. 返回setSucceed，则消耗读CU = 1，写CU = Value数据大小除以4KB向上取整。
b. 出现InvalidArgument或TryAgain异常，消耗读CU = 1，写CU = 0。
c. 出现其它异常，则消耗读CU = 0，写CU = 0。

### check_and_mutate操作：
a. 返回setSucceed，则消耗读CU = 1，写CU = MutationList中所有行的SortKey+Value数据大小之和除以4KB向上取整。
b. 出现InvalidArgument或TryAgain异常，消耗读CU = 1，写CU = 0。
c. 出现其它异常，则消耗读CU = 0，写CU = 0。

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5NDY5MTc1OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/236,https://api.github.com/repos/apache/incubator-pegasus/issues/236,incubator-pegasus,392497412,236,Support configurating different compression algorithms for different levels,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-12-19T08:51:24Z,2022-07-20T11:59:38Z,"最好是支持节点级和表级的配置：
* 节点级：类似现在的 rocksdb_compression_type = snappy，最好是在这上面扩展并保持兼容，譬如：
  * rocksdb_compression_type = snappy，兼容老的(只对level>=2进行设置)
  * rocksdb_compression_type = per_level:none,lz4,zstd  (数组下标从0开始；如果设置的数组长度大于num_levels，则将多余的忽略；如果设置的数组长度小于num_levels，则后面的都用最后一个type补齐)
* 表级：类似replica.write_throttling，用户可以通过env设置rocksdb.compression_type，该值会覆盖节点级配置rocksdb_compression_type 。如果env的value出现格式错误或者不支持的压缩方式，则使用节点级配置rocksdb_compression_type 。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/236/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/237,https://api.github.com/repos/apache/incubator-pegasus/issues/237,incubator-pegasus,392498731,237,Support check if the cluster is balanced,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-12-19T08:55:33Z,2019-02-11T03:42:57Z,"* in meta-server, check if the cluster is balanced, and add related perf-counter
* in shell, show if the cluster is balanced for `cluster_info` command","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/237/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/237,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ1MDQ2NDQwMg==,incubator-pegasus,450464402,237,NA,mentoswang,40587846,Shanshan Wang,,NA,2018-12-29T04:21:20Z,2018-12-29T04:21:20Z,"### Balance Checker Operation Count
- 仅在允许做balance的条件下有效, 如果集群有app unhealthy或者replica信息收集不全时, count = 0
  - **在balance过程中，balance operation count会出现非零-零-非零的现象，原因是balance过程中无法将replica信息收集完全，本轮balance checker或者balancer会终止，等待下一轮**
    ```
    D2019-01-09 15:20:37.455 (1547018437455479685 4e9f)   meta.meta_state0.020100000000001c: greedy_load_balancer.cpp:917:balance(): balance checker round
    D2019-01-09 15:20:34.455 (1547018434455271847 4e9f)   meta.meta_state0.020100000000001c: greedy_load_balancer.cpp:833:operator()(): meta server hasn't collected gpid(1.0)'s info of 10.239.35.124:34802
    D2019-01-09 15:20:34.455 (1547018434455281269 4e9f)   meta.meta_state0.020100000000001c: server_state.cpp:2519:check_all_partitions(): balance checker operation count = 0
    ```
- action list size returned by greed_load_balancer.check/balance
  - check: 所有balance操作数
  - balance: 本轮balance操作数：by app if balancer_in_turn，如有primary则本轮不再做secondary
- steady level下只做check
- lively level下先做balance, 如果返回操作数 = 0则做check
- 其他load_balancer不统计
### Score
""把各个机器上的primary个数(或者secondary的个数，或者总数)求个方差""
""作为balancer的一个接口(如score)，或者放到server_state/meta_service里面?""
- meta_service::on_query_cluster_info -> server_state::get_cluster_balance_score -> greedy_load_balancer::score
- request时分别计算当前meta view下各节点primary count和partition count的标准差，以衡量集群均衡程度
- 当meta view中存在unalive的节点，并且仍留有partition没有迁移时，采用部分样本标准差，反之采用样本总体标准差
### Shell command
- cluster_info: get from balancer
![shell-cluster_info](https://user-images.githubusercontent.com/40587846/51518008-8f7f8d80-1e57-11e9-8864-dff65233f47d.png)
- remote_command -t meta-server help
  - meta.lb.get_balance_operation_count [all | move_pri | copy_pri | copy_sec | detail]
![shell-remote_command_1](https://user-images.githubusercontent.com/40587846/51518017-94dcd800-1e57-11e9-9ded-f2b50a351fd7.png)
![shell-remote_command_2](https://user-images.githubusercontent.com/40587846/51518023-98705f00-1e57-11e9-8f0b-e71244f218c6.png)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ1MDQ2NDQwMg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/238,https://api.github.com/repos/apache/incubator-pegasus/issues/238,incubator-pegasus,392499226,238,Support white list of replica-servers in meta-server,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-12-19T08:56:56Z,2019-03-12T06:41:05Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/238/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/238,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NDY4MzQ1OA==,incubator-pegasus,464683458,238,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2019-02-18T10:49:24Z,2019-02-18T10:49:24Z,"failure_detector类可以设置use_allow_list为true，也就是开启allow list功能。当allow list功能开启时，收到new worker的ping时，会查询worker是否是allow list中的：
* 如果不在allow list中，就将其reject，并且不允许该worker再次ping；
* 如果在allow list中，则标记worker connected。

因此，meta-server要实现白名单功能，只允许指定的replica-servers与其连接，只需要开启meta-server持有的failure detector的allow list功能。

配置说明：将replica-servers白名单写入配置中，重启meta-server使其生效。
```
[meta_server]   
  enable_white_list = true  
  replica_white_list = %{replica.server.list}
```

辅助工具：
* 增加remote_command “meta.fd.allow_list”来查询meta-server现在的replica白名单。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NDY4MzQ1OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/239,https://api.github.com/repos/apache/incubator-pegasus/issues/239,incubator-pegasus,392500815,239,Fix failure detection bug,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2018-12-19T09:01:16Z,2020-08-18T14:45:53Z,"# Bug描述
这个bug会导致add learner时出core的异常 #189 。


# 重现与测试

# Bug修复","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/239/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/251,https://api.github.com/repos/apache/incubator-pegasus/issues/251,incubator-pegasus,397232266,251,support backup request to alleviate the long tail problem,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2019-01-09T06:41:27Z,2020-08-04T02:14:15Z,the backup requests will be sent to the secondary replicas.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/251/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/251,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU4NDE3NjY3Nw==,incubator-pegasus,584176677,251,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-02-10T15:25:59Z,2020-02-10T15:25:59Z,"# Proposal: Backup Request

- Author(s): @levy5307 @neverchanje 

## Background

For now, Pegasus only supports reading from the primary. This proposal aims to support reading from secondary. This may improve the condition where the primary is instable (e.g rebalancing, hotspot writing), while other secondaries can serve with lower latency. So reading secondary will lead to better availablity.

**Backup request** is also known as ""hedge request"", which was first introduced in [The Tail at Scale](http://www.cs.duke.edu/courses/cps296.4/fall13/838-CloudPapers/dean_longtail.pdf) Dean and Barroso 2013. [BRPC](https://github.com/apache/incubator-brpc/blob/master/docs/en/backup_request.md) also implements this mechanism.

## Design

Backup Request is designed to be directly driven by the client, which means when it wants to read from secondary, it can read immediately without any registration to meta or replica, neither any additional communications to the server.

The client sends a read request, with an option`is_backup_request` set in the RPC header, to secondary. When the replica handles the request identifies this option is true, it continues to DB retrieval, otherwise, it responds with `ERR_INVALID_STATE`.

## TODO

- [x] Supports `is_backup_request` option in RPC header, and allows the secondary replica to handle read requests: https://github.com/XiaoMi/rdsn/pull/255. (Because the previous RPC protocol was not extendable for one more field, we redesigned the RPC protocol, changed request parsing.)

- [x] Java client adaption. https://github.com/XiaoMi/pegasus-java-client/pull/93

- [x] Performance Testing

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU4NDE3NjY3Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/251,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU4NjY2NjExMg==,incubator-pegasus,586666112,251,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-02-16T03:57:38Z,2020-02-16T03:57:38Z,"I put the related paragraphs described ""hedge request"" in ""The tail at scale"" here:

> **Hedged requests.** A simple way to curb latency variability is to issue the same request to multiple replicas and use the results from whichever replica responds first. We term such requests “hedged requests” because a client first sends one request to the replica believed to be the most appropriate, but then falls back on sending a secondary request after some brief delay. The client cancels remaining outstanding requests once the first result is received. Although naive implementations of this technique typically add unacceptable additional load, many variations exist that give most of the latency-reduction effects while increasing load only modestly.
>
> One such approach is to defer sending a secondary request until the first choosing but rather enqueuing copies of a request in multiple servers simultaneously and allowing the servers to communicate updates on the status of these copies to each other. We call requests where servers perform cross-server status updates “tied requests.” The simplest form of a tied request has the client send the request to two different servers, each tagged with the identity of the other server (“tied”). When a request begins execution, it sends a cancellation message to its counterpart. The corresponding request, if still enqueued in the other server, can be aborted immediately or deprioritized substantially.
>
> There is a brief window of one average network message delay where both servers may start executing the request while the cancellation messages are both in flight to the other server. A common case where this situation can occur is if both server queues are completely empty. It is useful therefore for the client to introduce a small delay of two times the average network message delay (1ms or less in modern data-center networks) between sending the first request and sending the second request.
>
> Google’s implementation of this technique in the context of its cluster-level distributed file system is effective at reducing both median and tail latencies. Table 2 lists the times for servicing a small read request from a BigTable where the data is not cached in memory but must be read from the underlying file system; each file chunk has three replicas on distinct machines. The table includes read latencies observed with and without tied requests for two scenarios: The first is a cluster in which the benchmark is running in isolation, in which case latency variability is mostly from self-interference and regular cluster-management activities. In it, sending a tied request that does cross-server cancellation to another file system replica following 1ms reduces median latency by 16% and is increasingly effective along the tail of the latency distribution, achieving nearly 40% reduction at the 99.9th-percentile latency. The second scenario is like the first except there is also a large, concurrent sorting job running on the same cluster contending for the same disk resources in the shared file system. Although overall latencies are somewhat higher due to higher utilization, similar reductions in the latency profile are achieved with the tied-request technique discussed earlier. The latency profile with tied requests while running a concurrent large sorting job is nearly identical to the latency profile of a mostly idle cluster without tied requests. Tied requests allow the workloads to be consolidated into a single cluster, resulting in dramatic computing cost reductions. In both Table 2 scenarios, the overhead of tied requests in disk utilization is less than 1%, indicating the cancellation strategy is effective at eliminating redundant reads.
>
> An alternative to the tied-request and hedged-request schemes is to probe remote queues first, then submit the request to the least-loaded server. It can be beneficial but is less effective than submitting work to two queues simultaneously for three main reasons: load levels can change between probe and request time; request service times can be difficult to estimate due to underlying system and hardware variability, and clients can create temporary hot spots by all clients picking the same (least- loaded) server at the same time. The Distributed Shortest-Positioning Time First system uses another variation in which the request is sent to one server and forwarded to replicas only if the initial server does not have it in its cache and uses cross-server cancellations.
>
> Worth noting is this technique is not restricted to simple replication but is also applicable in more-complex coding schemes (such as Reed-Solomon) where a primary request is sent to the machine with the desired data block, and, if no response is received following a brief delay, a collection of requests is issued to a subset of the remaining replication group sufficient to reconstruct the desired data, with the whole ensemble forming a set of tied requests.
>
> Note, too, the class of techniques described here is effective only when the phenomena that causes variability does not tend to simultaneously affect multiple request replicas. We expect such uncorrelated phenomena are rather common in large-scale systems.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU4NjY2NjExMg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/251,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU5OTQyMTgxOA==,incubator-pegasus,599421818,251,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2020-03-16T09:05:06Z,2020-03-16T09:05:06Z,"****performance test****

set/get operation:
|  test case   | enable backup request  |  read/write propotion  |  qps | read avg  |  read p95  |  read p99  |  read p999  |  read p9999  |  write avg  |  write p95  |  write p99  |  write p999  |  write p9999  |  
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | --- |
| 3-clients 15-threads | no | 1 : 3 | 7076 | 880.6512836149132 | 428.0 | 727.0 | 138495.0 | 988671.0 | 2495.0710801540517 | 6319.0 | 9023.0 | 36319.0 | 531455.0|
| 3-clients 15-threads | yes, delay 138ms | 1 : 3 | 6987 | 1010.1412488662884 | 403.0  | 7747.0 | 138751.0 | 153599.0 | 2476.104380444753 | 6859.0 | 9119.0 | 13759.0 | 185855.0 |
| 3-clients 100-threads | no | 1 : 0 | 140607 | 707.98960978 | 1474.0 | 2731.0 | 5511.0 | 167551.0 |  | | |  | |
| 3-clients 100-threads | yes, delay 5ms | 1 : 0 | 77429 | 1288.01461934 | 2935.0 | 3487.0 | 6323.0 | 71743.0 | ---- | ---- | ---- | ---- | --- |
| 3-clients 30-threads | no | 30 : 1 | 87198 | 306.9600544730426 | 513.0 | 805.0 | 4863.0 | 28271.0 | 1369.4669874672938 | 2661.0 | 5795.0 | 22319.0 | 51359.0 |
| 3-clients 30-threads | yes, delay 5ms | 30 : 1 | 88541 | 298.22470022339127 | 493.0 | 711.0 | 4483.0 | 18479.0 | 1467.6130963728997 | 3263.0 | 6411.0 | 17439.0 | 50975.0 |

Multi-get/Batch-Set operation: 
|  test case  | enable backup request  | read/write porpotion  | qps |  read avg  |  read p95  |  read p99  |  read p999  |  read p9999  |  write avg  |  write p95  |  write p99  |  write p999  |  write p9999  |  
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | --- |
| 3-clients  7-threads | no | 20 : 1 | 24113 | 200.37956913733476 | 277.0 | 410.0 | 2317.0 | 21647.0 | 2034.1923768463382 | 4283.0 | 6427.0 | 18271.0 | 62687.0 |
| 3-clients  7-threads | yes, deley 2ms | 20 : 1 | 23756 | 197.48540031650361 | 268.0 | 351.0 | 2173.0 | 5759.0 | 2187.199077764627 | 4531.0 | 6551.0 | 21551.0 | 63999.0 |
| 3-clients  15-threads | no | 20 : 1 | 30980 | 236.7482510418767 | 348.0 | 526.0 | 3535.0 | 25695.0 | 5361.380053671262 | 14087.0 | 20223.0 | 40639.0 | 90815.0 |
| 3-clients  15-threads | yes, delay 3ms | 20 : 1 | 30483 | 244.1182599024727 | 386.0 | 540.0 | 3105.0 | 13287.0 | 5377.992155339365 | 14119.0 | 19535.0 | 31311.0 | 103103.0 |
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU5OTQyMTgxOA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/255,https://api.github.com/repos/apache/incubator-pegasus/issues/255,incubator-pegasus,398829939,255,Support limit on the number of connections to avoid network resources to be used out,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2019-01-14T10:37:09Z,2020-01-19T08:18:05Z,limit on connection count by client as well as in total for a server,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/255/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/255,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ3MjY5MzQ4OA==,incubator-pegasus,472693488,255,NA,mentoswang,40587846,Shanshan Wang,,NA,2019-03-14T03:45:55Z,2019-03-14T03:45:55Z,limit on connection count by endpoint (ip) for each server,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ3MjY5MzQ4OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/256,https://api.github.com/repos/apache/incubator-pegasus/issues/256,incubator-pegasus,398830808,256,Support client version negotiate and check,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,OPEN,2019-01-14T10:39:24Z,2019-01-31T08:24:24Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/256/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/256,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ1OTI1NzI2Nw==,incubator-pegasus,459257267,256,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2019-01-31T08:24:24Z,2019-01-31T08:24:24Z,"version negotiate和auth negotiate一样，同属于SS_NEGOTIATION阶段。出于简单性的考虑，version negotiate不单独增加开关用于平滑升级，因此认证功能开启就是既要version negotiate也要auth negotiate。

初步设想为：

    // version negotiation process:
    //
    //                       client               server
    //                          | ---- VERSION ---> |
    //                          | <-- MATCH/MIS --- |
    
    enum version_nego_status
    {
        INVALID = 0,
        MISMATCH,
        MATCH
    }
    
    struct version_nego_message
    {
        1: version_nego_status status;
        2: string version;
    }

也就是说client告诉server自己的版本号，server经过check后回答MATCH或MISMATCH。这样一问一答即可完成版本协商。

但version check这一步需要斟酌，原因是：version是pegasus的version，而check这一步却在rdsn的rpc_session层。
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ1OTI1NzI2Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/258,https://api.github.com/repos/apache/incubator-pegasus/issues/258,incubator-pegasus,398947234,258,statistics and monitor the count of error and warning logs,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2019-01-14T15:43:53Z,2019-04-24T05:35:17Z,"if the count of error and warning logs exceed some threshold, the system (maybe falcon) should alarm.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/258/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/258,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ4NjA3NDY5MA==,incubator-pegasus,486074690,258,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-04-24T05:35:16Z,2019-04-24T05:35:16Z,"This issue has been automatically marked as stale because it's inactive for a long time. It will be closed if no further activity occurs, reopen if you have further ideas.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ4NjA3NDY5MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/259,https://api.github.com/repos/apache/incubator-pegasus/issues/259,incubator-pegasus,398951460,259,add counter of read and write failure for alarm,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2019-01-14T15:52:31Z,2019-01-21T03:22:59Z,in on_client_read() and on_client_write(),"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/259/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/260,https://api.github.com/repos/apache/incubator-pegasus/issues/260,incubator-pegasus,398958250,260,manual compact coredump bug,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2019-01-14T16:06:38Z,2019-01-28T08:20:01Z,"server version: https://github.com/XiaoMi/pegasus/commit/359cfad298e56bcbee0ba67e5f7fb7757e191af3
```
[c3-st03 bin]$ /usr/local/bin/gdb ./pegasus_server /home/core/core.replica.replica.26937.1547454319
Program terminated with signal SIGABRT, Aborted.
#0  0x0000003f746328a5 in raise () from /lib64/libc.so.6
(gdb) bt
#0  0x0000003f746328a5 in raise () from /lib64/libc.so.6
#1  0x0000003f74634085 in abort () from /lib64/libc.so.6
#2  0x0000003f7466ffe7 in __libc_message () from /lib64/libc.so.6
#3  0x0000003f74675916 in malloc_printerr () from /lib64/libc.so.6
#4  0x00000000008862dd in rocksdb::WritableFileWriter::Append(rocksdb::Slice const&) ()
#5  0x000000000092b989 in rocksdb::BlockBasedTableBuilder::WriteRawBlock(rocksdb::Slice const&, rocksdb::CompressionType, rocksdb::BlockHandle*) ()
#6  0x000000000092bd66 in rocksdb::BlockBasedTableBuilder::WriteBlock(rocksdb::Slice const&, rocksdb::BlockHandle*, bool) ()
#7  0x000000000092c1fe in rocksdb::BlockBasedTableBuilder::WriteBlock(rocksdb::BlockBuilder*, rocksdb::BlockHandle*, bool) ()
#8  0x000000000092c29b in rocksdb::BlockBasedTableBuilder::Flush() ()
#9  0x000000000092c41e in rocksdb::BlockBasedTableBuilder::Add(rocksdb::Slice const&, rocksdb::Slice const&) ()
#10 0x00000000008e0779 in rocksdb::CompactionJob::ProcessKeyValueCompaction(rocksdb::CompactionJob::SubcompactionState*) ()
#11 0x00000000008e1e52 in rocksdb::CompactionJob::Run() ()
#12 0x00000000006de5b9 in rocksdb::DBImpl::BackgroundCompaction(bool*, rocksdb::JobContext*, rocksdb::LogBuffer*, rocksdb::DBImpl::PrepickedCompaction*) ()
#13 0x00000000006e233f in rocksdb::DBImpl::BackgroundCallCompaction(rocksdb::DBImpl::PrepickedCompaction*, rocksdb::Env::Priority) ()
#14 0x00000000006e28ec in rocksdb::DBImpl::BGWorkCompaction(void*) ()
#15 0x00000000009415b8 in rocksdb::ThreadPoolImpl::Impl::BGThread(unsigned long) ()
#16 0x000000000094179d in rocksdb::ThreadPoolImpl::Impl::BGThreadWrapper(void*) ()
#17 0x00007f56cb494600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>) at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#18 0x0000003f74e07851 in start_thread () from /lib64/libpthread.so.0
#19 0x0000003f746e811d in clone () from /lib64/libc.so.6
(gdb) 
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/260/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/260,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ1NTQ1MzE4Nw==,incubator-pegasus,455453187,260,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2019-01-18T07:32:37Z,2019-01-18T07:32:37Z,"在已有表中持续写数据，并在此过程中执行manual compact，问题复现，出现多次core，但是另外一种栈：
```
[New LWP 166326]
[New LWP 166262]
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib64/libthread_db.so.1"".
Core was generated by `/home/work/app/pegasus/****/replica/package/bin/pegasus_server con'.
Program terminated with signal SIGABRT, Aborted.
#0  0x00007f8d37acc1d7 in raise () from /lib64/libc.so.6
(gdb) bt
#0  0x00007f8d37acc1d7 in raise () from /lib64/libc.so.6
#1  0x00007f8d37acd8c8 in abort () from /lib64/libc.so.6
#2  0x00007f8d3b19eb8e in dsn_coredump () at /home/work/***/Pegasus/pegasus/rdsn/src/core/core/service_api_c.cpp:73
#3  0x0000000000574c45 in pegasus::server::pegasus_server_impl::sync_checkpoint (this=0x7f7f40c1a570) at /home/work/***/Pegasus/pegasus/src/server/pegasus_server_impl.cpp:1758
#4  0x00000000005790bd in pegasus::server::pegasus_server_impl::do_manual_compact (this=0x7f7f40c1a570, options=...) at /home/work/***/Pegasus/pegasus/src/server/pegasus_server_impl.cpp:2620
#5  0x00000000005e6387 in pegasus::server::pegasus_manual_compact_service::manual_compact (this=0x7f7f40c1abf8, options=...)
    at /home/work/***/Pegasus/pegasus/src/server/pegasus_manual_compact_service.cpp:282
#6  0x00007f8d3b1f0489 in dsn::task::exec_internal (this=this@entry=0x7f7c88008d7f) at /home/work/***/Pegasus/pegasus/rdsn/src/core/core/task.cpp:180
#7  0x00007f8d3b270d9d in dsn::task_worker::loop (this=0x261e960) at /home/work/***/Pegasus/pegasus/rdsn/src/core/core/task_worker.cpp:211
#8  0x00007f8d3b270f69 in dsn::task_worker::run_internal (this=0x261e960) at /home/work/***/Pegasus/pegasus/rdsn/src/core/core/task_worker.cpp:191
#9  0x00007f8d38424600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/***/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#10 0x00007f8d38c9ddc5 in start_thread () from /lib64/libpthread.so.0
#11 0x00007f8d37b8e73d in clone () from /lib64/libc.so.6
(gdb)
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ1NTQ1MzE4Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/260,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ1NTQ3ODA3OA==,incubator-pegasus,455478078,260,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2019-01-18T09:15:26Z,2019-01-18T09:15:26Z,"> 在已有表中持续写数据，并在此过程中执行manual compact，问题复现，出现多次core，但是另外一种栈：
> 
> ```
> [New LWP 166326]
> [New LWP 166262]
> [Thread debugging using libthread_db enabled]
> Using host libthread_db library ""/lib64/libthread_db.so.1"".
> Core was generated by `/home/work/app/pegasus/****/replica/package/bin/pegasus_server con'.
> Program terminated with signal SIGABRT, Aborted.
> #0  0x00007f8d37acc1d7 in raise () from /lib64/libc.so.6
> (gdb) bt
> #0  0x00007f8d37acc1d7 in raise () from /lib64/libc.so.6
> #1  0x00007f8d37acd8c8 in abort () from /lib64/libc.so.6
> #2  0x00007f8d3b19eb8e in dsn_coredump () at /home/work/***/Pegasus/pegasus/rdsn/src/core/core/service_api_c.cpp:73
> #3  0x0000000000574c45 in pegasus::server::pegasus_server_impl::sync_checkpoint (this=0x7f7f40c1a570) at /home/work/***/Pegasus/pegasus/src/server/pegasus_server_impl.cpp:1758
> #4  0x00000000005790bd in pegasus::server::pegasus_server_impl::do_manual_compact (this=0x7f7f40c1a570, options=...) at /home/work/***/Pegasus/pegasus/src/server/pegasus_server_impl.cpp:2620
> #5  0x00000000005e6387 in pegasus::server::pegasus_manual_compact_service::manual_compact (this=0x7f7f40c1abf8, options=...)
>     at /home/work/***/Pegasus/pegasus/src/server/pegasus_manual_compact_service.cpp:282
> #6  0x00007f8d3b1f0489 in dsn::task::exec_internal (this=this@entry=0x7f7c88008d7f) at /home/work/***/Pegasus/pegasus/rdsn/src/core/core/task.cpp:180
> #7  0x00007f8d3b270d9d in dsn::task_worker::loop (this=0x261e960) at /home/work/***/Pegasus/pegasus/rdsn/src/core/core/task_worker.cpp:211
> #8  0x00007f8d3b270f69 in dsn::task_worker::run_internal (this=0x261e960) at /home/work/***/Pegasus/pegasus/rdsn/src/core/core/task_worker.cpp:191
> #9  0x00007f8d38424600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
>     at /home/***/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
> #10 0x00007f8d38c9ddc5 in start_thread () from /lib64/libpthread.so.0
> #11 0x00007f8d37b8e73d in clone () from /lib64/libc.so.6
> (gdb)
> ```

这个core的原因是：
此处：https://github.com/XiaoMi/pegasus/blob/master/src/server/pegasus_server_impl.cpp#L1758
在checkpoint的过程中，他自己的flush之后，如果还有新的数据写入并进行了flush，两个值就不一样了","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ1NTQ3ODA3OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/260,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ1NzQyOTIzNQ==,incubator-pegasus,457429235,260,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-01-25T02:06:14Z,2019-01-25T02:06:14Z,"根本原因是：sync_checkpoint这个函数本来一开始设计的时候，使用场景就是受限的，是和写操作排斥的，这个函数值只在 init 和 learn 的时候使用，在这些情况下，不会有并发的write操作。
所以manual compact这里就不能用sync_checkpoint，而要用async_checkpoint。
参见 #268","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ1NzQyOTIzNQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/261,https://api.github.com/repos/apache/incubator-pegasus/issues/261,incubator-pegasus,400082477,261,improve assign strategy to balance multiple disks usage,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2019-01-17T02:21:26Z,2022-07-20T12:02:33Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/261/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/261,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU4NjE3MzgwNA==,incubator-pegasus,586173804,261,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2020-02-14T09:25:30Z,2020-02-14T09:25:30Z,"## 提案: Disk Rebalancer
* Author(s): @Shuo-Jia 
## 设计背景
Pegasus已经实现了节点级别的负载均衡，但是对于单个节点内磁盘的负载均衡却没有针对性的工具。虽然Pegasus在分片落盘的策略中尽量保证了分片级别的均衡，但由于不同表的单个分片容量不一致，会随着不同表的数据写入的增多，存在磁盘容量分布不均的问题。我们选取线上的一个集群可以发现，在节点负载基本均衡、磁盘分片基本均衡的情况下，节点内的磁盘容量并没有保证均衡：
```
// node state
// command(use pegasus-shell): nodes -d
address             status    replica_count  primary_count    secondary_count
node1               ALIVE               259             92                 167
node2               ALIVE               294             94                 200
node3               ALIVE               267             95                 172
node4               ALIVE               279             78                 201
node5               ALIVE               258             92                 166
node6               ALIVE               259             89                 170
node7               ALIVE               295             80                 215
node8               ALIVE               271             99                 172
node9               ALIVE               284             96                 188
node10              ALIVE               267             96                 171
node11              ALIVE               267             89                 178

// disk replica state（node7）
// command: tree {ssd_path} | grep ""\.pegasus"" | wc -l
ssd                 replica_count
/dev/sde1                 54  
/dev/sdh1                 52
/dev/sdg1                 55
/dev/sdf1                 57
/dev/sdc1                 54
/dev/sdd1                 56
/dev/sdb1                 57

// disk capacity state(node7)
// command(login node7): df -h
/dev/sdd1                    445G  255G  186G  58% /home/work/ssd3
/dev/sdb1                    445G  215G  226G  49% /home/work/ssd1
/dev/sde1                    445G   77G  364G  18% /home/work/ssd4
/dev/sdg1                    445G  257G  183G  59% /home/work/ssd6
/dev/sdf1                    445G  136G  305G  31% /home/work/ssd5
/dev/sdc1                    445G  258G  182G  59% /home/work/ssd2
/dev/sdh1                    445G  153G  287G  35% /home/work/ssd7
```

该提案旨在提出完整的磁盘负载均衡方案，使得在磁盘使用不合理的情况下，能够触发磁盘的均衡策略并实现磁盘分片数目和容量均衡。
## 设计概述
* 考虑到不影响系统已有的“node rebalancer”功能，设计的“disk rebalancer”功能是与其分离的，并类似于[HDFS DiskRebalancer](https://hadoop.apache.org/docs/r3.0.0/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html) 使用命令行触发的方式来执行：使用命令查看当前的磁盘均衡状态，并生成对应的磁盘均衡计划（步骤），随后执行该计划触发磁盘间的数据迁移以实现磁盘均衡。
* 整个功能分为两大部分：
  + 均衡评估：以表为单位，评估每个表的primary、secondary在磁盘上是否均衡，给出磁盘分片的分布情况、容量使用情况，并计算当前的均衡度量值。同时在节点级别以均方差的形式计算当前节点所有磁盘、所有表的总的均衡度。
  + 均衡策略：以表为最小决策单位，以分片为最小迁移单位，遍历每个表分别实现单表的primary、secondary均衡，进而实现磁盘的全局均衡。
* 整体功能有replica_server来执行，但需要把所有节点的均衡策略执行的状态交由meta_server来维护，这点区别node rebalancer中meta_server实现均衡状态计算和策略的生成。
## 总体设计
![选区_015](https://user-images.githubusercontent.com/23136769/74802815-e1723b80-5315-11ea-9fe8-44d48c401661.png)
* 黑色部分为Pegasus已有的类，红色为新增的类。
* 所有功能涉及三个部分：
  + command模块：负责balance相关命令的下发。
  + meta_server模块：command和replica的桥梁，一方面和replica_server同步当前的balance的状态，另一方面与command通信转发请求给replica_server并返回结果给command模块。功能的核心入口为server_state。
  + replica_server模块：实现核心的均衡状态查询和执行的逻，该功能的核心入口为replica_stub作为入口
* 所有的请求包括五种：
  + query_disk_info: 查看当前的个节点的磁盘均衡状态，包括磁盘利用率、分片分布等。该模块需要借助fs_manager实现。
  + create_disk_balance_policy: 创建某个节点的均衡策略的具体步骤。该模块依赖disk_balancer_policy实现，而disk_balancer_policy则需要借助fs_manager维护的disk信息进行计算
  + excute_disk_balance: 根据创建好的步骤执行数据迁移拷贝任务，迁移拷贝的实际执行依赖disk_balancer_service，并最终借助nfs_node、replica_chkpt来执行。实际迁移策略的执行，需要参照[replica_learn](https://github.com/XiaoMi/rdsn/blob/0db7b36840/src/dist/replication/lib/replica_learn.cpp)模块（该模块包含节点间数据迁移功能）的步骤来进行，即：先复制→后回放。
  + query_disk_balance_stats: 查看当前的执行状态。
  + cancel_disk_balance: 取消现在的磁盘均衡策略的执行。
## 核心模块设计
### 分片均衡状态设计
#### 方案一：参照meta_server维护的disk_load进行设计（舍弃）
![image2020-2-10_17-23-20](https://user-images.githubusercontent.com/23136769/74516340-2f202a00-4f4b-11ea-8a9b-bb96c821989e.png)
* 已有的meta_server模块的分片均衡状态计算的逻辑在[bool greedy_load_balancer::calc_disk_load](https://github.com/XiaoMi/rdsn/blob/0db7b36840/src/dist/replication/meta_server/greedy_load_balancer.cpp#L540)，原理如上图。可以看到，server_state通过rpc获取了app_info原始信息并生成app_mapper信息给greedy_load_balancer用，该模块在使用的时候通过meta_data的get_node_state和get_config_context提取的信息最终生成了disk_load，也就是说meta_server对从replica_stub获取的原始信息做了大量的重新封装，而后才计算出disk_load。
* 由于meta_server有自己特定“config信息”封装规则，所以与replica_server复用一些逻辑是很难的，一个推荐的做法的是：replica_stub把disk_load信息计算完成后可以直接同步给meta_server使用，而不用使用目前这么复杂的逻辑。
#### 方案二：基于fs_manager类重新设计和维护（选定）
* 在当前的设计中，倾向于重新为replica_stub设计自己的disk_load信息，设计如下：
![image2020-2-10_20-53-33 (1)](https://user-images.githubusercontent.com/23136769/74518089-385ec600-4f4e-11ea-9a62-041fd932aff2.png)
  + fs_manager管理所有的disk信息(_sorted_dir_nodes/_dir_nodes)，并根据disk的信息计算均衡度——compute_disk_density()
  + dir_node管理当前disk下的容量和分片信息（holding_replicas/holding_primary_replicas），并根据均衡度重写dir_node的排序信息
 + 定义节点利用率total_used_ratio，磁盘利用率disk_used_ratio，则均衡度为`disk_density = disk_used_ratio-total_used_ratio`
 + 定义磁盘容量disk_capacity，则可迁移最大数据量`max_move = abs(disk_capacity*total_used_ratio - disk_used)`
 + 定义节点均衡度:`node_density = sum(abs(disk_density))`
### 均衡策略生成设计
![image2020-2-14_10-22-29 (1)](https://user-images.githubusercontent.com/23136769/74518284-82e04280-4f4e-11ea-9bce-f8eff8e115d8.png)
* 均衡策略生成由disk_balancer_policy类来完成，其计算原理为：以表为单位，把某张表在所有磁盘下的分片数据按照均衡度(disk_density)进行排序，每次选举最大、最小容量的两个分区按照最大可迁移量（max_move）进行迁移，迁移完成后更新现在的的分区均衡状态，循环执行，直到认为当前已经均衡（参考[HDFS DiskBalancer](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/GreedyPlanner.java)）。需要注意的是：最大可迁移量在迁移的时候最终需要转换为“最大可迁移分片”：
`max_move_replica=max_move/replica_size`
* 如“分片均衡状态设计”所述，fs_manager存储了所有磁盘的容量均衡信息和分片均衡信息，disk_balancer_policy需要借助这些信息进行排序、计算。
* disk_balancer_actions存储了一次均衡操作的所有步骤，每个迁移（move_action）主要包含迁移源（source），迁移目的地（destination），迁移量（move_bytes/move_replica）
* 需要注意的是，每次生生成一个步骤，需要更新当前的磁盘均衡状态（使用compute_disk_density）
### 数据迁移执行设计
### 参考：node rebalancer
![image2020-2-14_11-23-40 (1)](https://user-images.githubusercontent.com/23136769/74518379-b28f4a80-4f4e-11ea-93e6-c4f004140441.png)
* 图示是现有的node rebalance时数据的迁移步骤，由于node rebalancer涉及不同节点间的通信，所以定义了learner和leanee两个角色，并且通过RPC进行通信。
* learner通过init_learn方法首先要更新当前未提交的request，并且通过“catch_up_with_logs”循环执行日志“同步操作”，随后发起RPC通信。learnee收到请求后比对当前的decree，并且分别处理mutation数据、checkpoint数据、plog数据，并返回给learner。其中返回的核心数据存储在learn_state中。learner通过on_learn_reply处理响应，通过nfs→copy_remote_files拷贝远端数据，最终通过on_copy_remote_state_completed分别处理checkpoint和plog，同样该逻辑是是循环执行的，最终依然要回到init再次发起同步请求，保证数据完备性。
* **疑问：mutation数据的处理没有明白。**
### 设计：disk rebalancer
* disk rebalancer少了网络通信的步骤，所以实际设计仅需要“learner”中的“init_learn”逻辑即可，即不断的catch_up_with_private_logs保证数据的完整和同步即可。
* 迁移步骤：learn过程数据迁移包含三个部分——迁移磁盘数据、迁移mutation数据、迁移plog数据，但对于disk rebalancer来说，不同的磁盘共用“mutation”数据，所以实际的步骤如下：
  + 为待迁移的数据创建“checkpoint”，目标路径为“目标迁移路径”
  + 使用plog回放新增的数据，并写入到新的路径下
  + 变更mutation的目标的路径为新的路径（需要调研具体实现）
* 数据完整：在线数据的迁移要保证数据的一致性和完整性。
  + 在迁移数据包括回放plog的过程中，新的请求数据会时刻写入到旧的replica路径下，所以为了保证数据完整性，需要参考replica_learn的catch_up_with_private_logs进行回放日志。
  + checkpoint的数据和原始路径的文件会有差异（log文件等），需要考虑是否需要二次拷贝。
* 状态变更：replica_learn在数据的迁移过程中，会使learner的replica置为“PS_POTENTIAL_SECONDARY”，disk rebalancer需要设计类似的状态机变化，以保证状态的切换对数据的访问的合理性
  + disk rebalancer的数据迁移可能会导致同一个node下产生两个相同的replica，所以需要注意同名replica对replica_server造成的影响
  + 新增的replica的状态可以设置成invalid状态，直至确定数据已经完整，随后改为secondary状态，并把旧的replica置为invalid
## 开发计划
### 总体计划：
command和meta_server（包括meta_server与command、replica_server通信与同步逻辑）暂时不予关注，首先专注replica_server模块的设计，随后再开发command和meta_server模块。
### 具体步骤：
* 磁盘的均衡状态信息功能开发
* 磁盘均衡策略开发
* 磁盘均衡策略模拟测试开发
* 磁盘均衡数据迁移功能开发
* meta_server和replica_server数据通信与同步开发
* command模块开发
### 潜在难点：
* 磁盘数据迁移的一致性和完整性
* 均衡策略的有效性
## 其他问题
* **为什么以表为单位进行均衡？** 对于任意一个表，我们认为每个replica的大小都是一样的。所以当每个表的primary、secondary在磁盘的分布均衡时，则理想情况下磁盘的所有primary、secondary是均衡的，全局磁盘利用率是均衡的。这种“磁盘均衡”是较为“彻底的均衡”，既满足“容量的均衡”，又满足“负载的均衡”——其本质上是和“node rebalancer”一个原理和效果。
* **以表为单位进行均衡，是否对的单个磁盘整体均衡有误差？**  当primary和secondary的数量无法整除磁盘数量时，将会带来累积误差，而且这种误差如果不在后续迭代每个表进行均衡的的时候处理妥善，将会随着表的数量的增加而越来越大，因此大致的设计思路是：
（1）初始状态，当无法整除的时候，将随机选择一些磁盘进行分配
（2）当继续其余表的均衡的时候，将优先选择磁盘分片数较少、且磁盘容量利用率小的磁盘进行分配
* **数据迁移的开销怎么样？** 基于每个表进行迁移以达到磁盘全局的磁盘均衡策略，将会出现很多“多余的”replica迁移(实际是不同表的replica交换)，例如2个磁盘，2个表,以primary为例：5个tableA_pri + 1个tableB_pri | 1个tableA_pri + 5个tableB_pri，如果从全局来看，磁盘容量均衡、请求负载均衡，则就达到了我们的“均衡目的”，但是该策略中表是最小的决策单位，所以我们依然需要更换不同的表的replica，这将增大均衡策略中迁移数据的开销。但是如果我们严格需要每个表在磁盘的分布都是均衡的，这种开销是不可避免的。
* **均衡策略的有效性怎么样？** 目前的磁盘均衡策略借鉴HDFS DiskBalancer的思想，并基于Pegasus的表级特性、分片特性进行实现。至于效果需要后续实现简单的Simulator进行测试。
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU4NjE3MzgwNA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/264,https://api.github.com/repos/apache/incubator-pegasus/issues/264,incubator-pegasus,402125820,264,support spark reader to read cold backup data on HDFS,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2019-01-23T08:35:52Z,2021-06-01T07:06:33Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/264/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/265,https://api.github.com/repos/apache/incubator-pegasus/issues/265,incubator-pegasus,402128067,265,seperate key and large value in rocksdb,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,OPEN,2019-01-23T08:42:46Z,2020-08-18T14:45:54Z,refer to WiscKey & HashKV,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/265/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/265,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ1NjcxNjk2NQ==,incubator-pegasus,456716965,265,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-01-23T08:43:14Z,2019-01-23T08:43:14Z,可以参考TiDB的TiTan实现：https://zhuanlan.zhihu.com/p/55521489,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ1NjcxNjk2NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/273,https://api.github.com/repos/apache/incubator-pegasus/issues/273,incubator-pegasus,406198584,273,祝大家2019年新春愉快！,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2019-02-04T06:36:28Z,2020-02-11T03:50:48Z,新的一年，祝Pegasus系统越来越稳健，业务越来越多，价值越来越大。,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/273/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/273,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2MDQ4OTgzNQ==,incubator-pegasus,460489835,273,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-02-05T02:22:11Z,2019-02-05T02:22:11Z,优秀！,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2MDQ4OTgzNQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/276,https://api.github.com/repos/apache/incubator-pegasus/issues/276,incubator-pegasus,411285642,276,coredump when manual compact,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2019-02-18T03:21:59Z,2020-01-19T08:16:32Z,"# 版本
Pegasus Server 1.11.2 (a186d383e113ea9e90cbb11460e8542ddde49613) Release

# 环境
c3srv-add
c3-hadoop-pegasus-srv-st06
/home/core/core.replica.replica.49746.1550276531 
正在执行manual compact

# coredump
```
Core was generated by `/home/work/app/pegasus/c3srv-add/replica/package/bin/pegasus_server config.ini'.
Program terminated with signal SIGABRT, Aborted.
#0  0x0000003dd24328a5 in raise () from /lib64/libc.so.6
(gdb) bt
#0  0x0000003dd24328a5 in raise () from /lib64/libc.so.6
#1  0x0000003dd2434085 in abort () from /lib64/libc.so.6
#2  0x0000003dd246ffe7 in __libc_message () from /lib64/libc.so.6
#3  0x0000003dd2475916 in malloc_printerr () from /lib64/libc.so.6
#4  0x00000000008762ed in rocksdb::WritableFileWriter::Append(rocksdb::Slice const&) ()
#5  0x000000000091b999 in rocksdb::BlockBasedTableBuilder::WriteRawBlock(rocksdb::Slice const&, rocksdb::CompressionType, rocksdb::BlockHandle*) ()
#6  0x000000000091bd76 in rocksdb::BlockBasedTableBuilder::WriteBlock(rocksdb::Slice const&, rocksdb::BlockHandle*, bool) ()
#7  0x000000000091c20e in rocksdb::BlockBasedTableBuilder::WriteBlock(rocksdb::BlockBuilder*, rocksdb::BlockHandle*, bool) ()
#8  0x000000000091c2ab in rocksdb::BlockBasedTableBuilder::Flush() ()
#9  0x000000000091c42e in rocksdb::BlockBasedTableBuilder::Add(rocksdb::Slice const&, rocksdb::Slice const&) ()
#10 0x00000000008d0789 in rocksdb::CompactionJob::ProcessKeyValueCompaction(rocksdb::CompactionJob::SubcompactionState*) ()
#11 0x00000000008d1e62 in rocksdb::CompactionJob::Run() ()
#12 0x00000000006ce5c9 in rocksdb::DBImpl::BackgroundCompaction(bool*, rocksdb::JobContext*, rocksdb::LogBuffer*, rocksdb::DBImpl::PrepickedCompaction*) ()
#13 0x00000000006d234f in rocksdb::DBImpl::BackgroundCallCompaction(rocksdb::DBImpl::PrepickedCompaction*, rocksdb::Env::Priority) ()
#14 0x00000000006d28fc in rocksdb::DBImpl::BGWorkCompaction(void*) ()
#15 0x00000000009315c8 in rocksdb::ThreadPoolImpl::Impl::BGThread(unsigned long) ()
#16 0x00000000009317ad in rocksdb::ThreadPoolImpl::Impl::BGThreadWrapper(void*) ()
#17 0x00007f27df3c5600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>) at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#18 0x0000003dd2c07851 in start_thread () from /lib64/libpthread.so.0
#19 0x0000003dd24e811d in clone () from /lib64/libc.so.6
(gdb)
```

# 相关
#260 ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/276/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/278,https://api.github.com/repos/apache/incubator-pegasus/issues/278,incubator-pegasus,411722771,278,refactor shell/commands.h,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2019-02-19T03:00:50Z,2019-02-21T11:23:54Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/278/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/280,https://api.github.com/repos/apache/incubator-pegasus/issues/280,incubator-pegasus,412207407,280,build failed on centos 7.2,clicx,13973702,,,CLOSED,2019-02-20T01:55:37Z,2019-02-20T06:27:31Z,"Build succeed
INFO: start build rocksdb...
WARNING_ALL=NO
ENABLE_GCOV=NO
BUILD_TYPE=release
Building...
RUN_VERBOSE=NO
make: *** No rule to make target `install'.  Stop.
ERROR: build rocksdb failed","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/280/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/280,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NTM5MTk0Mw==,incubator-pegasus,465391943,280,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-02-20T02:15:53Z,2019-02-20T02:15:53Z,"你用的哪个分支编译的？试试checkout到master分支？
另外你的CMake版本是多少？","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NTM5MTk0Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/280,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NTM5Mzg4Mg==,incubator-pegasus,465393882,280,NA,clicx,13973702,,,NA,2019-02-20T02:25:25Z,2019-02-20T02:25:25Z,"#git branch
* master
#cmake --version
cmake version 2.8.12.2","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NTM5Mzg4Mg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/280,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NTM5ODI2Ng==,incubator-pegasus,465398266,280,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-02-20T02:46:18Z,2019-02-20T02:46:18Z,你用 `./run.sh build -c -v` 重新编译一遍，看看报错是什么？,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NTM5ODI2Ng==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/280,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NTQ0MjU1Mw==,incubator-pegasus,465442553,280,NA,clicx,13973702,,,NA,2019-02-20T06:26:27Z,2019-02-20T06:26:27Z,"Thanks, now build successfully after changed the command that you provided.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NTQ0MjU1Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/286,https://api.github.com/repos/apache/incubator-pegasus/issues/286,incubator-pegasus,413224418,286,cpp client core when meta closes connection of query config if connection threshold is hit,mentoswang,40587846,Shanshan Wang,,OPEN,2019-02-22T03:18:21Z,2019-03-15T03:21:16Z,"The problem can be reproduced on the temp branch https://github.com/XiaoMi/pegasus/tree/timeout_core

Steps:
1. build
2. start onebox (3 meta, 5 replica, connection_threshold_endpoint is set to 7 for testing)
[network]
; how many network threads for network library(used by asio)
io_service_worker_count = 4
**connection_threshold_endpoint = 7**
3. start a shell, so primary meta connection count will be 6
4. build cpp client sample, run it (query config from primary meta will hit the connection threshold)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/286/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/286,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NjI2NzQyMQ==,incubator-pegasus,466267421,286,NA,mentoswang,40587846,Shanshan Wang,,NA,2019-02-22T04:14:33Z,2019-02-22T04:14:33Z,"change client log level to LOG_LEVEL_INFORMATION, to see more log info
change set timeout to 10000 for cpp client sample, more easy to recreate
    int ret = client->set(hash_key, sort_key, value, **10000**);","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NjI2NzQyMQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/286,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NjI3NjA2Nw==,incubator-pegasus,466276067,286,NA,mentoswang,40587846,Shanshan Wang,,NA,2019-02-22T05:11:14Z,2019-02-22T05:11:14Z,"Two different core stacks, one of them is as below:

Core was generated by `./pegasus_cpp_sample onebox temp'.
Program terminated with signal SIGSEGV, Segmentation fault.
-0  0x00007f5fe1ba2b88 in main_arena () from /lib/x86_64-linux-gnu/libc.so.6
[Current thread is 1 (Thread 0x7f5fb37fe700 (LWP 15557))]
(gdb) bt
-0  0x00007f5fe1ba2b88 in main_arena () from /lib/x86_64-linux-gnu/libc.so.6
-1  0x00000000004c5aa1 in dsn::task_queue::increase_count (count=1, this=0x2223bf0) at /home/mi/pegasus/rdsn/include/dsn/tool-api/task_queue.h:85
-2  dsn::task_queue::enqueue_internal (this=0x2223bf0, task=0x7f5f98000998) at /home/mi/pegasus/rdsn/src/core/core/task_queue.cpp:97
-3  0x000000000056ae12 in dsn::tools::simple_timer_service::<lambda(const boost::system::error_code&)>::operator() (__closure=0x7f5fb37fbd80, ec=...)
    at /home/mi/pegasus/rdsn/src/core/tools/common/simple_task_queue.cpp:72
-4  boost::asio::detail::binder1<dsn::tools::simple_timer_service::add_timer(dsn::task*)::<lambda(const boost::system::error_code&)>, boost::system::error_code>::operator() (
    this=0x7f5fb37fbd80) at /usr/local/include/boost/asio/detail/bind_handler.hpp:47
-5  boost::asio::asio_handler_invoke<boost::asio::detail::binder1<dsn::tools::simple_timer_service::add_timer(dsn::task*)::<lambda(const boost::system::error_code&)>, boost::system::error_code> > (function=...) at /usr/local/include/boost/asio/handler_invoke_hook.hpp:69
-6  boost_asio_handler_invoke_helpers::invoke<boost::asio::detail::binder1<dsn::tools::simple_timer_service::add_timer(dsn::task*)::<lambda(const boost::system::error_code&)>, boost::system::error_code>, dsn::tools::simple_timer_service::add_timer(dsn::task*)::<lambda(const boost::system::error_code&)> > (context=..., function=...)
    at /usr/local/include/boost/asio/detail/handler_invoke_helpers.hpp:37
-7  boost::asio::detail::wait_handler<dsn::tools::simple_timer_service::add_timer(dsn::task*)::<lambda(const boost::system::error_code&)> >::do_complete(boost::asio::detail::io_service_impl *, boost::asio::detail::operation *, const boost::system::error_code &, std::size_t) (owner=0x2225a40, base=0x7f5f94010ab0)
    at /usr/local/include/boost/asio/detail/wait_handler.hpp:70
-8  0x000000000056a6ef in boost::asio::detail::task_io_service_operation::complete (bytes_transferred=0, ec=..., owner=..., this=0x7f5f94010ab0)
    at /usr/local/include/boost/asio/detail/task_io_service_operation.hpp:38
-9  boost::asio::detail::task_io_service::do_run_one (ec=..., this_thread=..., lock=..., this=0x2225a40) at /usr/local/include/boost/asio/detail/impl/task_io_service.ipp:372
-10 boost::asio::detail::task_io_service::run (ec=..., this=0x2225a40) at /usr/local/include/boost/asio/detail/impl/task_io_service.ipp:149
-11 boost::asio::io_service::run (this=<optimized out>, ec=...) at /usr/local/include/boost/asio/impl/io_service.ipp:66
-12 dsn::tools::simple_timer_service::<lambda()>::operator() (__closure=<optimized out>) at /home/mi/pegasus/rdsn/src/core/tools/common/simple_task_queue.cpp:50
-13 std::_Bind_simple<dsn::tools::simple_timer_service::start()::<lambda()>()>::_M_invoke<> (this=<optimized out>) at /usr/include/c++/5/functional:1531
-14 std::_Bind_simple<dsn::tools::simple_timer_service::start()::<lambda()>()>::operator() (this=<optimized out>) at /usr/include/c++/5/functional:1520
-15 std::thread::_Impl<std::_Bind_simple<dsn::tools::simple_timer_service::start()::<lambda()>()> >::_M_run(void) (this=<optimized out>) at /usr/include/c++/5/thread:115
-16 0x00007f5fe1e76c80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
-17 0x00007f5fe296d6ba in start_thread (arg=0x7f5fb37fe700) at pthread_create.c:333
-18 0x00007f5fe18e541d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109

client log:
I2019-02-22 13:04:57.662 (1550811897662068659 3cab)  mimic.io-thrd.15531: temp.client: start query config, gpid = -1.-1
I2019-02-22 13:04:57.662 (1550811897662116335 3cab)  mimic.io-thrd.15531: rpc_name = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX, remote_addr = 127.0.0.1:34602, header_format = NET_HDR_DSN, channel = RPC_CHANNEL_TCP, seq_id = 2, trace_id = 407952f1ef59b91d
D2019-02-22 13:04:57.662 (1550811897662178095 3cab)  mimic.io-thrd.15531: client session created, remote_server = 127.0.0.1:34602, current_count = 1
I2019-02-22 13:04:57.662 (1550811897662416421 3cad)  mimic.io-thrd.15533: client session 127.0.0.1:34602 connected
I2019-02-22 13:04:57.662 (1550811897662461366 3cad)  mimic.io-thrd.15533: boost asio send buffer size is 1313280, set as 16MB, now is 212992
I2019-02-22 13:04:57.662 (1550811897662480418 3cad)  mimic.io-thrd.15533: boost asio recv buffer size is 530904, set as 16MB, now is 212992
I2019-02-22 13:04:57.662 (1550811897662512809 3cad)  mimic.io-thrd.15533: boost asio set no_delay = true
D2019-02-22 13:04:57.662 (1550811897662527924 3cad)  mimic.io-thrd.15533: client session connected, remote_server = 127.0.0.1:34602, current_count = 1
I2019-02-22 13:04:57.663 (1550811897663228862 3caf)  mimic.io-thrd.15535: rpc_name = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX, remote_addr = 10.239.35.124:34601, header_format = NET_HDR_DSN, channel = RPC_CHANNEL_TCP, seq_id = 2, trace_id = 407952f1ef59b91d
D2019-02-22 13:04:57.663 (1550811897663301738 3caf)  mimic.io-thrd.15535: client session created, remote_server = 10.239.35.124:34601, current_count = 2
I2019-02-22 13:04:57.663 (1550811897663665688 3cb0)  mimic.io-thrd.15536: client session 10.239.35.124:34601 connected
I2019-02-22 13:04:57.663 (1550811897663718216 3cb0)  mimic.io-thrd.15536: boost asio send buffer size is 1313280, set as 16MB, now is 212992
I2019-02-22 13:04:57.663 (1550811897663739619 3cb0)  mimic.io-thrd.15536: boost asio recv buffer size is 530904, set as 16MB, now is 212992
I2019-02-22 13:04:57.663 (1550811897663753633 3cb0)  mimic.io-thrd.15536: boost asio set no_delay = true
D2019-02-22 13:04:57.663 (1550811897663767529 3cb0)  mimic.io-thrd.15536: client session connected, remote_server = 10.239.35.124:34601, current_count = 2
D2019-02-22 13:04:57.663 (1550811897663955032 3cb0)  mimic.io-thrd.15536: asio read from 10.239.35.124:34601 failed: End of file
D2019-02-22 13:04:57.663 (1550811897663981374 3cb0)  mimic.io-thrd.15536: client session disconnected, remote_server = 10.239.35.124:34601, current_count = 1
W2019-02-22 13:04:57.664 (1550811897664023999 3cb0)  mimic.io-thrd.15536: asio socket shutdown failed, error = Transport endpoint is not connected

The other one is as below:

Core was generated by `./pegasus_cpp_sample onebox temp'.
Program terminated with signal SIGABRT, Aborted.
-0  0x00007f7067008428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54
54	../sysdeps/unix/sysv/linux/raise.c: 没有那个文件或目录.
[Current thread is 1 (Thread 0x7f7068a43080 (LWP 13369))]
(gdb) bt
-0  0x00007f7067008428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54
-1  0x00007f706700a02a in __GI_abort () at abort.c:89
-2  0x00000000004b203e in dsn_coredump () at /home/mi/temp/pegasus/rdsn/src/core/core/service_api_c.cpp:73
-3  0x000000000056816a in dsn::rpc_client_matcher::~rpc_client_matcher (this=0x17ddf10, __in_chrg=<optimized out>) at /home/mi/temp/pegasus/rdsn/src/core/core/rpc_engine.cpp:71
-4  0x00000000004be548 in dsn::rpc_engine::~rpc_engine (this=0x17ddeb0, __in_chrg=<optimized out>) at /home/mi/temp/pegasus/rdsn/src/core/core/rpc_engine.h:130
-5  std::default_delete<dsn::rpc_engine>::operator() (this=<optimized out>, __ptr=0x17ddeb0) at /usr/include/c++/5/bits/unique_ptr.h:76
-6  std::unique_ptr<dsn::rpc_engine, std::default_delete<dsn::rpc_engine> >::~unique_ptr (this=0x17d9b40, __in_chrg=<optimized out>) at /usr/include/c++/5/bits/unique_ptr.h:236
-7  dsn::service_node::io_engine::~io_engine (this=0x17d9b40, __in_chrg=<optimized out>) at /home/mi/temp/pegasus/rdsn/src/core/core/service_engine.h:67
-8  dsn::service_node::~service_node (this=0x17d9940, __in_chrg=<optimized out>) at /home/mi/temp/pegasus/rdsn/src/core/core/service_engine.h:64
-9  0x00000000004b740a in std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release (this=0x17d9930) at /usr/include/c++/5/bits/shared_ptr_base.h:150
-10 std::__shared_count<(__gnu_cxx::_Lock_policy)2>::~__shared_count (this=<optimized out>, __in_chrg=<optimized out>) at /usr/include/c++/5/bits/shared_ptr_base.h:659
-11 std::__shared_ptr<dsn::service_node, (__gnu_cxx::_Lock_policy)2>::~__shared_ptr (this=<optimized out>, __in_chrg=<optimized out>) at /usr/include/c++/5/bits/shared_ptr_base.h:925
-12 std::shared_ptr<dsn::service_node>::~shared_ptr (this=<optimized out>, __in_chrg=<optimized out>) at /usr/include/c++/5/bits/shared_ptr.h:93
-13 std::pair<int const, std::shared_ptr<dsn::service_node> >::~pair (this=<optimized out>, __in_chrg=<optimized out>) at /usr/include/c++/5/bits/stl_pair.h:96
-14 __gnu_cxx::new_allocator<std::_Rb_tree_node<std::pair<int const, std::shared_ptr<dsn::service_node> > > >::destroy<std::pair<int const, std::shared_ptr<dsn::service_node> > > (this=<optimized out>, __p=<optimized out>)
    at /usr/include/c++/5/ext/new_allocator.h:124
-15 std::allocator_traits<std::allocator<std::_Rb_tree_node<std::pair<int const, std::shared_ptr<dsn::service_node> > > > >::destroy<std::pair<int const, std::shared_ptr<dsn::service_node> > > (__a=..., __p=<optimized out>)
    at /usr/include/c++/5/bits/alloc_traits.h:542
-16 std::_Rb_tree<int, std::pair<int const, std::shared_ptr<dsn::service_node> >, std::_Select1st<std::pair<int const, std::shared_ptr<dsn::service_node> > >, std::less<int>, std::allocator<std::pair<int const, std::shared_ptr<dsn::service_node> > > >::_M_destroy_node (this=0x816408 <dsn::utils::singleton<dsn::service_engine>::instance()::_instance+808>, __p=0x17f93c0) at /usr/include/c++/5/bits/stl_tree.h:553
-17 std::_Rb_tree<int, std::pair<int const, std::shared_ptr<dsn::service_node> >, std::_Select1st<std::pair<int const, std::shared_ptr<dsn::service_node> > >, std::less<int>, std::allocator<std::pair<int const, std::shared_ptr<dsn::service_node> > > >::_M_drop_node (this=0x816408 <dsn::utils::singleton<dsn::service_engine>::instance()::_instance+808>, __p=0x17f93c0) at /usr/include/c++/5/bits/stl_tree.h:561
-18 std::_Rb_tree<int, std::pair<int const, std::shared_ptr<dsn::service_node> >, std::_Select1st<std::pair<int const, std::shared_ptr<dsn::service_node> > >, std::less<int>, std::allocator<std::pair<int const, std::shared_ptr<dsn::service_node> > > >::_M_erase (
    this=this@entry=0x816408 <dsn::utils::singleton<dsn::service_engine>::instance()::_instance+808>, __x=0x17f93c0) at /usr/include/c++/5/bits/stl_tree.h:1614
-19 0x00000000004ba957 in std::_Rb_tree<int, std::pair<int const, std::shared_ptr<dsn::service_node> >, std::_Select1st<std::pair<int const, std::shared_ptr<dsn::service_node> > >, std::less<int>, std::allocator<std::pair<int const, std::shared_ptr<dsn::service_node> > > >::~_Rb_tree (this=0x816408 <dsn::utils::singleton<dsn::service_engine>::instance()::_instance+808>, __in_chrg=<optimized out>) at /usr/include/c++/5/bits/stl_tree.h:858
-20 std::map<int, std::shared_ptr<dsn::service_node>, std::less<int>, std::allocator<std::pair<int const, std::shared_ptr<dsn::service_node> > > >::~map (this=0x816408 <dsn::utils::singleton<dsn::service_engine>::instance()::_instance+808>, 
    __in_chrg=<optimized out>) at /usr/include/c++/5/bits/stl_map.h:96
-21 dsn::service_engine::~service_engine (this=0x8160e0 <dsn::utils::singleton<dsn::service_engine>::instance()::_instance>, __in_chrg=<optimized out>) at /home/mi/temp/pegasus/rdsn/src/core/core/service_engine.h:121
-22 0x00007f706700cff8 in __run_exit_handlers (status=-1, listp=0x7f70673975f8 <__exit_funcs>, run_list_atexit=run_list_atexit@entry=true) at exit.c:82
-23 0x00007f706700d045 in __GI_exit (status=<optimized out>) at exit.c:104
-24 0x00007f7066ff3837 in __libc_start_main (main=0x45fe72 <main(int, char const**)>, argc=3, argv=0x7fff6c8b5a98, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fff6c8b5a88) at ../csu/libc-start.c:325
-25 0x000000000045c789 in _start ()

client log:
I2019-02-22 12:57:01.483 (1550811421483019254 3439)  mimic.io-thrd.13369: temp.client: start query config, gpid = -1.-1, timeout_ms = 9999
I2019-02-22 12:57:01.483 (1550811421483033448 3439)  mimic.io-thrd.13369: rpc_name = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX, remote_addr = 127.0.0.1:34602, header_format = NET_HDR_DSN, channel = RPC_CHANNEL_TCP, seq_id = 2, trace_id = 8f6aae70d46ec050
D2019-02-22 12:57:01.483 (1550811421483060980 3439)  mimic.io-thrd.13369: client session created, remote_server = 127.0.0.1:34602, current_count = 1
I2019-02-22 12:57:01.483 (1550811421483164247 343b)  mimic.io-thrd.13371: client session 127.0.0.1:34602 connected
I2019-02-22 12:57:01.483 (1550811421483186340 343b)  mimic.io-thrd.13371: boost asio send buffer size is 1313280, set as 16MB, now is 212992
I2019-02-22 12:57:01.483 (1550811421483193136 343b)  mimic.io-thrd.13371: boost asio recv buffer size is 530904, set as 16MB, now is 212992
I2019-02-22 12:57:01.483 (1550811421483197325 343b)  mimic.io-thrd.13371: boost asio set no_delay = true
D2019-02-22 12:57:01.483 (1550811421483202544 343b)  mimic.io-thrd.13371: client session connected, remote_server = 127.0.0.1:34602, current_count = 1
I2019-02-22 12:57:01.483 (1550811421483363470 343d)  mimic.io-thrd.13373: rpc_name = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX, remote_addr = 10.239.35.124:34601, header_format = NET_HDR_DSN, channel = RPC_CHANNEL_TCP, seq_id = 2, trace_id = 8f6aae70d46ec050
D2019-02-22 12:57:01.483 (1550811421483379830 343d)  mimic.io-thrd.13373: client session created, remote_server = 10.239.35.124:34601, current_count = 2
I2019-02-22 12:57:01.483 (1550811421483444047 343c)  mimic.io-thrd.13372: client session 10.239.35.124:34601 connected
I2019-02-22 12:57:01.483 (1550811421483456477 343c)  mimic.io-thrd.13372: boost asio send buffer size is 1313280, set as 16MB, now is 212992
I2019-02-22 12:57:01.483 (1550811421483461921 343c)  mimic.io-thrd.13372: boost asio recv buffer size is 530904, set as 16MB, now is 212992
I2019-02-22 12:57:01.483 (1550811421483465960 343c)  mimic.io-thrd.13372: boost asio set no_delay = true
D2019-02-22 12:57:01.483 (1550811421483469867 343c)  mimic.io-thrd.13372: client session connected, remote_server = 10.239.35.124:34601, current_count = 2
D2019-02-22 12:57:01.483 (1550811421483518515 343e)  mimic.io-thrd.13374: asio read from 10.239.35.124:34601 failed: End of file
D2019-02-22 12:57:01.483 (1550811421483531251 343e)  mimic.io-thrd.13374: client session disconnected, remote_server = 10.239.35.124:34601, current_count = 1
W2019-02-22 12:57:01.483 (1550811421483550167 343e)  mimic.io-thrd.13374: asio socket shutdown failed, error = Transport endpoint is not connected
E2019-02-22 12:57:06.483 (1550811426483692005 3456)  mimic.default2.0100343900030003: temp.client: query config reply, gpid = -1.-1, err = ERR_TIMEOUT
I2019-02-22 12:57:07.484 (1550811427484237893 3454)  mimic.default0.0101000200000001: temp.client: start query config, gpid = -1.-1, timeout_ms = 3998
I2019-02-22 12:57:07.484 (1550811427484275368 3454)  mimic.default0.0101000200000001: rpc_name = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX, remote_addr = 10.239.35.124:34601, header_format = NET_HDR_DSN, channel = RPC_CHANNEL_TCP, seq_id = 4, trace_id = c18b7b02ed7164ef
D2019-02-22 12:57:07.484 (1550811427484292568 3454)  mimic.default0.0101000200000001: client session created, remote_server = 10.239.35.124:34601, current_count = 2
I2019-02-22 12:57:07.484 (1550811427484410818 343c)  mimic.io-thrd.13372: client session 10.239.35.124:34601 connected
I2019-02-22 12:57:07.484 (1550811427484426118 343c)  mimic.io-thrd.13372: boost asio send buffer size is 1313280, set as 16MB, now is 212992
I2019-02-22 12:57:07.484 (1550811427484440641 343c)  mimic.io-thrd.13372: boost asio recv buffer size is 530904, set as 16MB, now is 212992
I2019-02-22 12:57:07.484 (1550811427484445384 343c)  mimic.io-thrd.13372: boost asio set no_delay = true
D2019-02-22 12:57:07.484 (1550811427484450224 343c)  mimic.io-thrd.13372: client session connected, remote_server = 10.239.35.124:34601, current_count = 2
D2019-02-22 12:57:07.484 (1550811427484486857 343c)  mimic.io-thrd.13372: asio read from 10.239.35.124:34601 failed: End of file
D2019-02-22 12:57:07.484 (1550811427484496456 343c)  mimic.io-thrd.13372: client session disconnected, remote_server = 10.239.35.124:34601, current_count = 1
W2019-02-22 12:57:07.484 (1550811427484510121 343c)  mimic.io-thrd.13372: asio socket shutdown failed, error = Transport endpoint is not connected
W2019-02-22 12:57:11.484 (1550811431484775537 343a)  mimic.io-thrd.13370: io_getevents returns -22, you probably want to try on another machine:-(
**F2019-02-22 12:57:11.547 (1550811431547641883 3439)  mimic.io-thrd.13369: assertion expression: _requests[i].size() == 0**
F2019-02-22 12:57:11.547 (1550811431547736586 3439)  mimic.io-thrd.13369: all rpc entries must be removed before the matcher ends","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NjI3NjA2Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/286,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NjI4Mjc2Mg==,incubator-pegasus,466282762,286,NA,mentoswang,40587846,Shanshan Wang,,NA,2019-02-22T05:53:03Z,2019-02-22T05:53:03Z,"The problem happened probably because we setup a timer for query_config in partition_resolver_simple::call according to the client request timeout, however the internal rpc request timeout is not set in partition_resolver_simple::query_config, so it will be 5 sec as default. In some of the cases, the timer will be triggered before the rpc timeout and back. In this case the client will core, I didn't find the exact root cause yet.
```
    // calculate timeout
    int timeout_ms;
    if (nts + 1000 >= request->timeout_ts_us)
        timeout_ms = 1;
    else
        timeout_ms = static_cast<int>(request->timeout_ts_us - nts) / 1000;

    // init timeout timer only when necessary
    {
        zauto_lock l(request->lock);
        if (request->timeout_timer == nullptr) {
            derror(""wss: i am here in call 3, timeout_ms = %d"", timeout_ms);
            request->timeout_timer =
                tasking::enqueue(LPC_REPLICATION_CLIENT_REQUEST_TIMEOUT,
                                 &_tracker,
                                 [ =, req2 = request ]() mutable { on_timeout(std::move(req2)); },
                                 0,
                                 std::chrono::milliseconds(timeout_ms));
        }
    }
```
`auto msg = dsn::message_ex::create_request(RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX);`","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NjI4Mjc2Mg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/286,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NjM3OTY4Nw==,incubator-pegasus,466379687,286,NA,mentoswang,40587846,Shanshan Wang,,NA,2019-02-22T12:25:37Z,2019-02-22T12:25:37Z,"Found the root cause of the 2nd core:
### bad case
data/log/log.3.txt:E2019-02-22 19:55:48.331 (1550836548331735179 738f)  mimic.io-thrd.29583: wss: i am here in partition_resolver_simple::call
data/log/log.3.txt:E2019-02-22 19:55:48.331 (1550836548331759499 738f)  mimic.io-thrd.29583: wss: i am here in partition_resolver_simple::call setup timer, timeout_ms = 9999 **>>> setup timer as per client request timeout**
data/log/log.3.txt:E2019-02-22 19:55:48.331 (1550836548331847643 738f)  mimic.io-thrd.29583: wss: i am here in rpc_client_matcher::on_call, insert rpc code = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX, hdr.id = 2
data/log/log.3.txt:E2019-02-22 19:55:48.332 (1550836548332311378 7394)  mimic.io-thrd.29588: wss: i am here in rpc_client_matcher::on_recv_reply, remove hdr.id = 2
data/log/log.3.txt:E2019-02-22 19:55:48.332 (1550836548332348397 7394)  mimic.io-thrd.29588: wss: i am here in rpc_client_matcher::on_call, insert rpc code = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX, hdr.id = 3
data/log/log.3.txt:E2019-02-22 19:55:53.332 (1550836553332442548 73ac)  mimic.default2.0100739400010001: wss: i am here in rpc_client_matcher::on_rpc_timeout **>>> rpc timeout after 5 sec**
data/log/log.3.txt:E2019-02-22 19:55:53.332 (1550836553332671392 73ac)  mimic.default2.0100739400010001: wss: i am here in rpc_client_matcher::on_rpc_timeout, remove hdr.id = 3
data/log/log.3.txt:E2019-02-22 19:55:53.332 (1550836553332725140 73ac)  mimic.default2.0100738f00030003: wss: i am here in partition_resolver_simple::query_config_reply **>>> callback of rpc call**
data/log/log.3.txt:E2019-02-22 19:55:53.332 (1550836553332753246 73ac)  mimic.default2.0100738f00030003: wss: i am here in partition_resolver_simple::query_config_reply, ERR_TIMEOUT
data/log/log.3.txt:E2019-02-22 19:55:53.332 (1550836553332866460 73ac)  mimic.default2.0100738f00030003: wss: i am here in partition_resolver_simple::handle_pending_requests ERR_OK
data/log/log.3.txt:E2019-02-22 19:55:53.332 (1550836553332899569 73ac)  mimic.default2.0100738f00030003: wss: i am here in partition_resolver_simple::handle_pending_requests partition_index = -1
data/log/log.3.txt:E2019-02-22 19:55:53.332 (1550836553332919252 73ac)  mimic.default2.0100738f00030003: wss: i am here in partition_resolver_simple::handle_pending_requests, ERR_OK, ERR_OBJECT_NOT_FOUND
data/log/log.3.txt:E2019-02-22 19:55:53.332 (1550836553332943926 73ac)  mimic.default2.0100738f00030003: wss: i am here in partition_resolver_simple::call
data/log/log.3.txt:E2019-02-22 19:55:53.332 (1550836553332986783 73ac)  mimic.default2.0100738f00030003: wss: i am here in partition_resolver_simple::call setup retry
data/log/log.3.txt:E2019-02-22 19:55:54.333 (1550836554333122157 73aa)  mimic.default0.0101000200000001: wss: i am here in partition_resolver_simple::call **>>> retry**
data/log/log.3.txt:E2019-02-22 19:55:54.333 (1550836554333331803 73aa)  mimic.default0.0101000200000001: wss: i am here in rpc_client_matcher::on_call, insert rpc code = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX, hdr.id = 4
data/log/log.3.txt:E2019-02-22 19:55:58.330 (1550836558330870275 73ab)  mimic.default1.0100738f00030002: wss: i am here in partition_resolver_simple::on_timeout **>>> timer timeout after 9999ms, back before retry rpc timeout**
data/log/log.3.txt:E2019-02-22 19:55:58.330 (1550836558330935300 73ab)  mimic.default1.0100738f00030002: wss: i am here in partition_resolver_simple::end_request
data/log/log.3.txt:E2019-02-22 19:55:58.330 (1550836558330950440 73ab)  mimic.default1.0100738f00030001: wss: i am here in partition_resolver::call_task callback, gpid = 0, err = ERR_TIMEOUT
data/log/log.3.txt:E2019-02-22 19:55:58.330 (1550836558330958699 73ab)  mimic.default1.0100738f00030001: wss: i am here in partition_resolver::call_task old callback
**data/log/log.3.txt:E2019-02-22 19:55:58.379 (1550836558379533479 738f)  mimic.io-thrd.29583: wss: i am here in rpc_client_matcher::~rpc_client_matcher, hdr.id = 4, timeout_task = LPC_RPC_TIMEOUT, response_task = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX_ACK >>> the problem is the retry rpc is still on the way**
F2019-02-22 19:55:58.379 (1550836558379565885 738f)  mimic.io-thrd.29583: assertion expression: _requests[i].size() == 0
F2019-02-22 19:55:58.379 (1550836558379572153 738f)  mimic.io-thrd.29583: all rpc entries must be removed before the matcher ends

### good case
data/log/log.5.txt:E2019-02-22 19:58:09.528 (1550836689528468694 74f4)  mimic.io-thrd.29940: wss: i am here in partition_resolver_simple::call
data/log/log.5.txt:E2019-02-22 19:58:09.528 (1550836689528482169 74f4)  mimic.io-thrd.29940: wss: i am here in partition_resolver_simple::call setup timer, timeout_ms = 5006
data/log/log.5.txt:E2019-02-22 19:58:09.528 (1550836689528528215 74f4)  mimic.io-thrd.29940: wss: i am here in rpc_client_matcher::on_call, insert rpc code = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX, hdr.id = 2
data/log/log.5.txt:E2019-02-22 19:58:09.528 (1550836689528818469 74f8)  mimic.io-thrd.29944: wss: i am here in rpc_client_matcher::on_recv_reply, remove hdr.id = 2
data/log/log.5.txt:E2019-02-22 19:58:09.528 (1550836689528854286 74f8)  mimic.io-thrd.29944: wss: i am here in rpc_client_matcher::on_call, insert rpc code = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX, hdr.id = 3
data/log/log.5.txt:E2019-02-22 19:58:14.528 (1550836694528882565 7511)  mimic.default2.010074f800010001: wss: i am here in rpc_client_matcher::on_rpc_timeout **>>> rpc timeout after 5 sec**
data/log/log.5.txt:E2019-02-22 19:58:14.528 (1550836694528918756 7511)  mimic.default2.010074f800010001: wss: i am here in rpc_client_matcher::on_rpc_timeout, remove hdr.id = 3
data/log/log.5.txt:E2019-02-22 19:58:14.528 (1550836694528934697 7511)  mimic.default2.010074f400030003: wss: i am here in partition_resolver_simple::query_config_reply **>>> callback of rpc call**
data/log/log.5.txt:E2019-02-22 19:58:14.528 (1550836694528944567 7511)  mimic.default2.010074f400030003: wss: i am here in partition_resolver_simple::query_config_reply, ERR_TIMEOUT
data/log/log.5.txt:E2019-02-22 19:58:14.528 (1550836694528990155 7511)  mimic.default2.010074f400030003: wss: i am here in partition_resolver_simple::handle_pending_requests ERR_OK
data/log/log.5.txt:E2019-02-22 19:58:14.529 (1550836694529003450 7511)  mimic.default2.010074f400030003: wss: i am here in partition_resolver_simple::handle_pending_requests partition_index = -1
data/log/log.5.txt:E2019-02-22 19:58:14.529 (1550836694529013904 7511)  mimic.default2.010074f400030003: wss: i am here in partition_resolver_simple::handle_pending_requests, ERR_OK, ERR_OBJECT_NOT_FOUND
data/log/log.5.txt:E2019-02-22 19:58:14.529 (1550836694529024170 7511)  mimic.default2.010074f400030003: wss: i am here in partition_resolver_simple::call
data/log/log.5.txt:E2019-02-22 19:58:14.529 (1550836694529039427 7511)  mimic.default2.010074f400030003: wss: i am here in partition_resolver_simple::call setup retry
data/log/log.5.txt:E2019-02-22 19:58:14.534 (1550836694534534820 750f)  mimic.default0.010074f400030002: wss: i am here in partition_resolver_simple::on_timeout **>>> timer timeout after 5006ms, back after rpc timeout**
data/log/log.5.txt:E2019-02-22 19:58:14.534 (1550836694534573070 750f)  mimic.default0.010074f400030002: wss: i am here in partition_resolver_simple::end_request
data/log/log.5.txt:E2019-02-22 19:58:14.534 (1550836694534591296 750f)  mimic.default0.010074f400030001: wss: i am here in partition_resolver::call_task callback, gpid = 0, err = ERR_TIMEOUT
data/log/log.5.txt:E2019-02-22 19:58:14.534 (1550836694534599697 750f)  mimic.default0.010074f400030001: wss: i am here in partition_resolver::call_task old callback","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NjM3OTY4Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/286,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NzQwMTM3OA==,incubator-pegasus,467401378,286,NA,mentoswang,40587846,Shanshan Wang,,NA,2019-02-26T11:22:56Z,2019-02-26T11:22:56Z,"Temporary fix the problem by passing the client request timeout into partition_resolver_simple::query_config to set the coresponding timeout for the actual rpc call.

```
        if (-1 != pindex) {
            // put into pending queue of querying target partition
            auto it = _pending_requests.find(pindex);
            if (it == _pending_requests.end()) {
                auto pc = new partition_context();
                it = _pending_requests.emplace(pindex, pc).first;
            }
            it->second->requests.push_back(std::move(request));

            // init configuration query task if necessary
            if (nullptr == it->second->query_config_task) {
                it->second->query_config_task = query_config(pindex, timeout_ms);
            }
        } else {
            _pending_requests_before_partition_count_unknown.push_back(std::move(request));
            if (_pending_requests_before_partition_count_unknown.size() == 1) {
                _query_config_task = query_config(pindex, timeout_ms);
            }
        }
```

```
    task_spec *sp = task_spec::get(RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX);
    if (timeout_ms >= sp->rpc_timeout_milliseconds)
        timeout_ms = 0;
    auto msg = dsn::message_ex::create_request(RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX, timeout_ms);

```
E2019-02-26 18:59:30.886 (1551178770886220913 4ca2)  mimic.io-thrd.19618: wss: i am here in partition_resolver_simple::call
E2019-02-26 18:59:30.886 (1551178770886270790 4ca2)  mimic.io-thrd.19618: wss: i am here in partition_resolver_simple::call setup timer, timeout_ms = 9999
I2019-02-26 18:59:30.886 (1551178770886358214 4ca2)  mimic.io-thrd.19618: temp.client: start query config, gpid = -1.-1, timeout_ms = 9994
I2019-02-26 18:59:30.886 (1551178770886393692 4ca2)  mimic.io-thrd.19618: rpc_name = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX, remote_addr = 127.0.0.1:34603, header_format = NET_HDR_DSN, channel = RPC_CHANNEL_TCP, seq_id = 2, trace_id = 83786c678ddd03ba
E2019-02-26 18:59:30.886 (1551178770886412859 4ca2)  mimic.io-thrd.19618: wss: i am here in rpc_client_matcher::on_call, insert hdr.id = 2, request_code = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX, timeout_task = LPC_RPC_TIMEOUT, response_task = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX_ACK
E2019-02-26 18:59:30.887 (1551178770887449353 4ca6)  mimic.io-thrd.19622: wss: i am here in rpc_client_matcher::on_recv_reply, remove hdr.id = 2, timeout_task = LPC_RPC_TIMEOUT, response_task = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX_ACK
I2019-02-26 18:59:30.887 (1551178770887540004 4ca6)  mimic.io-thrd.19622: rpc_name = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX, remote_addr = 10.239.35.124:34601, header_format = NET_HDR_DSN, channel = RPC_CHANNEL_TCP, seq_id = 2, trace_id = 83786c678ddd03ba
E2019-02-26 18:59:30.887 (1551178770887563375 4ca6)  mimic.io-thrd.19622: wss: i am here in rpc_client_matcher::on_call, insert hdr.id = 3, request_code = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX, timeout_task = LPC_RPC_TIMEOUT, response_task = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX_ACK
E2019-02-26 18:59:35.887 (1551178775887753170 4cc0)  mimic.default2.01004ca600010001: wss: i am here in rpc_client_matcher::on_rpc_timeout1, remove hdr.id = 3, timeout_task = LPC_RPC_TIMEOUT, response_task = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX_ACK
E2019-02-26 18:59:35.887 (1551178775887890489 4cc0)  mimic.default2.01004ca200030003: wss: i am here in partition_resolver_simple::query_config_reply
E2019-02-26 18:59:35.887 (1551178775887920602 4cc0)  mimic.default2.01004ca200030003: temp.client: query config reply, gpid = -1.-1, err = ERR_TIMEOUT
E2019-02-26 18:59:35.887 (1551178775887988750 4cc0)  mimic.default2.01004ca200030003: wss: i am here in partition_resolver_simple::handle_pending_requests ERR_OK
E2019-02-26 18:59:35.888 (1551178775888019130 4cc0)  mimic.default2.01004ca200030003: wss: i am here in partition_resolver_simple::handle_pending_requests, ERR_OK, ERR_OBJECT_NOT_FOUND
E2019-02-26 18:59:35.888 (1551178775888042311 4cc0)  mimic.default2.01004ca200030003: wss: i am here in partition_resolver_simple::call
E2019-02-26 18:59:35.888 (1551178775888063263 4cc0)  mimic.default2.01004ca200030003: wss: i am here in partition_resolver_simple::call setup retry
E2019-02-26 18:59:36.888 (1551178776888341178 4cbe)  mimic.default0.0101000200000001: wss: i am here in partition_resolver_simple::call
I2019-02-26 18:59:36.888 (1551178776888433571 4cbe)  mimic.default0.0101000200000001: temp.client: start query config, gpid = -1.-1, timeout_ms = 3992
I2019-02-26 18:59:36.888 (1551178776888525895 4cbe)  mimic.default0.0101000200000001: rpc_name = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX, remote_addr = 10.239.35.124:34601, header_format = NET_HDR_DSN, channel = RPC_CHANNEL_TCP, seq_id = 4, trace_id = 12b04fc20186042d
E2019-02-26 18:59:36.888 (1551178776888548869 4cbe)  mimic.default0.0101000200000001: wss: i am here in rpc_client_matcher::on_call, insert hdr.id = 4, request_code = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX, timeout_task = LPC_RPC_TIMEOUT, response_task = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX_ACK
E2019-02-26 18:59:40.880 (1551178780880812576 4cbf)  mimic.default1.0101000000000002: wss: i am here in rpc_client_matcher::on_rpc_timeout1, remove hdr.id = 4, timeout_task = LPC_RPC_TIMEOUT, response_task = RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX_ACK
E2019-02-26 18:59:40.880 (1551178780880941179 4cbf)  mimic.default1.0101000000000001: wss: i am here in partition_resolver_simple::query_config_reply
E2019-02-26 18:59:40.880 (1551178780880968663 4cbf)  mimic.default1.0101000000000001: temp.client: query config reply, gpid = -1.-1, err = ERR_TIMEOUT
E2019-02-26 18:59:40.881 (1551178780881054311 4cbf)  mimic.default1.0101000000000001: wss: i am here in partition_resolver_simple::handle_pending_requests ERR_OK
E2019-02-26 18:59:40.881 (1551178780881086681 4cbf)  mimic.default1.0101000000000001: wss: i am here in partition_resolver_simple::handle_pending_requests, ERR_OK, ERR_OBJECT_NOT_FOUND
E2019-02-26 18:59:40.881 (1551178780881112056 4cbf)  mimic.default1.0101000000000001: wss: i am here in partition_resolver_simple::call
E2019-02-26 18:59:40.881 (1551178780881135929 4cbf)  mimic.default1.0101000000000001: wss: i am here in partition_resolver_simple::call setup retry
E2019-02-26 18:59:40.885 (1551178780885621160 4cc0)  mimic.default2.01004ca200030002: wss: i am here in partition_resolver_simple::on_timeout
E2019-02-26 18:59:40.885 (1551178780885710194 4cc0)  mimic.default2.01004ca200030002: wss: i am here in partition_resolver_simple::end_request
E2019-02-26 18:59:40.885 (1551178780885751662 4cc0)  mimic.default2.01004ca200030001: wss: i am here in partition_resolver::call_task callback, gpid = 0, err = ERR_TIMEOUT
W2019-02-26 18:59:40.888 (1551178780888077128 4ca3)  mimic.io-thrd.19619: io_getevents returns -22, you probably want to try on another machine:-(","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2NzQwMTM3OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/286,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2ODEwMTc2MA==,incubator-pegasus,468101760,286,NA,mentoswang,40587846,Shanshan Wang,,NA,2019-02-28T01:40:33Z,2019-02-28T01:40:33Z,"Found the root cause of core1, it's a different problem, need to reconsider the solution to fix the two problems.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2ODEwMTc2MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/286,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2ODE4MzgyNQ==,incubator-pegasus,468183825,286,NA,mentoswang,40587846,Shanshan Wang,,NA,2019-02-28T08:32:29Z,2019-02-28T08:32:29Z,"The second problem is because end_request is called as timeout will happen within 100 us, and then the timer task is trying to enqueue shorlty. Considering to not end up the request in this case, but wait for the timer to clean up instead, but still need to make sure rpc in query_config is back before the timer.

```
// timeout will happen very soon, no way to get the rpc call done
if (nts + 100 >= request->timeout_ts_us) // within 100 us
{
    end_request(std::move(request), ERR_TIMEOUT, rpc_address());
    return;
}

// delay 1 second for further config query
if (from_meta_ack) {
    tasking::enqueue(LPC_REPLICATION_DELAY_QUERY_CONFIG,
                     &_tracker,
                     [ =, req2 = request ]() mutable { call(std::move(req2), false); },
                     0,
                     std::chrono::seconds(1));
    return;
}

// calculate timeout
int timeout_ms;
if (nts + 1000 >= request->timeout_ts_us)
    timeout_ms = 1;
else
    timeout_ms = static_cast<int>(request->timeout_ts_us - nts) / 1000;

// init timeout timer only when necessary
{
    zauto_lock l(request->lock);
    if (request->timeout_timer == nullptr) {
        request->timeout_timer =
            tasking::enqueue(LPC_REPLICATION_CLIENT_REQUEST_TIMEOUT,
                             &_tracker,
                             [ =, req2 = request ]() mutable { on_timeout(std::move(req2)); },
                             0,
                             std::chrono::milliseconds(timeout_ms));
    }
}
```

E2019-02-28 12:07:52.541 (1551326872541616898 455c)  mimic.default1.0101000000000001: temp.client: query config reply, gpid = -1.-1, err = ERR_TIMEOUT
E2019-02-28 12:07:52.541 (1551326872541685356 455c)  mimic.default1.0101000000000001: wss: i am here in partition_resolver_simple::handle_pending_requests ERR_OK
E2019-02-28 12:07:52.541 (1551326872541715626 455c)  mimic.default1.0101000000000001: wss: i am here in partition_resolver_simple::handle_pending_requests, ERR_OK, ERR_OBJECT_NOT_FOUND
E2019-02-28 12:07:52.541 (1551326872541738976 455c)  mimic.default1.0101000000000001: wss: i am here in partition_resolver_simple::call
E2019-02-28 12:07:52.541 (1551326872541763269 455c)  mimic.default1.0101000000000001: wss: i am here in partition_resolver_simple::call timeout immediately
E2019-02-28 12:07:52.541 (1551326872541786034 455c)  mimic.default1.0101000000000001: wss: i am here in partition_resolver_simple::end_request
E2019-02-28 12:07:52.541 (1551326872541811523 455c)  mimic.default1.0101000000000001: wss: i am here in partition_resolver_simple::end_request, timer LPC_REPLICATION_CLIENT_REQUEST_TIMEOUT was canceled
E2019-02-28 12:07:52.541 (1551326872541855416 455c)  mimic.default1.0100454000030001: wss: i am here in partition_resolver::call_task callback, gpid = 0, err = ERR_TIMEOUT
E2019-02-28 12:07:52.541 (1551326872541911690 455a)  mimic.io-thrd.17754: wss: i am here in simple_timer_service::add_timer, enqueue task = LPC_REPLICATION_CLIENT_REQUEST_TIMEOUT","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2ODE4MzgyNQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/287,https://api.github.com/repos/apache/incubator-pegasus/issues/287,incubator-pegasus,413398634,287,Better log failure handling instead of assert false ,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2019-02-22T13:14:12Z,2022-07-21T06:29:38Z,"
<h1>触发环境</h1>

- 机器：2台meta server，5台replica server
- server version:  hycdong的split branch [https://github.com/hycdong/pegasus/tree/split](https://github.com/hycdong/pegasus/tree/split)
- QPS：2w-get，3w-set，同时随机kill test
- 表：大小1T, partition_count为128，从fds中恢复c3tst-sample-krb集群的usertable表
- 其他条件： split 1次后，在split过程中或者split完成后均可能触发

<h1>coredump</h1>

```
(gdb) bt
#0 0x0000003f852328a5 in raise () from /lib64/libc.so.6
#1 0x0000003f85234085 in abort () from /lib64/libc.so.6
#2 0x00007f93ada6125e in dsn_coredump () at /home/heyuchen/split/pegasus/rdsn/src/core/core/service_api_c.cpp:73
#3 0x00007f93ad93caee in dsn::replication::replica_stub::handle_log_failure (this=<optimized out>, err=...) at /home/heyuchen/split/pegasus/rdsn/src/dist/replication/lib/replica_stub.cpp:1962
#4 0x00007f93ad98eef5 in dsn::replication::replica::on_append_log_completed (this=0x7f920d1eac60, mu=..., err=..., size=<optimized out>)
at /home/heyuchen/split/pegasus/rdsn/src/dist/replication/lib/replica_2pc.cpp:526
#5 0x00007f93ada5f5b8 in operator() (__args#1=<optimized out>, __args#0=..., this=<optimized out>) at /home/heyuchen/toolchain/output/include/c++/4.8.2/functional:2464
#6 dsn::aio_task::exec (this=<optimized out>) at /home/heyuchen/split/pegasus/rdsn/include/dsn/tool-api/task.h:597
#7 0x00007f93ada5d1f9 in dsn::task::exec_internal (this=this@entry=0x7f8cb6f11a88) at /home/heyuchen/split/pegasus/rdsn/src/core/core/task.cpp:180
#8 0x00007f93adab1d9d in dsn::task_worker::loop (this=0x2305f00) at /home/heyuchen/split/pegasus/rdsn/src/core/core/task_worker.cpp:211
#9 0x00007f93adab1f69 in dsn::task_worker::run_internal (this=0x2305f00) at /home/heyuchen/split/pegasus/rdsn/src/core/core/task_worker.cpp:191
#10 0x00007f93ab431600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>) at /home/heyuchen/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#11 0x0000003f85607851 in start_thread () from /lib64/libpthread.so.0
#12 0x0000003f852e811d in clone () from /lib64/libc.so.6
(gdb)
```

<h1>相关日志</h1>

```
E2019-02-20 18:30:54.767 (1550658654767145919 3211) replica.replica7.04050007005c7e26: native_aio_provider.linux.cpp:218:aio_internal(): io_submit error, ret = -11
E2019-02-20 18:30:54.767 (1550658654767175695 31ec) replica.default2.040100010017b3a7: mutation_log.cpp:193:operator()(): write shared log failed, err = ERR_FILE_OPERATION_FAILED
E2019-02-20 18:30:54.767 (1550658654767210218 31ef) replica.default5.04050001008ef578: mutation_log.cpp:457:operator()(): write private log failed, err = ERR_FILE_OPERATION_FAILED
E2019-02-20 18:30:54.767 (1550658654767285730 31ee) replica.default4.04050014007090e7: mutation_log.cpp:457:operator()(): write private log failed, err = ERR_FILE_OPERATION_FAILED
E2019-02-20 18:30:54.767 (1550658654767310415 31eb) replica.default1.040500170074149b: mutation_log.cpp:457:operator()(): write private log failed, err = ERR_FILE_OPERATION_FAILED
E2019-02-20 18:30:54.767 (1550658654767357562 31f0) replica.default6.040500150068bf23: mutation_log.cpp:457:operator()(): write private log failed, err = ERR_FILE_OPERATION_FAILED
E2019-02-20 18:30:54.767 (1550658654767400994 321e) replica.replica20.0405001400709062: native_aio_provider.linux.cpp:218:aio_internal(): io_submit error, ret = -11
```

- 从日志中看将问题定位到：native_aio_provider.linux.cpp的aio_internal的函数调用io_submit系统调用时返回了-11（EAGAIN），因系统资源不足导致。
- 关于io_submit相关可参见对linux io_submit的说明： [http://man7.org/linux/man-pages/man2/io_submit.2.html](http://man7.org/linux/man-pages/man2/io_submit.2.html)
- 我们目前的代码并不能处理这种io错误，会返回一个`ERR_FILE_OPERATION_FAILED`的error code，由于是写shared log出错，因此这个错误会返回到replica_stub上，最终直接assert false，产生coredump

<h1>解决方案思考</h1>

- 调研其他数据库或存储系统如何处理系统io错误，完善错误处理机制
- 添加对io速度，io busy相关的监控项，以便提前发现问题

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/287/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/287,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5G_qMk,incubator-pegasus,1191093028,287,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-07-21T06:29:38Z,2022-07-21T06:29:38Z,https://github.com/XiaoMi/rdsn/pull/818,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5G_qMk/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/293,https://api.github.com/repos/apache/incubator-pegasus/issues/293,incubator-pegasus,414913022,293,refactor shell to reduce duplicated code,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2019-02-27T02:20:44Z,2020-01-19T08:15:04Z,"like:
* option parsing of copy_data/clear_data/count_data","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/293/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/295,https://api.github.com/repos/apache/incubator-pegasus/issues/295,incubator-pegasus,415474047,295,rdsn core.aio test failed when using config-test-posix-aio.ini,vagetablechicken,24697960,HuangWei,huangwei@apache.org,OPEN,2019-02-28T06:52:59Z,2019-02-28T06:52:59Z,"在build机上编译时遇到使用posix-aio作为aio_provider时测试失败的问题。
运行：
`./dsn.core.tests --gtest_filter=core.aio config-test-posix-aio.ini`
错误日志：
`E2019-02-28 11:49:39.890 (1551325779890758008 2d09) client.io-thrd.11529: native_aio_provider.posix.cpp:187:aio_internal(): file op failed, err = 22 (Invalid argument). On FreeBSD, you may need to load aio kernel module by running 'sudo kldload aio'.
`

测试了c3build01,c3build03,c4build04三台，均为gcc4.8.2，c3build01和03的glibc2.12，c4build04 glibc2.17。

c3build01,c3build03都遇到这个问题，c4build04测试成功。
问题在于aio_read/aio_write调用时期。
c3build03能跑通测试aio的简单实例，在rdsn框架下将TEST(core,aio)替换为aio简单实例，也会得到Invalid argument错误。strace系统调用，aio简单实例可以trace到`open(""tmp"", O_RDWR|O_CREAT, 0666)       = 3
`这样的信息，rdsn的dsn.core.tests却不能trace到相关信息，没有办法进行下一步调试。

c3build03编译得到的dsn.core.tests拷贝到c4build04上却可以测试通过。

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/295/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/299,https://api.github.com/repos/apache/incubator-pegasus/issues/299,incubator-pegasus,417643323,299,test failed with assert,clicx,13973702,,,CLOSED,2019-03-06T06:26:30Z,2019-06-09T14:51:02Z,"./run.sh test
============
start global_env()
meta1 pid: 32764
pegasus_function_test: /root/pegasus/src/test/function_test/global_env.cpp:39: void global_env::get_dirs(): Assertion `dsn::utils::pipe_execute(cmd2, ss2) == 0' failed.
./run.sh: line 18: 94425 Aborted                 GTEST_OUTPUT=""xml:$REPORT_DIR/basic.xml"" GTEST_FILTER=""basic.*"" ./$test_case $config_file $table_name
run test basic failed: pegasus_function_test config.ini temp
run test ""pegasus_function_test"" in /root/pegasus/src/builder/bin/pegasus_function_test failed

I'm interesting in playing with it, do you have any guide to learn and toubleshot it?
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/299/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/299,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2OTk4Nzk2OA==,incubator-pegasus,469987968,299,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-03-06T06:38:54Z,2019-03-06T06:38:54Z,"Can you exec ""readline /proc/xxx/cwd"" (where xxx is any process id) in shell ?

the related code is at https://github.com/XiaoMi/pegasus/blob/master/src/test/function_test/global_env.cpp#L35:
```
    // get the dir of a process in onebox, say: $PEGASUS/onebox/meta1
    char cmd2[512];
    sprintf(cmd2, ""readlink /proc/%d/cwd"", meta1_pid);
    std::stringstream ss2;
    assert(dsn::utils::pipe_execute(cmd2, ss2) == 0);
    std::string meta1_dir;
    ss2 >> meta1_dir;
    std::cout << ""meta1 dir: "" << meta1_dir << std::endl;
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ2OTk4Nzk2OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/299,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMDIxNzg4OA==,incubator-pegasus,500217888,299,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-06-09T14:50:34Z,2019-06-09T14:50:34Z,"This issue has been automatically marked as stale because it's inactive for a long time. It will be closed if no further activity occurs, reopen if you have further ideas. Thank you for your contributions!","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMDIxNzg4OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/304,https://api.github.com/repos/apache/incubator-pegasus/issues/304,incubator-pegasus,419831870,304,replication data consistency verify tool,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,OPEN,2019-03-12T07:06:05Z,2020-08-18T14:45:54Z,"功能：对建立了replication关系的两个表进行数据验证，保证数据的一致性

使用场景：对于建立了replication关系的两个表，可以定期（譬如一周）在低峰时段进行数据一致性验证

注意点：
* 要考虑异步复制的延迟，譬如可以考虑获取数据的timestamp，对最近1小时写入的数据不进行比较；
* 可以考虑分布式并发执行，每个并发只比较一个partition，提升速度；","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/304/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/307,https://api.github.com/repos/apache/incubator-pegasus/issues/307,incubator-pegasus,421785380,307,pegasus coredump at boost asio,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2019-03-16T08:41:54Z,2024-09-11T03:54:28Z,"## Server Version

Pegasus Server 1.11.3 (b45cb06ec27f89eec6c430cecb9deed6f61675a6) Release
Occurred in XiaoMi's c3srv-browser cluster.

## Coredump Stack

```
(gdb) bt
#0  0x00007f00795f3b56 in boost::asio::detail::epoll_reactor::start_op (this=0x35b6210, op_type=op_type@entry=1, descriptor=251, descriptor_data=@0x220cd2a8: 0x0, op=op@entry=0x156ead900, 
    is_continuation=is_continuation@entry=false, allow_speculative=allow_speculative@entry=true)
    at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/impl/epoll_reactor.ipp:219
#1  0x00007f007960d755 in start_op (noop=false, is_non_blocking=true, is_continuation=false, op=0x156ead900, op_type=1, impl=..., this=0x358f078)
    at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/impl/reactive_socket_service_base.ipp:214
#2  async_send<boost::asio::detail::consuming_buffers<boost::asio::const_buffer, std::vector<boost::asio::const_buffer> >, boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::write(uint64_t)::__lambda3> > (flags=0, handler=..., buffers=..., impl=..., 
    this=0x358f078) at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/reactive_socket_service_base.hpp:216
#3  async_send<boost::asio::detail::consuming_buffers<boost::asio::const_buffer, std::vector<boost::asio::const_buffer> >, boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::write(uint64_t)::__lambda3> > (flags=0, 
    handler=<unknown type in /home/work/app/pegasus/c3srv-feedprofile/replica/package/bin/libdsn_replica_server.so, CU 0x3770fbb, DIE 0x37e9f76>, buffers=..., impl=..., this=0x358f050)
    at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/stream_socket_service.hpp:330
#4  async_write_some<boost::asio::detail::consuming_buffers<boost::asio::const_buffer, std::vector<boost::asio::const_buffer> >, boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::write(uint64_t)::__lambda3> > (
    handler=<unknown type in /home/work/app/pegasus/c3srv-feedprofile/replica/package/bin/libdsn_replica_server.so, CU 0x3770fbb, DIE 0x37e9118>, buffers=..., this=0x220cd2a0)
    at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/basic_stream_socket.hpp:732
#5  boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp, boost::asio::stream_socket_service<boost::asio::ip::tcp> >, std::vector<boost::asio::const_buffer, std::allocator<boost::asio::const_buffer> >, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::write(uint64_t)::__lambda3>::operator()(const boost::system::error_code &, std::size_t, int)
    (this=0x7f006213edd0, ec=..., bytes_transferred=<optimized out>, start=<optimized out>) at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/impl/write.hpp:181
#6  0x00007f007960dd4c in async_write<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, dsn::tools::asio_rpc_session::write(uint64_t)::__lambda3> (
    handler=<optimized out>, buffers=..., s=...) at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/impl/write.hpp:621
#7  dsn::tools::asio_rpc_session::write (this=0x1d2833b00, signature=2) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/tools/common/asio_rpc_session.cpp:162
#8  0x00007f00795e0f2c in dsn::rpc_session::on_send_completed (this=0x1d2833b00, signature=<optimized out>) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/network.cpp:326
#9  0x00007f007960d9a6 in operator() (length=<optimized out>, __closure=0x7f006213f088, ec=...) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/tools/common/asio_rpc_session.cpp:158
#10 boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp, boost::asio::stream_socket_service<boost::asio::ip::tcp> >, std::vector<boost::asio::const_buffer, std::allocator<boost::asio::const_buffer> >, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::write(uint64_t)::__lambda3>::operator()(const boost::system::error_code &, std::size_t, int)
    (this=0x7f006213f030, ec=..., bytes_transferred=<optimized out>, start=<optimized out>) at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/impl/write.hpp:192
#11 0x00007f007960e06d in operator() (this=0x7f006213f030) at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/bind_handler.hpp:127
#12 asio_handler_invoke<boost::asio::detail::binder2<boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::write(uint64_t)::__lambda3>, boost::system::error_code, long unsigned int> > (function=...)
    at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/handler_invoke_hook.hpp:69
#13 invoke<boost::asio::detail::binder2<boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::write(uint64_t)::__lambda3>, boost::system::error_code, long unsigned int>, dsn::tools::asio_rpc_session::write(uint64_t)::__lambda3> (context=..., 
    function=...) at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/handler_invoke_helpers.hpp:37
#14 asio_handler_invoke<boost::asio::detail::binder2<boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::write(uint64_t)::__lambda3>, boost::system::error_code, long unsigned int>, boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::write(uint64_t)::__lambda3> (this_handler=<optimized out>, function=...)
    at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/impl/write.hpp:565
#15 invoke<boost::asio::detail::binder2<boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::write(uint64_t)::__lambda3>, boost::system::error_code, long unsigned int>, boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::write(uint64_t)::__lambda3> > (context=..., function=...)
    at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/handler_invoke_helpers.hpp:37
#16 boost::asio::detail::reactive_socket_send_op<boost::asio::detail::consuming_buffers<boost::asio::const_buffer, std::vector<boost::asio::const_buffer, std::allocator<boost::asio::const_buffer> > >, boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp, boost::asio::stream_socket_service<boost::asio::ip::tcp> >, std::vector<boost::asio::const_buffer, std::allocator<boost::asio::const_buffer> >, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::write(uint64_t)::__lambda3> >::do_complete(boost::asio::detail::io_service_impl *, boost::asio::detail::operation *, const boost::system::error_code &, std::size_t) (owner=0x35b27e0, base=<optimized out>)
    at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/reactive_socket_send_op.hpp:107
#17 0x0000000000620659 in complete (bytes_transferred=<optimized out>, ec=..., owner=..., this=<optimized out>)
    at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/task_io_service_operation.hpp:38
#18 do_run_one (ec=..., this_thread=..., lock=..., this=0x35b27e0) at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/impl/task_io_service.ipp:372
#19 boost::asio::detail::task_io_service::run (this=0x35b27e0, ec=...) at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/impl/task_io_service.ipp:149
#20 0x00007f00795eed46 in run (this=<optimized out>, ec=...) at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/impl/io_service.ipp:66
#21 operator() (__closure=0x358d6b0) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/tools/common/asio_net_provider.cpp:73
#22 _M_invoke<> (this=0x358d6b0) at /home/work/qinzuoyan/Pegasus/toolchain/output/include/c++/4.8.2/functional:1732
#23 operator() (this=0x358d6b0) at /home/work/qinzuoyan/Pegasus/toolchain/output/include/c++/4.8.2/functional:1720
#24 std::thread::_Impl<std::_Bind_simple<dsn::tools::asio_network_provider::start(dsn::rpc_channel, int, bool)::__lambda2()> >::_M_run(void) (this=0x358d698)
    at /home/work/qinzuoyan/Pegasus/toolchain/output/include/c++/4.8.2/thread:115
#25 0x00007f00763a4600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#26 0x00007f0077011dc5 in start_thread () from /lib64/libpthread.so.0
#27 0x00007f0075b0e73d in clone () from /lib64/libc.so.6
(gdb) 
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/307/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/307,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMjU3NjE4MA==,incubator-pegasus,502576180,307,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-06-17T07:58:49Z,2019-06-17T07:58:49Z,"## Server Version
Pegasus Server 1.11.3 (b45cb06) Release
Occurred in XiaoMi's c3srv-account cluster (2019/6/17).

## Coredump Stack

```
(gdb) bt
#0  0x00007fcd0217fb56 in boost::asio::detail::epoll_reactor::start_op (this=0x383a6e0, op_type=op_type@entry=0, descriptor=5656, descriptor_data=@0xd1c75708: 0x0, op=op@entry=0x6b7061a00, 
    is_continuation=is_continuation@entry=false, allow_speculative=allow_speculative@entry=true) at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/impl/epoll_reactor.ipp:219
#1  0x00007fcd0219732c in start_op (noop=false, is_non_blocking=true, is_continuation=false, op=0x6b7061a00, op_type=0, impl=..., this=0x3834ef8)
    at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/impl/reactive_socket_service_base.ipp:214
#2  async_receive<boost::asio::mutable_buffers_1, dsn::tools::asio_rpc_session::do_read(int)::__lambda2> (flags=0, handler=..., buffers=..., impl=..., this=0x3834ef8)
    at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/reactive_socket_service_base.hpp:287
#3  async_receive<boost::asio::mutable_buffers_1, dsn::tools::asio_rpc_session::do_read(int)::__lambda2> (handler=<optimized out>, flags=0, buffers=..., impl=..., this=0x3834ed0)
    at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/stream_socket_service.hpp:357
#4  async_read_some<boost::asio::mutable_buffers_1, dsn::tools::asio_rpc_session::do_read(int)::__lambda2> (handler=<optimized out>, buffers=..., this=0xd1c75700)
    at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/basic_stream_socket.hpp:845
#5  dsn::tools::asio_rpc_session::do_read (this=<optimized out>, read_next=<optimized out>) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/tools/common/asio_rpc_session.cpp:136
#6  0x00007fcd0216b501 in dsn::rpc_session::start_read_next (this=0x380c00480, read_next=<optimized out>) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/network.cpp:216
#7  0x00007fcd021978e8 in operator() (length=<optimized out>, __closure=0x7fcceaccb090, ec=...) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/tools/common/asio_rpc_session.cpp:131
#8  operator() (this=0x7fcceaccb090) at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/bind_handler.hpp:127
#9  asio_handler_invoke<boost::asio::detail::binder2<dsn::tools::asio_rpc_session::do_read(int)::__lambda2, boost::system::error_code, long unsigned int> > (function=...)
    at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/handler_invoke_hook.hpp:69
#10 invoke<boost::asio::detail::binder2<dsn::tools::asio_rpc_session::do_read(int)::__lambda2, boost::system::error_code, long unsigned int>, dsn::tools::asio_rpc_session::do_read(int)::__lambda2> (
    context=..., function=...) at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/handler_invoke_helpers.hpp:37
#11 boost::asio::detail::reactive_socket_recv_op<boost::asio::mutable_buffers_1, dsn::tools::asio_rpc_session::do_read(int)::__lambda2>::do_complete(boost::asio::detail::io_service_impl *, boost::asio::detail::operation *, const boost::system::error_code &, std::size_t) (owner=<optimized out>, base=<optimized out>)
    at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/reactive_socket_recv_op.hpp:110
#12 0x0000000000620659 in complete (bytes_transferred=<optimized out>, ec=..., owner=..., this=<optimized out>)
    at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/task_io_service_operation.hpp:38
#13 do_run_one (ec=..., this_thread=..., lock=..., this=0x38567e0) at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/impl/task_io_service.ipp:372
#14 boost::asio::detail::task_io_service::run (this=0x38567e0, ec=...) at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/detail/impl/task_io_service.ipp:149
#15 0x00007fcd0217ad46 in run (this=<optimized out>, ec=...) at /home/work/qinzuoyan/software/boost_1_58_0/output/include/boost/asio/impl/io_service.ipp:66
#16 operator() (__closure=0x3831730) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/tools/common/asio_net_provider.cpp:73
#17 _M_invoke<> (this=0x3831730) at /home/work/qinzuoyan/Pegasus/toolchain/output/include/c++/4.8.2/functional:1732
#18 operator() (this=0x3831730) at /home/work/qinzuoyan/Pegasus/toolchain/output/include/c++/4.8.2/functional:1720
#19 std::thread::_Impl<std::_Bind_simple<dsn::tools::asio_network_provider::start(dsn::rpc_channel, int, bool)::__lambda2()> >::_M_run(void) (this=0x3831718)
    at /home/work/qinzuoyan/Pegasus/toolchain/output/include/c++/4.8.2/thread:115
#20 0x00007fccfef30600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#21 0x00007fccffb9ddc5 in start_thread () from /lib64/libpthread.so.0
#22 0x00007fccfe69a73d in clone () from /lib64/libc.so.6
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMjU3NjE4MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/307,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMjU4NTAyOQ==,incubator-pegasus,502585029,307,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-06-17T08:26:26Z,2019-06-17T08:26:26Z,"https://www.cnblogs.com/ruizhang3/p/6418269.html
Solution: protect _socket in [asio_rpc_session.h](https://github.com/XiaoMi/rdsn/blob/master/src/core/tools/common/asio_rpc_session.h) by mutex","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMjU4NTAyOQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/307,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMjU5OTk3MQ==,incubator-pegasus,502599971,307,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-06-17T09:08:03Z,2019-06-17T09:08:03Z,https://stackoverflow.com/questions/7362894/boostasiosocket-thread-safety,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMjU5OTk3MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/307,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6KUb9U,incubator-pegasus,2320613204,307,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-08-30T09:18:20Z,2024-08-30T09:18:20Z,"This issue appears on 2.4.x (https://github.com/XiaoMi/rdsn/pull/1016 included), with the backtrace like:
```
(gdb) bt
#0  boost::asio::detail::epoll_reactor::start_op (this=0x272f520, op_type=1, descriptor=39541, descriptor_data=@0x15f8f7580: 0x0, op=0x4d426b6c0, is_continuation=<optimized out>, allow_speculative=true)
    at /home/laiyingchun/dev/skv_240/thirdparty/output/include/boost/asio/detail/impl/epoll_reactor.ipp:245
#1  0x00007ffa59d29a76 in boost::asio::detail::reactive_socket_service_base::start_op (noop=false, is_non_blocking=true, is_continuation=<optimized out>, op=0x4d426b6c0, op_type=1, impl=..., this=0x2ce82a8)
    at /home/laiyingchun/dev/skv_240/thirdparty/output/include/boost/asio/detail/impl/reactive_socket_service_base.ipp:246
#2  boost::asio::detail::reactive_socket_service_base::async_send<boost::asio::detail::prepared_buffers<boost::asio::const_buffer, 64>, boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, __gnu_cxx::__normal_iterator<const boost::asio::const_buffer*, std::vector<boost::asio::const_buffer> >, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::send(uint64_t)::<lambda(boost::system::error_code, std::size_t)> > > (flags=0, handler=..., buffers=...,
    impl=..., this=0x2ce82a8) at /home/laiyingchun/dev/skv_240/thirdparty/output/include/boost/asio/detail/reactive_socket_service_base.hpp:278
#3  boost::asio::basic_stream_socket<boost::asio::ip::tcp>::async_write_some<boost::asio::detail::prepared_buffers<boost::asio::const_buffer, 64>, boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, __gnu_cxx::__normal_iterator<const boost::asio::const_buffer*, std::vector<boost::asio::const_buffer> >, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::send(uint64_t)::<lambda(boost::system::error_code, std::size_t)> > > (handler=..., buffers=...,
    this=0x15f8f7570) at /home/laiyingchun/dev/skv_240/thirdparty/output/include/boost/asio/basic_stream_socket.hpp:787
#4  boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer, std::allocator<boost::asio::const_buffer> >, __gnu_cxx::__normal_iterator<const boost::asio::const_buffer*, std::vector<boost::asio::const_buffer, std::allocator<boost::asio::const_buffer> > >, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::send(uint64_t)::<lambda(boost::system::error_code, std::size_t)> >::operator() (start=1, bytes_transferred=0, ec=..., this=0x7ffa36a58220)
    at /home/laiyingchun/dev/skv_240/thirdparty/output/include/boost/asio/impl/write.hpp:259
#5  boost::asio::detail::start_write_buffer_sequence_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, __gnu_cxx::__normal_iterator<const boost::asio::const_buffer*, std::vector<boost::asio::const_buffer> >, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::send(uint64_t)::<lambda(boost::system::error_code, std::size_t)> > (handler=..., buffers=<synthetic pointer>..., stream=..., completion_condition=...)
    at /home/laiyingchun/dev/skv_240/thirdparty/output/include/boost/asio/impl/write.hpp:345
#6  boost::asio::async_write<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, dsn::tools::asio_rpc_session::send(uint64_t)::<lambda(boost::system::error_code, std::size_t)> > (handler=..., buffers=<synthetic pointer>...,
    s=...) at /home/laiyingchun/dev/skv_240/thirdparty/output/include/boost/asio/impl/write.hpp:435
#7  dsn::tools::asio_rpc_session::send (this=<optimized out>, signature=<optimized out>) at /home/laiyingchun/dev/skv_240/src/rdsn/src/runtime/rpc/asio_rpc_session.cpp:142
#8  0x00007ffa59ca8e42 in dsn::rpc_session::send_message (this=this@entry=0x951f8dc0, msg=msg@entry=0x4c6fbcd10) at /home/laiyingchun/dev/skv_240/src/rdsn/src/runtime/rpc/network.cpp:281
#9  0x00007ffa59cafa52 in dsn::rpc_engine::reply (this=0x2cb6000, response=response@entry=0x4c6fbcd10, err=...) at /home/laiyingchun/dev/skv_240/src/rdsn/src/runtime/rpc/rpc_engine.cpp:761
#10 0x00007ffa59c75e2d in dsn_rpc_reply (response=response@entry=0x4c6fbcd10, err=...) at /home/laiyingchun/dev/skv_240/src/rdsn/src/runtime/service_api_c.cpp:164
#11 0x00007ffa59c9da49 in dsn::rpc_holder<dsn::security::negotiation_request, dsn::security::negotiation_response>::internal::reply (this=this@entry=0x58ee78da0) at /home/laiyingchun/dev/skv_240/src/rdsn/include/dsn/cpp/rpc_holder.h:304
#12 0x00007ffa59c9e169 in dsn::rpc_holder<dsn::security::negotiation_request, dsn::security::negotiation_response>::internal::~internal (this=0x58ee78da0, __in_chrg=<optimized out>) at /home/laiyingchun/dev/skv_240/src/rdsn/include/dsn/cpp/rpc_holder.h:310
#13 __gnu_cxx::new_allocator<dsn::rpc_holder<dsn::security::negotiation_request, dsn::security::negotiation_response>::internal>::destroy<dsn::rpc_holder<dsn::security::negotiation_request, dsn::security::negotiation_response>::internal> (this=<optimized out>,
    __p=<optimized out>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/ext/new_allocator.h:140
#14 std::allocator_traits<std::allocator<dsn::rpc_holder<dsn::security::negotiation_request, dsn::security::negotiation_response>::internal> >::destroy<dsn::rpc_holder<dsn::security::negotiation_request, dsn::security::negotiation_response>::internal> (__a=...,
    __p=<optimized out>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/alloc_traits.h:487
#15 std::_Sp_counted_ptr_inplace<dsn::rpc_holder<dsn::security::negotiation_request, dsn::security::negotiation_response>::internal, std::allocator<dsn::rpc_holder<dsn::security::negotiation_request, dsn::security::negotiation_response>::internal>, (__gnu_cxx::_Lock_policy)2>::_M_dispose (this=0x58ee78d90) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/shared_ptr_base.h:535
#16 0x00000000005f12d6 in std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release (this=0x58ee78d90) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/shared_ptr_base.h:154
#17 0x00007ffa59c9d144 in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::~__shared_count (this=0x7ffa36a58698, __in_chrg=<optimized out>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/shared_ptr_base.h:684
#18 std::__shared_ptr<dsn::rpc_holder<dsn::security::negotiation_request, dsn::security::negotiation_response>::internal, (__gnu_cxx::_Lock_policy)2>::~__shared_ptr (this=0x7ffa36a58690, __in_chrg=<optimized out>)
    at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/shared_ptr_base.h:1123
#19 std::shared_ptr<dsn::rpc_holder<dsn::security::negotiation_request, dsn::security::negotiation_response>::internal>::~shared_ptr (this=0x7ffa36a58690, __in_chrg=<optimized out>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/shared_ptr.h:93
#20 dsn::rpc_holder<dsn::security::negotiation_request, dsn::security::negotiation_response>::~rpc_holder (this=0x7ffa36a58690, __in_chrg=<optimized out>) at /home/laiyingchun/dev/skv_240/src/rdsn/include/dsn/cpp/rpc_holder.h:66
#21 dsn::serverlet<dsn::security::negotiation_manager>::register_rpc_handler_with_rpc_holder<dsn::rpc_holder<dsn::security::negotiation_request, dsn::security::negotiation_response> >(dsn::task_code, char const*, void (dsn::security::negotiation_manager::*)(dsn::rpc_holder<dsn::security::negotiation_request, dsn::security::negotiation_response>))::{lambda(dsn::message_ex*)#1}::operator()(dsn::message_ex*) const (request=<optimized out>, __closure=<optimized out>) at /home/laiyingchun/dev/skv_240/src/rdsn/include/dsn/cpp/serverlet.h:200
#22 std::_Function_handler<void (dsn::message_ex*), dsn::serverlet<dsn::security::negotiation_manager>::register_rpc_handler_with_rpc_holder<dsn::rpc_holder<dsn::security::negotiation_request, dsn::security::negotiation_response> >(dsn::task_code, char const*, void (dsn::security::negotiation_manager::*)(dsn::rpc_holder<dsn::security::negotiation_request, dsn::security::negotiation_response>))::{lambda(dsn::message_ex*)#1}>::_M_invoke(std::_Any_data const&, dsn::message_ex*&&) (__functor=..., __args#0=<optimized out>)
    at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/std_function.h:316
#23 0x00007ffa59cb85c2 in std::function<void (dsn::message_ex*)>::operator()(dsn::message_ex*) const (__args#0=<optimized out>, this=0x3970d1cd0) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/std_function.h:706
#24 dsn::rpc_request_task::exec (this=0x3970d1c00) at /home/laiyingchun/dev/skv_240/src/rdsn/include/dsn/tool-api/task.h:435
#25 0x00007ffa59cb93a1 in dsn::task::exec_internal (this=this@entry=0x3970d1c00) at /home/laiyingchun/dev/skv_240/src/rdsn/src/runtime/task/task.cpp:176
#26 0x00007ffa59cce8a2 in dsn::task_worker::loop (this=0x242f8c0) at /home/laiyingchun/dev/skv_240/src/rdsn/src/runtime/task/task_worker.cpp:224
#27 0x00007ffa59ccea20 in dsn::task_worker::run_internal (this=0x242f8c0) at /home/laiyingchun/dev/skv_240/src/rdsn/src/runtime/task/task_worker.cpp:204
#28 0x00007ffa594a328f in execute_native_thread_routine () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_utils.so
#29 0x00007ffa5762f1ca in start_thread () from /lib64/libpthread.so.0
#30 0x00007ffa559b3e73 in clone () from /lib64/libc.so.6
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6KUb9U/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/307,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6KnNNc,incubator-pegasus,2325533532,307,NA,ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,NA,2024-09-03T03:18:07Z,2024-09-03T03:18:07Z,"@acelyc111 I want merge #1016  in Xiaomi's 2.3.X to upgrade 2.2.2-asiofix cluster. Could you tell more information about it influence？ 
1. Many nodes coredump?
2. one nodes always core?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6KnNNc/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/307,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6K194K,incubator-pegasus,2329402890,307,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-09-04T15:39:14Z,2024-09-04T15:39:14Z,"> @acelyc111 I want merge #1016 in Xiaomi's 2.3.X to upgrade 2.2.2-asiofix cluster. Could you tell more information about it influence？
> 
> 1. Many nodes coredump?
> 2. one nodes always core?

All of the replica servers in the cluster crash in a certain period, but not at the same time. According to the coredump backtrace, it's related to the negotiate failure.

I guess the cause is the boost::asio::ip::tcp::socket is not thread-safe, but we call the async* and close() APIs in multi-threads. I'm trying to fix it.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6K194K/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/307,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6K2IIM,incubator-pegasus,2329444876,307,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-09-04T15:56:54Z,2024-09-04T15:56:54Z,Related issue: https://github.com/apache/incubator-pegasus/issues/387,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6K2IIM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/307,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6LTI0a,incubator-pegasus,2337049882,307,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-09-09T03:41:53Z,2024-09-09T03:41:53Z,"Suppose this case:

- Thread 1: calls `rpc_session::send_message()`, which then calls `boost::asio::async_write(*_socket, ...)`. Note that the `_socket` is passed to `async_write()`, it means the `rpc_session::send_message()` will return immediatly but the `_socket` is still asynchronous used.

- Thread 2: calls `rpc_session::on_failure()`, which then calls `_socket->close()`. Note that `_socket` is synchronous closed, it means the socket can not be used then.

If the timeline is:
send_message() -> async_write(*_socket...) -> on_failure() -> _socket->close() -> actual write on _socket

Then it causes the crash.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6LTI0a/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/307,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6LTK3I,incubator-pegasus,2337058248,307,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-09-09T03:52:17Z,2024-09-09T03:52:17Z,"Q: Why https://github.com/XiaoMi/rdsn/pull/263 can't fix it?
A: Because the `_socket` is used in `async_*` APIs, the lock just ensures the APIs are not called concurrently, but can't ensure the `_socket` are actually used serially. It's possible the socket is `close()d` before actually used in `async_*` callbacks.

Q: Why https://github.com/XiaoMi/rdsn/pull/1016 can't fix it?
A: It just ensures the thread count in io_service is 1, the callback functions of `async_*()` are running in 1 thread, but it doesn't limit the callers, which call `async_*()` and `close()`, are in 1 thread. It's also possible the socket is `close()d` before actually used in `async_*` callbacks.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6LTK3I/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/308,https://api.github.com/repos/apache/incubator-pegasus/issues/308,incubator-pegasus,422524953,308,get latest several config change history from meta-server,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,OPEN,2019-03-19T03:17:25Z,2019-03-19T03:17:25Z,"to help us trace problem.
we can add sub-command `last_config_change` in shell.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/308/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/309,https://api.github.com/repos/apache/incubator-pegasus/issues/309,incubator-pegasus,424026030,309,rdsn coredump in remove_directory while deleting backup tmp file,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2019-03-22T02:59:57Z,2020-01-10T10:29:43Z,"# Server Version
- Pegasus Server 1.11.3 (b45cb06) Release
- browser cluster (Date: 3.22 03:39)

# Coredump Stack
```
(gdb) bt
#0  0x00007f88e99ae1d7 in raise () from /lib64/libc.so.6
#1  0x00007f88e99af8c8 in abort () from /lib64/libc.so.6
#2  0x00007f88ed47a3ee in dsn_coredump () at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/service_api_c.cpp:73
#3  0x00007f88ed477104 in operator() (__closure=<optimized out>, ftwbuf=<optimized out>, typeflag=3, 
    fpath=0x466674000 ""/home/work/ssd3/pegasus/c3srv-browser/replica/reps/16.5.pegasus/backup/backup_tmp.every_day.1553196873544.1553196874651.tmp/008028.dbtmp"")
    at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/filesystem.cpp:327
#4  std::_Function_handler<int(char const*, int, FTW*), dsn::utils::filesystem::remove_directory(const string&)::__lambda4>::_M_invoke(const std::_Any_data &, const char *, int, FTW *) (__functor=..., 
    __args#0=0x466674000 ""/home/work/ssd3/pegasus/c3srv-browser/replica/reps/16.5.pegasus/backup/backup_tmp.every_day.1553196873544.1553196874651.tmp/008028.dbtmp"", __args#1=3, __args#2=<optimized out>)
    at /home/work/qinzuoyan/Pegasus/toolchain/output/include/c++/4.8.2/functional:2056
#5  0x00007f88e9a6364a in process_entry () from /lib64/libc.so.6
#6  0x00007f88e9a6397b in ftw_dir () from /lib64/libc.so.6
#7  0x00007f88e9a6413b in ftw_startup () from /lib64/libc.so.6
#8  0x00007f88ed47739e in dsn::utils::filesystem::file_tree_walk(std::string const&, std::function<int (char const*, int, FTW*)>, bool) (dirpath=..., handler=..., recursive=recursive@entry=true)
    at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/filesystem.cpp:149
#9  0x00007f88ed477b30 in remove_directory (npath=...) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/filesystem.cpp:336
#10 dsn::utils::filesystem::remove_path (path=...) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/filesystem.cpp:360
#11 0x00007f88ed3c1728 in dsn::replication::replica::clear_backup_checkpoint (this=0x2bed680, policy_name=...) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/dist/replication/lib/replica_backup.cpp:424
#12 0x00007f88ed4cbce9 in dsn::task::exec_internal (this=this@entry=0x42303265b) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/task.cpp:180
#13 0x00007f88ed54c42d in dsn::task_worker::loop (this=0x2af1550) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/task_worker.cpp:211
#14 0x00007f88ed54c5f9 in dsn::task_worker::run_internal (this=0x2af1550) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/task_worker.cpp:191
#15 0x00007f88ea306600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#16 0x00007f88eaf73dc5 in start_thread () from /lib64/libpthread.so.0
#17 0x00007f88e9a7073d in clone () from /lib64/libc.so.6
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/309/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/311,https://api.github.com/repos/apache/incubator-pegasus/issues/311,incubator-pegasus,427213088,311,coredumped on rocksdb::CopyFile during cold backup.,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2019-03-29T23:29:57Z,2020-01-19T10:01:32Z,"c3srv-browser, 
2019/3/20

```
#0  0x000000000088bc95 in rocksdb::CopyFile(rocksdb::Env*, std::string const&, std::string const&, unsigned long, bool) ()
#1  0x00000000008a4c03 in rocksdb::CheckpointImpl::CreateCheckpointQuick(std::string const&, unsigned long*) ()
#2  0x0000000000570f0a in pegasus::server::pegasus_server_impl::copy_checkpoint_to_dir_unsafe (this=this@entry=0x2f9b57000, 
    checkpoint_dir=0x9895d8018 ""/home/work/ssd5/pegasus/c3srv-browser/replica/reps/16.3.pegasus/backup/backup_tmp.every_day.1553888104417.1553888105354"", checkpoint_decree=0x7fb9b3e92ff8)
    at /home/work/qinzuoyan/Pegasus/pegasus/src/server/pegasus_server_impl.cpp:1937
#3  0x0000000000571280 in pegasus::server::pegasus_server_impl::copy_checkpoint_to_dir (this=0x2f9b57000, checkpoint_dir=<optimized out>, last_decree=<optimized out>)
    at /home/work/qinzuoyan/Pegasus/pegasus/src/server/pegasus_server_impl.cpp:1909
#4  0x00007fb9fb2113e7 in dsn::replication::replica::local_create_backup_checkpoint (this=0x2f6c0a480, backup_context=...)
    at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/dist/replication/lib/replica_backup.cpp:683
#5  0x00007fb9fb211b04 in operator() (__closure=<optimized out>) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/dist/replication/lib/replica_backup.cpp:653
#6  std::_Function_handler<void(), dsn::replication::replica::wait_async_checkpoint_for_backup(dsn::replication::cold_backup_context_ptr)::__lambda23>::_M_invoke(const std::_Any_data &) (
    __functor=...) at /home/work/qinzuoyan/Pegasus/toolchain/output/include/c++/4.8.2/functional:2071
#7  0x00007fb9fb31dce9 in dsn::task::exec_internal (this=this@entry=0x9a4597ac5) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/task.cpp:180
#8  0x00007fb9fb39e42d in dsn::task_worker::loop (this=0x23eb4a0) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/task_worker.cpp:211
#9  0x00007fb9fb39e5f9 in dsn::task_worker::run_internal (this=0x23eb4a0) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/task_worker.cpp:191
#10 0x00007fb9f8158600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#11 0x00007fb9f8dc5dc5 in start_thread () from /lib64/libpthread.so.0
#12 0x00007fb9f78c273d in clone () from /lib64/libc.so.6
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/311/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/311,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwOTE1Mzk1MQ==,incubator-pegasus,509153951,311,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2019-07-08T09:35:21Z,2019-07-08T09:35:21Z,"这个bug还是需要rocksdb的symbol table，目前rocksdb都是release模式编译的。
如果改成debug模式，没有-O3优化，可能会影响性能。
rocksdb有一个RelWithDebInfo模式，具体为
```
//Flags used by the CXX compiler during RELWITHDEBINFO builds.
CMAKE_CXX_FLAGS_RELWITHDEBINFO:STRING=-O2 -g -DNDEBUG
```
可以考虑用这个模式，得到更多信息，以便定位bug。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwOTE1Mzk1MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/311,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUyNjQzNzEwNQ==,incubator-pegasus,526437105,311,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-08-30T03:00:44Z,2019-08-30T03:00:44Z,"c4srv-msg, c4-hadoop-pegasus-srv-st22, 2019/8/30

## Coredump stack

```
(gdb) bt
#0  0x00000000008f0c95 in rocksdb::CopyFile(rocksdb::Env*, std::string const&, std::string const&, unsigned long, bool) ()
#1  0x0000000000901a43 in rocksdb::CheckpointImpl::CreateCheckpointQuick(std::string const&, unsigned long*) ()
#2  0x00000000006ea09a in pegasus::server::pegasus_server_impl::copy_checkpoint_to_dir_unsafe (this=this@entry=0x70513800, 
    checkpoint_dir=0x2da800d08 ""/home/work/ssd5/pegasus/c4srv-msg/replica/reps/14.4.pegasus/backup/backup_tmp.every_day.1567108847214.1567108847817"", 
    checkpoint_decree=0x7ffc890e4ff8) at /home/wutao1/pegasus-release/src/server/pegasus_server_impl.cpp:1949
#3  0x00000000006ea410 in pegasus::server::pegasus_server_impl::copy_checkpoint_to_dir (this=0x70513800, checkpoint_dir=<optimized out>, last_decree=<optimized out>)
    at /home/wutao1/pegasus-release/src/server/pegasus_server_impl.cpp:1921
#4  0x00007ffccec8a8e7 in dsn::replication::replica::local_create_backup_checkpoint (this=0x1d0f0900, backup_context=...)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_backup.cpp:683
#5  0x00007ffccec8b004 in operator() (__closure=<optimized out>) at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_backup.cpp:653
#6  std::_Function_handler<void(), dsn::replication::replica::wait_async_checkpoint_for_backup(dsn::replication::cold_backup_context_ptr)::__lambda24>::_M_invoke(const std::_Any_data &) (__functor=...) at /home/wutao1/app/include/c++/4.8.2/functional:2071
#7  0x00007ffcceda0cd9 in dsn::task::exec_internal (this=this@entry=0x4e7c6c600) at /home/wutao1/pegasus-release/rdsn/src/core/core/task.cpp:180
#8  0x00007ffccedb4a6d in dsn::task_worker::loop (this=0x27fb290) at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:211
#9  0x00007ffccedb4c39 in dsn::task_worker::run_internal (this=0x27fb290) at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:191
#10 0x00007ffccbba7600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#11 0x00007ffccc81adc5 in start_thread () from /lib64/libpthread.so.0
#12 0x00007ffccb31173d in clone () from /lib64/libc.so.6
```

## Server version

Pegasus Server 1.11.6 (9f4e5aeab5798b487cb54374f28b1a480c0749c5)  release

RelWithDebInfo enabled.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUyNjQzNzEwNQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/311,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUyNjQ5ODM0OQ==,incubator-pegasus,526498349,311,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2019-08-30T07:50:24Z,2019-08-30T07:50:24Z,"发布版应该是BUILD_TYPE=release吧, https://github.com/XiaoMi/pegasus/blob/master/run.sh#L232 rocksdb实际应该是release版本. 我在core的主机上gdb, 也是""no symbol table info available"". 如果是relwithdeginfo, 应该是有symbol的.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUyNjQ5ODM0OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/311,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUyOTI4NDY0MQ==,incubator-pegasus,529284641,311,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2019-09-09T03:17:06Z,2019-09-09T03:17:06Z,"c3srv-browser c3-hadoop-pegasus-srv-st161.bj, 2019/9/9
### Coredump stack

```
(gdb) bt
#0  0x00000000008f0c95 in rocksdb::CopyFile(rocksdb::Env*, std::string const&, std::string const&, unsigned long, bool) ()
#1  0x0000000000901a43 in rocksdb::CheckpointImpl::CreateCheckpointQuick(std::string const&, unsigned long*) ()
#2  0x00000000006ea09a in pegasus::server::pegasus_server_impl::copy_checkpoint_to_dir_unsafe (this=this@entry=0x2b8098800, 
    checkpoint_dir=0x492b780a8 ""/home/work/ssd5/pegasus/c3srv-browser/replica/reps/2.29.pegasus/backup/backup_tmp.every_day.1567971025551.1567971027178"", checkpoint_decree=0x7fdbe8fe1ff8)
    at /home/wutao1/pegasus-release/src/server/pegasus_server_impl.cpp:1949
#3  0x00000000006ea410 in pegasus::server::pegasus_server_impl::copy_checkpoint_to_dir (this=0x2b8098800, checkpoint_dir=<optimized out>, last_decree=<optimized out>)
    at /home/wutao1/pegasus-release/src/server/pegasus_server_impl.cpp:1921
#4  0x00007fdc2f3888e7 in dsn::replication::replica::local_create_backup_checkpoint (this=0x323f06d00, backup_context=...)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_backup.cpp:683
#5  0x00007fdc2f389004 in operator() (__closure=<optimized out>) at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_backup.cpp:653
#6  std::_Function_handler<void(), dsn::replication::replica::wait_async_checkpoint_for_backup(dsn::replication::cold_backup_context_ptr)::__lambda24>::_M_invoke(const std::_Any_data &) (__functor=...)
    at /home/wutao1/app/include/c++/4.8.2/functional:2071
#7  0x00007fdc2f49ecd9 in dsn::task::exec_internal (this=this@entry=0xf43baf2ee) at /home/wutao1/pegasus-release/rdsn/src/core/core/task.cpp:180
#8  0x00007fdc2f4b2a6d in dsn::task_worker::loop (this=0x2389340) at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:211
#9  0x00007fdc2f4b2c39 in dsn::task_worker::run_internal (this=0x2389340) at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:191
#10 0x00007fdc2c2a5600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#11 0x00007fdc2cf18dc5 in start_thread () from /lib64/libpthread.so.0
#12 0x00007fdc2ba0f73d in clone () from /lib64/libc.so.6
```
### Server version
[pegasus-server-1.11.6-9f4e5ae-glibc2.12-release](https://github.com/XiaoMi/pegasus/commit/9f4e5aeab5798b487cb54374f28b1a480c0749c5)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUyOTI4NDY0MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/311,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUyOTI4ODE1Mw==,incubator-pegasus,529288153,311,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2019-09-09T03:41:24Z,2019-09-09T03:41:24Z,尽快编译debug版本升级吧,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUyOTI4ODE1Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/311,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU3NTk3ODQzNw==,incubator-pegasus,575978437,311,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2020-01-19T08:12:56Z,2020-01-19T08:12:56Z,"> 尽快编译debug版本升级吧

release 版本加-g 也是可以的吧？","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU3NTk3ODQzNw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/311,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU3NTk4NjEwMQ==,incubator-pegasus,575986101,311,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2020-01-19T09:46:46Z,2020-01-19T09:46:46Z,"> > 尽快编译debug版本升级吧
> 
> release 版本加-g 也是可以的吧？

当时的目的是为了查bug，所以比较希望优化O等级低一点，不然optimizedout的话，就查不出什么东西了","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU3NTk4NjEwMQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/313,https://api.github.com/repos/apache/incubator-pegasus/issues/313,incubator-pegasus,428296279,313,rocksdb cmd tools should support to operate on both pegasus and original data,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2019-04-02T15:29:36Z,2020-01-19T08:09:41Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/313/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/314,https://api.github.com/repos/apache/incubator-pegasus/issues/314,incubator-pegasus,428633435,314,Clean up 'run.sh test',acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2019-04-03T08:34:39Z,2019-04-03T08:34:39Z,"Too many log and command output, it's not easy to check whether there is any failure.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/314/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/315,https://api.github.com/repos/apache/incubator-pegasus/issues/315,incubator-pegasus,429054855,315,rdsn coredump - add primary replica as a learner,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2019-04-04T02:30:51Z,2019-09-16T09:55:12Z,"# Server Version
- Pegasus Server 1.11.3 (b45cb06) Release
- c6 ai cluster (Date: 4.4 09:38)

# Coredump Stack
```
(gdb) bt
#0  0x00007f838254f1d7 in raise () from /lib64/libc.so.6
#1  0x00007f83825508c8 in abort () from /lib64/libc.so.6
#2  0x00007f838601b3ee in dsn_coredump () at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/service_api_c.cpp:73
#3  0x00007f8385f1c2b7 in dsn::replication::replica::update_local_configuration (this=this@entry=0x394d600, config=..., same_ballot=same_ballot@entry=true)
    at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/dist/replication/lib/replica_config.cpp:807
#4  0x00007f8385f811cb in dsn::replication::replica::on_add_learner (this=0x394d600, request=...) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/dist/replication/lib/replica_learn.cpp:1377
#5  0x00007f8385ee2ce2 in dsn::replication::replica_stub::on_add_learner (this=0x31b6580, request=...) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/dist/replication/lib/replica_stub.cpp:1004
#6  0x00007f8385efd9e0 in operator() (request=<optimized out>, __closure=0x3a399660) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/include/dsn/cpp/serverlet.h:169
#7  std::_Function_handler<void (dsn::message_ex*), bool dsn::serverlet<dsn::replication::replica_stub>::register_rpc_handler<dsn::replication::group_check_request>(dsn::task_code, char const*, void (dsn::replication::replica_stub::*)(dsn::replication::group_check_request const&))::{lambda(dsn::message_ex*)#1}>::_M_invoke(std::_Any_data const&, dsn::message_ex*) (__functor=..., __args#0=<optimized out>)
    at /home/work/qinzuoyan/Pegasus/toolchain/output/include/c++/4.8.2/functional:2071
#8  0x00007f838606cce9 in dsn::task::exec_internal (this=this@entry=0x57be163e4) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/task.cpp:180
#9  0x00007f83860ed42d in dsn::task_worker::loop (this=0x2f65ce0) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/task_worker.cpp:211
#10 0x00007f83860ed5f9 in dsn::task_worker::run_internal (this=0x2f65ce0) at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/core/core/task_worker.cpp:191
#11 0x00007f8382ea7600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>) at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#12 0x00007f8383b14dc5 in start_thread () from /lib64/libpthread.so.0
#13 0x00007f838261173d in clone () from /lib64/libc.so.6
```
## frame 3 local
```
(gdb) f 3
#3  0x00007f8385f1c2b7 in dsn::replication::replica::update_local_configuration (this=this@entry=0x394d600, config=..., same_ballot=same_ballot@entry=true)
    at /home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/dist/replication/lib/replica_config.cpp:807
807	/home/work/qinzuoyan/Pegasus/pegasus/rdsn/src/dist/replication/lib/replica_config.cpp: No such file or directory.
(gdb) i locals
__FUNCTION__ = ""update_local_configuration""
old_status = dsn::replication::partition_status::PS_PRIMARY
old_ballot = 3
r = false
oldTs = 1553249692836
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/315/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/315,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MDc3MTEwNw==,incubator-pegasus,490771107,315,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2019-05-09T07:06:29Z,2019-05-09T07:06:29Z,"现有的failure detection机制如下图所示：
```
                        |--- lease period ----| lease IsExpired, commit suicide
             |--- lease period ---|
worker: ---------------------------------------------------------------->
             \    /     \    /      _\
         beacon ack  beacon ack       x (beacon deliver failed)
              _\/        _\/
master: ---------------------------------------------------------------->
                |---- grace period ----|
                           |--- grace period ----| grace IsExpired, declare worker dead
```

replica server(worker) 周期性(3秒)给meta server(maser)发送心跳
worker和master都会周期性(2秒)检查lease是否到期
- 对worker而言，基于最近一次成功发送的心跳包的时间计算master是否失联
  - 若worker认为自己与master失联，将自己所有replica状态转化为inactive
- 对master而言，基于最近一次接收到心跳包的时间计算worker是否失联
  - 若master认为某个worker失联，会处理这个节点上的所有replica
    - 对于secondary，给这个replica的primary发propose，下线这个secondary
    - 对于primary，master会从secondary中选择一个replica发propose，assign primary

维持现有failure detection机制正确性的关键点在于：master使用的grace lease比worker的lease稍长一点，确保worker首先发现自己与master失联，master才发现这个worker失联。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MDc3MTEwNw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/315,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MDc4NzUxOA==,incubator-pegasus,490787518,315,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2019-05-09T07:34:35Z,2019-05-09T07:34:35Z,"出现core的原因就是违背了上述前提，master发现某个worker失联，而worker并没有发现自己失联，即某个worker“假失联”。

具体来说，master在认为worker失联后，会为worker上的所有primary重新assign primary，在此期间“假失联”的worker通过心跳又被master认为alive，新primary选择这个“假失联”的replica server作为新的secondary，并且meta向它发送propose，而“假失联”的server依旧认为自己是replica的primary。我们认为replica不能从primary转化成secondary，这是一种非法的状态转化，因此产生了这个core。

在实际环境中，我们还发现出现master认为worker“假失联”，但是并没有产生core的情况。
replica server和meta server之间会通过on_config_sync同步config信息，当master认为worker“假失联”后，依旧能够收到replica发来的on_config_sync请求并回复ack给replica。当replica server收到ack后发现meta认为自己没有任何replica，就会主动remove自己所有的replica，这样当收到add secondary propose时就不会产生非法状态转换。同时，该集群“_add_secondary_max_count_for_one_node”配置个数较小，相当于延迟了propose add secondary的时间。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MDc4NzUxOA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/315,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MDc5Mjk0NA==,incubator-pegasus,490792944,315,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2019-05-09T07:51:58Z,2019-05-09T07:51:58Z,"通过分析日志发现，worker没有发现自己失联。第一，没有失联和切换master的log打印出来; 第二，当心跳包的error code不为ERR_OK时，pref-counter _recent_beacon_fail_count会加1，而出问题的集群该pref-counter值都为0; 第三，当master认为worker失联后，它依旧能收到worker的心跳包，并将worker重新设置为alive，因此，初步判断worker的行为是正常的。

目前我在1.11.3的代码中添加了一些log，在测试集群上运行了3周均未复现该问题，但是线上集群却不规律的出现这个bug，出现问题的集群并不是总在灌数据，出bug的时间也没有规律。

现在我计划在出现bug的集群的meta server上搭建meta节点，在测试集群上搭建replica节点，尝试先复现这个问题。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MDc5Mjk0NA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/315,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUzMTcxMTY0OA==,incubator-pegasus,531711648,315,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-09-16T09:55:08Z,2019-09-16T09:55:08Z,"This bug is fixed in release 1.11.6 https://github.com/XiaoMi/pegasus/releases/tag/v1.11.6.
Close it now.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUzMTcxMTY0OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/316,https://api.github.com/repos/apache/incubator-pegasus/issues/316,incubator-pegasus,430249669,316,Use new tags to serialize extended fields in manifest file introduced by Pegasus,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2019-04-08T04:27:30Z,2023-11-09T17:13:55Z,Or store these fields in another column family,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/316/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/317,https://api.github.com/repos/apache/incubator-pegasus/issues/317,incubator-pegasus,431348565,317,refactor pegasus_upgrade_test,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2019-04-10T07:35:14Z,2019-04-10T07:35:52Z,"There are too much duplicate code in pegasus_upgrade_test with pegasus_kill_test, need do some refactor","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/317/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/319,https://api.github.com/repos/apache/incubator-pegasus/issues/319,incubator-pegasus,433090473,319,too many ERR_BUSY logs when write throttling enabled,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2019-04-15T04:32:12Z,2019-04-24T05:30:40Z,"like:
```
E2019-04-16 08:52:44.089 (1555375964089515071 1a218) replica.replica22.040500165af5496f: replica_stub.cpp:1348:response_client(): 2.0@10.142.92.27:30801: write fail: client = 10.152.139.24:60348, code = RPC_RRDB_RRDB_PUT, timeout = 1000, status = replication::partition_status::PS_PRIMARY, error = ERR_BUSY
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/319/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/320,https://api.github.com/repos/apache/incubator-pegasus/issues/320,incubator-pegasus,436518888,320,Record P99 bytes length of request/response,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2019-04-24T06:30:58Z,2019-04-29T06:04:55Z,"Currently pegasus doesn't support recording bytes length of RPC, these are of use to us to know which request is of large size and which table is serving such requests.

We can rely on CU(capacity unit #235) to induct the real record size, but it's probably not accurate for several reasons: 
1. CU only calculates the throughput, for table serving multi-put and put together we are unable to know the size per data.
2. CU is designed for billing, not for trouble-shooting.
3. CU doesn't support internal RPC types like nfs_copy.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/320/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/321,https://api.github.com/repos/apache/incubator-pegasus/issues/321,incubator-pegasus,436522208,321,Support rack awareness,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,OPEN,2019-04-24T06:41:54Z,2020-01-17T07:34:03Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/321/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/322,https://api.github.com/repos/apache/incubator-pegasus/issues/322,incubator-pegasus,437610005,322,restore task can't be stoped manually,foreverneverer,23136769,Jiashuo,js982986555@live.com,OPEN,2019-04-26T10:34:26Z,2019-05-06T11:11:41Z,"1、pegasus don‘t provide the tool to stop the “restore_app” task manualy.
2、when try stop it by “drop {app}”，the task still run，but the “query_restore_status” show “restore failed, because some partition's data is damaged on cold backup media”.
3、when restore anther app，the task must wait for the last task to complete，otherwise you'll see that progress is zero.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/322/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/325,https://api.github.com/repos/apache/incubator-pegasus/issues/325,incubator-pegasus,442108550,325,"when I change to boost1.68,complie error",zdy0009,23186415,,,CLOSED,2019-05-09T08:16:18Z,2019-05-09T09:40:45Z,"```
-- Parsed Thrift package version: 0.9.3
-- Parsed Thrift version: 0.9.3 (0.9.3)
-- libevent NOT found.
CMake Error at /usr/share/cmake/Modules/FindOpenSSL.cmake:278 (list):
  list GET given empty list
Call Stack (most recent call first):
  build/cmake/DefineOptions.cmake:62 (find_package)
  CMakeLists.txt:45 (include)


CMake Error at /usr/share/cmake/Modules/FindOpenSSL.cmake:279 (list):
  list GET given empty list
Call Stack (most recent call first):
  build/cmake/DefineOptions.cmake:62 (find_package)
  CMakeLists.txt:45 (include)


CMake Error at /usr/share/cmake/Modules/FindOpenSSL.cmake:281 (list):
  list GET given empty list
Call Stack (most recent call first):
  build/cmake/DefineOptions.cmake:62 (find_package)
  CMakeLists.txt:45 (include)


CMake Error at /usr/share/cmake/Modules/FindOpenSSL.cmake:283 (list):
  list GET given empty list
Call Stack (most recent call first):
  build/cmake/DefineOptions.cmake:62 (find_package)
  CMakeLists.txt:45 (include)


-- Building without tests
-- Boost version: 1.68.0
CMake Error at /usr/share/cmake/Modules/FindOpenSSL.cmake:278 (list):
  list GET given empty list
Call Stack (most recent call first):
  lib/cpp/CMakeLists.txt:101 (find_package)


CMake Error at /usr/share/cmake/Modules/FindOpenSSL.cmake:279 (list):
  list GET given empty list
Call Stack (most recent call first):
  lib/cpp/CMakeLists.txt:101 (find_package)


CMake Error at /usr/share/cmake/Modules/FindOpenSSL.cmake:281 (list):
  list GET given empty list
Call Stack (most recent call first):
  lib/cpp/CMakeLists.txt:101 (find_package)


CMake Error at /usr/share/cmake/Modules/FindOpenSSL.cmake:283 (list):
  list GET given empty list
Call Stack (most recent call first):
  lib/cpp/CMakeLists.txt:101 (find_package)


-- ----------------------------------------------------------
-- Thrift version:                       0.9.3 (0.9.3)
-- Thrift package version:               0.9.3
-- Build configuration Summary
--   Build Thrift compiler:              OFF
--   Build with unit tests:              OFF
--   Build examples:                     OFF
--   Build Thrift libraries:             ON
--  Language libraries:
--   Build C++ library:                  ON
--   Build C (GLib) library:             OFF
--    - Disabled by via WITH_C_GLIB=OFF
--   Build Java library:                 OFF
--    - Disabled by via WITH_JAVA=OFF
--    - Ant missing
--   Build Python library:               OFF
--    - Disabled by via WITH_PYTHON=OFF
--  Library features:
--   Build shared libraries:             OFF
--   Build static libraries:             ON
--   Build with ZLIB support:            ON
--   Build with libevent support:        OFF
--   Build with Qt4 support:             OFF
--   Build with Qt5 support:             OFF
--   Build with OpenSSL support:         OFF
--   Build with Boost thread support:    OFF
--   Build with C++ std::thread support: OFF
-- ----------------------------------------------------------
-- Configuring incomplete, errors occurred!
See also ""/home/zengdingyong/soft/pegasus/rdsn/thirdparty/build/thrift-0.9.3/CMakeFiles/CMakeOutput.log"".
See also ""/home/zengdingyong/soft/pegasus/rdsn/thirdparty/build/thrift-0.9.3/CMakeFiles/CMakeError.log"".
make: *** No targets specified and no makefile found.  Stop.
build thrift failed
ERROR: build rdsn failed
```

CMakeError.log中的内容：

```
File /home/zengdingyong/soft/pegasus/rdsn/thirdparty/build/thrift-0.9.3/CMakeFiles/CMakeTmp/CheckSymbolExists.c:
/* */
#include <pthread.h>

int main(int argc, char** argv)
{
  (void)argv;
#ifndef pthread_create
  return ((int*)(&pthread_create))[argc];
#else
  (void)argc;
  return 0;
#endif
}

Determining if the function pthread_create exists in the pthreads failed with the following output:
Change Dir: /home/zengdingyong/soft/pegasus/rdsn/thirdparty/build/thrift-0.9.3/CMakeFiles/CMakeTmp

Run Build Command:/bin/gmake ""cmTryCompileExec776809177/fast""
/bin/gmake -f CMakeFiles/cmTryCompileExec776809177.dir/build.make CMakeFiles/cmTryCompileExec776809177.dir/build
gmake[1]: Entering directory `/home/zengdingyong/soft/pegasus/rdsn/thirdparty/build/thrift-0.9.3/CMakeFiles/CMakeTmp'
/usr/bin/cmake -E cmake_progress_report /home/zengdingyong/soft/pegasus/rdsn/thirdparty/build/thrift-0.9.3/CMakeFiles/CMakeTmp/CMakeFiles 1
Building C object CMakeFiles/cmTryCompileExec776809177.dir/CheckFunctionExists.c.o
/home/zengdingyong/feature_XCloudTP/gcc-v7.2.0-rhel7x/bin/gcc   -DCHECK_FUNCTION_EXISTS=pthread_create -fPIE   -o CMakeFiles/cmTryCompileExec776809177.dir/CheckFunctionExists.c.o   -c /usr/share/cmake/Modules/CheckFunctionExists.c
Linking C executable cmTryCompileExec776809177
/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTryCompileExec776809177.dir/link.txt --verbose=1
/home/zengdingyong/feature_XCloudTP/gcc-v7.2.0-rhel7x/bin/gcc   -DCHECK_FUNCTION_EXISTS=pthread_create    CMakeFiles/cmTryCompileExec776809177.dir/CheckFunctionExists.c.o  -o cmTryCompileExec776809177 -rdynamic -lpthreads
/bin/ld: cannot find -lpthreads
collect2: error: ld returned 1 exit status
gmake[1]: *** [cmTryCompileExec776809177] Error 1
gmake[1]: Leaving directory `/home/zengdingyong/soft/pegasus/rdsn/thirdparty/build/thrift-0.9.3/CMakeFiles/CMakeTmp'
gmake: *** [cmTryCompileExec776809177/fast] Error 2
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/325/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/325,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MDgwNzU1Ng==,incubator-pegasus,490807556,325,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-05-09T08:24:21Z,2019-05-09T08:24:21Z,"Refer to this doc: https://github.com/XiaoMi/pegasus/wiki/%E7%BC%96%E8%AF%91%E6%9E%84%E5%BB%BA#%E4%BD%BF%E7%94%A8%E9%9D%9E%E7%B3%BB%E7%BB%9F%E8%87%AA%E5%B8%A6%E7%9A%84boost%E5%BA%93

Try this command out:

```
./run.sh build -b /home/work/software/boost_1_58_0/output --clear_thirdparty
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MDgwNzU1Ng==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/325,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MDgxMjIyOQ==,incubator-pegasus,490812229,325,NA,zdy0009,23186415,,,NA,2019-05-09T08:37:57Z,2019-05-09T08:37:57Z,"Sorry, It not ok .the errors are same","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MDgxMjIyOQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/325,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MDgxNDUwNw==,incubator-pegasus,490814507,325,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-05-09T08:44:33Z,2019-05-09T08:44:33Z,"You should give more information for troubleshooting.
- HOW you build Pegasus?
- Have you installed the required dependencies following the doc?https://github.com/XiaoMi/pegasus/wiki/%E7%BC%96%E8%AF%91%E6%9E%84%E5%BB%BA
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MDgxNDUwNw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/325,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MDgxNjQ0Mg==,incubator-pegasus,490816442,325,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-05-09T08:50:07Z,2019-05-09T08:50:07Z,"It looks like the core problem is building failed on thrift-0.9.3, you can take a try on separately building thrift according to our scripts: https://github.com/XiaoMi/rdsn/blob/master/thirdparty/build-thirdparty.sh#L93.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MDgxNjQ0Mg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/325,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MDgzNDA0NA==,incubator-pegasus,490834044,325,NA,zdy0009,23186415,,,NA,2019-05-09T09:40:45Z,2019-05-09T09:40:45Z,"It's my fault,my enviroment is mixed","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MDgzNDA0NA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/327,https://api.github.com/repos/apache/incubator-pegasus/issues/327,incubator-pegasus,443081083,327,onebox集群，停止调某个replica进程后，通过shell进行数据操作报ERROR:timeout,jackeylove01,50524485,,,CLOSED,2019-05-12T08:56:55Z,2019-06-03T11:30:43Z,"体验 `onebox` 集群，通过 `shell` 进行数据操作。创建一个新的表后，执行数据操作没有问题，然后运行  `./run.sh stop_onebox_instance -r <id>` 命令，或者直接 `kill`调 `replica` 进程后，再次执行数据操作，就会报错`ERROR：timeout`。

![2019-05-12 16-37-23 的屏幕截图](https://user-images.githubusercontent.com/50524485/57579920-f9d73f00-74d5-11e9-882a-82c6d34f5b37.png)

![2019-05-12 16-38-22 的屏幕截图](https://user-images.githubusercontent.com/50524485/57579954-808c1c00-74d6-11e9-9153-2b387d250112.png)


![2019-05-12 16-44-24 的屏幕截图](https://user-images.githubusercontent.com/50524485/57579942-576b8b80-74d6-11e9-9235-6ee72b595e81.png)


![2019-05-12 16-39-22 的屏幕截图](https://user-images.githubusercontent.com/50524485/57579941-50dd1400-74d6-11e9-8fc7-c6b7378277cf.png)

![2019-05-12 16-51-01 的屏幕截图](https://user-images.githubusercontent.com/50524485/57579983-bf21d680-74d6-11e9-88ef-c0a467b8d22e.png)

以上为运行的截图，麻烦帮忙看下什么问题，thanks~","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/327/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/327,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MTU5ODU1MQ==,incubator-pegasus,491598551,327,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-05-12T14:02:23Z,2019-05-12T14:02:23Z,"Because there is no primary replica alive for partition 2 (your pic 5), you must wait for new primary elected to be able to read/write. Normally it doesn't take long.

Btw we suggest using [v1.11.3](https://github.com/XiaoMi/pegasus/releases/tag/v1.11.3) which is the latest stable version.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MTU5ODU1MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/327,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MTg4MzUwNw==,incubator-pegasus,491883507,327,NA,jackeylove01,50524485,,,NA,2019-05-13T16:01:11Z,2019-05-13T16:01:11Z,"> Because there is no primary replica alive for partition 2 (your pic 5), you must wait for new primary elected to be able to read/write. Normally it doesn't take long.
> 
> Btw we suggest to use [v1.11.3](https://github.com/XiaoMi/pegasus/releases/tag/v1.11.3) which is the latest stable version.

But I've been waiting for at least ten minutes, and I still can't do data manipulation.I don't think it's normal.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MTg4MzUwNw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/327,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MjQ4NTc2NA==,incubator-pegasus,492485764,327,NA,zhangyifan27,19500115,Zhang Yifan,chinazhangyifan@163.com,NA,2019-05-15T03:04:46Z,2019-05-15T03:04:46Z,"由于只剩下了2个replica server（图3），而create建表是默认三副本的，一个replica server挂掉后partition 2没有primary所以无法读写（图5）。

可以用start_onebox -r 指定replica个数，启动多个（大于3）replica server，再stop其中一个试试。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MjQ4NTc2NA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/327,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MjUxNDA1NA==,incubator-pegasus,492514054,327,NA,jackeylove01,50524485,,,NA,2019-05-15T05:53:06Z,2019-05-15T05:53:06Z,"> 由于只剩下了2个replica server（图3），而create建表是默认三副本的，一个replica server挂掉后partition 2没有primary所以无法读写（图5）。
> 
> 可以用start_onebox -r 指定replica个数，启动多个（大于3）replica server，再stop其中一个试试。

我尝试了，你说的是对的，我再学习一下，感谢。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MjUxNDA1NA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/327,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MjUxNzEzMQ==,incubator-pegasus,492517131,327,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-05-15T06:07:19Z,2019-05-15T06:07:19Z,"@zhangyifan27 is wrong.
@jackeylove01 

This behavior is controlled by a configuration item of meta server:
```
[meta_server]
min_live_node_count_for_unfreeze = 3
```

If only 2 (< `min_live_node_count_for_unfreeze`) servers alive at last, the cluster will turn into a ""freeze"" state, which means some of the partitions may not gonna have its primary, and the cluster will become
unavailable.

```
>>> app temp -d
[Parameters]
app_name  : temp
detailed  : true

[Result]
app_name           : temp
app_id             : 1   
partition_count    : 8   
max_replica_count  : 3   
details            :     
pidx  ballot  replica_count  primary              secondaries                                
0     3       3/3            0.0.0.0:34803  [0.0.0.0:34801,0.0.0.0:34802]  
1     4       2/3            -              [0.0.0.0:34802,0.0.0.0:34803]  
2     3       3/3            0.0.0.0:34802  [0.0.0.0:34803,0.0.0.0:34801]  
3     3       3/3            0.0.0.0:34803  [0.0.0.0:34802,0.0.0.0:34801]  
4     4       2/3            -              [0.0.0.0:34802,0.0.0.0:34803]  
5     3       3/3            0.0.0.0:34802  [0.0.0.0:34801,0.0.0.0:34803]  
6     3       3/3            0.0.0.0:34803  [0.0.0.0:34801,0.0.0.0:34802]  
7     4       2/3            -              [0.0.0.0:34803,0.0.0.0:34802]  

node                 primary  secondary  total  
0.0.0.0:34801        0        5          5      
0.0.0.0:34802        2        6          8      
0.0.0.0:34803        3        5          8      
                     5        16         21     

fully_healthy_partition_count    : 5
unhealthy_partition_count        : 3
write_unhealthy_partition_count  : 3
read_unhealthy_partition_count   : 3

list app temp succeed
```

If you set this item to ""min_live_node_count_for_unfreeze = 2"":

```
>>> app temp -d
[Parameters]
app_name  : temp
detailed  : true

[Result]
app_name           : temp
app_id             : 1   
partition_count    : 8   
max_replica_count  : 3   
details            :     
pidx  ballot  replica_count  primary              secondaries            
0     4       2/3            0.0.0.0:34803  [0.0.0.0:34802]  
1     5       2/3            0.0.0.0:34802  [0.0.0.0:34803]  
2     4       2/3            0.0.0.0:34802  [0.0.0.0:34803]  
3     4       2/3            0.0.0.0:34803  [0.0.0.0:34802]  
4     5       2/3            0.0.0.0:34803  [0.0.0.0:34802]  
5     4       2/3            0.0.0.0:34802  [0.0.0.0:34803]  
6     4       2/3            0.0.0.0:34803  [0.0.0.0:34802]  
7     5       2/3            0.0.0.0:34802  [0.0.0.0:34803]  

node                 primary  secondary  total  
0.0.0.0:34802        4        4          8      
0.0.0.0:34803        4        4          8      
                     8        8          16     

fully_healthy_partition_count    : 0
unhealthy_partition_count        : 8
write_unhealthy_partition_count  : 0
read_unhealthy_partition_count   : 0

list app temp succeed
```

your cluster can still be able to read/write.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MjUxNzEzMQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/327,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MzkyNzkzOA==,incubator-pegasus,493927938,327,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-05-20T10:23:25Z,2019-05-20T10:23:25Z,@jackeylove01 参见楼上neverchanje的解释，原来有3个replica-server，你kill掉一个后，只有两个了，就会进入freezed模式，这种模式下会禁止所有的config更新，所以无法切换新的primary。你可以在shell中执行`cluster_info`命令 ，可以看到meta_function_level是freezed。,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5MzkyNzkzOA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/330,https://api.github.com/repos/apache/incubator-pegasus/issues/330,incubator-pegasus,446990470,330,Release 1.11.4,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2019-05-22T08:05:22Z,2019-11-18T09:45:24Z,"What we gonna have in this release:

- [x] p999 latency perf-counters (https://github.com/XiaoMi/rdsn/pull/236, https://github.com/XiaoMi/pegasus/pull/332)

- [x] rpc size statistics (https://github.com/XiaoMi/rdsn/pull/247, https://github.com/XiaoMi/pegasus/pull/323)

- [x] **bugfix:** too much error logs by write-throttling reject (https://github.com/XiaoMi/rdsn/pull/243)

- [x] **bugfix:** add debug logs for failure detection (https://github.com/XiaoMi/rdsn/pull/256)

- [x] capacity unit (https://github.com/XiaoMi/pegasus/pull/318 https://github.com/XiaoMi/rdsn/pull/239 https://github.com/XiaoMi/rdsn/pull/253)

- [x] **bugfix:** fix simple_logger gc log file problem (https://github.com/XiaoMi/rdsn/pull/258)

- [x] updates on utils (https://github.com/XiaoMi/rdsn/pull/240, https://github.com/XiaoMi/rdsn/pull/253, https://github.com/XiaoMi/rdsn/pull/239)

- [x] **bugfix:** http_message_parser (https://github.com/XiaoMi/rdsn/pull/259)

- [x] perf-counter: improve log printing when processing http response (https://github.com/XiaoMi/pegasus/pull/333)

- [x]  **bugfix:** pegasus_server: fix set usage scenario problem when open db (https://github.com/XiaoMi/pegasus/pull/334)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/330/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/330,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5NTk3ODUwMQ==,incubator-pegasus,495978501,330,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-05-26T08:01:27Z,2019-05-26T08:01:27Z,关于rpc size statistics，是否也需要在falcon_screen.json里面添加相关的screen？,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5NTk3ODUwMQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/330,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5NTk5MTUyMA==,incubator-pegasus,495991520,330,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-05-26T11:23:33Z,2019-05-26T11:23:33Z,另外可以加上这个：fix simple_logger gc log file problem  https://github.com/XiaoMi/rdsn/pull/258,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5NTk5MTUyMA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/330,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5NzI5NzA3Mw==,incubator-pegasus,497297073,330,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-05-30T11:21:10Z,2019-05-30T11:21:10Z,"Configuration updates:


```
[pegasus.server]
...
perf_counter_read_capacity_unit_size = 4096
perf_counter_write_capacity_unit_size = 4096

[pegasus.collector]
...
cu_stat_app = stat
cu_fetch_interval_seconds = 8

[task.RPC_RRDB_RRDB_PUT]
...
profiler::size.request.server = true

[task.RPC_RRDB_RRDB_MULTI_PUT]
...
profiler::size.request.server = true

[task.RPC_RRDB_RRDB_GET_ACK]
...
profiler::size.response.server = true

[task.RPC_RRDB_RRDB_MULTI_GET_ACK]
...
profiler::size.response.server = true
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5NzI5NzA3Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/330,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5NzI5OTIyOA==,incubator-pegasus,497299228,330,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-05-30T11:30:42Z,2019-05-30T11:30:42Z,"Check List:

- [ ] Ensure no configuration incompatibility when rolling back.
- [x] Ensure failover of a replica instance won't cause CU statistic to miscount.
- [x] Ensure rejects on write throttling don't generate ERROR logs.
- [x] Ensure CU works correctly when `capacity_unit_size` is different.
- [x] Ensure set usage scenario problem when open db is fixed.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDQ5NzI5OTIyOA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/330,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMDMxMzcxNQ==,incubator-pegasus,500313715,330,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-06-10T07:01:25Z,2019-06-10T07:01:25Z,"https://github.com/XiaoMi/pegasus/releases/tag/v1.11.4
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMDMxMzcxNQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/330,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1NDcwNzUxMA==,incubator-pegasus,554707510,330,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-11-17T05:35:24Z,2019-11-17T05:35:24Z,"# Release Notes

The following are the highlights in this release:

## Support Capacity Unit Calculation

To meet the demand for cost and pricing of Pegasus, we introduce ""Capacity Unit"" in this release. This concept is equivalent to AWS DynamoDB's ""Request Unit"", and HBase's ""Capacity Unit"".

Related PR: #318, XiaoMi/rdsn#239, XiaoMi/rdsn#253
Related Docs: TBD
Related Issues:

## Bug Fixes

- bugfix: too much error logs by write-throttling reject (XiaoMi/rdsn#243)

- bugfix: add debug logs for failure detection (XiaoMi/rdsn#256)

- bugfix: fix simple_logger gc log file problem (XiaoMi/rdsn#258)

- bugfix: http_message_parser (XiaoMi/rdsn#259)

- bugfix: pegasus_server: fix set usage scenario problem when open db (#334)

## Perf-Counters

The new perf counters introduced are:

## Upgrades from the previous version

```ini
[pegasus.server]
...
perf_counter_read_capacity_unit_size = 4096
perf_counter_write_capacity_unit_size = 4096

[pegasus.collector]
...
cu_stat_app = stat
cu_fetch_interval_seconds = 8

[task.RPC_RRDB_RRDB_PUT]
...
profiler::size.request.server = true

[task.RPC_RRDB_RRDB_MULTI_PUT]
...
profiler::size.request.server = true

[task.RPC_RRDB_RRDB_GET_ACK]
...
profiler::size.response.server = true

[task.RPC_RRDB_RRDB_MULTI_GET_ACK]
...
profiler::size.response.server = true
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1NDcwNzUxMA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/337,https://api.github.com/repos/apache/incubator-pegasus/issues/337,incubator-pegasus,454474477,337,Release 1.12.0,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2019-06-11T04:25:12Z,2019-11-06T03:27:37Z,"This is a feature release. The main target is to make backup-request available.

What we gonna have in 1.12.0:

Scripts: 
- [x] https://github.com/XiaoMi/pegasus/pull/305
- [x] https://github.com/XiaoMi/pegasus/pull/324
- [x] https://github.com/XiaoMi/pegasus/pull/301
- [x] https://github.com/XiaoMi/pegasus/pull/298
- [x] https://github.com/XiaoMi/pegasus/pull/296

No code: 
- [x] https://github.com/XiaoMi/pegasus/pull/331
- [x] https://github.com/XiaoMi/pegasus/pull/297
- [x] https://github.com/XiaoMi/rdsn/pull/260

Tiny fixes:

- [x] https://github.com/XiaoMi/pegasus/pull/326
- [x] https://github.com/XiaoMi/rdsn/pull/238
- [x] https://github.com/XiaoMi/rdsn/pull/233
- [x] https://github.com/XiaoMi/rdsn/pull/231
- [x] https://github.com/XiaoMi/rdsn/pull/229
- [x] https://github.com/XiaoMi/rdsn/pull/227

Shell & Tools(sample/bench/redisproxy) updates
- [x] https://github.com/XiaoMi/pegasus/pull/329
- [x] https://github.com/XiaoMi/pegasus/pull/303
- [x] https://github.com/XiaoMi/pegasus/pull/312
- [x] https://github.com/XiaoMi/pegasus/pull/302
- [x] https://github.com/XiaoMi/pegasus/pull/290
- [x] https://github.com/XiaoMi/rdsn/pull/254
- [x] https://github.com/XiaoMi/rdsn/pull/223, 

Backup Request: 
- [x] https://github.com/XiaoMi/rdsn/pull/251
- [x] https://github.com/XiaoMi/rdsn/pull/252, 

[Memory utilization optimization] support tcmalloc heap profiling:
- [x] https://github.com/XiaoMi/rdsn/pull/225
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/337/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/337,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMDcwNjI0OQ==,incubator-pegasus,500706249,337,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-06-11T06:31:57Z,2019-06-11T06:31:57Z,Backup Request会修改rpc request header的格式，存在兼容性问题，只升级小版本号恐怕不太合适，考虑升级到1.12.0？,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMDcwNjI0OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/337,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMDcwNjU5NA==,incubator-pegasus,500706594,337,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-06-11T06:33:23Z,2019-06-11T06:33:23Z, @qinzuoyan OK,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMDcwNjU5NA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/337,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMTk0ODY2MQ==,incubator-pegasus,501948661,337,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-06-14T02:52:40Z,2019-06-14T02:52:40Z,"https://github.com/XiaoMi/pegasus/releases/tag/v1.12.0-RC1
https://github.com/XiaoMi/rdsn/releases/tag/v1.12.0-RC1
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMTk0ODY2MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/340,https://api.github.com/repos/apache/incubator-pegasus/issues/340,incubator-pegasus,456750206,340,community: make config more user-friendly,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2019-06-17T05:32:19Z,2019-11-14T08:49:00Z,"The main target is to make our config.ini more user-friendly for new-comers of Pegasus, while now the config.ini has around 600 lines of code, which has become the primary barrier keeping users away.

The following is my plan to **make the config file with fewer entries**:

- [x] Remove the configurations that are no use anymore. https://github.com/XiaoMi/pegasus/pull/338

- [x] Provide `config.min.ini` / `config.full.ini`, one with minimal configurable options, one with a full list of options. (https://github.com/XiaoMi/pegasus/pull/370)

- [x] Hard code the recommended value of options as the default value. (https://github.com/XiaoMi/pegasus/pull/350)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/340/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/341,https://api.github.com/repos/apache/incubator-pegasus/issues/341,incubator-pegasus,458393635,341,Release 1.11.5,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2019-06-20T07:20:28Z,2019-11-06T03:19:40Z,"This is a minor release, including some improvements on CU calculation.

- [x] https://github.com/XiaoMi/pegasus/pull/339
- [x] https://github.com/XiaoMi/rdsn/pull/261

with http memory profiling support:
- [x] https://github.com/XiaoMi/rdsn/pull/225

Scripts: 
- [x] https://github.com/XiaoMi/pegasus/pull/305
- [x] https://github.com/XiaoMi/pegasus/pull/324
- [x] https://github.com/XiaoMi/pegasus/pull/301
- [x] https://github.com/XiaoMi/pegasus/pull/298
- [x] https://github.com/XiaoMi/pegasus/pull/296

No code: 
- [x] https://github.com/XiaoMi/pegasus/pull/331
- [x] https://github.com/XiaoMi/pegasus/pull/297
- [x] https://github.com/XiaoMi/rdsn/pull/260

Shell & Tools(sample/bench/redisproxy) updates
- [x] https://github.com/XiaoMi/pegasus/pull/329
- [x] https://github.com/XiaoMi/pegasus/pull/303
- [x] https://github.com/XiaoMi/pegasus/pull/312
- [x] https://github.com/XiaoMi/pegasus/pull/302
- [x] https://github.com/XiaoMi/pegasus/pull/290
- [x] https://github.com/XiaoMi/rdsn/pull/254
- [x] https://github.com/XiaoMi/rdsn/pull/223, 

Bugfix:
- [x]  XiaoMi/rdsn#263,","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/341/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/341,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMzk0MDIzNQ==,incubator-pegasus,503940235,341,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-06-20T08:53:31Z,2019-06-20T08:53:31Z,"https://github.com/XiaoMi/pegasus/releases/tag/v1.11.5-RC1
https://github.com/XiaoMi/rdsn/releases/tag/v1.11.5-RC1","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMzk0MDIzNQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/341,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMzk0MjIzMw==,incubator-pegasus,503942233,341,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-06-20T08:59:11Z,2019-06-20T08:59:11Z,"https://travis-ci.org/XiaoMi/rdsn/builds/548101434
https://travis-ci.org/XiaoMi/pegasus/builds/548106467","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwMzk0MjIzMw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/341,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwNDMxMzU3NA==,incubator-pegasus,504313574,341,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-06-21T06:52:22Z,2019-06-21T06:52:22Z,"## Configuration Updates

- Upgrade from 1.11.3

```ini
[pegasus.server]
...
perf_counter_read_capacity_unit_size = 4096
perf_counter_write_capacity_unit_size = 4096

[pegasus.collector]
...
usage_stat_app = stat
capacity_unit_fetch_interval_seconds = 8
storage_size_fetch_interval_seconds = 3600

[task.RPC_RRDB_RRDB_PUT]
...
profiler::size.request.server = true

[task.RPC_RRDB_RRDB_MULTI_PUT]
...
profiler::size.request.server = true

[task.RPC_RRDB_RRDB_GET_ACK]
...
profiler::size.response.server = true

[task.RPC_RRDB_RRDB_MULTI_GET_ACK]
...
profiler::size.response.server = true
```

- Upgrade from 1.11.4

```ini
[pegasus.collector]
--  cu_stat_app = stat
--  cu_fetch_interval_seconds = 8
  usage_stat_app = stat
  capacity_unit_fetch_interval_seconds = 8
  storage_size_fetch_interval_seconds = 3600
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwNDMxMzU3NA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/341,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwNDk2ODkzMA==,incubator-pegasus,504968930,341,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-06-24T11:24:00Z,2019-06-24T11:24:00Z,"https://github.com/XiaoMi/pegasus/releases/tag/v1.11.5
https://github.com/XiaoMi/rdsn/releases/tag/v1.11.5

Passed: https://travis-ci.org/XiaoMi/rdsn/builds/549679408
Passed: https://travis-ci.org/XiaoMi/pegasus/builds/549679120
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwNDk2ODkzMA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/342,https://api.github.com/repos/apache/incubator-pegasus/issues/342,incubator-pegasus,459034169,342,Toolset - migrate HBase files to Pegasus,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2019-06-21T06:33:57Z,2021-05-31T09:48:29Z,"Many users used to write their data to HBase, for some reasons, they would like to use Pegasus now. Besides, they also would like to migrate their large amount of data from HBase to Pegasus. 
Unfortunately, bulk load function is still in development, we need to develop a simpler solution to solve creating table with large data.

Usage situation:
- migrate data when creating table

Basic idea:
1. Use Spark or MR to write HFiles data to RocksDB run over HDFS
2. Get checkpoint of RocksDB and construct a valid cold backup
3. Restore table from cold backup
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/342/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/343,https://api.github.com/repos/apache/incubator-pegasus/issues/343,incubator-pegasus,459430310,343,Could Pegasus support multiple versions for single value?,windshg,2142001,Vinson,,OPEN,2019-06-22T03:43:47Z,2019-07-09T02:24:55Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/343/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/343,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwOTQ2MTU0MQ==,incubator-pegasus,509461541,343,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2019-07-09T02:24:55Z,2019-07-09T02:24:55Z,It is not supported.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwOTQ2MTU0MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/346,https://api.github.com/repos/apache/incubator-pegasus/issues/346,incubator-pegasus,464028391,346,bug: coredump in dsn::utils::filesystem::file_tree_walk.,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2019-07-04T02:22:24Z,2019-07-16T07:56:58Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
Normal bootstrap.
7/4/2019, in XiaoMi c4srv-msg cluster.

2. What did you expect to see?
No coredump.

3. What did you see instead?

```
(gdb) bt
#0  0x00007f7f8ecf51d7 in raise () from /lib64/libc.so.6
#1  0x00007f7f8ecf68c8 in abort () from /lib64/libc.so.6
#2  0x00007f7f9282f43e in dsn_coredump () at /home/wutao1/pegasus-release/rdsn/src/core/core/service_api_c.cpp:73
#3  0x00007f7f928243b4 in operator() (__closure=<optimized out>, ftwbuf=<optimized out>, typeflag=3, 
    fpath=0x47f2e000 ""/home/work/ssd5/pegasus/c4srv-msg/replica/reps/10.119.pegasus/backup/backup_tmp.every_day.1562184334957.1562184336589.tmp/MANIFEST-011929"")
    at /home/wutao1/pegasus-release/rdsn/src/core/core/filesystem.cpp:327
#4  std::_Function_handler<int(char const*, int, FTW*), dsn::utils::filesystem::remove_directory(const string&)::__lambda4>::_M_invoke(const std::_Any_data &, const char *, int, FTW *) (
    __functor=..., __args#0=0x47f2e000 ""/home/work/ssd5/pegasus/c4srv-msg/replica/reps/10.119.pegasus/backup/backup_tmp.every_day.1562184334957.1562184336589.tmp/MANIFEST-011929"", __args#1=3, 
    __args#2=<optimized out>) at /home/wutao1/app/include/c++/4.8.2/functional:2056
#5  0x00007f7f8edaa64a in process_entry () from /lib64/libc.so.6
#6  0x00007f7f8edaa97b in ftw_dir () from /lib64/libc.so.6
#7  0x00007f7f8edab13b in ftw_startup () from /lib64/libc.so.6
#8  0x00007f7f9282464e in dsn::utils::filesystem::file_tree_walk(std::string const&, std::function<int (char const*, int, FTW*)>, bool) (dirpath=..., handler=..., recursive=recursive@entry=true)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/filesystem.cpp:149
#9  0x00007f7f92824de0 in remove_directory (npath=...) at /home/wutao1/pegasus-release/rdsn/src/core/core/filesystem.cpp:336
#10 dsn::utils::filesystem::remove_path (path=...) at /home/wutao1/pegasus-release/rdsn/src/core/core/filesystem.cpp:360
#11 0x00007f7f9272dbd8 in dsn::replication::replica::clear_backup_checkpoint (this=0x46dd8000, policy_name=...) at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_backup.cpp:424
#12 0x00007f7f92841609 in dsn::task::exec_internal (this=this@entry=0x55c0cc2d0) at /home/wutao1/pegasus-release/rdsn/src/core/core/task.cpp:180
#13 0x00007f7f9285529d in dsn::task_worker::loop (this=0x2867130) at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:211
#14 0x00007f7f92855469 in dsn::task_worker::run_internal (this=0x2867130) at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:191
#15 0x00007f7f8f64d600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#16 0x00007f7f902c0dc5 in start_thread () from /lib64/libpthread.so.0
#17 0x00007f7f8edb773d in clone () from /lib64/libc.so.6
```

4. What version of Pegasus are you using?
pegasus-server-1.11.5-ba0661d--release.tar.gz
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/346/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/346,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwODMzMzk4OQ==,incubator-pegasus,508333989,346,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-07-04T04:31:57Z,2019-07-04T04:31:57Z,Identical with #309 ,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwODMzMzk4OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/346,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwODM1MzIyMQ==,incubator-pegasus,508353221,346,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-07-04T06:15:06Z,2019-07-04T06:15:06Z,"This bug occurred in the following code:

```cpp
static bool remove_directory(const std::string &npath)
{
    return dsn::utils::filesystem::file_tree_walk(
        npath,
        [](const char *fpath, int typeflag, struct FTW *ftwbuf) {
            bool succ;

            dassert( // ==> COREDUMP POINT
                (typeflag == FTW_F) || (typeflag == FTW_DP), ""Invalid typeflag = %d."", typeflag);
            succ = (::remove(fpath) == 0);
            if (!succ) {
                dwarn(""remove file %s failed, err = %s"", fpath, safe_strerror(errno).c_str());
            }

            return (succ ? FTW_CONTINUE : FTW_STOP);
        },
        true);
}
```

And the server outputs:

```
F2019-07-04 04:05:56.592 (1562184356592616649 17aa3) replica.rep_long1.0404000d05dcdde3: filesystem.cpp:328:operator()(): assertion expression: (typeflag == FTW_F) || (typeflag == FTW_DP)
F2019-07-04 04:05:56.592 (1562184356592632714 17aa3) replica.rep_long1.0404000d05dcdde3: filesystem.cpp:328:operator()(): Invalid typeflag = 3.
```

The code assumes `typeflag` must be either `FTW_F/FTW_DP`, however from `ftw.h`,
we can see type 3 (`FTW_NS`) is also possible:

```
       FTW_NS The stat(2) call failed on fpath, which is not a symbolic
              link.  The probable cause for this is that the caller had read
              permission on the parent directory, so that the filename fpath
              could be seen, but did not have execute permission, so that
              the file could not be reached for stat(2).  The contents of
              the buffer pointed to by sb are undefined.
```

```c
/* Values for the FLAG argument to the user function passed to `ftw'
   and 'nftw'.  */
enum
{
  FTW_F,		/* Regular file.  */
#define FTW_F	 FTW_F
  FTW_D,		/* Directory.  */
#define FTW_D	 FTW_D
  FTW_DNR,		/* Unreadable directory.  */
#define FTW_DNR	 FTW_DNR
  FTW_NS,		/* Unstatable file.  */
#define FTW_NS	 FTW_NS

  FTW_SL,		/* Symbolic link.  */
# define FTW_SL	 FTW_SL

/* These flags are only passed from the `nftw' function.  */
  FTW_DP,		/* Directory, all subdirs have been visited. */
# define FTW_DP	 FTW_DP
  FTW_SLN		/* Symbolic link naming non-existing file.  */
# define FTW_SLN FTW_SLN
};
```
But the reason why file `/home/work/ssd5/pegasus/c4srv-msg/replica/reps/10.119.pegasus/backup/backup_tmp.every_day.1562184334957.1562184336589.tmp/MANIFEST-011929` was unstatable is still not clear.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUwODM1MzIyMQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/347,https://api.github.com/repos/apache/incubator-pegasus/issues/347,incubator-pegasus,464132957,347,cold backup core - no provider,vagetablechicken,24697960,HuangWei,huangwei@apache.org,CLOSED,2019-07-04T08:24:34Z,2019-07-12T02:11:43Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
restore app using cold backup

2. What did you expect to see?
if the provider is unknown, it should return err instead of assert.

3. What did you see instead?
```E2019-07-04 16:12:29.822 (1562227949822055788 0868)   meta.THREAD_POOL_META_SERVER2.0202000200000099: block_service_manager.cpp:58:get_block_filesystem(): acquire block filesystem failed, provider = fds_c3, provider_type =
F2019-07-04 16:12:29.822 (1562227949822103320 0868)   meta.THREAD_POOL_META_SERVER2.0202000200000099: server_state_restore.cpp:47:sync_app_from_backup_media(): assertion expression: blk_fs != nullptr
F2019-07-04 16:12:29.822 (1562227949822116855 0868)   meta.THREAD_POOL_META_SERVER2.0202000200000099: server_state_restore.cpp:47:sync_app_from_backup_media(): acquire block_filesyetem failed
```

4. What version of Pegasus are you using?
1.11.5","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/347/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/347,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUxMDcxNzM5MA==,incubator-pegasus,510717390,347,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2019-07-12T02:11:43Z,2019-07-12T02:11:43Z,https://github.com/XiaoMi/rdsn/commit/e726a817ed2ae74056a3f3038c71d2a2ea9414c8 fixed,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUxMDcxNzM5MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/351,https://api.github.com/repos/apache/incubator-pegasus/issues/351,incubator-pegasus,466126134,351,Support querying system stat using RESTful API.,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2019-07-10T06:33:58Z,2019-11-14T08:50:17Z,"Currently, Pegasus supports a few RESTful APIs. Users who want to get system status still must go through pegasus shell, which internally uses `replication_ddl_client`, and `remote_command`. Both of the methods retrieve system status by sending RPC in rDSN-specific network protocol. They are not suitable for building tools in other languages except for C++. Each of the languages has to implement the client-side RPC (including thrift serialization, payload decompression, etc.) to communicates with the server.

Most of our administration tools (rolling update, create_table, pegasus_check_clusters, clusters_show) now heavily rely on pegasus shell executing commands for system status. It is not convenient for either the development of tools or using tools.

## Solution:

Expose system status retrieval through RESTful APIs.

Including:
- [x] http://meta03/nodes
- [x] http://meta03/apps
- [x] http://meta03/app/<APP_NAME>
- [ ] http://meta03/cluster
- [ ] http://replica03/replicaInfo
- [x] http://replica03/version
- [x] http://replica03/recenStartTime

RESTful APIs can be registered using https://github.com/XiaoMi/rdsn/tree/master/src/dist/http module.
Refer to header file: https://github.com/XiaoMi/rdsn/blob/master/include/dsn/tool-api/http_server.h.

### nodes

shell alternative:
```
> nodes -j
```

SEE:
- src/dist/replication/meta_server/meta_service::on_list_nodes
- src/dist/replication/ddl_lib/replication_ddl_client::list_nodes

### apps

shell alternative:
```
> ls -j
```

SEE:
- src/dist/replication/meta_server/meta_service::on_list_apps
- src/dist/replication/ddl_lib/replication_ddl_client::list_apps

### app/<APP_NAME>

shell alternative:
```
> app <APP_NAME> -j
```

SEE:
- src/dist/replication/meta_server/meta_service::on_query_configuration_by_index
- src/dist/replication/ddl_lib/replication_ddl_client::list_app

### cluster

shell alternative:
```
> cluster_info -j
```

SEE:
- src/dist/replication/meta_server/meta_service::on_query_cluster_info
- src/dist/replication/ddl_lib/replication_ddl_client::cluster_info

### version

shell alternative:
```
> remote-command -l <ip:port> server-info
```

SEE:
- src/server/main.cpp::dsn_app_registration_pegasus

### recentStartTime

shell alternative:
```
> remote-command -l <ip:port> server-info
```
SEE:
- src/server/main.cpp::dsn_app_registration_pegasus
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/351/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/353,https://api.github.com/repos/apache/incubator-pegasus/issues/353,incubator-pegasus,466230867,353,Support write throttling using total byte-size of requests,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2019-07-10T10:30:04Z,2019-11-14T08:51:20Z,"-------
update: Fixed by https://github.com/XiaoMi/rdsn/pull/298","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/353/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/354,https://api.github.com/repos/apache/incubator-pegasus/issues/354,incubator-pegasus,467206522,354,Release 1.11.6,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2019-07-12T03:45:01Z,2019-11-18T03:46:32Z,"This is a minor release, including some bugfixes.

- https://github.com/XiaoMi/rdsn/pull/265 (https://github.com/XiaoMi/rdsn/commit/83959dacc288040f756d269790e2a0e2b85d4224)

- https://github.com/XiaoMi/rdsn/pull/267 (https://github.com/XiaoMi/rdsn/commit/e726a817ed2ae74056a3f3038c71d2a2ea9414c8)

- https://github.com/XiaoMi/rdsn/pull/268 (https://github.com/XiaoMi/rdsn/commit/314cd90aaadb4e16c00f813125fc9b9930f9bacd)

- https://github.com/XiaoMi/pegasus/pull/355 (https://github.com/XiaoMi/pegasus/commit/bfc342144d9987a5609c3e649d0f32a7db674068)

Scripts and docs:

- https://github.com/XiaoMi/pegasus/pull/335 (https://github.com/XiaoMi/pegasus/commit/040af1e80cb13e7b3d58f239474c14cf9d4f41a9)

- https://github.com/XiaoMi/pegasus/pull/348 (https://github.com/XiaoMi/pegasus/commit/288787367aa5a2a8abed8d81e520ffd2ee05a658)

- https://github.com/XiaoMi/pegasus/pull/349 (https://github.com/XiaoMi/pegasus/commit/547f647aa52e029b0704a3d180227a7a850a1286)

Minor fixes:

- https://github.com/XiaoMi/rdsn/pull/229 (https://github.com/XiaoMi/rdsn/commit/b46426745697c25121968d8e7dd72d14c443cfd0)

- https://github.com/XiaoMi/rdsn/pull/230 (https://github.com/XiaoMi/rdsn/commit/a493678b195d8fb1945eb2518a01e13c707018ec)

- https://github.com/XiaoMi/rdsn/pull/231 (https://github.com/XiaoMi/rdsn/commit/fc0f32486d243cb6dac72c316bf9724b5c511d56)

- https://github.com/XiaoMi/pegasus/pull/326 (https://github.com/XiaoMi/pegasus/commit/5cf8b7a5eeb44b0d62f12c230c768a14f3c4aa6f)

- https://github.com/XiaoMi/pegasus/pull/338 (https://github.com/XiaoMi/pegasus/commit/3c59ffdb5ae2696d4966b3ed456684a5929a145c)

- https://github.com/XiaoMi/pegasus/pull/352 (https://github.com/XiaoMi/pegasus/commit/7ff7cd3243901606dc408a585e37adb4744e2aae)

Configuration updates:

```
[core]
- ;tool = fastrun
- cli_local = false
- cli_remote = false
- start_nfs = false
- ;logging_factory_name = dsn::tools::hpc_logger

- [tools.hpc_logger]
- per_thread_buffer_bytes = 8192

[threadpool.THREAD_POOL_DEFAULT]
- max_input_queue_length = 1024

[replication]
+ duplication_disabled = true

[pegasus.server]
- updating_rocksdb_sstsize_interval_seconds = 600
+ update_rdb_stat_interval = 600

[task..default]
- fast_execution_in_network_thread = false
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/354/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/354,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUxMjc5NDY0NA==,incubator-pegasus,512794644,354,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-07-18T12:29:17Z,2019-07-18T12:29:17Z,"The above commits have been cherry-picked in 1.11.6-RC1.
Additional commits to be added to 1.11.6-RC2:

Important bugfix on failure detection!

- https://github.com/XiaoMi/rdsn/pull/272 (https://github.com/XiaoMi/rdsn/commit/75029e09258257d8814872b4ad392747ae66f035)

- https://github.com/XiaoMi/rdsn/pull/279

- https://github.com/XiaoMi/rdsn/pull/277

Whitelist support:

- XiaoMi/rdsn#228 (XiaoMi/rdsn@f9bb246)

- XiaoMi/rdsn#226 (XiaoMi/rdsn@ddaa1ad)

Server connection threshold support:

- XiaoMi/rdsn#234 (XiaoMi/rdsn@ca636f2)

Minor fixes:

- XiaoMi/rdsn#238 (XiaoMi/rdsn@0048ae4)

- XiaoMi/rdsn#227 (XiaoMi/rdsn@36607b7)

- XiaoMi/rdsn#251 (XiaoMi/rdsn@3989922)

- XiaoMi/rdsn#252 (XiaoMi/rdsn@270a42c)

Periodically memory release:

- https://github.com/XiaoMi/rdsn/pull/278

Redis Proxy:

- https://github.com/XiaoMi/pegasus/pull/357

- https://github.com/XiaoMi/pegasus/pull/369

- https://github.com/XiaoMi/pegasus/pull/381

Tools and Scripts:

- https://github.com/XiaoMi/pegasus/pull/375

- https://github.com/XiaoMi/pegasus/pull/382

Configuration updates:

```
[core]
+ tcmalloc_release_rate = 1.0 # default

[replication]
+ mem_release_interval_ms = 86400000 # default
+ mem_release_enabled = true  # default
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUxMjc5NDY0NA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/354,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1NDczMTg5Mg==,incubator-pegasus,554731892,354,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-11-17T10:22:25Z,2019-11-17T10:22:25Z,"# Release Notes of v1.11.6

The following are the highlights in this release:

## Important bugfix on failure detection!

This bug occurred a few times in a week in our production. Each time causes one server restart. For user who experiences this bug we suggest you to upgrade to 1.11.6.

Related PR: [XiaoMi/rdsn#272](https://github.com/XiaoMi/rdsn/pull/272), [XiaoMi/rdsn#279](https://github.com/XiaoMi/rdsn/pull/279), [XiaoMi/rdsn#277](https://github.com/XiaoMi/rdsn/pull/277)

Related Issue: [#315](https://github.com/XiaoMi/pegasus/issues/315)

## Stability Fixes

Server connection threshold support: [XiaoMi/rdsn#234](https://github.com/XiaoMi/rdsn/pull/234)

Whitelist support: [XiaoMi/rdsn#228](https://github.com/XiaoMi/rdsn/pull/228), [XiaoMi/rdsn#226](https://github.com/XiaoMi/rdsn/pull/226)

Related Docs: <https://pegasus-kv.github.io/administration/whitelist>

## Optimization of Memory Usage

In XiaoMi, some of our clusters experience unreasonably high memory load. To reduce the unknown-where memory cost we introduce ""auto-memory-release"", thanks to @linlinhaohao888.

Currently this feature is disable by default due to severe performance downgrade during each run of memory release.

Related PR: [XiaoMi/rdsn#278](https://github.com/XiaoMi/rdsn/pull/278)

Related Docs: TBD

## Upgrade from the previous version

```ini
[core]
- ;tool = fastrun
- cli_local = false
- cli_remote = false
- start_nfs = false
- ;logging_factory_name = dsn::tools::hpc_logger
+ tcmalloc_release_rate = 1.0 # default

- [tools.hpc_logger]
- per_thread_buffer_bytes = 8192

[threadpool.THREAD_POOL_DEFAULT]
- max_input_queue_length = 1024

[replication]
+ duplication_disabled = true
+ mem_release_interval_ms = 86400000 # 1day
+ mem_release_enabled = false # disable by default

[pegasus.server]
- updating_rocksdb_sstsize_interval_seconds = 600
+ update_rdb_stat_interval = 600

[task..default]
- fast_execution_in_network_thread = false
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1NDczMTg5Mg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/356,https://api.github.com/repos/apache/incubator-pegasus/issues/356,incubator-pegasus,469005830,356,core in dsn::thrift_message_parser::parse_message,mentoswang,40587846,Shanshan Wang,,OPEN,2019-07-17T06:14:27Z,2019-07-17T06:14:27Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
If possible, provide a recipe for reproducing the error.

2. What did you expect to see?

3. What did you see instead?
time: 20190717 11:39:15
cluster: tjwqtst-staging
node: tj1-hadoop-pegasus-tst-ts09
version: 1.11.5
CALL [replica-server] [10.38.162.236:31801] succeed: Pegasus Server 1.11.5 (ba0661d17a96143164d7a0a5c17bb88c0c1dd44d) Release, Started at 2019-07-17 11:39:15
core file: core.replica.asio.3.120767.1563334709
code:
https://github.com/XiaoMi/rdsn/blob/709ea4117fd31b2bd2788dddb1b41f94e8307210/src/core/tools/common/thrift_message_parser.cpp
call stack:
```
#0  0x00007f19d0d7d1d7 in raise () from /lib64/libc.so.6
#1  0x00007f19d0d7e8c8 in abort () from /lib64/libc.so.6
#2  0x00007f19d48b743e in dsn_coredump () at /home/wutao1/pegasus-release/rdsn/src/core/core/service_api_c.cpp:73
#3  0x00007f19d493e843 in dsn::thrift_message_parser::parse_message (thrift_header=..., message_data=...)
    at /home/wutao1/pegasus-release/rdsn/src/core/tools/common/thrift_message_parser.cpp:275
#4  0x00007f19d493eb13 in dsn::thrift_message_parser::get_message_on_receive (this=0x50a07ee10, reader=0x59bc4a80, read_next=@0x7f19bc46e04c: 4096)
    at /home/wutao1/pegasus-release/rdsn/src/core/tools/common/thrift_message_parser.cpp:72
#5  0x00007f19d495a5ff in operator() (length=<optimized out>, __closure=0x7f19bc46e090, ec=...) at /home/wutao1/pegasus-release/rdsn/src/core/tools/common/asio_rpc_session.cpp:114
#6  operator() (this=0x7f19bc46e090) at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/bind_handler.hpp:127
#7  asio_handler_invoke<boost::asio::detail::binder2<dsn::tools::asio_rpc_session::do_read(int)::__lambda2, boost::system::error_code, long unsigned int> > (function=...)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/handler_invoke_hook.hpp:69
#8  invoke<boost::asio::detail::binder2<dsn::tools::asio_rpc_session::do_read(int)::__lambda2, boost::system::error_code, long unsigned int>, dsn::tools::asio_rpc_session::do_read(int)::__lambda2> (context=..., function=...) at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/handler_invoke_helpers.hpp:37
#9  boost::asio::detail::reactive_socket_recv_op<boost::asio::mutable_buffers_1, dsn::tools::asio_rpc_session::do_read(int)::__lambda2>::do_complete(boost::asio::detail::io_service_impl *, boost::asio::detail::operation *, const boost::system::error_code &, std::size_t) (owner=<optimized out>, base=<optimized out>)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/reactive_socket_recv_op.hpp:110
#10 0x000000000074fec9 in complete (bytes_transferred=<optimized out>, ec=..., owner=..., this=<optimized out>)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/task_io_service_operation.hpp:38
#11 do_run_one (ec=..., this_thread=..., lock=..., this=0x29c4620) at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/impl/task_io_service.ipp:372
#12 boost::asio::detail::task_io_service::run (this=0x29c4620, ec=...) at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/impl/task_io_service.ipp:149
#13 0x00007f19d4952cc6 in run (this=<optimized out>, ec=...) at /home/wutao1/boost_1_58_0/output/include/boost/asio/impl/io_service.ipp:66
#14 operator() (__closure=0x299d930) at /home/wutao1/pegasus-release/rdsn/src/core/tools/common/asio_net_provider.cpp:73
#15 _M_invoke<> (this=0x299d930) at /home/wutao1/app/include/c++/4.8.2/functional:1732
#16 operator() (this=0x299d930) at /home/wutao1/app/include/c++/4.8.2/functional:1720
#17 std::thread::_Impl<std::_Bind_simple<dsn::tools::asio_network_provider::start(dsn::rpc_channel, int, bool)::__lambda2()> >::_M_run(void) (this=0x299d918)
    at /home/wutao1/app/include/c++/4.8.2/thread:115
#18 0x00007f19d16d5600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#19 0x00007f19d2348dc5 in start_thread () from /lib64/libpthread.so.0
#20 0x00007f19d0e3f73d in clone () from /lib64/libc.so.6
```
4. What version of Pegasus are you using?
1.11.5","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/356/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/361,https://api.github.com/repos/apache/incubator-pegasus/issues/361,incubator-pegasus,472766430,361,CMake improvements,vagetablechicken,24697960,HuangWei,huangwei@apache.org,OPEN,2019-07-25T09:40:54Z,2019-09-19T08:00:17Z,"需要改进的几点：
- DSN_IN_CORE等不需要的逻辑可以去掉了
- boost库在每次dsn_add_project都会find一次，无意义的重复
- #86 需要去掉link_directories()，因此需要改变三方库的搜索方法，并使用export()生成rdsn的package
- https://github.com/XiaoMi/rdsn/issues/190 ，整理第三方的时候可以顺便处理了
- 在rdsn作为package的基础上，可以清理下DSN_ROOT相关的东西，比如，解除run.sh对环境变量的重度依赖，不再必须将rdsn库copy到pegasus目录下等等
- 在链接库均由package管理，与cmake语法支持的基础上，CMakeList的target_link_libraries可以减少显式链接，可以让CMakeList更易读
- CMakeList规范化，更易于上手

补充：

- dsn_replica_server 和 dsn_meta_server 看看能不能做成静态库，项目统一用一种方式，有必要的时候再用 ""BUILD_SHARED_LIBS=ON"" 做动态库","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/361/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/361,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUxNDk3NTk4Ng==,incubator-pegasus,514975986,361,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2019-07-25T09:44:10Z,2019-07-25T09:44:10Z,"- [x] 去掉DSN_IN_CORE等不需要的逻辑
- [x] boost库在每次dsn_add_project都会find一次，fix","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUxNDk3NTk4Ng==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/361,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUxNTY1NjY1Mg==,incubator-pegasus,515656652,361,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2019-07-27T06:22:32Z,2019-07-27T06:22:32Z,"cleanup ‘remove this’ comments in dsn.cmake
- [x] remove MY_RPOJ_LIB_PATH

- [x] remove MY_PROJ_INC_PATH-等bench重构好done

  - pegasus: 
    - [x] remove rocksdb::HistogramImpl, cause it's not exposed.-- use rocksdb::Statistics
    - [x] remove MY_PROJ_INC_PATH
  - rdsn: done
- [ ] https://github.com/XiaoMi/rdsn/issues/190 & fmt加入默认链接库 & DSN_SYSTEM_LIBS和默认链接库的命名关系 & curl build without idn
  - [ ] rdsn: cleanup crypto todo
  - [x] pegasus cleanup","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUxNTY1NjY1Mg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/361,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUyMjg2NTk0OQ==,incubator-pegasus,522865949,361,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2019-08-20T05:59:18Z,2019-08-20T05:59:18Z,"- [ ] thirdparty的链接方式通过find来做 doing

现有问题：

- [x] prometheus需要curl，而curl在有idn的环境中会链接idn，在使用curl时就必须要跟上idn。

    目前pegasus写死了需要idn，但其实无idn的环境下就反而会导致ld can't find -lidn。
通过find使链接隐式化，就不会再有这种问题了
ps. curl使用也用不着idn, 可以--without-idn","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUyMjg2NTk0OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/364,https://api.github.com/repos/apache/incubator-pegasus/issues/364,incubator-pegasus,473656536,364,backup: unexpected cancel operation,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,OPEN,2019-07-27T16:49:54Z,2019-07-27T16:54:13Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

### 1. What did you do?
If possible, provide a recipe for reproducing the error.

Normal setup. 2019/7/28.
4-day period of backup, scheduled at 3:00 am.

### 2. What did you expect to see?

The most recent backup was ended at 7/26, which means no backup task should be run before 7/30.

### 3. What did you see instead?

A backup cancel operation heavily blocked the replica-server‘s read/write handling.

The logs:

```
D2019-07-28 00:25:39.307 (1564244739307908603 a21b) replica.replica13.0404000d395dfa0e: replica_backup.cpp:760:set_backup_context_cancel(): 1.110@10.114.11.26:60801: cancel backup progress, backup_request = backup_request(pid=1.110, policy=policy_info(policy_name=daily_bak, backup_provider_type=fds_wq), app_name=feeds_user_history, backup_id=1564081494642)
D2019-07-28 00:25:39.307 (1564244739307919494 a215) replica.replica7.0404000757e03026: replica_backup.cpp:760:set_backup_context_cancel(): 1.32@10.114.11.26:60801: cancel backup progress, backup_request = backup_request(pid=1.32, policy=policy_info(policy_name=daily_bak, backup_provider_type=fds_wq), app_name=feeds_user_history, backup_id=1564081494642)
D2019-07-28 00:25:39.307 (1564244739307930085 a225) replica.replica23.04040017b116af54: replica_backup.cpp:760:set_backup_context_cancel(): 1.72@10.114.11.26:60801: cancel backup progress, backup_request = backup_request(pid=1.72, policy=policy_info(policy_name=daily_bak, backup_provider_type=fds_wq), app_name=feeds_user_history, backup_id=1564081494642)
D2019-07-28 00:25:39.307 (1564244739307941464 a21c) replica.replica14.0404000edf16ed14: replica_backup.cpp:760:set_backup_context_cancel(): 1.111@10.114.11.26:60801: cancel backup progress, backup_request = backup_request(pid=1.111, policy=policy_info(policy_name=daily_bak, backup_provider_type=fds_wq), app_name=feeds_user_history, backup_id=1564081494642)
E2019-07-28 00:25:39.308 (1564244739308003269 a23e) replica.local_app23.0400a1e6f74f8101: replica_stub.cpp:1392:response_client(): 1.111@10.114.11.26:60801: read fail: client = 10.132.17.44:19602, code = RPC_RRDB_RRDB_MULTI_GET, timeout = 110, status = replication::partition_status::PS_INACTIVE, error = ERR_INVALID_STATE
```

The perf-counter `replica*eon.replica_stub*cold.backup.recent.cancel.count` was raised up to 9 at the moment. (0 for normal)

### 4. What version of Pegasus are you using?

1.11.3 (modified version for duplication).","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/364/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/365,https://api.github.com/repos/apache/incubator-pegasus/issues/365,incubator-pegasus,473707429,365,Assert from rocksdb posix_logger,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,OPEN,2019-07-28T05:28:46Z,2019-07-28T05:37:57Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

### 1. What did you do?
If possible, provide a recipe for reproducing the error.

Normal rolling-update on an existing cluster. (c4tst-perfomance)

### 2. What did you expect to see?

No error.

### 3. What did you see instead?

pegasus_server: /home/wutao1/pegasus/rocksdb/env/posix_logger.h:151: virtual void rocksdb::PosixLogger::Logv(const char*, __va_list_tag*): Assertion `sz == write_size' failed.

```
(gdb) bt
#0  0x00007f7845b641d7 in raise () from /lib64/libc.so.6
#1  0x00007f7845b658c8 in abort () from /lib64/libc.so.6
#2  0x00007f7845b5d146 in __assert_fail_base () from /lib64/libc.so.6
#3  0x00007f7845b5d1f2 in __assert_fail () from /lib64/libc.so.6
#4  0x0000000000924572 in rocksdb::PosixLogger::Logv(char const*, __va_list_tag*) ()
#5  0x0000000000896cb3 in rocksdb::Log(rocksdb::InfoLogLevel, rocksdb::Logger*, char const*, ...) ()
#6  0x00000000008a35ac in rocksdb::ImmutableDBOptions::Dump(rocksdb::Logger*) const ()
#7  0x00000000007e6fd3 in rocksdb::DBImpl::DBImpl(rocksdb::DBOptions const&, std::string const&) ()
#8  0x000000000081c0ef in rocksdb::DB::Open(rocksdb::DBOptions const&, std::string const&, std::vector<rocksdb::ColumnFamilyDescriptor, std::allocator<rocksdb::ColumnFamilyDescriptor> > const&, std::vector<rocksdb::ColumnFamilyHandle*, std::allocator<rocksdb::ColumnFamilyHandle*> >*, rocksdb::DB**) ()
#9  0x000000000081d856 in rocksdb::DB::Open(rocksdb::Options const&, std::string const&, rocksdb::DB**) ()
#10 0x0000000000701e98 in pegasus::server::pegasus_server_impl::start (this=0x57d0000, argc=<optimized out>, argv=<optimized out>)
    at /home/wutao1/pegasus/src/server/pegasus_server_impl.cpp:1515
#11 0x00007f784964e08b in dsn::replication::replication_app_base::open (this=this@entry=0x57d0000)
    at /home/wutao1/pegasus/rdsn/src/dist/replication/lib/replication_app_base.cpp:425
#12 0x00007f7849650968 in dsn::replication::replication_app_base::open_internal (this=0x57d0000, r=r@entry=0x2f6f700)
    at /home/wutao1/pegasus/rdsn/src/dist/replication/lib/replication_app_base.cpp:346
#13 0x00007f784960e0af in dsn::replication::replica::init_app_and_prepare_list (this=this@entry=0x2f6f700, create_new=create_new@entry=false)
    at /home/wutao1/pegasus/rdsn/src/dist/replication/lib/replica_init.cpp:211
#14 0x00007f784960f45a in dsn::replication::replica::initialize_on_load (this=this@entry=0x2f6f700) at /home/wutao1/pegasus/rdsn/src/dist/replication/lib/replica_init.cpp:121
#15 0x00007f784960f88a in dsn::replication::replica::load (stub=0x27de580, dir=0x2e185b8 ""/home/work/ssd6/pegasus/c4tst-perfomance/replica/reps/60.33.pegasus"")
    at /home/wutao1/pegasus/rdsn/src/dist/replication/lib/replica_init.cpp:162
#16 0x00007f784962e801 in operator() (__closure=0x2cf8fe0) at /home/wutao1/pegasus/rdsn/src/dist/replication/lib/replica_stub.cpp:397
#17 std::_Function_handler<void(), dsn::replication::replica_stub::initialize(const dsn::replication::replication_options&, bool)::__lambda14>::_M_invoke(const std::_Any_data &) (
    __functor=...) at /home/wutao1/app/include/c++/4.8.2/functional:2071
#18 0x00007f784970b149 in dsn::task::exec_internal (this=this@entry=0x2e1ec60) at /home/wutao1/pegasus/rdsn/src/core/core/task.cpp:180
#19 0x00007f784971eedd in dsn::task_worker::loop (this=0x2c704d0) at /home/wutao1/pegasus/rdsn/src/core/core/task_worker.cpp:211
#20 0x00007f784971f0a9 in dsn::task_worker::run_internal (this=0x2c704d0) at /home/wutao1/pegasus/rdsn/src/core/core/task_worker.cpp:191
#21 0x00007f78464bc600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#22 0x00007f784712fdc5 in start_thread () from /lib64/libpthread.so.0
#23 0x00007f7845c2673d in clone () from /lib64/libc.so.6
```

### 4. What version of Pegasus are you using?

pegasus (bdc029937bee99025c69b6a7f71ef4d31a1c5abb)
rocksdb (dc91f8a9567b65cea9cc7d1cc792ddd764141638)

The cause for this error may be the incompatible upgrade of rocksdb.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/365/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/365,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUxNTczNDUzNg==,incubator-pegasus,515734536,365,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2019-07-28T05:32:05Z,2019-07-28T05:32:05Z,磁盘满了吗？,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUxNTczNDUzNg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/365,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUxNTczNDc4MA==,incubator-pegasus,515734780,365,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-07-28T05:37:57Z,2019-07-28T05:37:57Z,"@acelyc111 不知道，我现在已经清了，清的时候没有看磁盘容量，好像很满了。
之前也有社区用户有这个问题。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUxNTczNDc4MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/366,https://api.github.com/repos/apache/incubator-pegasus/issues/366,incubator-pegasus,474543346,366,jepsen for pegasus,windshg,2142001,Vinson,,CLOSED,2019-07-30T12:00:45Z,2019-11-14T08:47:25Z,Will pegasus support jepsen for consistency verification?,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/366/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/366,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUxNjQzMzI2OQ==,incubator-pegasus,516433269,366,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-07-30T14:03:40Z,2019-07-30T14:03:40Z,"Yes, we have implemented a simple testing tool in Jespen, but it's not run as part of our CI.
This tool is hosted here: https://github.com/neverchanje/jepsen-pegasus.
This test still doesn't bring us many benefits, since it finishes too fast even for 10 runs (1min for every run).","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUxNjQzMzI2OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/383,https://api.github.com/repos/apache/incubator-pegasus/issues/383,incubator-pegasus,484794592,383,"Weekly Digest (17 August, 2019 - 24 August, 2019)",weekly-digest,,,,CLOSED,2019-08-24T08:40:49Z,2019-09-04T05:12:02Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 3 issues were created.
Of these, 3 issues have been closed and 0 issues are still open.
## CLOSED ISSUES
:heart: #382 [fix: improve shell get_app_stat and ls_nodes & fix scripts](https://github.com/XiaoMi/pegasus/pull/382), by [qinzuoyan](https://github.com/qinzuoyan)
:heart: #381 [GEO: add GEOADD API for redis proxy to support full GEO function](https://github.com/XiaoMi/pegasus/pull/381), by [acelyc111](https://github.com/acelyc111)
:heart: #380 [compile/link: fix dependency error of curl](https://github.com/XiaoMi/pegasus/pull/380), by [acelyc111](https://github.com/acelyc111)
## NOISY ISSUE
:speaker: #380 [compile/link: fix dependency error of curl](https://github.com/XiaoMi/pegasus/pull/380), by [acelyc111](https://github.com/acelyc111)
It received 6 comments.

 - - - 
# PULL REQUESTS
Last week, 5 pull requests were created, updated or merged.
## MERGED PULL REQUEST
Last week, 5 pull requests were merged.
:purple_heart: #382 [fix: improve shell get_app_stat and ls_nodes & fix scripts](https://github.com/XiaoMi/pegasus/pull/382), by [qinzuoyan](https://github.com/qinzuoyan)
:purple_heart: #381 [GEO: add GEOADD API for redis proxy to support full GEO function](https://github.com/XiaoMi/pegasus/pull/381), by [acelyc111](https://github.com/acelyc111)
:purple_heart: #380 [compile/link: fix dependency error of curl](https://github.com/XiaoMi/pegasus/pull/380), by [acelyc111](https://github.com/acelyc111)
:purple_heart: #378 [proxy: do some refactor](https://github.com/XiaoMi/pegasus/pull/378), by [acelyc111](https://github.com/acelyc111)
:purple_heart: #372 [benchmark: remove dependency of unexport rocksdb objects](https://github.com/XiaoMi/pegasus/pull/372), by [levy5307](https://github.com/levy5307)

 - - - 
# COMMITS
Last week there were 5 commits.
:hammer_and_wrench: [fix: improve shell get_app_stat and ls_nodes & fix scripts (#382)](https://github.com/XiaoMi/pegasus/commit/695ad274a3c8b5d64272ad0324c5fb4b0486513b) by [qinzuoyan](https://github.com/qinzuoyan)
:hammer_and_wrench: [GEO: add GEOADD API for redis proxy to support full GEO function (#381)](https://github.com/XiaoMi/pegasus/commit/b0bd9f52acdfa37a0c1d9c1e3b70373305c60276) by [acelyc111](https://github.com/acelyc111)
:hammer_and_wrench: [benchmark: remove dependency of unexport rocksdb objects (#372)](https://github.com/XiaoMi/pegasus/commit/7208249303c9df97a7db4ccbb939c56c4cbd3d38) by [levy5307](https://github.com/levy5307)
:hammer_and_wrench: [proxy: do some refactor (#378)](https://github.com/XiaoMi/pegasus/commit/006b9da2e5785220b51046b0abb5b285bdfc24af) by [acelyc111](https://github.com/acelyc111)
:hammer_and_wrench: [compile: fix dependency error of curl (#380)](https://github.com/XiaoMi/pegasus/commit/4c38fc974610fbf92c5337b272becaa58d03c4b6) by [acelyc111](https://github.com/acelyc111)

 - - - 
# CONTRIBUTORS
Last week there were 3 contributors.
:bust_in_silhouette: [qinzuoyan](https://github.com/qinzuoyan)
:bust_in_silhouette: [acelyc111](https://github.com/acelyc111)
:bust_in_silhouette: [levy5307](https://github.com/levy5307)

 - - - 
# STARGAZERS
Last week there were 7 stagazers.
:star: [chuanlei](https://github.com/chuanlei)
:star: [kindlejiang](https://github.com/kindlejiang)
:star: [hawkingrei](https://github.com/hawkingrei)
:star: [MrHeer](https://github.com/MrHeer)
:star: [zTgx](https://github.com/zTgx)
:star: [wuleying](https://github.com/wuleying)
:star: [zhangsean](https://github.com/zhangsean)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there was 1 release.
:rocket: [v1.11.6 v1.11.6](https://github.com/XiaoMi/pegasus/releases/tag/v1.11.6)

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/383/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/387,https://api.github.com/repos/apache/incubator-pegasus/issues/387,incubator-pegasus,486962165,387,Pegasus Coredump at boost.asio epoll_reactor 1.11.5,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,OPEN,2019-08-29T13:34:15Z,2022-01-07T09:36:25Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
If possible, provide a recipe for reproducing the error.

Normal bootstrap. c3srv-xiaoai, on replica-server (2019/08/29).

2. What did you expect to see?

No coredump.

3. What did you see instead?

Coredump Stack:

```
(gdb) bt
#0  0x00007fcc302b7ad6 in boost::asio::detail::epoll_reactor::start_op (this=0x309a6e0, op_type=op_type@entry=1, descriptor=1352, descriptor_data=@0x583560e88: 0x0, 
    op=op@entry=0x6cdf6e900, is_continuation=is_continuation@entry=true, allow_speculative=allow_speculative@entry=true)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/impl/epoll_reactor.ipp:219
#1  0x00007fcc302bca55 in start_op (noop=false, is_non_blocking=true, is_continuation=true, op=0x6cdf6e900, op_type=1, impl=..., this=0x309c418)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/impl/reactive_socket_service_base.ipp:214
#2  async_send<boost::asio::detail::consuming_buffers<boost::asio::const_buffer, std::vector<boost::asio::const_buffer> >, boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::send(uint64_t)::__lambda3> > (flags=0, 
    handler=..., buffers=..., impl=..., this=0x309c418) at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/reactive_socket_service_base.hpp:216
#3  async_send<boost::asio::detail::consuming_buffers<boost::asio::const_buffer, std::vector<boost::asio::const_buffer> >, boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::send(uint64_t)::__lambda3> > (flags=0, 
    handler=<unknown type in /home/work/app/pegasus/c3srv-xiaoai/replica/package/bin/libdsn_replica_server.so, CU 0x34e2bd9, DIE 0x355c99c>, buffers=..., impl=..., this=0x309c3f0)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/stream_socket_service.hpp:330
#4  async_write_some<boost::asio::detail::consuming_buffers<boost::asio::const_buffer, std::vector<boost::asio::const_buffer> >, boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::send(uint64_t)::__lambda3> > (
    handler=<unknown type in /home/work/app/pegasus/c3srv-xiaoai/replica/package/bin/libdsn_replica_server.so, CU 0x34e2bd9, DIE 0x355bab4>, buffers=..., this=0x583560e80)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/basic_stream_socket.hpp:732
#5  boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp, boost::asio::stream_socket_service<boost::asio::ip::tcp> >, std::vector<boost::asio::const_buffer, std::allocator<boost::asio::const_buffer> >, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::send(uint64_t)::__lambda3>::operator()(const boost::system::error_code &, std::size_t, int) (this=0x7fcc185cf030, ec=..., bytes_transferred=<optimized out>, start=<optimized out>)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/impl/write.hpp:181
#6  0x00007fcc302bd39d in operator() (this=0x7fcc185cf030) at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/bind_handler.hpp:127
#7  asio_handler_invoke<boost::asio::detail::binder2<boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::send(uint64_t)::__lambda3>, boost::system::error_code, long unsigned int> > (function=...)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/handler_invoke_hook.hpp:69
#8  invoke<boost::asio::detail::binder2<boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::send(uint64_t)::__lambda3>, boost::system::error_code, long unsigned int>, dsn::tools::asio_rpc_session::send(uint64_t)::__lambda3> (context=..., function=...) at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/handler_invoke_helpers.hpp:37
#9  asio_handler_invoke<boost::asio::detail::binder2<boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::send(uint64_t)::__lambda3>, boost::system::error_code, long unsigned int>, boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::send(uint64_t)::__lambda3> (
    this_handler=<optimized out>, function=...) at /home/wutao1/boost_1_58_0/output/include/boost/asio/impl/write.hpp:565
#10 invoke<boost::asio::detail::binder2<boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::send(uint64_t)::__lambda3>, boost::system::error_code, long unsigned int>, boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp>, std::vector<boost::asio::const_buffer>, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::send(uint64_t)::__lambda3> > (
    context=..., function=...) at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/handler_invoke_helpers.hpp:37
#11 boost::asio::detail::reactive_socket_send_op<boost::asio::detail::consuming_buffers<boost::asio::const_buffer, std::vector<boost::asio::const_buffer, std::allocator<boost::asio::const_buffer> > >, boost::asio::detail::write_op<boost::asio::basic_stream_socket<boost::asio::ip::tcp, boost::asio::stream_socket_service<boost::asio::ip::tcp> >, std::vector<boost::asio::const_buffer, std::allocator<boost::asio::const_buffer> >, boost::asio::detail::transfer_all_t, dsn::tools::asio_rpc_session::send(uint64_t)::__lambda3> >::do_complete(boost::asio::detail::io_service_impl *, boost::asio::detail::operation *, const boost::system::error_code &, std::size_t) (owner=0x30b6620, base=<optimized out>)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/reactive_socket_send_op.hpp:107
#12 0x000000000074fec9 in complete (bytes_transferred=<optimized out>, ec=..., owner=..., this=<optimized out>)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/task_io_service_operation.hpp:38
---Type <return> to continue, or q <return> to quit---
#13 do_run_one (ec=..., this_thread=..., lock=..., this=0x30b6620) at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/impl/task_io_service.ipp:372
#14 boost::asio::detail::task_io_service::run (this=0x30b6620, ec=...) at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/impl/task_io_service.ipp:149
#15 0x00007fcc302b2cc6 in run (this=<optimized out>, ec=...) at /home/wutao1/boost_1_58_0/output/include/boost/asio/impl/io_service.ipp:66
#16 operator() (__closure=0x30958b0) at /home/wutao1/pegasus-release/rdsn/src/core/tools/common/asio_net_provider.cpp:73
#17 _M_invoke<> (this=0x30958b0) at /home/wutao1/app/include/c++/4.8.2/functional:1732
#18 operator() (this=0x30958b0) at /home/wutao1/app/include/c++/4.8.2/functional:1720
#19 std::thread::_Impl<std::_Bind_simple<dsn::tools::asio_network_provider::start(dsn::rpc_channel, int, bool)::__lambda2()> >::_M_run(void) (this=0x3095898)
    at /home/wutao1/app/include/c++/4.8.2/thread:115
#20 0x00007fcc2d035600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#21 0x00007fcc2dca8dc5 in start_thread () from /lib64/libpthread.so.0
#22 0x00007fcc2c79f73d in clone () from /lib64/libc.so.6
```

4. What version of Pegasus are you using?

Pegasus-server 1.11.5","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/387/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/387,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUyNjE4OTQ0OQ==,incubator-pegasus,526189449,387,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-08-29T13:39:08Z,2019-08-29T13:39:08Z,"The coredump point (boost/asio/detail/impl/epoll_reactor.ipp:219) is:

```

void epoll_reactor::start_op(int op_type, socket_type descriptor,
    epoll_reactor::per_descriptor_data& descriptor_data, reactor_op* op,
    bool is_continuation, bool allow_speculative)
{
  if (!descriptor_data)
  {
    op->ec_ = boost::asio::error::bad_descriptor;
    post_immediate_completion(op, is_continuation);
    return;
  }

  mutex::scoped_lock descriptor_lock(descriptor_data->mutex_);

  if (descriptor_data->shutdown_)
  {
    post_immediate_completion(op, is_continuation);
    return;
  }
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUyNjE4OTQ0OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/387,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUyNjQ0MTgxMw==,incubator-pegasus,526441813,387,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-08-30T03:29:15Z,2019-08-30T03:29:15Z,我怀疑锁使用得不对。现在是用的读写锁，在调用`async_read_some()`和`async_write()`时都是用的read_lock。如果用普通的排它锁是不是就不会有问题了？当然，用排它锁可能影响性能，但是这个需要测试才知道。,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUyNjQ0MTgxMw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/387,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU2Mzg5ODQ4Mw==,incubator-pegasus,563898483,387,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2019-12-10T07:12:14Z,2019-12-10T07:12:14Z,"http://wen-studio.blogspot.com/2018/12/ （need vpn）
https://svn.boost.org/trac10/ticket/7611#no2
看看是否有参考性？
@neverchanje ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU2Mzg5ODQ4Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/387,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48Bw0B,incubator-pegasus,1007095041,387,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2022-01-07T02:40:30Z,2022-01-07T02:40:30Z,"这里应该不能简单地用锁来防止竞态条件
asio_rpc_session.cpp:86
```
  utils::auto_read_lock socket_guard(_socket_lock);

    _socket->async_read_some(
        boost::asio::buffer(ptr, remaining),
        [this](boost::system::error_code ec, std::size_t length) {
            if (!!ec) {
                if (ec == boost::asio::error::make_error_code(boost::asio::error::eof)) {
                    ddebug(""asio read from %s failed: %s"",
                           _remote_addr.to_string(),
                           ec.message().c_str());
                } else {
                    derror(""asio read from %s failed: %s"",
                           _remote_addr.to_string(),
                           ec.message().c_str());
                }
                on_failure();
            } else {
```
async_read_some 执行的时候是在 io_service 中排队执行的，但是有同步的方法会立即关闭 socket。
```
void asio_rpc_session::close()
{
    utils::auto_write_lock socket_guard(_socket_lock);

    boost::system::error_code ec;
    _socket->shutdown(boost::asio::socket_base::shutdown_type::shutdown_both, ec);
    if (ec)
        dwarn(""asio socket shutdown failed, error = %s"", ec.message().c_str());
    _socket->close(ec);
    if (ec)
        dwarn(""asio socket close failed, error = %s"", ec.message().c_str());
}
```
由于我们的 io_service 是多线程环境下运行的，所以会产生 A 线程关闭了 socket 之后，B线程又尝试去读取，引发 issue 中所提及的 coredump 问题。

一个最简单的方法是，把 worker_count 设置为 1，单线程运行 io_service。但需要测试性能","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48Bw0B/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/387,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48CaUk,incubator-pegasus,1007265060,387,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2022-01-07T09:36:25Z,2022-01-07T09:36:25Z,"> 这里应该不能简单地用锁来防止竞态条件 asio_rpc_session.cpp:86
> 
> ```
>   utils::auto_read_lock socket_guard(_socket_lock);
> 
>     _socket->async_read_some(
>         boost::asio::buffer(ptr, remaining),
>         [this](boost::system::error_code ec, std::size_t length) {
>             if (!!ec) {
>                 if (ec == boost::asio::error::make_error_code(boost::asio::error::eof)) {
>                     ddebug(""asio read from %s failed: %s"",
>                            _remote_addr.to_string(),
>                            ec.message().c_str());
>                 } else {
>                     derror(""asio read from %s failed: %s"",
>                            _remote_addr.to_string(),
>                            ec.message().c_str());
>                 }
>                 on_failure();
>             } else {
> ```
> 
> async_read_some 执行的时候是在 io_service 中排队执行的，但是有同步的方法会立即关闭 socket。
> 
> ```
> void asio_rpc_session::close()
> {
>     utils::auto_write_lock socket_guard(_socket_lock);
> 
>     boost::system::error_code ec;
>     _socket->shutdown(boost::asio::socket_base::shutdown_type::shutdown_both, ec);
>     if (ec)
>         dwarn(""asio socket shutdown failed, error = %s"", ec.message().c_str());
>     _socket->close(ec);
>     if (ec)
>         dwarn(""asio socket close failed, error = %s"", ec.message().c_str());
> }
> ```
> 
> 由于我们的 io_service 是多线程环境下运行的，所以会产生 A 线程关闭了 socket 之后，B线程又尝试去读取，引发 issue 中所提及的 coredump 问题。
> 
> 一个最简单的方法是，把 worker_count 设置为 1，单线程运行 io_service。但需要测试性能

性能测试结果不好，asio 线程把单核负载打满了都没法撑起原来 4 线程 asio 的流量","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48CaUk/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/388,https://api.github.com/repos/apache/incubator-pegasus/issues/388,incubator-pegasus,487854483,388,"Weekly Digest (25 August, 2019 - 1 September, 2019)",weekly-digest,,,,CLOSED,2019-09-01T07:59:21Z,2019-10-25T09:44:17Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 4 issues were created.
Of these, 3 issues have been closed and 1 issues are still open.
## OPEN ISSUES
:green_heart: #387 [Pegasus Coredump at boost.asio epoll_reactor 1.11.5](https://github.com/XiaoMi/pegasus/issues/387), by [neverchanje](https://github.com/neverchanje)
## CLOSED ISSUES
:heart: #386 [build: remove MY_PROJ_INC_PATH](https://github.com/XiaoMi/pegasus/pull/386), by [vagetablechicken](https://github.com/vagetablechicken)
:heart: #385 [fix: unit tests & travis.sh exit with err](https://github.com/XiaoMi/pegasus/pull/385), by [vagetablechicken](https://github.com/vagetablechicken)
:heart: #384 [fix: unit test failed and update rdsn](https://github.com/XiaoMi/pegasus/pull/384), by [hycdong](https://github.com/hycdong)
## LIKED ISSUE
:+1: #384 [fix: unit test failed and update rdsn](https://github.com/XiaoMi/pegasus/pull/384), by [hycdong](https://github.com/hycdong)
It received :+1: x1, :smile: x0, :tada: x0 and :heart: x0.
## NOISY ISSUE
:speaker: #385 [fix: unit tests & travis.sh exit with err](https://github.com/XiaoMi/pegasus/pull/385), by [vagetablechicken](https://github.com/vagetablechicken)
It received 2 comments.

 - - - 
# PULL REQUESTS
Last week, 3 pull requests were created, updated or merged.
## MERGED PULL REQUEST
Last week, 3 pull requests were merged.
:purple_heart: #386 [build: remove MY_PROJ_INC_PATH](https://github.com/XiaoMi/pegasus/pull/386), by [vagetablechicken](https://github.com/vagetablechicken)
:purple_heart: #385 [fix: unit tests & travis.sh exit with err](https://github.com/XiaoMi/pegasus/pull/385), by [vagetablechicken](https://github.com/vagetablechicken)
:purple_heart: #384 [fix: unit test failed and update rdsn](https://github.com/XiaoMi/pegasus/pull/384), by [hycdong](https://github.com/hycdong)

 - - - 
# COMMITS
Last week there were 3 commits.
:hammer_and_wrench: [build: remove MY_PROJ_INC_PATH (#386)](https://github.com/XiaoMi/pegasus/commit/6dfd6e9abb57974233bda04caac8b3fe6229ba98) by [vagetablechicken](https://github.com/vagetablechicken)
:hammer_and_wrench: [fix: unit tests & travis.sh exit with err (#385)](https://github.com/XiaoMi/pegasus/commit/af174a4d039b2fd7a5b8897c3ae349b8116b7a92) by [vagetablechicken](https://github.com/vagetablechicken)
:hammer_and_wrench: [fix: unit test failed and update rdsn (#384)](https://github.com/XiaoMi/pegasus/commit/dea2bd9b4e3b95a93ffe5d37c7c2b2129d2392f3) by [hycdong](https://github.com/hycdong)

 - - - 
# CONTRIBUTORS
Last week there were 2 contributors.
:bust_in_silhouette: [vagetablechicken](https://github.com/vagetablechicken)
:bust_in_silhouette: [hycdong](https://github.com/hycdong)

 - - - 
# STARGAZERS
Last week there were 3 stagazers.
:star: [Haze-Lan](https://github.com/Haze-Lan)
:star: [Constlin](https://github.com/Constlin)
:star: [IngrownMink4](https://github.com/IngrownMink4)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/388/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/390,https://api.github.com/repos/apache/incubator-pegasus/issues/390,incubator-pegasus,490609988,390,Coredump in complete_aio,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,OPEN,2019-09-07T08:34:53Z,2019-09-08T11:18:50Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
If possible, provide a recipe for reproducing the error.

Test bootstrap, after a previous cleanup. Though this is a new bootstrap,
there was still some log files not cleared. The pegasus-server replayed those
log files during initialization, which caused this core dump.

```
D2019-09-07 16:13:18.846 (1567843998846027369 1e500) replica.default0.0000e4a500010001: mutation_log.cpp:864:replay(): start to replay mutation log /home/work/app/pegasus/c4tst-performance/replica/replica/slog/log.3356.462664068974, offset = [462664068974, 462902133466), size = 238064492
D2019-09-07 16:13:19.873 (1567843999873498904 1e500) replica.default0.0000e4a500010001: mutation_log.cpp:914:replay(): finish to replay mutation log /home/work/app/pegasus/c4tst-performance/replica/replica/slog/log.3356.462664068974, err = ERR_HANDLE_EOF
D2019-09-07 16:13:19.873 (1567843999873593538 1e500) replica.default0.0000e4a500010001: mutation_log.cpp:864:replay(): start to replay mutation log /home/work/app/pegasus/c4tst-performance/replica/replica/slog/log.3357.462902133466, offset = [462902133466, 463173023339), size = 270889873
D2019-09-07 16:13:21.035 (1567844001035586779 1e500) replica.default0.0000e4a500010001: mutation_log.cpp:914:replay(): finish to replay mutation log /home/work/app/pegasus/c4tst-performance/replica/replica/slog/log.3357.462902133466, err = ERR_HANDLE_EOF
D2019-09-07 16:13:21.035 (1567844001035664808 1e500) replica.default0.0000e4a500010001: mutation_log.cpp:864:replay(): start to replay mutation log /home/work/app/pegasus/c4tst-performance/replica/replica/slog/log.3358.463173023339, offset = [463173023339, 463345410168), size = 172386829
D2019-09-07 16:13:21.788 (1567844001788814689 1e500) replica.default0.0000e4a500010001: mutation_log.cpp:914:replay(): finish to replay mutation log /home/work/app/pegasus/c4tst-performance/replica/replica/slog/log.3358.463173023339, err = ERR_HANDLE_EOF
D2019-09-07 16:13:21.789 (1567844001789227538 1e500) replica.default0.0000e4a500010001: mutation_log.cpp:864:replay(): start to replay mutation log /home/work/app/pegasus/c4tst-performance/replica/replica/slog/log.3359.463345410168, offset = [463345410168, 463526005305), size = 180595137
D2019-09-07 16:13:22.568 (1567844002568888601 1e500) replica.default0.0000e4a500010001: mutation_log.cpp:914:replay(): finish to replay mutation log /home/work/app/pegasus/c4tst-performance/replica/replica/slog/log.3359.463345410168, err = ERR_HANDLE_EOF
D2019-09-07 16:13:22.568 (1567844002568946492 1e500) replica.default0.0000e4a500010001: mutation_log.cpp:864:replay(): start to replay mutation log /home/work/app/pegasus/c4tst-performance/replica/replica/slog/log.3360.463526005305, offset = [463526005305, 463665560054), size = 139554749
```

2. What did you expect to see?

No coredump.

3. What did you see instead?

```
#0  load (__m=std::memory_order_relaxed, this=0x3d343e3c7b553a69) at /home/wutao1/app/include/c++/4.8.2/bits/atomic_base.h:496
#1  signal (this=0x3d343e3c7b553a69) at /home/wutao1/pegasus/rdsn/include/dsn/utility/hpc_locks/autoresetevent.h:33
#2  notify (this=0x3d343e3c7b553a69) at /home/wutao1/pegasus/rdsn/include/dsn/utility/synchronize.h:99
#3  dsn::tools::native_linux_aio_provider::complete_aio (this=this@entry=0x36f6f60, io=0xe462728, bytes=2145385340, err=<optimized out>)
    at /home/wutao1/pegasus/rdsn/src/core/tools/common/native_aio_provider.linux.cpp:160
#4  0x00007f9d5ae50d01 in dsn::tools::native_linux_aio_provider::get_event (this=0x36f6f60) at /home/wutao1/pegasus/rdsn/src/core/tools/common/native_aio_provider.linux.cpp:134
#5  0x00007f9d577e5600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#6  0x00007f9d58ddedc5 in start_thread () from /lib64/libpthread.so.0
#7  0x00007f9d56f4f73d in clone () from /lib64/libc.so.6
```

error logs
```
tcmalloc: large alloc 1502150656 bytes == 0x6dfcc000 @  0x7f9d580f1e07 0x7f9d581141a9 0x5b5d32 0x7f9d5ad89bde 0x7f9d5ad8a2b2 0x7f9d5ad8a50e 0x7f9d5ac68b8d 0x7f9d5ac5ec76 0x7f9d5ac5ef3c 0x7f9d5ac5f47f 0x7f9d5ac60b52 0x7f9d5acd22a2 0x7f9d5acf3bfc 0x5da801 0x7f9d5adbb1e2 0x7f9d5addea7b 0x7f9d5adc6859 0x7f9d5adda50d 0x7f9d5adda6d9 0x7f9d577e5600
tcmalloc: large alloc 1674551296 bytes == 0x1dbd6000 @  0x7f9d580f1e07 0x7f9d581141a9 0x5b5d32 0x7f9d5ad89bde 0x7f9d5ad8a2b2 0x7f9d5ad8a50e 0x7f9d5ac68b8d 0x7f9d5ac5ec76 0x7f9d5ac5ef3c 0x7f9d5ac5f47f 0x7f9d5ac60b52 0x7f9d5acd22a2 0x7f9d5acf3bfc 0x5da801 0x7f9d5adbb1e2 0x7f9d5addea7b 0x7f9d5adc6859 0x7f9d5adda50d 0x7f9d5adda6d9 0x7f9d577e5600
task_spec.cpp:94:register_task_code():overwrite default thread pool for task RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX from THREAD_POOL_META_SERVER to THREAD_POOL_DEFAULT
task_spec.cpp:94:register_task_code():overwrite default thread pool for task RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX_ACK from THREAD_POOL_META_SERVER to THREAD_POOL_DEFAULT
main.cpp:92:main():pegasus server starting, pid(122326), version($Version: Pegasus Server 1.12.SNAPSHOT (38dccf6d774751a1f5bd67d6572c2156412979a5) Release, built with rDSN 1.0.0 (f631ed925c291ac40570fdcbd447655650a607cd), built by gcc 4.8.2, built on c3-hadoop-build03.bj, built at Sep  5 2019 16:10:19 $)
got signal id: 11
```

4. What version of Pegasus are you using?

Pegasus Server 1.12.SNAPSHOT (38dccf6d774751a1f5bd67d6572c2156412979a5)  release
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/390/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/390,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUyOTA4OTgwNg==,incubator-pegasus,529089806,390,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-09-07T09:14:18Z,2019-09-07T09:14:18Z,"## Code point

The bug is caused by inaccessible address 0x3d343e3c7b553a69, which is
`aio->evt` in `dsn::tools::native_linux_aio_provider::complete_aio`, which indicates memory corruption of `linux_disk_aio_context`.

```cpp
void native_linux_aio_provider::complete_aio(struct iocb *io, int bytes, int err)
{
    linux_disk_aio_context *aio = CONTAINING_RECORD(io, linux_disk_aio_context, cb);
    error_code ec;
    if (err != 0) {
        derror(""aio error, err = %s"", strerror(err));
        ec = ERR_FILE_OPERATION_FAILED;
    } else {
        ec = bytes > 0 ? ERR_OK : ERR_HANDLE_EOF;
    }

    if (!aio->evt) {
        aio_task *aio_ptr(aio->tsk);
        aio->this_->complete_io(aio_ptr, ec, bytes);
    } else {
        aio->err = ec; // ec = 0
        aio->bytes = bytes; // bytes = 2,145,385,340, nearly exceeds int max = 2,147,483,647
        aio->evt->notify(); // coredump point, evt is inaccessible
    }
}
```

Memory dump of `linux_disk_aio_context`:

```
$6 = {
  <dsn::disk_aio> = {
    file = 0x5c3d225c76292532, 
    buffer = 0x316e3a3271492e22, 
    support_write_vec = 92, 
    write_buffer_vec = 0x212f423a774e3673, 
    buffer_size = 1161178450, 
    file_offset = 5346938506896954679, 
    type = (dsn::AIO_Read | unknown: 808398908), 
    engine = 0x6a2530363730614f, 
    file_object = 0x422563473b7c3f3c
  }, 
  members of dsn::tools::native_linux_aio_provider::linux_disk_aio_context: 
  cb = {
    data = 0x343b523e3a2c242f, 
    key = 1178021723, 
    __pad2 = 1029324393, 
    aio_lio_opcode = 19250, 
    aio_reqprio = 13949, 
    aio_fildes = 892874815, 
    u = {
      c = {
        buf = 0x3d24242a39482b68, 
        nbytes = 5278068530193660960, 
        offset = 3475146331730683773, 
        __pad3 = 8806003118902164527, 
        flags = 913847603, 
        resfd = 1546202665
      }, 
      v = {
        vec = 0x3d24242a39482b68, 
        nr = 576486432, 
        offset = 3475146331730683773
      }, 
      poll = {
        events = 961031016, 
        __pad1 = 1025778730
      }, 
      saddr = {
        addr = 0x3d24242a39482b68, 
        len = 576486432
      }
    }
  }, 
  tsk = 0x3353212d5223755c, 
  this_ = 0x472937503a7c3c31, 
  evt = 0x3d343e3c7b553a69, 
  err = {
    _internal_code = 0
  }, 
  bytes = 2145385340
}
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDUyOTA4OTgwNg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/391,https://api.github.com/repos/apache/incubator-pegasus/issues/391,incubator-pegasus,490723701,391,"Weekly Digest (1 September, 2019 - 8 September, 2019)",weekly-digest,,,,CLOSED,2019-09-08T07:59:22Z,2019-10-25T09:44:45Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 2 issues were created.
Of these, 1 issues have been closed and 1 issues are still open.
## OPEN ISSUES
:green_heart: #390 [Coredump in complete_aio](https://github.com/XiaoMi/pegasus/issues/390), by [neverchanje](https://github.com/neverchanje)
## CLOSED ISSUES
:heart: #389 [build: pack boost.regex with binaries](https://github.com/XiaoMi/pegasus/pull/389), by [neverchanje](https://github.com/neverchanje)
## NOISY ISSUE
:speaker: #390 [Coredump in complete_aio](https://github.com/XiaoMi/pegasus/issues/390), by [neverchanje](https://github.com/neverchanje)
It received 1 comments.

 - - - 
# PULL REQUESTS
Last week, 1 pull request was created, updated or merged.
## MERGED PULL REQUEST
Last week, 1 pull request was merged.
:purple_heart: #389 [build: pack boost.regex with binaries](https://github.com/XiaoMi/pegasus/pull/389), by [neverchanje](https://github.com/neverchanje)

 - - - 
# COMMITS
Last week there was 1 commit.
:hammer_and_wrench: [build: pack boost.regex with binaries (#389)](https://github.com/XiaoMi/pegasus/commit/38dccf6d774751a1f5bd67d6572c2156412979a5) by [neverchanje](https://github.com/neverchanje)

 - - - 
# CONTRIBUTORS
Last week there was 1 contributor.
:bust_in_silhouette: [neverchanje](https://github.com/neverchanje)

 - - - 
# STARGAZERS
Last week there were 4 stagazers.
:star: [Tomilla](https://github.com/Tomilla)
:star: [narutoPro](https://github.com/narutoPro)
:star: [jaogoy](https://github.com/jaogoy)
:star: [magicsilence](https://github.com/magicsilence)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/391/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/392,https://api.github.com/repos/apache/incubator-pegasus/issues/392,incubator-pegasus,491062528,392,info collector can't get reply from metaserver,levy5307,22141103,赵立伟,zlw5307@163.com,CLOSED,2019-09-09T12:21:44Z,2019-12-05T10:49:32Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
If possible, provide a recipe for reproducing the error.
run onebox with info collector server
command: ./run.sh start_onebox -c
2. What did you expect to see?
The Info collector server can get reply from meta server
3. What did you see instead?
The info collector server can't get reply after it send list_apps(list_nodes and so no) to meta server. It is blocked waiting for the reply of meta server.
4. What version of Pegasus are you using?
master branch, commit 38dccf6d774751a1f5bd67d6572c2156412979a5 ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/392/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/393,https://api.github.com/repos/apache/incubator-pegasus/issues/393,incubator-pegasus,493707868,393,"Weekly Digest (8 September, 2019 - 15 September, 2019)",weekly-digest,,,,CLOSED,2019-09-15T07:59:22Z,2019-10-25T09:44:27Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 1 issue was created.
It is still open.
## OPEN ISSUES
:green_heart: #392 [info collector can't get reply from metaserver](https://github.com/XiaoMi/pegasus/issues/392), by [levy5307](https://github.com/levy5307)

 - - - 
# PULL REQUESTS
Last week, no pull requests were created, updated or merged.

 - - - 
# COMMITS
Last week there were no commits.

 - - - 
# CONTRIBUTORS
Last week there were no contributors.

 - - - 
# STARGAZERS
Last week there were 3 stagazers.
:star: [lee87902407](https://github.com/lee87902407)
:star: [Renkai](https://github.com/Renkai)
:star: [xjj59307](https://github.com/xjj59307)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/393/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/396,https://api.github.com/repos/apache/incubator-pegasus/issues/396,incubator-pegasus,496746381,396,"Weekly Digest (15 September, 2019 - 22 September, 2019)",weekly-digest,,,,CLOSED,2019-09-22T07:59:22Z,2019-10-25T09:44:33Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 2 issues were created.
Of these, 2 issues have been closed and 0 issues are still open.
## CLOSED ISSUES
:heart: #395 [feat(shell): add debugging commands for hex and escaped-string conversion](https://github.com/XiaoMi/pegasus/pull/395), by [neverchanje](https://github.com/neverchanje)
:heart: #394 [build(cmake): cleanup lib links](https://github.com/XiaoMi/pegasus/pull/394), by [vagetablechicken](https://github.com/vagetablechicken)
## NOISY ISSUE
:speaker: #394 [build(cmake): cleanup lib links](https://github.com/XiaoMi/pegasus/pull/394), by [vagetablechicken](https://github.com/vagetablechicken)
It received 2 comments.

 - - - 
# PULL REQUESTS
Last week, 2 pull requests were created, updated or merged.
## MERGED PULL REQUEST
Last week, 2 pull requests were merged.
:purple_heart: #395 [feat(shell): add debugging commands for hex and escaped-string conversion](https://github.com/XiaoMi/pegasus/pull/395), by [neverchanje](https://github.com/neverchanje)
:purple_heart: #394 [build(cmake): cleanup lib links](https://github.com/XiaoMi/pegasus/pull/394), by [vagetablechicken](https://github.com/vagetablechicken)

 - - - 
# COMMITS
Last week there were 2 commits.
:hammer_and_wrench: [feat(shell): add debugging commands for hex and escaped-string conversion (#395)](https://github.com/XiaoMi/pegasus/commit/e5fcffd5d94bf72c647d724bfc30a18f05edfb10) by [neverchanje](https://github.com/neverchanje)
:hammer_and_wrench: [build(cmake): cleanup lib links (#394)](https://github.com/XiaoMi/pegasus/commit/da4bc6799430f9579b8b4c181bb40d645b499025) by [vagetablechicken](https://github.com/vagetablechicken)

 - - - 
# CONTRIBUTORS
Last week there were 2 contributors.
:bust_in_silhouette: [neverchanje](https://github.com/neverchanje)
:bust_in_silhouette: [vagetablechicken](https://github.com/vagetablechicken)

 - - - 
# STARGAZERS
Last week there were 3 stagazers.
:star: [imet](https://github.com/imet)
:star: [kiminh](https://github.com/kiminh)
:star: [suteelng](https://github.com/suteelng)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/396/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/401,https://api.github.com/repos/apache/incubator-pegasus/issues/401,incubator-pegasus,499861591,401,"Weekly Digest (22 September, 2019 - 29 September, 2019)",weekly-digest,,,,CLOSED,2019-09-29T05:38:12Z,2019-10-25T09:44:52Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 4 issues were created.
Of these, 2 issues have been closed and 2 issues are still open.
## OPEN ISSUES
:green_heart: #400 [feat: table level slow query](https://github.com/XiaoMi/pegasus/pull/400), by [levy5307](https://github.com/levy5307)
:green_heart: #399 [feat(dup): implement pegasus_mutation_duplicator](https://github.com/XiaoMi/pegasus/pull/399), by [neverchanje](https://github.com/neverchanje)
## CLOSED ISSUES
:heart: #398 [refactor: change the way to call replication_ddl_client::set_app_envs](https://github.com/XiaoMi/pegasus/pull/398), by [levy5307](https://github.com/levy5307)
:heart: #397 [build(cmake): fix curl link of prometheus](https://github.com/XiaoMi/pegasus/pull/397), by [vagetablechicken](https://github.com/vagetablechicken)
## NOISY ISSUE
:speaker: #399 [feat(dup): implement pegasus_mutation_duplicator](https://github.com/XiaoMi/pegasus/pull/399), by [neverchanje](https://github.com/neverchanje)
It received 2 comments.

 - - - 
# PULL REQUESTS
Last week, 4 pull requests were created, updated or merged.
## UPDATED PULL REQUEST
Last week, 2 pull requests were updated.
:yellow_heart: #400 [feat: table level slow query](https://github.com/XiaoMi/pegasus/pull/400), by [levy5307](https://github.com/levy5307)
:yellow_heart: #399 [feat(dup): implement pegasus_mutation_duplicator](https://github.com/XiaoMi/pegasus/pull/399), by [neverchanje](https://github.com/neverchanje)
## MERGED PULL REQUEST
Last week, 2 pull requests were merged.
:purple_heart: #398 [refactor: change the way to call replication_ddl_client::set_app_envs](https://github.com/XiaoMi/pegasus/pull/398), by [levy5307](https://github.com/levy5307)
:purple_heart: #397 [build(cmake): fix curl link of prometheus](https://github.com/XiaoMi/pegasus/pull/397), by [vagetablechicken](https://github.com/vagetablechicken)

 - - - 
# COMMITS
Last week there were 2 commits.
:hammer_and_wrench: [build(cmake): fix curl link of prometheus (#397)](https://github.com/XiaoMi/pegasus/commit/1b482833057ab6bed7073b18442281942752c9af) by [vagetablechicken](https://github.com/vagetablechicken)
:hammer_and_wrench: [refactor: change the way to call replication_ddl_client::set_app_envs (#398)](https://github.com/XiaoMi/pegasus/commit/8fb00997878bb3658c25e58c95f073b1bff300bd) by [levy5307](https://github.com/levy5307)

 - - - 
# CONTRIBUTORS
Last week there were 2 contributors.
:bust_in_silhouette: [vagetablechicken](https://github.com/vagetablechicken)
:bust_in_silhouette: [levy5307](https://github.com/levy5307)

 - - - 
# STARGAZERS
Last week there were 5 stagazers.
:star: [2012-wangjiaqi](https://github.com/2012-wangjiaqi)
:star: [zeroggz](https://github.com/zeroggz)
:star: [jqsl2012](https://github.com/jqsl2012)
:star: [ke4nec](https://github.com/ke4nec)
:star: [Nobervem](https://github.com/Nobervem)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/401/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/402,https://api.github.com/repos/apache/incubator-pegasus/issues/402,incubator-pegasus,503055384,402,"Weekly Digest (29 September, 2019 - 6 October, 2019)",weekly-digest,,,,CLOSED,2019-10-06T05:38:12Z,2019-11-14T08:46:21Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week, no issues were created.

 - - - 
# PULL REQUESTS
Last week, 2 pull requests were created, updated or merged.
## UPDATED PULL REQUEST
Last week, 2 pull requests were updated.
:yellow_heart: #400 [feat: table level slow query](https://github.com/XiaoMi/pegasus/pull/400), by [levy5307](https://github.com/levy5307)
:yellow_heart: #399 [feat(dup): implement pegasus_mutation_duplicator](https://github.com/XiaoMi/pegasus/pull/399), by [neverchanje](https://github.com/neverchanje)

 - - - 
# COMMITS
Last week there were no commits.

 - - - 
# CONTRIBUTORS
Last week there were no contributors.

 - - - 
# STARGAZERS
Last week there was 1 stargazer.
:star: [GeorgeWang1994](https://github.com/GeorgeWang1994)
You are the star! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/402/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/403,https://api.github.com/repos/apache/incubator-pegasus/issues/403,incubator-pegasus,506278307,403,"Weekly Digest (6 October, 2019 - 13 October, 2019)",weekly-digest,,,,CLOSED,2019-10-13T05:38:13Z,2019-11-14T08:46:27Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week, no issues were created.

 - - - 
# PULL REQUESTS
Last week, 1 pull request was created, updated or merged.
## MERGED PULL REQUEST
Last week, 1 pull request was merged.
:purple_heart: #400 [feat: table level slow query](https://github.com/XiaoMi/pegasus/pull/400), by [levy5307](https://github.com/levy5307)

 - - - 
# COMMITS
Last week there was 1 commit.
:hammer_and_wrench: [feat: table level slow query (#400)](https://github.com/XiaoMi/pegasus/commit/92a55b2305de98e05c680fdecc200fd612dcb58c) by [levy5307](https://github.com/levy5307)

 - - - 
# CONTRIBUTORS
Last week there was 1 contributor.
:bust_in_silhouette: [levy5307](https://github.com/levy5307)

 - - - 
# STARGAZERS
Last week there were 3 stagazers.
:star: [jl0x61](https://github.com/jl0x61)
:star: [xdxiaodao](https://github.com/xdxiaodao)
:star: [yiqingGit](https://github.com/yiqingGit)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/403/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/405,https://api.github.com/repos/apache/incubator-pegasus/issues/405,incubator-pegasus,509549393,405,"Weekly Digest (13 October, 2019 - 20 October, 2019)",weekly-digest,,,,CLOSED,2019-10-20T05:38:13Z,2019-11-14T08:46:32Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 1 issue was created.
It is closed now.
## CLOSED ISSUES
:heart: #404 [fix: fix the bug of deadlock in info collector in onebox environment](https://github.com/XiaoMi/pegasus/pull/404), by [levy5307](https://github.com/levy5307)
## LIKED ISSUE
:+1: #404 [fix: fix the bug of deadlock in info collector in onebox environment](https://github.com/XiaoMi/pegasus/pull/404), by [levy5307](https://github.com/levy5307)
It received :+1: x2, :smile: x0, :tada: x0 and :heart: x0.
## NOISY ISSUE
:speaker: #404 [fix: fix the bug of deadlock in info collector in onebox environment](https://github.com/XiaoMi/pegasus/pull/404), by [levy5307](https://github.com/levy5307)
It received 8 comments.

 - - - 
# PULL REQUESTS
Last week, 2 pull requests were created, updated or merged.
## UPDATED PULL REQUEST
Last week, 1 pull request was updated.
:yellow_heart: #399 [feat(dup): implement pegasus_mutation_duplicator](https://github.com/XiaoMi/pegasus/pull/399), by [neverchanje](https://github.com/neverchanje)
## MERGED PULL REQUEST
Last week, 1 pull request was merged.
:purple_heart: #404 [fix: fix the bug of deadlock in info collector in onebox environment](https://github.com/XiaoMi/pegasus/pull/404), by [levy5307](https://github.com/levy5307)

 - - - 
# COMMITS
Last week there was 1 commit.
:hammer_and_wrench: [fix: fix the bug of deadlock in info collector in onebox environment (#404)](https://github.com/XiaoMi/pegasus/commit/2a1110532a9449afeac5185c8a4912bbde97ea7e) by [levy5307](https://github.com/levy5307)

 - - - 
# CONTRIBUTORS
Last week there was 1 contributor.
:bust_in_silhouette: [levy5307](https://github.com/levy5307)

 - - - 
# STARGAZERS
Last week there were 3 stagazers.
:star: [michaelzhanga](https://github.com/michaelzhanga)
:star: [SuperC25](https://github.com/SuperC25)
:star: [mysunnyshine](https://github.com/mysunnyshine)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/405/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/406,https://api.github.com/repos/apache/incubator-pegasus/issues/406,incubator-pegasus,510389283,406,add table level latency,levy5307,22141103,赵立伟,zlw5307@163.com,CLOSED,2019-10-22T02:04:27Z,2020-02-11T06:11:09Z,"**增加表级别延迟统计&表级别慢查询配置**
======================

****一、背景****
------------

目前pegasus的一个node中关于get和multi-get接口的latency perf-counter是基于表分片的。有时我们需要查询一个node上的表延迟，由于node中的分片有可能有多个，所以就会很杂乱，基本没什么使用价值。

另外，由于不同表的结构不同，有的表读写40KB，有的表读写4KB，其读写latency肯定不同，如果设置一个统一的慢查询时长会导致很多无效的日志，所以需要针对不同的表提供不同的慢查询配置。

****二、实现方案****
----------------------

### ****2.1 增加表级别延迟统计****

perf_counters单例中保存了当前node上的所有的perf counter，并维护了不同perf counter的引用计数。当某个perf counter引用计数变为0的时候将会被删除。

另外提供了创建新的perf counter的接口。在创建时，先判断该perf counter是否已经存在，如果已经存在，直接返回对应的perf counter指针；否则则创建一个新的。

#### (1) 数据结构

为replica增加一个unordered\_map<int, perf\_counter\_wrapper>类型成员变量\_counters\_table\_level\_latency，用于存储task\_code与perf\_counter的映射。其中task\_code包括RPC\_RRDB\_RRDB\_GET、RPC\_RRDB\_RRDB\_MULTI\_GET、RPC\_RRDB\_RRDB\_SET和RPC\_RRDB\_RRDB\_MULTI\_SET。

#### (2) 初始化

由于在replica_stub中创建replica的地方有多处，并最终都由会执行replica构造函数，所以选择在replica的构造函数中插入perf counter创建的逻辑实现。对于同一个app的不同replica，其对应的perf counter的name相同，以保证同一app的不同replica中使用同一组perf counter。

当新创建一个replica时，通过相应的perf counter name调用perf\_counters的创建接口。如上所述，如果该perf counter不存在，则创建并初始化一个新的perf\_counter；如果存在，说明该perf counter已经被其他的replica在初始化的时候创建好了，只需要获取其指针就可以了。

#### (3) 计数

当请求到来时，以get为例，在replica::on\_client\_read函数的\_app->on\_request前后分别记录请求开始时间和结束时间，相减便可以获取该次请求的latency, 计入到其perf counter中。

#### (4) 销毁

在replica的析构函数中，replica本地的所有perf\_counter\_wrapper将会被析构。当一个perf\_counter\_wrapper被析构时，对应的perf counter的计数会被减1。当其的引用计数变为0时，该perf counter将会被删除。

### ****2.2 表级别慢查询配置****

慢查询配置可以通过在表的环境变量中配置的方式来实现。命令如下：set\_app\_envs  replica.slow\_query\_threshold  {threshold_ms}

其中threshold_ms必须大于20ms。

当get/multi-get操作duration > threshold_ms时，将会写入慢查询日志。

****三、目标预期****
----------------

在不影响性能的前提下，实现各node的表级操作延迟统计(get/set/multi-get/muti-set)功能，以及表级别的慢查询配置功能。

但是目前无法将各node的表级perf counter进行汇总，因而无法实现集群的表级延迟统计。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/406/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/406,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0NDc3OTM3NQ==,incubator-pegasus,544779375,406,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-10-22T02:09:32Z,2019-10-22T02:09:32Z,get_latency_perf_counters 不是只有 get 才会记 latency，你这个名字有问题。如果说 get/set/multiset/multiget 都要记 latency，你这个设计是不是不行？你肯定要设计的有可扩展性，能够支持不同的RPC类型。,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0NDc3OTM3NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/406,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0NDc4MDAyOQ==,incubator-pegasus,544780029,406,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2019-10-22T02:13:31Z,2019-10-22T02:13:31Z,"> get_latency_perf_counters 不是只有 get 才会记 latency，你这个名字有问题。如果说 get/set/multiset/multiget 都要记 latency，你这个设计是不是不行？你肯定要设计的有可扩展性，能够支持不同的RPC类型。

不是get和multi-get这两个吗？还有set和multi-set？我理解的是get操作和set操作的latency不应该放在同一个perf counter里面吧","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0NDc4MDAyOQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/406,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0NDc4MDg0MQ==,incubator-pegasus,544780841,406,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2019-10-22T02:18:43Z,2019-10-22T02:18:43Z,"首先, 调整下格式成markdown吧","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0NDc4MDg0MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/407,https://api.github.com/repos/apache/incubator-pegasus/issues/407,incubator-pegasus,510486929,407,build question,mysunnyshine,24480430,,,CLOSED,2019-10-22T07:51:42Z,2019-10-31T05:34:01Z,"the error is showed in the picture
cmake 2.8.12
os:   Centos 7.2
![image](https://user-images.githubusercontent.com/24480430/67266772-92410900-f4e3-11e9-881b-11e54142002d.png)

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/407/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/407,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0NDg2MzQ3NQ==,incubator-pegasus,544863475,407,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-10-22T08:45:15Z,2019-10-22T08:45:15Z,"your cmake version is too old, use >=3.5.2.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0NDg2MzQ3NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/407,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0NTI0ODM4Ng==,incubator-pegasus,545248386,407,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2019-10-23T03:24:43Z,2019-10-23T03:24:43Z,"这是fds编译时没有找到gtest的lib，建议查一下[$TP_OUTPUT/lib](https://github.com/XiaoMi/rdsn/blob/master/thirdparty/build-thirdparty.sh#L213)里有没有gtest的lib。
thirdparty编译只要求cmake>=2.8，但我们的pegasus要求cmake>=3.5.2，还是需要你升级cmake","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0NTI0ODM4Ng==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/407,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0NTI1ODM3NQ==,incubator-pegasus,545258375,407,NA,mysunnyshine,24480430,,,NA,2019-10-23T04:17:30Z,2019-10-23T04:17:30Z,"> 这是fds编译时没有找到gtest的lib，建议查一下[$TP_OUTPUT/lib](https://github.com/XiaoMi/rdsn/blob/master/thirdparty/build-thirdparty.sh#L213)里有没有gtest的lib。
> thirdparty编译只要求cmake>=2.8，但我们的pegasus要求cmake>=3.5.2，还是需要你升级cmake

ok, I will try","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0NTI1ODM3NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/408,https://api.github.com/repos/apache/incubator-pegasus/issues/408,incubator-pegasus,511753627,408,periodic memory release results in increased latency,foreverneverer,23136769,Jiashuo,js982986555@live.com,CLOSED,2019-10-24T07:13:47Z,2019-12-05T10:51:10Z,"## Pegasus version
pegasus-server-1.11.6-9f4e5ae-glibc2.12-release
## Problem
periodic memory release results in increased latency
## Reason
When build and  run  rdsn，the macro `ENABLE_GPERF`  and  the variable `mem_release_enabled`default value is `ON`:
```
#ifdef DSN_ENABLE_GPERF
    if (_options.mem_release_enabled) {
        _mem_release_timer_task =
            tasking::enqueue_timer(LPC_MEM_RELEASE,
                                   &_tracker,
                                   []() {
                                       ddebug(""Memory release has started..."");
                                       ::MallocExtension::instance()->ReleaseFreeMemory();
                                       ddebug(""Memory release has ended..."");
                                   },
                                   std::chrono::milliseconds(_options.mem_release_interval_ms),
                                   0,
                                   std::chrono::milliseconds(_options.mem_release_interval_ms));
    }
#endif
```
but the strategies for freeing memory is unreasonable. for example, this time frees too much memory (5GB) at once  and results in the significant latency.
## Solution
Now the config.ini is not set the value about ""release memory"", we can only add the section and set the `mem_release_enabled` value is false.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/408/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/408,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU2MjA3NzMwNg==,incubator-pegasus,562077306,408,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-12-05T10:51:07Z,2019-12-05T10:51:07Z,This problem is resolved in [v1.12.1](https://github.com/XiaoMi/pegasus/releases/tag/v1.12.1). Refer to this PR: https://github.com/XiaoMi/rdsn/pull/343.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU2MjA3NzMwNg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/409,https://api.github.com/repos/apache/incubator-pegasus/issues/409,incubator-pegasus,512437884,409,Release 1.12.0,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2019-10-25T10:33:32Z,2019-11-21T01:10:49Z,"|       PR        |                                   TITLE                                    |
|-----------------|----------------------------------------------------------------------------|
| XiaoMi/rdsn#332 | fix: meta unnecessary assert                                               |
| XiaoMi/rdsn#290 | feat(http): support CPU profiling using gperf                              |
| XiaoMi/rdsn#323 | refactor: move aio tests out from service_api_c                            |
| XiaoMi/rdsn#326 | refactor: remove useless functions from binary_reader                      |
| XiaoMi/rdsn#327 | fix(coldbackup): delay clean request when chkpting                         |
| XiaoMi/rdsn#321 | feat(http): add http interface for get_app_envs                            |
| XiaoMi/rdsn#324 | refactor: move replay related codes to mutation_log_replay                 |
| XiaoMi/rdsn#317 | feat(dup): implement procedure load_from_private_log                       |
| XiaoMi/rdsn#314 | feat: support table-level slow query on meta server                        |
| XiaoMi/rdsn#309 | feat(split): child replica learn parent prepare list and checkpoint        |
| XiaoMi/rdsn#315 | feat(dup): verify private log validity before starting to duplicate        |
| XiaoMi/rdsn#312 | feat(dup): implement ship_mutation stage and mutation_batch                |
| XiaoMi/rdsn#298 | feat(throttle): support size-based write throttling                        |
| XiaoMi/rdsn#311 | refactor: rename disk_aio to aio_context                                   |
| XiaoMi/rdsn#302 | refactor: introduce mutation_log::replay_block                             |
| XiaoMi/rdsn#310 | refactor: remove empty_aio_provider and posix aio_provider                 |
| XiaoMi/rdsn#299 | feat(split): parent replica prepare states                                 |
| XiaoMi/rdsn#304 | feat(dup): add interface mutation_duplicator & duplication procedure       |
| XiaoMi/rdsn#307 | build: change thrift compiler, set default 3rdlibs                         |
| XiaoMi/rdsn#305 | add unit tests for task                                                    |
| XiaoMi/rdsn#303 | build: remove MY_PROJ_INC_PATH                                             |
| XiaoMi/rdsn#300 | fix(network): use derror rather than dwarn for failed network bootstrap    |
| XiaoMi/rdsn#297 | feat(dup): implement duplication_sync on meta server side                  |
| XiaoMi/rdsn#291 | split: parent replica create child replica                                 |
| XiaoMi/rdsn#296 | http: improvement on http api                                              |
| XiaoMi/rdsn#295 | build: fix dsn.cmake to add thirdparty/output/lib64 into link directories  |
| XiaoMi/rdsn#294 | utils: add unlikely for dverify/dreturn/dstop macros                       |
| XiaoMi/rdsn#293 | utility: enhance fail point functionality                                  |
| XiaoMi/rdsn#292 | dup: complete implementation of replica_duplicator_manager                 |
| XiaoMi/rdsn#249 | dup: add duplication recovery on meta server                               |
| XiaoMi/rdsn#285 | CMake: remove MY_PROJ_LIB_PATH                                             |
| XiaoMi/rdsn#286 | split: meta start partition split                                          |
| XiaoMi/rdsn#280 | HTTP service for meta & replica server                                     |
| XiaoMi/rdsn#289 | utils: add test utils on meta server                                       |
| XiaoMi/rdsn#288 | build curl :add without-ssl                                                |
| XiaoMi/rdsn#287 | thirdparty: add curl to help build prometheus                              |
| XiaoMi/rdsn#284 | add prometheus to thirdparty                                               |
| XiaoMi/rdsn#281 | dup: add duplication_sync_timer                                            |
| XiaoMi/rdsn#283 | *: remove DSN_IN_CORE, improve BOOST linking, fix build.sh                 |
| XiaoMi/rdsn#282 | minor refactoring on replica-server and meta-server                        |
| XiaoMi/rdsn#276 | configs: remove useless configs                                            |
| XiaoMi/rdsn#273 | duplication: bugfix on pause_dup                                           |
| XiaoMi/rdsn#266 | http: move http as a submodule of dist                                     |
| XiaoMi/rdsn#262 | Revert ""use c++14 mode for compilation (#257)""                             |
| XiaoMi/rdsn#257 | use c++14 mode for compilation                                             |
| XiaoMi/rdsn#248 | dup: add change_duplication_status on meta_server                          |
| XiaoMi/rdsn#246 | duplication: add meta_duplication_service (part 3)                         |
| XiaoMi/rdsn#245 | duplication: add meta_duplication_service (part 2)                         |
| XiaoMi/rdsn#244 | duplication: add meta_duplication_service (part 1)                         |
| XiaoMi/rdsn#242 | duplication: add meta_duplication_service (part 0)                         |
| XiaoMi/rdsn#237 | meta: add duplication_info and refactor test utils                         |
| XiaoMi/rdsn#235 | ddl_client: add duplication related commands                               |
| XiaoMi/rdsn#233 | replica: add unit tests for mutation log replay                            |
| XiaoMi/rdsn#232 | replication: update thrift definitions for duplication                     |

|         PR         |                                      TITLE                                      |
|--------------------|---------------------------------------------------------------------------------|
| XiaoMi/pegasus#400 | feat: table level slow query                                                    |
| XiaoMi/pegasus#397 | build(cmake): fix curl link of prometheus                                       |
| XiaoMi/pegasus#398 | refactor: change the way to call replication_ddl_client::set_app_envs           |
| XiaoMi/pegasus#395 | feat(shell): add debugging commands for hex and escaped-string conversion       |
| XiaoMi/pegasus#394 | build(cmake): cleanup lib links                                                 |
| XiaoMi/pegasus#389 | build: pack boost.regex with binaries                                           |
| XiaoMi/pegasus#386 | build: remove MY_PROJ_INC_PATH                                                  |
| XiaoMi/pegasus#385 | fix: unit tests & travis.sh exit with err                                       |
| XiaoMi/pegasus#384 | fix: unit test failed and update rdsn                                           |
| XiaoMi/pegasus#372 | benchmark: remove dependency of unexport rocksdb objects                        |
| XiaoMi/pegasus#380 | compile: fix dependency error of curl                                           |
| XiaoMi/pegasus#379 | build: fix travis                                                               |
| XiaoMi/pegasus#377 | fix cmake file; update rdsn                                                     |
| XiaoMi/pegasus#376 | CMake: fix client_lib                                                           |
| XiaoMi/pegasus#373 | CMake: remove MY_PROJ_LIB_PATH, improve client_lib, use rocksdb target to link  |
| XiaoMi/pegasus#370 | configs: minimize config-server.ini to config.min.ini                           |
| XiaoMi/pegasus#358 | scripts: travis supports format                                                 |
| XiaoMi/pegasus#360 | HTTP service for replica server                                                 |
| XiaoMi/pegasus#367 | shell & bench: use rocksdb::Statistics to calc histogram                        |
| XiaoMi/pegasus#368 | add prometheus for monitor                                                      |
| XiaoMi/pegasus#362 | CMakeList: remove DSN_IN_CORE & improve BOOST linking                           |
| XiaoMi/pegasus#359 | update rdsn & fix config                                                        |
| XiaoMi/pegasus#350 | refactor: keep default value in code the same as what it in config.ini          |
| XiaoMi/pegasus#310 | rocksdb: add an option to use rocksdb to reduce intrusive modifications         |
| XiaoMi/pegasus#306 | *: add server config for connection threshold per endpoint                      |
| XiaoMi/pegasus#300 | *: add whitelist config & optimize pegasus_offline_node.sh                      |
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/409/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/409,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0NzMxOTQ2Nw==,incubator-pegasus,547319467,409,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-10-29T08:56:12Z,2019-10-29T08:56:12Z,"Pegasus v1.12.0-RC1 has released:
- https://github.com/XiaoMi/pegasus/releases/tag/v1.12.0-RC1
- https://github.com/XiaoMi/rdsn/releases/tag/v1.12.0-RC1","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0NzMxOTQ2Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/409,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1NDIzODUxNQ==,incubator-pegasus,554238515,409,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-11-15T06:56:38Z,2019-11-15T06:56:38Z,"# Release Note

The following are the highlights in this release:

## HTTP Support

We exposed many metadata of a Pegasus cluster and some system states of a Pegasus server through HTTP interfaces. Thanks to @Skysheepwang.

Related PR: [XiaoMi/rdsn#280](https://github.com/XiaoMi/rdsn/pull/280), [#360](https://github.com/XiaoMi/pegasus/pull/360), [XiaoMi/rdsn#321](https://github.com/XiaoMi/rdsn/pull/321), [XiaoMi/rdsn#296](https://github.com/XiaoMi/rdsn/pull/296)

Related Docs: <https://pegasus-kv.github.io/api/http>

## Multi-tenant Support

In XiaoMi as our user base grows, improvements on our multi-tenant support become increasingly needed for stability. For example, we lack monitoring support for table-level latency (only server-level latency). When a table is observed unreasonably slow, we need to refine our slow query mechanism to dynamically configure the ""slow"" threshold per table without system reboot. Another requirement is to support size-based write throttling to reduce the influence of a high-throughput-low-QPS table to other tables in the same cluster.

Related PR: [XiaoMi/rdsn#314](https://github.com/XiaoMi/rdsn/pull/314), [XiaoMi/rdsn#298](https://github.com/XiaoMi/rdsn/pull/298), [#400](https://github.com/XiaoMi/pegasus/pull/400), [XiaoMi/rdsn#336](https://github.com/XiaoMi/rdsn/pull/336), [XiaoMi/rdsn#340](https://github.com/XiaoMi/rdsn/pull/340)

Docs on table-level latency: TBD

Docs on table-level slow query: TBD

Docs on size-based throttling: TBD

## Prometheus Support

Pegasus is continuously lowering our access costs for community users. In this release, we provide our experimental support on Promentheus, thanks to @ChenQShmily.

Related PR: [XiaoMi/rdsn#287](https://github.com/XiaoMi/rdsn/pull/287), [XiaoMi/rdsn#284](https://github.com/XiaoMi/rdsn/pull/284), [#397](https://github.com/XiaoMi/pegasus/pull/397), [#368](https://github.com/XiaoMi/pegasus/pull/368)

Related Docs: TBD

## CPU Profiling Support

Thanks to @linlinhaohao888.

Related PR: [XiaoMi/rdsn#290](https://github.com/XiaoMi/rdsn/pull/290)

Related Docs: TBD

## Bug Fixes

- fix: meta unnecessary assert [XiaoMi/rdsn#332](https://github.com/XiaoMi/rdsn/pull/332)
- fix(coldbackup): delay clean request when chkpting [XiaoMi/rdsn#327](https://github.com/XiaoMi/rdsn/pull/327)

## Upgrade from the previous version

No configuration update is needed in this release.

For table-level latency, some new perf-counters are added. Every read/write operation to a table has  two perf-counters (p99&p999) for latency.
```
replica*eon.replica*table.level.RPC_RRDB_RRDB_PUT.latency(ns)@${for.each.table}
replica*eon.replica*table.level.RPC_RRDB_RRDB_PUT.latency(ns)@${for.each.table}.p999
replica*eon.replica*table.level.RPC_RRDB_RRDB_GET.latency(ns)@${for.each.table}
replica*eon.replica*table.level.RPC_RRDB_RRDB_GET.latency(ns)@${for.each.table}.p999
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1NDIzODUxNQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/409,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1NTM2NjUwNQ==,incubator-pegasus,555366505,409,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-11-19T07:10:36Z,2019-11-19T07:10:36Z,"|       PR        |                                   TITLE                                    |
|-----|-----|
| #411 | fix(config): disable auto mem release by default |
| #416 |  refactor: remove the assert in get_app_stat |
| #420 |  feat(shell): add qps and p99 statistics while list_node |
| XiaoMi/rdsn#336 | feat: add table level latency perf counters |
| XiaoMi/rdsn#340 | fix(bug): resolve static initialization order problem of s_storage_rpc_req_codes |

Pegasus v1.12.0-RC2 has released:
- https://github.com/XiaoMi/pegasus/releases/tag/v1.12.0-RC2
- https://github.com/XiaoMi/rdsn/releases/tag/v1.12.0-RC2","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1NTM2NjUwNQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/409,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1NTM4OTAyOA==,incubator-pegasus,555389028,409,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-11-19T08:25:06Z,2019-11-19T08:25:06Z,"Pegasus v1.12.0 has released:

https://github.com/XiaoMi/pegasus/releases/tag/v1.12.0
https://github.com/XiaoMi/rdsn/releases/tag/v1.12.0
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1NTM4OTAyOA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/410,https://api.github.com/repos/apache/incubator-pegasus/issues/410,incubator-pegasus,512933943,410,"Weekly Digest (20 October, 2019 - 27 October, 2019)",weekly-digest,,,,CLOSED,2019-10-27T07:59:24Z,2019-11-14T08:46:40Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 4 issues were created.
Of these, 0 issues have been closed and 4 issues are still open.
## OPEN ISSUES
:green_heart: #409 [Release 1.11.7](https://github.com/XiaoMi/pegasus/issues/409), by [neverchanje](https://github.com/neverchanje)
:green_heart: #408 [periodic memory release results in increased latency](https://github.com/XiaoMi/pegasus/issues/408), by [Shuo-Jia](https://github.com/Shuo-Jia)
:green_heart: #407 [build question](https://github.com/XiaoMi/pegasus/issues/407), by [mysunnyshine](https://github.com/mysunnyshine)
:green_heart: #406 [add table level latency](https://github.com/XiaoMi/pegasus/issues/406), by [levy5307](https://github.com/levy5307)
## LIKED ISSUE
:+1: #409 [Release 1.11.7](https://github.com/XiaoMi/pegasus/issues/409), by [neverchanje](https://github.com/neverchanje)
It received :+1: x1, :smile: x0, :tada: x0 and :heart: x0.
## NOISY ISSUE
:speaker: #406 [add table level latency](https://github.com/XiaoMi/pegasus/issues/406), by [levy5307](https://github.com/levy5307)
It received 3 comments.

 - - - 
# PULL REQUESTS
Last week, no pull requests were created, updated or merged.

 - - - 
# COMMITS
Last week there were no commits.

 - - - 
# CONTRIBUTORS
Last week there were no contributors.

 - - - 
# STARGAZERS
Last week there were 4 stagazers.
:star: [beckhamaaa](https://github.com/beckhamaaa)
:star: [liehuoqiangni](https://github.com/liehuoqiangni)
:star: [daniel2643](https://github.com/daniel2643)
:star: [xiayurain95](https://github.com/xiayurain95)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/410/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/412,https://api.github.com/repos/apache/incubator-pegasus/issues/412,incubator-pegasus,515142000,412,git clone --resursive failed,mysunnyshine,24480430,,,CLOSED,2019-10-31T03:51:03Z,2020-01-18T02:14:32Z,"## General Question

Before asking a question, make sure you have:

- Searched open and closed [GitHub issues](https://github.com/XiaoMi/pegasus/issues)
- Read the documentation:
  - [Pegasus Doc](https://pegasus-kv.github.io)


![image](https://user-images.githubusercontent.com/24480430/67917623-8ab0ec80-fbd4-11e9-956d-a68c609d5c88.png)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/412/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/412,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0ODIwNzEwNw==,incubator-pegasus,548207107,412,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-10-31T03:58:32Z,2019-10-31T03:58:32Z,"You can clone rdsn origin/master on your pegasus directory.

```
cd pegasus/
git clone https://github.com/XiaoMi/rdsn.git
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0ODIwNzEwNw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/412,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0ODIyMjExMw==,incubator-pegasus,548222113,412,NA,mysunnyshine,24480430,,,NA,2019-10-31T05:27:13Z,2019-10-31T05:27:13Z,"> You can clone rdsn origin/master on your pegasus directory.
> 
> ```
> cd pegasus/
> git clone https://github.com/XiaoMi/rdsn.git
> ```
Os: CentOS7
Cmake: 3.10
Gcc:  8.0

Now,  anthoer question is as below:

![image](https://user-images.githubusercontent.com/24480430/67921227-0e250a80-fbe2-11e9-94a9-15c21effe3f7.png)

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0ODIyMjExMw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/412,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0ODIyMjM2OQ==,incubator-pegasus,548222369,412,NA,mysunnyshine,24480430,,,NA,2019-10-31T05:28:41Z,2019-10-31T05:28:41Z,"> You can clone rdsn origin/master on your pegasus directory.
> 
> ```
> cd pegasus/
> git clone https://github.com/XiaoMi/rdsn.git
> ```
![image](https://user-images.githubusercontent.com/24480430/67921331-547a6980-fbe2-11e9-84e9-560b6345f6f8.png)

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0ODIyMjM2OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/412,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0ODIzMjAwMQ==,incubator-pegasus,548232001,412,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-10-31T06:18:54Z,2019-10-31T06:18:54Z,"This is the problem of zookeeper c client, not of Pegasus. You can use an older version GCC.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0ODIzMjAwMQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/413,https://api.github.com/repos/apache/incubator-pegasus/issues/413,incubator-pegasus,515960439,413,Pegasus crashed when I use the incorrect cluster,Smityz,22953824,Smilencer,smityz@qq.com,OPEN,2019-11-01T07:46:02Z,2019-11-04T10:49:11Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
If possible, provide a recipe for reproducing the error.
```
./run.sh shell
>>> cc 23232323
```
2. What did you expect to see?
some Error messages,like 'Cluster dosen't exist'
3. What did you see instead?
Pegasus crashed
```
>>> cc 23232323
F2019-11-01 15:36:15.609 (1572593775609414389 4057)  mimic.io-thrd.16471: replication_common.cpp:573:load_meta_servers(): assertion expression: servers.size() > 0
F2019-11-01 15:36:15.609 (1572593775609467709 4057)  mimic.io-thrd.16471: replication_common.cpp:573:load_meta_servers(): no meta server specified in config [pegasus.clusters].23232323
./run.sh: 行 1621: 16471 已放弃               (核心已转储) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
```

4. What version of Pegasus are you using?
Pegasus Shell 1.12.SNAPSHOT (404079781f91cd869949eb00c525155b330c9d7a) Release","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/413/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/413,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0OTMwMTMwOQ==,incubator-pegasus,549301309,413,NA,mysunnyshine,24480430,,,NA,2019-11-04T10:49:11Z,2019-11-04T10:49:11Z,"May  I add your WeChat？ I'm building the Pegasus, but something happed. So  I want to ask some questions. My wechat: wlxy211314","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU0OTMwMTMwOQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/414,https://api.github.com/repos/apache/incubator-pegasus/issues/414,incubator-pegasus,516794407,414,"Weekly Digest (27 October, 2019 - 3 November, 2019)",weekly-digest,,,,CLOSED,2019-11-03T07:59:23Z,2019-12-05T10:49:09Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 3 issues were created.
Of these, 1 issues have been closed and 2 issues are still open.
## OPEN ISSUES
:green_heart: #413 [Pegasus crashed when I use the incorrect cluster](https://github.com/XiaoMi/pegasus/issues/413), by [Smityz](https://github.com/Smityz)
:green_heart: #412 [git clone --resursive failed](https://github.com/XiaoMi/pegasus/issues/412), by [mysunnyshine](https://github.com/mysunnyshine)
## CLOSED ISSUES
:heart: #411 [fix(config): disable auto mem release by default](https://github.com/XiaoMi/pegasus/pull/411), by [neverchanje](https://github.com/neverchanje)
## NOISY ISSUE
:speaker: #412 [git clone --resursive failed](https://github.com/XiaoMi/pegasus/issues/412), by [mysunnyshine](https://github.com/mysunnyshine)
It received 4 comments.

 - - - 
# PULL REQUESTS
Last week, 1 pull request was created, updated or merged.
## MERGED PULL REQUEST
Last week, 1 pull request was merged.
:purple_heart: #411 [fix(config): disable auto mem release by default](https://github.com/XiaoMi/pegasus/pull/411), by [neverchanje](https://github.com/neverchanje)

 - - - 
# COMMITS
Last week there was 1 commit.
:hammer_and_wrench: [fix(config): disable auto mem release by default (#411)](https://github.com/XiaoMi/pegasus/commit/404079781f91cd869949eb00c525155b330c9d7a) by [neverchanje](https://github.com/neverchanje)

 - - - 
# CONTRIBUTORS
Last week there was 1 contributor.
:bust_in_silhouette: [neverchanje](https://github.com/neverchanje)

 - - - 
# STARGAZERS
Last week there were 9 stagazers.
:star: [gvvynplaine](https://github.com/gvvynplaine)
:star: [wangchen1ren](https://github.com/wangchen1ren)
:star: [whitewum](https://github.com/whitewum)
:star: [yixinglu](https://github.com/yixinglu)
:star: [TGBP](https://github.com/TGBP)
:star: [wklken](https://github.com/wklken)
:star: [cosyman](https://github.com/cosyman)
:star: [arystan-sw](https://github.com/arystan-sw)
:star: [jptiancai](https://github.com/jptiancai)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/414/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/415,https://api.github.com/repos/apache/incubator-pegasus/issues/415,incubator-pegasus,516971565,415,单机启动问题,mysunnyshine,24480430,,,OPEN,2019-11-04T06:03:36Z,2019-11-09T07:59:14Z,"目的： 单机环境搭建
过程： 1、下载 server 二进制包，而后修改 config 文件。启动 meta-server 和 replica-server
            2、下载 tool 二进制包，启动 shell
问题： 1、在启动shell后，告诉我meta_list is 127.0.0.1:34601,127.0.0.1:34602,127.0.0.1:34603。
            但是我的配置里，apps.meta.count 是1 。应该只有 34601 端口才对。
            2、shell 启动后，use table_name。 这个表名字，随便写一个，返回的都是 ok。那我可不可以 
             理解为，use 会帮忙创建表，即使表不存在。
            3、我后来通过在server端指定 meta@1  和 shell 指定 cluster ，只访问 34601端口。 但是依然连不通，所以问下，单机binary包搭建的时候，除了改配置，还需要做什么？
            4、ssh 报错。
                [root@debugboxcreate21x2 openssl-1.0.2]# ssh
ssh: /data/pegasus-server-1.11.6-9f4e5ae-glibc2.12-release/bin/libcrypto.so.10: version `OPENSSL_1.0.2' not found (required by ssh)
![image](https://user-images.githubusercontent.com/24480430/68106560-f076de80-ff1c-11e9-8534-5c98d603994c.png)


![image](https://user-images.githubusercontent.com/24480430/68101980-8ace2680-ff0b-11e9-8de3-322ef03cba65.png)


![image](https://user-images.githubusercontent.com/24480430/68101963-696d3a80-ff0b-11e9-9d7b-f173f5e5e18f.png)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/415/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/415,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1MjA3NjY2Ng==,incubator-pegasus,552076666,415,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2019-11-09T07:59:14Z,2019-11-09T07:59:14Z,"> 1、在启动shell后，告诉我meta_list is 127.0.0.1:34601,127.0.0.1:34602,127.0.0.1:34603。
但是我的配置里，apps.meta.count 是1 。应该只有 34601 端口才对。

pegasus的配置文件既供meta server使用也供replica server使用，因此配置项有部分对于meta server来说是多余的，也不会使用。
shell的meta list是读取的shell配置文件https://github.com/XiaoMi/pegasus/blob/e5fcffd5d94bf72c647d724bfc30a18f05edfb10/src/shell/main.cpp#L567
，[pegasus.clusters]这个section。

> 2、shell 启动后，use table_name。 这个表名字，随便写一个，返回的都是 ok。那我可不可以
> 理解为，use 会帮忙创建表，即使表不存在。

看代码可以知道，use不会创建。

> 3、我后来通过在server端指定 meta@1 和 shell 指定 cluster ，只访问 34601端口。 但是依然连不通，所以问下，单机binary包搭建的时候，除了改配置，还需要做什么？
4、ssh 报错。

问题看不大明白，如果需要，请详细说明
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1MjA3NjY2Ng==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/418,https://api.github.com/repos/apache/incubator-pegasus/issues/418,incubator-pegasus,519664301,418,Need to init environment variables before the unit test,Smityz,22953824,Smilencer,smityz@qq.com,CLOSED,2019-11-08T03:47:48Z,2019-11-08T11:05:33Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
When I made a unit test,it reported
```
./run.sh: 7: ./run.sh: ./clear.sh: Permission denied
./dsn.replica.test: error while loading shared libraries: libPocoCrypto.so.48: cannot open shared object file: No such file or directory
```
2. What did you expect to see?
I should set environment variables before the unit test.
```
export LD_LIBRARY_PATH=~/code/pegasus/rdsn/thirdparty/output/lib/:$LD_LIBRARY_PATH
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/418/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/418,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1MTU5MDYwNA==,incubator-pegasus,551590604,418,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2019-11-08T11:05:32Z,2019-11-08T11:05:32Z,clean & build thirdparty(poco&fds),"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1MTU5MDYwNA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/419,https://api.github.com/repos/apache/incubator-pegasus/issues/419,incubator-pegasus,520567961,419,"Weekly Digest (3 November, 2019 - 10 November, 2019)",weekly-digest,,,,CLOSED,2019-11-10T07:59:23Z,2020-02-16T16:45:23Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 4 issues were created.
Of these, 2 issues have been closed and 2 issues are still open.
## OPEN ISSUES
:green_heart: #417 [feat: add a interface to get perf-counters info of all partitions of all apps](https://github.com/XiaoMi/pegasus/pull/417), by [levy5307](https://github.com/levy5307)
:green_heart: #415 [单机启动问题](https://github.com/XiaoMi/pegasus/issues/415), by [mysunnyshine](https://github.com/mysunnyshine)
## CLOSED ISSUES
:heart: #418 [Need to init environment variables before the unit test](https://github.com/XiaoMi/pegasus/issues/418), by [Smityz](https://github.com/Smityz)
:heart: #416 [refactor: remove the assert in get_app_stat](https://github.com/XiaoMi/pegasus/pull/416), by [levy5307](https://github.com/levy5307)
## LIKED ISSUE
:+1: #416 [refactor: remove the assert in get_app_stat](https://github.com/XiaoMi/pegasus/pull/416), by [levy5307](https://github.com/levy5307)
It received :+1: x1, :smile: x0, :tada: x0 and :heart: x0.
## NOISY ISSUE
:speaker: #415 [单机启动问题](https://github.com/XiaoMi/pegasus/issues/415), by [mysunnyshine](https://github.com/mysunnyshine)
It received 1 comments.

 - - - 
# PULL REQUESTS
Last week, 2 pull requests were created, updated or merged.
## UPDATED PULL REQUEST
Last week, 1 pull request was updated.
:yellow_heart: #417 [feat: add a interface to get perf-counters info of all partitions of all apps](https://github.com/XiaoMi/pegasus/pull/417), by [levy5307](https://github.com/levy5307)
## MERGED PULL REQUEST
Last week, 1 pull request was merged.
:purple_heart: #416 [refactor: remove the assert in get_app_stat](https://github.com/XiaoMi/pegasus/pull/416), by [levy5307](https://github.com/levy5307)

 - - - 
# COMMITS
Last week there was 1 commit.
:hammer_and_wrench: [refactor: remove the assert in get_app_stat (#416)](https://github.com/XiaoMi/pegasus/commit/b16fe0b16db6f45914598d253ff8f32789c911b7) by [levy5307](https://github.com/levy5307)

 - - - 
# CONTRIBUTORS
Last week there was 1 contributor.
:bust_in_silhouette: [levy5307](https://github.com/levy5307)

 - - - 
# STARGAZERS
Last week there was 1 stargazer.
:star: [TWS-YIFEI](https://github.com/TWS-YIFEI)
You are the star! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/419/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/421,https://api.github.com/repos/apache/incubator-pegasus/issues/421,incubator-pegasus,522780635,421,perf: enable rocksdb prefix seek mode to improve hashkey scan.,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2019-11-14T10:54:32Z,2020-02-11T06:10:38Z,"Currently, our storage engine uses rocksdb's default options to construct a bloom filter, which only checks the existence of ""hashkey+sortkey"". For the situation where users scan over the range of hashkey (`multigetrange`, `hashscan`), the bloom filter makes no optimization if the hashkey does not exist. And rocksdb will search the corresponding SST data block, until it confirms the inexistence.

So what can we do with it? RocksDB provides an option called [prefix seek mode](https://github.com/facebook/rocksdb/wiki/Prefix-Seek-API-Changes). Its major improvement is adding prefix (the hashkey) as well as the whole key to the bloom filter. (https://github.com/facebook/rocksdb/wiki/RocksDB-Bloom-Filter#prefix-vs-whole-key)

For point lookups (multiget/get), only the whole key blooms are used. For range queries (multigetrange, hashscan, unorderedscan+hashkey prefix filtering), rocksdb responds without searching SST after the hashkey misses in the bloom filter.

One other improvement is to use [hash-based memtable](https://github.com/facebook/rocksdb/wiki/MemTable#hashskiplist-memtable). Since Pegasus doesn't support cross-partition ordering, this implementation can be taken into consideration.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/421/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/423,https://api.github.com/repos/apache/incubator-pegasus/issues/423,incubator-pegasus,523794155,423,如何限制内存占用,mysunnyshine,24480430,,,OPEN,2019-11-16T05:13:57Z,2021-08-27T03:42:45Z,我们的业务场景要求，必须有效的限制内存。我改了RocksDB的配置，将buffer改小。 测试发现，内存占用还是1.5G左右，期望控制在1G内。 这个具体有哪些参数可以调节，能分享下经验么？,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/423/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/423,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1NDgyMDM4NA==,incubator-pegasus,554820384,423,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-11-18T02:07:41Z,2019-11-18T02:07:41Z,请问你微信号多少？我把你拉群讨论吧,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1NDgyMDM4NA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/423,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1NDg0NTQxMA==,incubator-pegasus,554845410,423,NA,mysunnyshine,24480430,,,NA,2019-11-18T04:24:05Z,2019-11-18T04:24:05Z,"> 请问你微信号多少？我把你拉群讨论吧

已经在群里了，二群。问了，也没有人回复，只好在这里问了。
主要就是目前期望能控制 Pegasus 占用的内存，不希望他随着数据量的增加，内存也增加。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1NDg0NTQxMA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/423,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM42DjWp,incubator-pegasus,906900905,423,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2021-08-27T03:41:48Z,2021-08-27T03:41:48Z,"1G 内存过于严苛了，rocksdb 的运行时占用都可能不止 1G
你可以考虑
1. 把 rocksdb 的 blockcache 关掉，并参考 https://github.com/facebook/rocksdb/wiki/Memory-usage-in-RocksDB 这个页面把所有对内存有影响的参数调低。
2. 单机分片数尽可能少，降低 rocksdb 实例个数
3. 使用新版本(v2.2 以上)的 pegasus，我们做了不少关于内存使用的优化

这是 Onebox 场景下使用 valgrind massif 查看内存分布的用量图，可以看出，大部分内存还是由 rocksdb 产生的
![image](https://user-images.githubusercontent.com/22953824/131068394-c41c78e0-0ab6-4cb9-b9e2-203f175c4134.png)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM42DjWp/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/424,https://api.github.com/repos/apache/incubator-pegasus/issues/424,incubator-pegasus,523794625,424,关于multi_set的时间问题,mysunnyshine,24480430,,,OPEN,2019-11-16T05:18:11Z,2019-11-18T05:37:09Z,三台机器，每个节点都有一个 meta replica ，单次 mset 200个数据，测试发现，居然只要1-2ms。 因为之前调研过 TIkv ScyllaDB，基本8ms-10ms 左右。个人认为大部分数据是写内存了，但是又不知道哪个配置是改这个内存限制的。 ,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/424/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/424,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1NDczMDA2OA==,incubator-pegasus,554730068,424,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-11-17T09:57:18Z,2019-11-17T09:57:18Z,写数据肯定要先写WAL（write ahead log），保证数据不会丢，然后apply到内存的memtable中。不知道你有什么问题？是担心数据没有落地吗？,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1NDczMDA2OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/424,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1NDg1OTk5MA==,incubator-pegasus,554859990,424,NA,mysunnyshine,24480430,,,NA,2019-11-18T05:37:09Z,2019-11-18T05:37:09Z,"> 写数据肯定要先写WAL（write ahead log），保证数据不会丢，然后apply到内存的memtable中。不知道你有什么问题？是担心数据没有落地吗？

我想尽可能的限制内存占用，比如 在 Tikv 也是基于 RocksDB，我修改了部分 buffer 相关的配置后，Tikv的内存占用，至少能显著的控制一部分。但是Pegasus 我改了配置，内存还是难以控制。同样的数据集，Tikv 内存可以控制在1.1G左右，而 Pegasus 一度到2.5G。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1NDg1OTk5MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/425,https://api.github.com/repos/apache/incubator-pegasus/issues/425,incubator-pegasus,523951683,425,"Weekly Digest (10 November, 2019 - 17 November, 2019)",weekly-digest,,,,CLOSED,2019-11-17T07:59:24Z,2020-02-16T16:45:20Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 5 issues were created.
Of these, 1 issues have been closed and 4 issues are still open.
## OPEN ISSUES
:green_heart: #424 [关于multi_set的时间问题](https://github.com/XiaoMi/pegasus/issues/424), by [mysunnyshine](https://github.com/mysunnyshine)
:green_heart: #423 [如何限制内存占用](https://github.com/XiaoMi/pegasus/issues/423), by [mysunnyshine](https://github.com/mysunnyshine)
:green_heart: #421 [perf: enable rocksdb prefix seek mode to improve hashkey scan.](https://github.com/XiaoMi/pegasus/issues/421), by [neverchanje](https://github.com/neverchanje)
:green_heart: #420 [feat(shell): add qps and p99 statistics while list_node](https://github.com/XiaoMi/pegasus/pull/420), by [hycdong](https://github.com/hycdong)
## CLOSED ISSUES
:heart: #422 [小米能做一个termux软件商店吗？](https://github.com/XiaoMi/pegasus/issues/422), by [ZihaoXingstudy](https://github.com/ZihaoXingstudy)
## LIKED ISSUE
:+1: #421 [perf: enable rocksdb prefix seek mode to improve hashkey scan.](https://github.com/XiaoMi/pegasus/issues/421), by [neverchanje](https://github.com/neverchanje)
It received :+1: x0, :smile: x0, :tada: x0 and :heart: x1.
## NOISY ISSUE
:speaker: #420 [feat(shell): add qps and p99 statistics while list_node](https://github.com/XiaoMi/pegasus/pull/420), by [hycdong](https://github.com/hycdong)
It received 2 comments.

 - - - 
# PULL REQUESTS
Last week, 2 pull requests were created, updated or merged.
## UPDATED PULL REQUEST
Last week, 2 pull requests were updated.
:yellow_heart: #420 [feat(shell): add qps and p99 statistics while list_node](https://github.com/XiaoMi/pegasus/pull/420), by [hycdong](https://github.com/hycdong)
:yellow_heart: #417 [feat: add a interface to get perf-counters info of all partitions of all apps](https://github.com/XiaoMi/pegasus/pull/417), by [levy5307](https://github.com/levy5307)

 - - - 
# COMMITS
Last week there were no commits.

 - - - 
# CONTRIBUTORS
Last week there were no contributors.

 - - - 
# STARGAZERS
Last week there were 2 stagazers.
:star: [scoone](https://github.com/scoone)
:star: [di-stars](https://github.com/di-stars)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/425/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/428,https://api.github.com/repos/apache/incubator-pegasus/issues/428,incubator-pegasus,527656691,428,"Weekly Digest (17 November, 2019 - 24 November, 2019)",weekly-digest,,,,CLOSED,2019-11-24T06:29:34Z,2020-02-16T16:45:15Z,"Here's the Weekly Digest for [XiaoMi/pegasus](https://github.com/XiaoMi/pegasus):
# ISSUES 
This week, 9 issues were created. Of these, 6 issues have been closed and 3 issues are still open. 
## OPEN ISSUES 
:green_heart: #427 [feat: add statistics for partition hotspot](https://github.com/XiaoMi/pegasus/pull/427), by [levy5307](https://github.com/levy5307)
:green_heart: #424 [关于multi_set的时间问题](https://github.com/XiaoMi/pegasus/issues/424), by [mysunnyshine](https://github.com/mysunnyshine)
:green_heart: #423 [如何限制内存占用](https://github.com/XiaoMi/pegasus/issues/423), by [mysunnyshine](https://github.com/mysunnyshine)
## CLOSED ISSUES 
:heart: #426 [feat(shell): support resolve IP address in some shell commands](https://github.com/XiaoMi/pegasus/pull/426), by [Smityz](https://github.com/Smityz)
:heart: #420 [feat(shell): add qps and p99 statistics while list_node](https://github.com/XiaoMi/pegasus/pull/420), by [hycdong](https://github.com/hycdong)
:heart: #417 [feat: add a interface to get perf-counters info of all partitions of all apps](https://github.com/XiaoMi/pegasus/pull/417), by [levy5307](https://github.com/levy5307)
:heart: #409 [Release 1.12.0](https://github.com/XiaoMi/pegasus/issues/409), by [neverchanje](https://github.com/neverchanje)
:heart: #354 [Release 1.11.6](https://github.com/XiaoMi/pegasus/issues/354), by [neverchanje](https://github.com/neverchanje)
:heart: #330 [Release 1.11.4](https://github.com/XiaoMi/pegasus/issues/330), by [neverchanje](https://github.com/neverchanje)
## LIKED ISSUE 
The issue most liked this week has been:
:+1: #330 [Release 1.11.4](https://github.com/XiaoMi/pegasus/issues/330), by [neverchanje](https://github.com/neverchanje)
It received :+1: x3, :smile: x0, :tada: x0 and :heart: x0.
## NOISY ISSUE 
The issue most discussed this week has been:
:speaker: #330 [Release 1.11.4](https://github.com/XiaoMi/pegasus/issues/330), by [neverchanje](https://github.com/neverchanje)
It received 6 comments.

# PULL REQUESTS
This week, 2 pull requests were proposed. Of these, 0 pull requests have been merged and 1 are still open.
## OPEN PRs 
:green_heart: #427 [feat: add statistics for partition hotspot](https://github.com/XiaoMi/pegasus/pull/427), by [levy5307](https://github.com/levy5307)

# CONTRIBUTORS 
This week, 5 users have contributed to this repository. 
They are [levy5307](https://github.com/levy5307), [Smityz](https://github.com/Smityz), [mysunnyshine](https://github.com/mysunnyshine), [hycdong](https://github.com/hycdong), and [neverchanje](https://github.com/neverchanje).

# STARGAZERS
This week, no user has starred this repository.

# COMMITS
This week, there have been 3 commits in the repository.
These are: 
:hammer_and_wrench: [feat(shell): support resolve IP address in some shell commands (#426)](https://github.com/XiaoMi/pegasus/commit/1cdd522cd72f1f03b144d9f96399c3a907708f9c) by [Smityz](https://github.com/Smityz)
:hammer_and_wrench: [feat: add a interface to get perf-counters info of all partitions of all apps (#417)](https://github.com/XiaoMi/pegasus/commit/50eb5ad1ef6f2917e186cda8f83cdf92d27199a9) by [levy5307](https://github.com/levy5307)
:hammer_and_wrench: [feat(shell): add qps and p99 statistics while list_node (#420)](https://github.com/XiaoMi/pegasus/commit/e160f4dc1df552b0649a2f52d88852bc54b51a0a) by [hycdong](https://github.com/hycdong)

 # RELEASES
This week, 1 release was published. It is: 
:rocket: v1.12.0 [v1.12.0](https://github.com/XiaoMi/pegasus/releases/tag/v1.12.0)
See #409 for details.

Highlights in this release:

- HTTP Support
- Multi-tenant Support
- Prometheus Support
- CPU Profiling Support
- Bug Fixes


That's all for this week, please watch :eyes: and star :star: [XiaoMi/pegasus](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/428/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/429,https://api.github.com/repos/apache/incubator-pegasus/issues/429,incubator-pegasus,528620141,429,pegasus meta 进程内存泄露,sunwsh,4278126,,,CLOSED,2019-11-26T10:11:41Z,2020-06-19T08:04:55Z,"## Bug Report
最近看到pegasus meta 的primary进程内存一直在增长， 
版本： v1.11.1 

用pmap 看到
root@iZwz9a47t80hnkbpiuyzoeZ:/proc/29363# pmap -d 29363
29363:   ./pegasus_server config_meta.ini -app_list meta
Address           Kbytes Mode  Offset           Device    Mapping
0000000000400000    7004 r-x-- 0000000000000000 0fd:00010 pegasus_server
0000000000cde000     112 rw--- 0000000000000000 000:00000   [ anon ]
0000000001f58000 11359232 rw--- 0000000000000000 000:00000   [ anon ]

用valgrind --tool=memcheck --leak-check=yes --show-reachable=yes --run-libc-freeres=yes --log-file=./valgrind_report.log ./pegasus_server  config.ini -app_list meta 
==26730== Mismatched free() / delete / delete []
==26730==    at 0x4C2EF90: free (vg_replace_malloc.c:540)
==26730==    by 0x5EA5719: deallocate (new_allocator.h:110)

非常多Mismatched free，
==26730== HEAP SUMMARY:
==26730==     in use at exit: 35,786,093 bytes in 61,548 blocks
==26730==   total heap usage: 118,319 allocs, 56,771 frees, 86,003,342 bytes allocated
==26730== 
==26730== LEAK SUMMARY:
==26730==    definitely lost: 2,477 bytes in 51 blocks
==26730==    indirectly lost: 4,288 bytes in 90 blocks
==26730==      possibly lost: 89,490 bytes in 253 blocks
==26730==    still reachable: 35,689,838 bytes in 61,154 blocks
==26730==                       of which reachable via heuristic:
==26730==                         newarray           : 18,752 bytes in 1 blocks
==26730==         suppressed: 0 bytes in 0 blocks
==26730== Rerun with --leak-check=full to see details of leaked memory
==26730== 
==26730== For lists of detected and suppressed errors, rerun with: -s
==26730== ERROR SUMMARY: 49352 errors from 1000 contexts (suppressed: 0 from 0)

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/429/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/429,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY0NjUwMDY4MA==,incubator-pegasus,646500680,429,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2020-06-19T08:04:51Z,2020-06-19T08:04:51Z,Fixed by https://github.com/XiaoMi/rdsn/pull/477,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY0NjUwMDY4MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/430,https://api.github.com/repos/apache/incubator-pegasus/issues/430,incubator-pegasus,529161295,430,cold backup计时bug,vagetablechicken,24697960,HuangWei,huangwei@apache.org,OPEN,2019-11-27T07:12:47Z,2019-11-27T07:12:47Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
冷备策略设置中，当间隔时间<1天时，会无法正常工作。现象为上一个冷备结束后立即进行下一次冷备，但不会一直下去，大概四五次就会长时间不进行下一次备份（不确定是不是再也不触发）。

2. What did you expect to see?

3. What did you see instead?

4. What version of Pegasus are you using?
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/430/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/431,https://api.github.com/repos/apache/incubator-pegasus/issues/431,incubator-pegasus,529666524,431,Coredump on abnormal multiget,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2019-11-28T03:02:57Z,2019-12-05T10:48:49Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
If possible, provide a recipe for reproducing the error.

Normal bootstrap.

2. What did you expect to see?

No coredump.

3. What did you see instead?

Coredump:

```
#0  0x00007f85a80475c1 in vfprintf () from /lib64/libc.so.6
#1  0x00007f85ac03a5a6 in dsn::tools::simple_logger::dsn_logv (this=0x17f4a80, file=<optimized out>, function=<optimized out>, line=<optimized out>, log_level=LOG_LEVEL_WARNING, 
    fmt=0x52bad38 ""[] rocksdb abnormal multi_get from : hash_key = bikan:5c6ba3b20bc5cb2bb241aba4e3e87567, start_sort_key = \\x00\\x00\\x01n/%nm (inclusive), stop_sort_key =  (exclu""..., args=0x7f85705b1628) at /home/wutao1/pegasus-release/rdsn/src/core/tools/common/simple_logger.cpp:235
#2  0x00007f85abfb260f in dsn_logv (file=0x8b5b92 ""pegasus_server_impl.cpp"", 
    function=0x8b967b <pegasus::server::pegasus_server_impl::on_multi_get(dsn::apps::multi_get_request const&, dsn::rpc_replier<dsn::apps::multi_get_response>&)::__FUNCTION__> ""on_multi_get"", line=955, 
    log_level=LOG_LEVEL_WARNING, 
    fmt=0x52bad38 ""[] rocksdb abnormal multi_get from: hash_key = bikan:5c6ba3b20bc5cb2bb241aba4e3e87567, start_sort_key = \\x00\\x00\\x01n/%nm (inclusive), stop_sort_key =  (exclu""..., args=0x7f85705b1628) at /home/wutao1/pegasus-release/rdsn/src/core/core/logging.cpp:124
#3  0x00007f85abfb273f in dsn_logf (file=file@entry=0x8b5b92 ""pegasus_server_impl.cpp"", 
    function=function@entry=0x8b967b <pegasus::server::pegasus_server_impl::on_multi_get(dsn::apps::multi_get_request const&, dsn::rpc_replier<dsn::apps::multi_get_response>&)::__FUNCTION__> ""on_multi_get"", 
    line=line@entry=955, log_level=log_level@entry=LOG_LEVEL_WARNING, fmt=<optimized out>) at /home/wutao1/pegasus-release/rdsn/src/core/core/logging.cpp:141
#4  0x00000000005f4aa2 in pegasus::server::pegasus_server_impl::on_multi_get (this=0x14449c800, request=..., reply=...) at /home/wutao1/pegasus-release/src/server/pegasus_server_impl.cpp:932
#5  0x00000000005daf5d in bool dsn::replication::storage_serverlet<dsn::apps::rrdb_service>::register_async_rpc_handler<dsn::apps::multi_get_request, dsn::apps::multi_get_response>(dsn::task_code, char const*, void (*)(dsn::apps::rrdb_service*, dsn::apps::multi_get_request const&, dsn::rpc_replier<dsn::apps::multi_get_response>&))::{lambda(dsn::apps::rrdb_service*, dsn::message_ex*)#1}::operator()(dsn::apps::rrdb_service*, dsn::message_ex*) const () at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/dist/replication/storage_serverlet.h:29
#6  0x00000000005f92cc in operator() (__args#1=0x985c5d3c8, __args#0=0x14449c800, this=<optimized out>) at /home/wutao1/app/include/c++/4.8.2/functional:2464
#7  handle_request (request=0x985c5d3c8, this=0x14449c800) at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/dist/replication/storage_serverlet.h:80
#8  dsn::apps::rrdb_service::on_request (this=0x14449c800, request=0x985c5d3c8) at /home/wutao1/pegasus-release/src/include/rrdb/rrdb.server.h:17
#9  0x00007f85abe71e75 in dsn::replication::replica::on_client_read (this=0xe1f10f00, request=request@entry=0x985c5d3c8) at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica.cpp:171
#10 0x00007f85abed036f in dsn::replication::replica_stub::on_client_read (this=0x1c12580, id=..., request=0x985c5d3c8) at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_stub.cpp:770
#11 0x00007f85abfd20e9 in dsn::task::exec_internal (this=this@entry=0x985c5d560) at /home/wutao1/pegasus-release/rdsn/src/core/core/task.cpp:180
#12 0x00007f85abfe62ad in dsn::task_worker::loop (this=0x20b2a50) at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:211
#13 0x00007f85abfe6479 in dsn::task_worker::run_internal (this=0x20b2a50) at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:191
#14 0x00007f85a898a600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>) at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#15 0x00007f85a94efdc5 in start_thread () from /lib64/libpthread.so.0
#16 0x00007f85a80f473d in clone () from /lib64/libc.so.6
```

Related code point: pegasus/src/server/pegasus_server_impl.cpp:932

```
    uint64_t time_used = dsn_now_ns() - start_time;
    if (is_multi_get_abnormal(time_used, size, iterate_count)) {
        dwarn_replica(
            ""rocksdb abnormal multi_get from {}: hash_key = {}, ""
            ""start_sort_key = {} ({}), stop_sort_key = {} ({}), ""
            ""sort_key_filter_type = {}, sort_key_filter_pattern = {}, ""
            ""max_kv_count = {}, max_kv_size = {}, reverse = {}, ""
            ""result_count = {}, result_size = {}, iterate_count = {}, ""
            ""expire_count = {}, filter_count = {}, time_used = {} ns"",
            reply.to_address().to_string(),
            ::pegasus::utils::c_escape_string(request.hash_key),
            ::pegasus::utils::c_escape_string(request.start_sortkey),
            request.start_inclusive ? ""inclusive"" : ""exclusive"",
            ::pegasus::utils::c_escape_string(request.stop_sortkey),
            request.stop_inclusive ? ""inclusive"" : ""exclusive"",
            ::dsn::apps::_filter_type_VALUES_TO_NAMES.find(request.sort_key_filter_type)->second,
            ::pegasus::utils::c_escape_string(request.sort_key_filter_pattern),
            request.max_kv_count,
            request.max_kv_size,
            request.reverse ? ""true"" : ""false"",
            count,
            size,
            iterate_count,
            expire_count,
            filter_count,
            time_used);
        _pfc_recent_abnormal_count->increment();
    }
```

4. What version of Pegasus are you using?

1.12.0 Pegasus Server 1.12.0 (44a5293872a6f9fd3c6f6059e39d74e226422c78)  release","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/431/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/431,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1OTY0MjM4Ng==,incubator-pegasus,559642386,431,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2019-11-29T02:51:50Z,2019-11-29T02:51:50Z,"1.通过gdb查看core文件，是在打印`rocksdb abnormal multi_get from: hash_key = bikan:5c6ba3b20bc5cb2bb241aba4e3e87567, ""
                  ""start_sort_key = \x00\x00\x01%n/nm (inclusive), stop_sort_key =  (exclusive), sort_key_filter_type = FT_NO_FILTER, ""
                  ""sort_key_filter_pattern = , max_kv_count = 8000, max_kv_size = 0, reverse = true, result_count = 2736, result_size = 395854, ""
                  ""iterate_count = 2736, expire_count = 0, filter_count = 0, time_used = 6033235 ns`的时候coredump的。

2.在onebox上尝试打印该log内容，同样coredump，并提示`*** %n in writable segment detected ***`。将%n去掉就不再coredump。

3.由于1.11.6是使用dwarn打印，1.12.0使用dwarn_replica打印，但是1.11.6一直没有问题，所以尝试使用
`std::string sort_key = ""\\x00\\x00\\x01%n/nm""；`
`dwarn(""sort_key = %s"", sort_key.c_str())`，
发现是正常的，但是用dwarn_replcia和dwarn_f就会coredump

4.由于dwarn_replcia和dwarn_f相比dwarn使用了fmt进行了一次format操作，所以怀疑是fmt的问题，然后升级fmt到6.0.0，发现仍然存在同样的问题

5.分别将使用dwarn和dwarn_f/dwarn_replcia最终执行到simple_logger::dsn_logv接口里的fmt变量获取到，发现两者不同：
`dwarn: sort_key = %s`
`dwarn_f/warn_replica: sort_key = \x00\x00\x01%n/nm`

6.最后通过查看代码发现，由于dwarn_f/dwarn_replcia最开始做了一次format，将fmt从`sort_key = %s`转换成了`sort_key = \x00\x00\x01%n/nm`，然后底层的simple_logger写的时候，使用的`vfprintf(_log, fmt, args)`，由于字符串里包含了%n，会尝试再做一次格式化，然而此时的可变参数列表是空的，所以出现coredump。所以最终确定是dwarn_f和dwarn_replica的接口实现问题

解决方案：重新实现dwan_f和dwarn_replica的实现，不要依赖于dwarn实现。重新实现底层的日志接口供dwan_f和dwarn_replica调用，该底层日志接口不要使用`vfprintf(_log, fmt, args)`，而是使用`fprintf(_log, ""%s"", fmt)`（已经试用过，改成fpintf没问题）","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1OTY0MjM4Ng==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/431,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1OTY0NjQ3Mw==,incubator-pegasus,559646473,431,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2019-11-29T03:19:56Z,2019-11-29T03:19:56Z,"> 解决方案：重新实现dwan_f和dwarn_replica的实现，不要依赖于dwarn实现。

不依赖与dwarn实现是什么意思？","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1OTY0NjQ3Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/431,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1OTY0ODQzNA==,incubator-pegasus,559648434,431,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2019-11-29T03:33:12Z,2019-11-29T03:33:12Z,"> > 解决方案：重新实现dwan_f和dwarn_replica的实现，不要依赖于dwarn实现。
> 
> 不依赖与dwarn实现是什么意思？

dwarn_f和dwarn_replica最后通过调用dwarn来实现的：`#define dwarn_f(...) dwarn(fmt::format(__VA_ARGS__).c_str())`，然后如上面所说出现了coredump，后面想不依赖这个接口实现了，重新实现一个。我再看看可不可以通过修改底层接口方式，改动小一点","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1OTY0ODQzNA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/431,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1OTY4NjA3MA==,incubator-pegasus,559686070,431,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2019-11-29T07:16:48Z,2019-11-29T07:16:48Z,"> > > 解决方案：重新实现dwan_f和dwarn_replica的实现，不要依赖于dwarn实现。
> > 
> > 
> > 不依赖与dwarn实现是什么意思？
> 
> dwarn_f和dwarn_replica最后通过调用dwarn来实现的：`#define dwarn_f(...) dwarn(fmt::format(__VA_ARGS__).c_str())`，然后如上面所说出现了coredump，后面想不依赖这个接口实现了，重新实现一个。我再看看可不可以通过修改底层接口方式，改动小一点

dwarn_replica和dwarn_f都只是固定格式的dwarn。
简单来看的话，避免它们转义是不是就可以了？
如果考虑更仔细的话，dwarn是不是也有可能被转义字符搞挂？","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1OTY4NjA3MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/431,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1OTY4Nzg0Mw==,incubator-pegasus,559687843,431,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-11-29T07:24:41Z,2019-11-29T07:24:41Z,"@vagetablechicken 是不是立伟没讲清楚，是这样：

```
dwarn(""%s"", ""%n"") -> vfprintf(""%s"", ""%n"");
```

```
dwarn_f(""{}"", ""%n"") -> dwarn(""%n"") -> vfprintf(""%n"")
```

上面那个跑的是没问题的，下面这个是有问题的。这个问题怎么改呢，一种改的方式是下面这个：

```
dwarn_f(""{}"", ""%n"") -> vfprintf(""%s"", fmt::format(""{}"", ""%n""))
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1OTY4Nzg0Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/431,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1OTY5Mjk1NQ==,incubator-pegasus,559692955,431,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2019-11-29T07:46:26Z,2019-11-29T07:46:26Z,"> > > > 解决方案：重新实现dwan_f和dwarn_replica的实现，不要依赖于dwarn实现。
> > > 
> > > 
> > > 不依赖与dwarn实现是什么意思？
> > 
> > 
> > dwarn_f和dwarn_replica最后通过调用dwarn来实现的：`#define dwarn_f(...) dwarn(fmt::format(__VA_ARGS__).c_str())`，然后如上面所说出现了coredump，后面想不依赖这个接口实现了，重新实现一个。我再看看可不可以通过修改底层接口方式，改动小一点
> 
> dwarn_replica和dwarn_f都只是固定格式的dwarn。
> 简单来看的话，避免它们转义是不是就可以了？
> 如果考虑更仔细的话，dwarn是不是也有可能被转义字符搞挂？

dwarn确实是有可能的，我试过了，如果`dwarn(""\x00\x00\x01%n/nm"")`的话，一样会coredump。printf的时候如果参数列表和fmt不匹配的话会提示，但是dwarn没有","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU1OTY5Mjk1NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/434,https://api.github.com/repos/apache/incubator-pegasus/issues/434,incubator-pegasus,530691286,434,"Weekly Digest (24 November, 2019 - 1 December, 2019)",weekly-digest,,,,CLOSED,2019-12-01T07:59:22Z,2020-02-16T16:45:26Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 5 issues were created.
Of these, 2 issues have been closed and 3 issues are still open.
## OPEN ISSUES
:green_heart: #431 [Coredump on abnormal multiget](https://github.com/XiaoMi/pegasus/issues/431), by [neverchanje](https://github.com/neverchanje)
:green_heart: #430 [cold backup计时bug](https://github.com/XiaoMi/pegasus/issues/430), by [vagetablechicken](https://github.com/vagetablechicken)
:green_heart: #429 [pegasus meta 进程内存泄露](https://github.com/XiaoMi/pegasus/issues/429), by [sunwsh](https://github.com/sunwsh)
## CLOSED ISSUES
:heart: #433 [feat: update config for tcmalloc release memory optimization](https://github.com/XiaoMi/pegasus/pull/433), by [hycdong](https://github.com/hycdong)
:heart: #432 [WIP (feat)rocksdb: Adapt prefix bloom filter to speedup scan by hashkey](https://github.com/XiaoMi/pegasus/pull/432), by [acelyc111](https://github.com/acelyc111)
## NOISY ISSUE
:speaker: #431 [Coredump on abnormal multiget](https://github.com/XiaoMi/pegasus/issues/431), by [neverchanje](https://github.com/neverchanje)
It received 6 comments.

 - - - 
# PULL REQUESTS
Last week, 2 pull requests were created, updated or merged.
## UPDATED PULL REQUEST
Last week, 1 pull request was updated.
:yellow_heart: #427 [feat(collector): add statistics for partition hotspot](https://github.com/XiaoMi/pegasus/pull/427), by [levy5307](https://github.com/levy5307)
## MERGED PULL REQUEST
Last week, 1 pull request was merged.
:purple_heart: #433 [feat: update config for tcmalloc release memory optimization](https://github.com/XiaoMi/pegasus/pull/433), by [hycdong](https://github.com/hycdong)

 - - - 
# COMMITS
Last week there was 1 commit.
:hammer_and_wrench: [feat: update config for tcmalloc release memory optimization (#433)](https://github.com/XiaoMi/pegasus/commit/d46429d0e5c44895aa8da8faaf9cd6f41a72a8ad) by [hycdong](https://github.com/hycdong)

 - - - 
# CONTRIBUTORS
Last week there was 1 contributor.
:bust_in_silhouette: [hycdong](https://github.com/hycdong)

 - - - 
# STARGAZERS
Last week there was 1 stargazer.
:star: [lilac](https://github.com/lilac)
You are the star! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/434/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/439,https://api.github.com/repos/apache/incubator-pegasus/issues/439,incubator-pegasus,534506562,439,"Weekly Digest (1 December, 2019 - 8 December, 2019)",weekly-digest,,,,CLOSED,2019-12-08T07:59:23Z,2020-02-16T16:47:10Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 4 issues were created.
Of these, 3 issues have been closed and 1 issues are still open.
## OPEN ISSUES
:green_heart: #438 [feat(rocksdb): Adapt prefix bloom filter to speedup scan by hashkey](https://github.com/XiaoMi/pegasus/pull/438), by [acelyc111](https://github.com/acelyc111)
## CLOSED ISSUES
:heart: #437 [feat(collector): add statistics for estimate key number of table](https://github.com/XiaoMi/pegasus/pull/437), by [Shuo-Jia](https://github.com/Shuo-Jia)
:heart: #436 [chore: upgrade dev version to 1.13.SNAPSHOT](https://github.com/XiaoMi/pegasus/pull/436), by [neverchanje](https://github.com/neverchanje)
:heart: #435 [feat(collector): add statistics for estimate key number of partition](https://github.com/XiaoMi/pegasus/pull/435), by [Shuo-Jia](https://github.com/Shuo-Jia)

 - - - 
# PULL REQUESTS
Last week, 5 pull requests were created, updated or merged.
## UPDATED PULL REQUEST
Last week, 2 pull requests were updated.
:yellow_heart: #427 [feat(collector): add statistics for partition hotspot](https://github.com/XiaoMi/pegasus/pull/427), by [levy5307](https://github.com/levy5307)
:yellow_heart: #399 [feat(dup): implement pegasus_mutation_duplicator](https://github.com/XiaoMi/pegasus/pull/399), by [neverchanje](https://github.com/neverchanje)
## MERGED PULL REQUEST
Last week, 3 pull requests were merged.
:purple_heart: #437 [feat(collector): add statistics for estimate key number of table](https://github.com/XiaoMi/pegasus/pull/437), by [Shuo-Jia](https://github.com/Shuo-Jia)
:purple_heart: #436 [chore: upgrade dev version to 1.13.SNAPSHOT](https://github.com/XiaoMi/pegasus/pull/436), by [neverchanje](https://github.com/neverchanje)
:purple_heart: #435 [feat(collector): add statistics for estimate key number of partition](https://github.com/XiaoMi/pegasus/pull/435), by [Shuo-Jia](https://github.com/Shuo-Jia)

 - - - 
# COMMITS
Last week there were 3 commits.
:hammer_and_wrench: [feat(collector): add statistics for estimate key number of table (#437)](https://github.com/XiaoMi/pegasus/commit/59e54d67e6356d7030bc4dc6767eec8d4a9c5dc0) by [Shuo-Jia](https://github.com/Shuo-Jia)
:hammer_and_wrench: [chore: upgrade dev version to 1.13.SNAPSHOT (#436)](https://github.com/XiaoMi/pegasus/commit/c072e5ddfe238202c5abef0457053db38f2b9cc8) by [neverchanje](https://github.com/neverchanje)
:hammer_and_wrench: [feat(collector): add statistics for estimate key number of partition (#435)](https://github.com/XiaoMi/pegasus/commit/7ee9fb2d0406bc57cd42ebd643e803c7775fcd66) by [Shuo-Jia](https://github.com/Shuo-Jia)

 - - - 
# CONTRIBUTORS
Last week there were 2 contributors.
:bust_in_silhouette: [Shuo-Jia](https://github.com/Shuo-Jia)
:bust_in_silhouette: [neverchanje](https://github.com/neverchanje)

 - - - 
# STARGAZERS
Last week there were 9 stagazers.
:star: [mt21625457](https://github.com/mt21625457)
:star: [bootsrc](https://github.com/bootsrc)
:star: [SeasonLee](https://github.com/SeasonLee)
:star: [JiangChenYang](https://github.com/JiangChenYang)
:star: [WingsGo](https://github.com/WingsGo)
:star: [chucongqing](https://github.com/chucongqing)
:star: [byrJZhong22](https://github.com/byrJZhong22)
:star: [XyuWang](https://github.com/XyuWang)
:star: [Smityz](https://github.com/Smityz)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there was 1 release.
:rocket: [v1.12.1 v1.12.1](https://github.com/XiaoMi/pegasus/releases/tag/v1.12.1)

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/439/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/440,https://api.github.com/repos/apache/incubator-pegasus/issues/440,incubator-pegasus,535638231,440,http url中含有‘#’的处理问题,levy5307,22141103,赵立伟,zlw5307@163.com,CLOSED,2019-12-10T10:33:09Z,2019-12-20T05:31:46Z,"在http请求中，'#'之后的部分叫做锚部分，该部分代表网页中的一个位置，用于浏览器读取这个url后，自动滚动到锚部分指定的区域。

由于锚部分是用来指导浏览器动作的，对服务器完全无用，所以HTTP请求中不会包含锚部分，也就是说，会将其截断、不发送给服务器。

目前我们有一个http接口，用于获取perf counter信息，例如：
```http://127.0.0.1:34101/perfCounter?name=collector*app.pegasus*app.stat.read_qps#_all_```用于获取名字为 `collector*app.pegasus*app.stat.read_qps#_all_`的perf counter信息，但是由于该名字中包含了‘#’，所以会将#_all_这部分截断，服务器获取到的请求就变成了```http://127.0.0.1:34101/perfCounter?name=collector*app.pegasus*app.stat.read_qps```，导致无法获取正确的perf counter信息。

目前我想到的办法是:
1.对用户端发出的请求和服务端收到的请求添加编解码，对这种特殊字符进行转义。
2.实现http_message_parser中对header部分的解析，将name={perf_counter_name}放到header部分中

1是比较常规的做法，业内对于传递特殊字符都需要编解码。2这里感觉也比较有必要，因为目前我们的http实现不健全，对于header和body部分都没有实现读取","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/440/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/440,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU2NDgzNzYwNQ==,incubator-pegasus,564837605,440,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2019-12-12T03:47:31Z,2019-12-12T03:47:31Z,"`name=collector*app.pegasus*app.stat.read_qps#_all_`
其实根本原因是我们的perf counter名字太诡异了，包含各种符号，用户是难以记忆的，后面的思路应该是怎么去优化名字的规则","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU2NDgzNzYwNQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/440,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU2NDg2NTExNA==,incubator-pegasus,564865114,440,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2019-12-12T06:10:18Z,2019-12-12T06:10:18Z,"> `name=collector*app.pegasus*app.stat.read_qps#_all_`
> 其实根本原因是我们的perf counter名字太诡异了，包含各种符号，用户是难以记忆的，后面的思路应该是怎么去优化名字的规则

名字诡异确实是一方面，但是uri编解码也是必要的","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU2NDg2NTExNA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/440,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU2Nzc5NDQ5Ng==,incubator-pegasus,567794496,440,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2019-12-20T05:31:46Z,2019-12-20T05:31:46Z,"This problem was solved by https://github.com/XiaoMi/rdsn/pull/357, reopen it when there is futher issue.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU2Nzc5NDQ5Ng==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/441,https://api.github.com/repos/apache/incubator-pegasus/issues/441,incubator-pegasus,536219133,441,coredump when replay private log ,hycdong,17868458,HeYuchen,377710264@qq.com,OPEN,2019-12-11T08:36:04Z,2019-12-11T08:36:04Z,"### Bug Report
Time: 2019/12/11
Version: 1.11.6

### Coredump
```
(gdb) bt
#0  0x00007f217c4ab1d7 in raise () from /lib64/libc.so.6
#1  0x00007f217c4ac8c8 in abort () from /lib64/libc.so.6
#2  0x00007f217ffea9fe in dsn_coredump () at /home/wutao1/pegasus-release/rdsn/src/core/core/service_api_c.cpp:76
#3  0x00007f217fecf589 in dsn::replication::prepare_list::commit (this=this@entry=0x7f213933f010, d=16635787, ct=ct@entry=dsn::replication::COMMIT_TO_DECREE_HARD)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/prepare_list.cpp:141
#4  0x00007f217fecfa12 in dsn::replication::prepare_list::prepare (this=0x7f213933f010, mu=..., status=status@entry=dsn::replication::partition_status::PS_SECONDARY)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/prepare_list.cpp:80
#5  0x00007f217ff136f9 in operator() (log_length=<optimized out>, mu=..., __closure=0x48418c258) at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_learn.cpp:1415
#6  std::_Function_handler<bool(int, dsn::ref_ptr<dsn::replication::mutation>&), dsn::replication::replica::apply_learned_state_from_private_log(dsn::replication::learn_state&)::__lambda24>::_M_invoke(const std::_Any_data &, int, dsn::ref_ptr<dsn::replication::mutation> &) (__functor=..., __args#0=<optimized out>, __args#1=...) at /home/wutao1/app/include/c++/4.8.2/functional:2057
#7  0x00007f217fec4aa1 in dsn::replication::mutation_log::replay(dsn::ref_ptr<dsn::replication::log_file>, std::function<bool (int, dsn::ref_ptr<dsn::replication::mutation>&)>, long&) (log=..., callback=..., 
    end_offset=@0x7f213933efb8: 3490466037) at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/mutation_log.cpp:885
#8  0x00007f217fec4e9f in dsn::replication::mutation_log::replay(std::map<int, dsn::ref_ptr<dsn::replication::log_file>, std::less<int>, std::allocator<std::pair<int const, dsn::ref_ptr<dsn::replication::log_file> > > >&, std::function<bool (int, dsn::ref_ptr<dsn::replication::mutation>&)>, long&) (logs=..., callback=..., end_offset=@0x7f213933efb8: 3490466037) at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/mutation_log.cpp:967
#9  0x00007f217fec7415 in dsn::replication::mutation_log::replay(std::vector<std::string, std::allocator<std::string> >&, std::function<bool (int, dsn::ref_ptr<dsn::replication::mutation>&)>, long&) (log_files=..., callback=..., 
    end_offset=@0x7f213933efb8: 3490466037) at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/mutation_log.cpp:927
#10 0x00007f217ff13ea9 in dsn::replication::replica::apply_learned_state_from_private_log (this=this@entry=0xa37885600, state=...) at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_learn.cpp:1418
#11 0x00007f217fef2ae6 in dsn::replication::replica::catch_up_with_private_logs (this=0xa37885600, s=dsn::replication::partition_status::PS_POTENTIAL_SECONDARY)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_chkpt.cpp:335
#12 0x00007f217fffccd9 in dsn::task::exec_internal (this=this@entry=0x11c99c59d4) at /home/wutao1/pegasus-release/rdsn/src/core/core/task.cpp:180
#13 0x00007f2180010a6d in dsn::task_worker::loop (this=0x2a7b3f0) at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:211
#14 0x00007f2180010c39 in dsn::task_worker::run_internal (this=0x2a7b3f0) at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:191
#15 0x00007f217ce03600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>) at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#16 0x00007f217da76dc5 in start_thread () from /lib64/libpthread.so.0
#17 0x00007f217c56d73d in clone () from /lib64/libc.so.6
```
```
(gdb) f 3
#3  0x00007f217fecf589 in dsn::replication::prepare_list::commit (this=this@entry=0x7f213933f010, d=16635787, ct=ct@entry=dsn::replication::COMMIT_TO_DECREE_HARD)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/prepare_list.cpp:141
141	/home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/prepare_list.cpp: No such file or directory.
(gdb) p mu
$1 = {_obj = 0x0}
```
It seems that mutation whose decree is 16635787 is nullptr and is missing in prepare list.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/441/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/443,https://api.github.com/repos/apache/incubator-pegasus/issues/443,incubator-pegasus,538017322,443,"Weekly Digest (8 December, 2019 - 15 December, 2019)",weekly-digest,,,,CLOSED,2019-12-15T07:59:23Z,2020-02-16T16:44:55Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 3 issues were created.
Of these, 1 issues have been closed and 2 issues are still open.
## OPEN ISSUES
:green_heart: #441 [coredump when replay private log ](https://github.com/XiaoMi/pegasus/issues/441), by [hycdong](https://github.com/hycdong)
:green_heart: #440 [http url中含有‘#’的处理问题](https://github.com/XiaoMi/pegasus/issues/440), by [levy5307](https://github.com/levy5307)
## CLOSED ISSUES
:heart: #442 [feat: add http_server for info collector](https://github.com/XiaoMi/pegasus/pull/442), by [levy5307](https://github.com/levy5307)
## NOISY ISSUE
:speaker: #440 [http url中含有‘#’的处理问题](https://github.com/XiaoMi/pegasus/issues/440), by [levy5307](https://github.com/levy5307)
It received 2 comments.

 - - - 
# PULL REQUESTS
Last week, 2 pull requests were created, updated or merged.
## UPDATED PULL REQUEST
Last week, 1 pull request was updated.
:yellow_heart: #438 [feat(rocksdb): Adapt prefix bloom filter to speedup scans by hashkey](https://github.com/XiaoMi/pegasus/pull/438), by [acelyc111](https://github.com/acelyc111)
## MERGED PULL REQUEST
Last week, 1 pull request was merged.
:purple_heart: #442 [feat: add http_server for info collector](https://github.com/XiaoMi/pegasus/pull/442), by [levy5307](https://github.com/levy5307)

 - - - 
# COMMITS
Last week there was 1 commit.
:hammer_and_wrench: [feat: add http_server for info collector (#442)](https://github.com/XiaoMi/pegasus/commit/58d924910e93533d888bbc1f2f79907049ed923b) by [levy5307](https://github.com/levy5307)

 - - - 
# CONTRIBUTORS
Last week there was 1 contributor.
:bust_in_silhouette: [levy5307](https://github.com/levy5307)

 - - - 
# STARGAZERS
Last week there were 3 stagazers.
:star: [bainiu87](https://github.com/bainiu87)
:star: [snczl](https://github.com/snczl)
:star: [awkj](https://github.com/awkj)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/443/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/445,https://api.github.com/repos/apache/incubator-pegasus/issues/445,incubator-pegasus,538903608,445,增加流量热点检测功能,Smityz,22953824,Smilencer,smityz@qq.com,CLOSED,2019-12-17T08:06:52Z,2021-05-14T16:57:53Z,"# 增加流量热点检测功能

## 1 背景

目前pegasus针对热点流量检测还是比较原始地罗列监控数值，使用人工的方式进行故障检测和故障排查，在运维上显得略微繁琐。为了使得数据能更加直观地呈现，我们增加了流量热点检测模块，将热点问题相关数值进行打分并反馈到falcon，减少了运维人员的负担。

此模块的功能为实时收集partition级别的信息，并根据对应算法，计算出每个partition的热点值并反馈到falcon上。通过各个partition热点值的变化曲线，运维人员可以更加迅速地推断出出现热点的位置。

同时此工作也为热点数据读写问题的解决方案的工作之一。

## 2 算法框架

### 2.1 调用关系 		

在[info_collector.cpp](https://github.com/XiaoMi/pegasus/blob/master/src/server/info_collector.cpp#L130)中各个app中partition的数据将会按照`_app_stat_interval_seconds`的配置进行采集(默认间隔为10s)。

### 2.2 数据结构

#### 2.2.1 数据库级

使用了`std::map<std::string, hotpot_calculator *> _calculator_store`保存了所有app的hotpot_calculator的指针，以便在遍历app时使用对应的算法与数据进行运算。

#### 2.2.2 app级

在每个app对应的hotpot_calculator中存有:

```c++
class hotpot_calculator
{
public:
    std::vector<std::queue<Data_store>> data_stores;
    hotpot_calculator(const std::string &name, const int &app_size);
    void aggregate(std::vector<row_data> partitions);
    void start_alg();

private:
    std::string _app_name;
    Hotspot_policy *_policy;
    std::vector<dsn::perf_counter> _hotpot_points;
};
```

其中`_app_name`为app名,` _policy`为对应的算法,`_hotpot_points`为当前app中输出到perf_counter的每个partition的热点值。 `data_stores`保存了当前app中每个partition的历史记录，queue则是将历史数据做了缓存，便于算法查询。

#### 2.2.3 paritition级

对于每个partition, 我们使用了一个data_store来存储单次采集收集的数据。目前确定可以收集到的信息为:

```c++
double total_get_qps = 0;
double total_multi_get_qps = 0;
double total_put_qps = 0;
double total_multi_put_qps = 0;
double total_remove_qps = 0;
double total_multi_remove_qps = 0;
double total_incr_qps = 0;
double total_check_and_set_qps = 0;
double total_check_and_mutate_qps = 0;
double total_scan_qps = 0;
double total_recent_read_cu = 0;
double total_recent_write_cu = 0;
```

我们的算法将基于这些信息去做打分。

#### 2.2.4 类图
![Screenshot from 2019-12-17 16-22-25](https://user-images.githubusercontent.com/22953824/70977785-90fa1a00-20e9-11ea-9764-23bb8426ca5c.png)



### 2.3 使用方法

#### 代码示例

```c++
std::map<std::string, std::vector<row_data>> all_rows;
if (!get_app_partition_stat(&_shell_context, all_rows)) {
    derror(""call get_app_stat() failed"");
    return;
}
//首先取出当前所有app的信息
for (auto app_rows : all_rows) {
    hotpot_calculator *app_store = nullptr;
//取出map中对应app
    get_store_handler(app_rows.first, app_rows.second.size(), app_store);
//收集当前app所有partition的数据
    app_store->aggregate(app_rows.second);
//使用对应算法计算结果并输出
    app_store->start_alg();
}
```




","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/445/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/445,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU2NjkxODUxOQ==,incubator-pegasus,566918519,445,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2019-12-18T07:51:07Z,2019-12-18T07:51:07Z,Good job~,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU2NjkxODUxOQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/445,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU2NjkyMDg5Mw==,incubator-pegasus,566920893,445,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2019-12-18T07:59:34Z,2019-12-18T07:59:34Z,"hotpot_calculator里的_name改成_app_name吧，不看你这边文章还以为是caculator的名字。
还有
```
//首先取出当前所有app的信息
for (auto app_rows : all_rows) {
    hotpot_calculator *app_store = nullptr;
    get_store_handler(app_rows.first, app_rows.second.size(), app_store);
//取出map中对应app
    app_store->aggregate(app_rows.second);
//收集当前app所有partition的数据
    app_store->start_alg();
//使用对应算法计算结果并输出
}```


这里注释写到上面一行吧","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU2NjkyMDg5Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/445,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU2NjkyMjY4MQ==,incubator-pegasus,566922681,445,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2019-12-18T08:05:48Z,2019-12-18T08:05:48Z,"> hotpot_calculator里的_name改成_app_name吧，不看你这边文章还以为是caculator的名字。
> 还有
> 
> ```
> //首先取出当前所有app的信息
> for (auto app_rows : all_rows) {
>     hotpot_calculator *app_store = nullptr;
>     get_store_handler(app_rows.first, app_rows.second.size(), app_store);
> //取出map中对应app
>     app_store->aggregate(app_rows.second);
> //收集当前app所有partition的数据
>     app_store->start_alg();
> //使用对应算法计算结果并输出
> }```
> 
> 
> 这里注释写到上面一行吧
> ```

ok~","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU2NjkyMjY4MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/447,https://api.github.com/repos/apache/incubator-pegasus/issues/447,incubator-pegasus,539411491,447,Coredump in c++ client,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,OPEN,2019-12-18T02:18:21Z,2019-12-18T02:19:02Z,"## Bug Report

小爱服务 yuyongrong 报告 coredump，有可能是Pegasus Cilent引起的。

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
If possible, provide a recipe for reproducing the error.

2. What did you expect to see?

3. What did you see instead?
coredump
```
(gdb) bt
#0  0x00007f86b8e3d1d7 in raise () from /lib64/libc.so.6
#1  0x00007f86b8e3e8c8 in abort () from /lib64/libc.so.6
#2  0x00007f86ba2dfa95 in __gnu_cxx::__verbose_terminate_handler ()
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/libsupc++/vterminate.cc:95
#3  0x00007f86ba2ddc06 in __cxxabiv1::__terminate (handler=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/libsupc++/eh_terminate.cc:38
#4  0x00007f86ba2ddc33 in std::terminate () at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/libsupc++/eh_terminate.cc:48
#5  0x00007f86ba334695 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:92
#6  0x00007f86b91d0dc5 in start_thread () from /lib64/libpthread.so.0
#7  0x00007f86b8eff73d in clone () from /lib64/libc.so.6
```

4. What version of Pegasus are you using?
1.5.6","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/447/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/448,https://api.github.com/repos/apache/incubator-pegasus/issues/448,incubator-pegasus,541420749,448,"Weekly Digest (15 December, 2019 - 22 December, 2019)",weekly-digest,,,,CLOSED,2019-12-22T07:59:24Z,2020-02-16T16:44:52Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 4 issues were created.
Of these, 1 issues have been closed and 3 issues are still open.
## OPEN ISSUES
:green_heart: #447 [Coredump in c++ client](https://github.com/XiaoMi/pegasus/issues/447), by [qinzuoyan](https://github.com/qinzuoyan)
:green_heart: #445 [增加流量热点检测功能](https://github.com/XiaoMi/pegasus/issues/445), by [Smityz](https://github.com/Smityz)
:green_heart: #444 [feat(collector): add statistics for partition hotspot](https://github.com/XiaoMi/pegasus/pull/444), by [Smityz](https://github.com/Smityz)
## CLOSED ISSUES
:heart: #446 [build: add sanitizer support](https://github.com/XiaoMi/pegasus/pull/446), by [Shuo-Jia](https://github.com/Shuo-Jia)
## LIKED ISSUE
:+1: #445 [增加流量热点检测功能](https://github.com/XiaoMi/pegasus/issues/445), by [Smityz](https://github.com/Smityz)
It received :+1: x1, :smile: x0, :tada: x0 and :heart: x0.
## NOISY ISSUE
:speaker: #445 [增加流量热点检测功能](https://github.com/XiaoMi/pegasus/issues/445), by [Smityz](https://github.com/Smityz)
It received 3 comments.

 - - - 
# PULL REQUESTS
Last week, 4 pull requests were created, updated or merged.
## UPDATED PULL REQUEST
Last week, 2 pull requests were updated.
:yellow_heart: #444 [feat(collector): add statistics for partition hotspot](https://github.com/XiaoMi/pegasus/pull/444), by [Smityz](https://github.com/Smityz)
:yellow_heart: #427 [feat(collector): add statistics for partition hotspot](https://github.com/XiaoMi/pegasus/pull/427), by [levy5307](https://github.com/levy5307)
## MERGED PULL REQUEST
Last week, 2 pull requests were merged.
:purple_heart: #446 [build: add sanitizer support](https://github.com/XiaoMi/pegasus/pull/446), by [Shuo-Jia](https://github.com/Shuo-Jia)
:purple_heart: #438 [feat(rocksdb): Adapt prefix bloom filter to speedup scans by hashkey](https://github.com/XiaoMi/pegasus/pull/438), by [acelyc111](https://github.com/acelyc111)

 - - - 
# COMMITS
Last week there were 2 commits.
:hammer_and_wrench: [build: add sanitizer support (#446)](https://github.com/XiaoMi/pegasus/commit/07b2e0ac8d4ba6c61e1c7e39fbd03d9ae24066a4) by [Shuo-Jia](https://github.com/Shuo-Jia)
:hammer_and_wrench: [feat(rocksdb): Adapt prefix bloom filter to speedup scans by hashkey (#438)](https://github.com/XiaoMi/pegasus/commit/cecef440cfe46a33fe3b44f21b1fbadaf5f3f34d) by [acelyc111](https://github.com/acelyc111)

 - - - 
# CONTRIBUTORS
Last week there were 2 contributors.
:bust_in_silhouette: [Shuo-Jia](https://github.com/Shuo-Jia)
:bust_in_silhouette: [acelyc111](https://github.com/acelyc111)

 - - - 
# STARGAZERS
Last week there were 3 stagazers.
:star: [Yi-Hunter](https://github.com/Yi-Hunter)
:star: [fuxiangduan](https://github.com/fuxiangduan)
:star: [yaocong](https://github.com/yaocong)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/448/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/451,https://api.github.com/repos/apache/incubator-pegasus/issues/451,incubator-pegasus,542245314,451,build error in thirdparty zk when using gcc 8.3,vagetablechicken,24697960,HuangWei,huangwei@apache.org,OPEN,2019-12-25T03:35:01Z,2019-12-25T03:35:01Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
gcc8.3.1 build thirdparty zk3.4.10

2. What did you expect to see?
No error

3. What did you see instead?
https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=897892

4. What version of Pegasus are you using?

## Fix solutions

https://salsa.debian.org/java-team/zookeeper/commit/8f12c964bbc1fc1c025d5fc5e7011a5a67d75181","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/451/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/452,https://api.github.com/repos/apache/incubator-pegasus/issues/452,incubator-pegasus,543334688,452,"Weekly Digest (22 December, 2019 - 29 December, 2019)",weekly-digest,,,,CLOSED,2019-12-29T07:59:24Z,2020-02-16T16:44:50Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 3 issues were created.
Of these, 1 issues have been closed and 2 issues are still open.
## OPEN ISSUES
:green_heart: #451 [build error in thirdparty zk when using gcc 8.3](https://github.com/XiaoMi/pegasus/issues/451), by [vagetablechicken](https://github.com/vagetablechicken)
:green_heart: #450 [feat(rocksdb): Select the option of Direct-IO in Rocksdb](https://github.com/XiaoMi/pegasus/pull/450), by [Smityz](https://github.com/Smityz)
## CLOSED ISSUES
:heart: #449 [feat(rocksdb): Select the option of Direct-IO in Rocksdb](https://github.com/XiaoMi/pegasus/pull/449), by [Smityz](https://github.com/Smityz)

 - - - 
# PULL REQUESTS
Last week, 2 pull requests were created, updated or merged.
## UPDATED PULL REQUEST
Last week, 2 pull requests were updated.
:yellow_heart: #450 [feat(rocksdb): Select the option of Direct-IO in Rocksdb](https://github.com/XiaoMi/pegasus/pull/450), by [Smityz](https://github.com/Smityz)
:yellow_heart: #399 [feat(dup): implement pegasus_mutation_duplicator](https://github.com/XiaoMi/pegasus/pull/399), by [neverchanje](https://github.com/neverchanje)

 - - - 
# COMMITS
Last week there were no commits.

 - - - 
# CONTRIBUTORS
Last week there were no contributors.

 - - - 
# STARGAZERS
Last week there were 3 stagazers.
:star: [dneht](https://github.com/dneht)
:star: [frank19900731](https://github.com/frank19900731)
:star: [guanbear](https://github.com/guanbear)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/452/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/454,https://api.github.com/repos/apache/incubator-pegasus/issues/454,incubator-pegasus,545386718,454,"Weekly Digest (29 December, 2019 - 5 January, 2020)",weekly-digest,,,,CLOSED,2020-01-05T07:59:24Z,2020-02-16T16:44:46Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 1 issue was created.
It is closed now.
## CLOSED ISSUES
:heart: #453 [fix: retry starting zookeeper if it failed on onebox environment](https://github.com/XiaoMi/pegasus/pull/453), by [neverchanje](https://github.com/neverchanje)

 - - - 
# PULL REQUESTS
Last week, 3 pull requests were created, updated or merged.
## UPDATED PULL REQUEST
Last week, 2 pull requests were updated.
:yellow_heart: #450 [feat(rocksdb): Select the option of Direct-IO in Rocksdb](https://github.com/XiaoMi/pegasus/pull/450), by [Smityz](https://github.com/Smityz)
:yellow_heart: #399 [feat(dup): implement pegasus_mutation_duplicator](https://github.com/XiaoMi/pegasus/pull/399), by [neverchanje](https://github.com/neverchanje)
## MERGED PULL REQUEST
Last week, 1 pull request was merged.
:purple_heart: #453 [fix: retry starting zookeeper if it failed on onebox environment](https://github.com/XiaoMi/pegasus/pull/453), by [neverchanje](https://github.com/neverchanje)

 - - - 
# COMMITS
Last week there was 1 commit.
:hammer_and_wrench: [fix: retry starting zookeeper if it failed on onebox environment (#453)](https://github.com/XiaoMi/pegasus/commit/65d86cdb812bee532961aafdf5d43323de9c51eb) by [neverchanje](https://github.com/neverchanje)

 - - - 
# CONTRIBUTORS
Last week there was 1 contributor.
:bust_in_silhouette: [neverchanje](https://github.com/neverchanje)

 - - - 
# STARGAZERS
Last week there were 4 stagazers.
:star: [my3157](https://github.com/my3157)
:star: [codievilky](https://github.com/codievilky)
:star: [anoriqq](https://github.com/anoriqq)
:star: [sntdevco](https://github.com/sntdevco)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there was 1 release.
:rocket: [v1.12.2 v1.12.2](https://github.com/XiaoMi/pegasus/releases/tag/v1.12.2)

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/454/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/457,https://api.github.com/repos/apache/incubator-pegasus/issues/457,incubator-pegasus,547583123,457,coredump on on_append_log_completed,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,OPEN,2020-01-09T16:24:56Z,2021-07-31T03:20:16Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!



1. What did you do?
If possible, provide a recipe for reproducing the error.

- Compaction writes per second: 4GB.
- Max flushed writes per second 1GB.
- MultiSet CU = 600K
- MultiSet QPS = 40K.

2. What did you expect to see?

No coredump.

3. What did you see instead?

Coredump stack:

```
#0  0x00007f98eb83b1d7 in raise () from /lib64/libc.so.6
#1  0x00007f98eb83c8c8 in abort () from /lib64/libc.so.6
#2  0x00007f98ef7cb7be in dsn_coredump () at /home/wutao1/pegasus-release/rdsn/src/core/core/service_api_c.cpp:76
#3  0x00007f98ef6d5bde in dsn::replication::replica_stub::handle_log_failure (this=<optimized out>, err=...)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_stub.cpp:1919
#4  0x00007f98ef684215 in dsn::replication::replica::on_append_log_completed (this=0x389c300, mu=..., err=..., size=<optimized out>)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_2pc.cpp:455
#5  0x00007f98ef7dfd08 in operator() (__args#1=<optimized out>, __args#0=..., this=<optimized out>) at /home/wutao1/app/include/c++/4.8.2/functional:2464
#6  dsn::aio_task::exec (this=<optimized out>) at /home/wutao1/pegasus-release/rdsn/include/dsn/tool-api/task.h:600
#7  0x00007f98ef7dd8a9 in dsn::task::exec_internal (this=this@entry=0xb2ed0acd7) at /home/wutao1/pegasus-release/rdsn/src/core/core/task.cpp:180
#8  0x00007f98ef7f1a6d in dsn::task_worker::loop (this=0x2e3f8c0) at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:211
#9  0x00007f98ef7f1c39 in dsn::task_worker::run_internal (this=0x2e3f8c0) at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:191
#10 0x00007f98ec193600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#11 0x00007f98eccf8dc5 in start_thread () from /lib64/libpthread.so.0
#12 0x00007f98eb8fd73d in clone () from /lib64/libc.so.6
```

The mutation to append:

```
  _appro_data_bytes = 1265, 

```

The error:

```
$5 = {
  _internal_code = 20 // ERR_FILE_OPERATION_FAILED
}
```

4. What version of Pegasus are you using?

1.12.1

Related codes:

```cpp
void replica_stub::handle_log_failure(error_code err)
{
    derror(""handle log failure: %s"", err.to_string());
    if (!s_not_exit_on_log_failure) {
        dassert(false, ""TODO: better log failure handling ..."");
    }
}
```

```cpp
void replica::on_append_log_completed(mutation_ptr &mu, error_code err, size_t size)
{
    if (err == ERR_OK) {
        mu->set_logged();
    } else {
        derror(""%s: append shared log failed for mutation %s, err = %s"",
               name(),
               mu->name(),
               err.to_string());
    }

    ...

    if (err != ERR_OK) {
        // mutation log failure, propagate to all replicas
        _stub->handle_log_failure(err);
    }
```
```cpp
        mu->log_task() = _stub->_log->append(mu,
                                             LPC_WRITE_REPLICATION_LOG,
                                             &_tracker,
                                             std::bind(&replica::on_append_log_completed,
                                                       this,
                                                       mu,
                                                       std::placeholders::_1,
                                                       std::placeholders::_2),
                                             get_gpid().thread_hash(),
                                             &pending_size);
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/457/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/457,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM41EKYL,incubator-pegasus,890283531,457,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2021-07-31T03:20:04Z,2021-07-31T03:20:04Z,Has the bug been fixed?,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM41EKYL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/458,https://api.github.com/repos/apache/incubator-pegasus/issues/458,incubator-pegasus,548535881,458,"Weekly Digest (5 January, 2020 - 12 January, 2020)",weekly-digest,,,,CLOSED,2020-01-12T07:59:25Z,2020-02-16T16:44:30Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 3 issues were created.
Of these, 1 issues have been closed and 2 issues are still open.
## OPEN ISSUES
:green_heart: #457 [coredump on on_append_log_completed](https://github.com/XiaoMi/pegasus/issues/457), by [neverchanje](https://github.com/neverchanje)
:green_heart: #456 [feat(dup): implement server handling of duplicate rpc (part 1)](https://github.com/XiaoMi/pegasus/pull/456), by [neverchanje](https://github.com/neverchanje)
## CLOSED ISSUES
:heart: #455 [fix: fix the bug that threads don't stop when pegasus_io_service is released](https://github.com/XiaoMi/pegasus/pull/455), by [levy5307](https://github.com/levy5307)

 - - - 
# PULL REQUESTS
Last week, 5 pull requests were created, updated or merged.
## UPDATED PULL REQUEST
Last week, 2 pull requests were updated.
:yellow_heart: #456 [feat(dup): implement server handling of duplicate rpc (part 1)](https://github.com/XiaoMi/pegasus/pull/456), by [neverchanje](https://github.com/neverchanje)
:yellow_heart: #444 [feat(collector): add statistics for partition hotspot](https://github.com/XiaoMi/pegasus/pull/444), by [Smityz](https://github.com/Smityz)
## MERGED PULL REQUEST
Last week, 3 pull requests were merged.
:purple_heart: #455 [fix: fix the bug that threads don't stop when pegasus_io_service is released](https://github.com/XiaoMi/pegasus/pull/455), by [levy5307](https://github.com/levy5307)
:purple_heart: #450 [feat(rocksdb): Select the option of Direct-IO in Rocksdb](https://github.com/XiaoMi/pegasus/pull/450), by [Smityz](https://github.com/Smityz)
:purple_heart: #399 [feat(dup): implement pegasus_mutation_duplicator](https://github.com/XiaoMi/pegasus/pull/399), by [neverchanje](https://github.com/neverchanje)

 - - - 
# COMMITS
Last week there were 3 commits.
:hammer_and_wrench: [feat(rocksdb): Select the option of Direct-IO in Rocksdb (#450)](https://github.com/XiaoMi/pegasus/commit/02685300c3128069005e2d1742993ab83b16c355) by [Smityz](https://github.com/Smityz)
:hammer_and_wrench: [fix: fix the bug that threads don't stop when pegasus_io_service is released (#455)](https://github.com/XiaoMi/pegasus/commit/bbae381aaad080c7930ffb4ea674dbd4a81e12d0) by [levy5307](https://github.com/levy5307)
:hammer_and_wrench: [feat(dup): implement pegasus_mutation_duplicator (#399)](https://github.com/XiaoMi/pegasus/commit/b8dea57451b6bbf31b51788bab1f17e3030753fb) by [neverchanje](https://github.com/neverchanje)

 - - - 
# CONTRIBUTORS
Last week there were 3 contributors.
:bust_in_silhouette: [Smityz](https://github.com/Smityz)
:bust_in_silhouette: [levy5307](https://github.com/levy5307)
:bust_in_silhouette: [neverchanje](https://github.com/neverchanje)

 - - - 
# STARGAZERS
Last week there were 3 stagazers.
:star: [dmqm](https://github.com/dmqm)
:star: [wangtuo](https://github.com/wangtuo)
:star: [skyrocknroll](https://github.com/skyrocknroll)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/458/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/461,https://api.github.com/repos/apache/incubator-pegasus/issues/461,incubator-pegasus,551882940,461,"Weekly Digest (12 January, 2020 - 19 January, 2020)",weekly-digest,,,,CLOSED,2020-01-19T07:59:25Z,2020-02-16T16:44:26Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 2 issues were created.
Of these, 1 issues have been closed and 1 issues are still open.
## OPEN ISSUES
:green_heart: #459 [feat(dup): write pegasus value in new data version for duplication](https://github.com/XiaoMi/pegasus/pull/459), by [neverchanje](https://github.com/neverchanje)
## CLOSED ISSUES
:heart: #460 [refactor: update rdsn and remove DSN_CORE_VERSION](https://github.com/XiaoMi/pegasus/pull/460), by [neverchanje](https://github.com/neverchanje)

 - - - 
# PULL REQUESTS
Last week, 2 pull requests were created, updated or merged.
## UPDATED PULL REQUEST
Last week, 1 pull request was updated.
:yellow_heart: #444 [feat(collector): add statistics for partition hotspot](https://github.com/XiaoMi/pegasus/pull/444), by [Smityz](https://github.com/Smityz)
## MERGED PULL REQUEST
Last week, 1 pull request was merged.
:purple_heart: #456 [feat(dup): implement server handling of duplicate rpc (part 1)](https://github.com/XiaoMi/pegasus/pull/456), by [neverchanje](https://github.com/neverchanje)

 - - - 
# COMMITS
Last week there was 1 commit.
:hammer_and_wrench: [feat(dup): implement server handling of duplicate rpc (part 1) (#456)](https://github.com/XiaoMi/pegasus/commit/c5dec62fd2eb3932caa307355bc7c29f5175bfc1) by [neverchanje](https://github.com/neverchanje)

 - - - 
# CONTRIBUTORS
Last week there was 1 contributor.
:bust_in_silhouette: [neverchanje](https://github.com/neverchanje)

 - - - 
# STARGAZERS
Last week there were 6 stagazers.
:star: [jsanant](https://github.com/jsanant)
:star: [LakithKarunaratne](https://github.com/LakithKarunaratne)
:star: [raisingstar](https://github.com/raisingstar)
:star: [pangpangzhu5200](https://github.com/pangpangzhu5200)
:star: [xinlc](https://github.com/xinlc)
:star: [singlegraph](https://github.com/singlegraph)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/461/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/464,https://api.github.com/repos/apache/incubator-pegasus/issues/464,incubator-pegasus,555193121,464,"Weekly Digest (19 January, 2020 - 26 January, 2020)",weekly-digest,,,,CLOSED,2020-01-26T07:59:26Z,2020-02-16T15:58:22Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 2 issues were created.
Of these, 1 issues have been closed and 1 issues are still open.
## OPEN ISSUES
:green_heart: #463 [refactor: refactor the code about prometheus in pegasus_counter_reporter ](https://github.com/XiaoMi/pegasus/pull/463), by [levy5307](https://github.com/levy5307)
## CLOSED ISSUES
:heart: #462 [refactor(config): remove duplicated config items for cluster name](https://github.com/XiaoMi/pegasus/pull/462), by [neverchanje](https://github.com/neverchanje)
## NOISY ISSUE
:speaker: #463 [refactor: refactor the code about prometheus in pegasus_counter_reporter ](https://github.com/XiaoMi/pegasus/pull/463), by [levy5307](https://github.com/levy5307)
It received 3 comments.

 - - - 
# PULL REQUESTS
Last week, 5 pull requests were created, updated or merged.
## UPDATED PULL REQUEST
Last week, 3 pull requests were updated.
:yellow_heart: #463 [refactor: refactor the code about prometheus in pegasus_counter_reporter ](https://github.com/XiaoMi/pegasus/pull/463), by [levy5307](https://github.com/levy5307)
:yellow_heart: #459 [feat(dup): write pegasus value in new data version for duplication](https://github.com/XiaoMi/pegasus/pull/459), by [neverchanje](https://github.com/neverchanje)
:yellow_heart: #444 [feat(collector): add statistics for partition hotspot](https://github.com/XiaoMi/pegasus/pull/444), by [Smityz](https://github.com/Smityz)
## MERGED PULL REQUEST
Last week, 2 pull requests were merged.
:purple_heart: #462 [refactor(config): remove duplicated config items for cluster name](https://github.com/XiaoMi/pegasus/pull/462), by [neverchanje](https://github.com/neverchanje)
:purple_heart: #460 [refactor: update rdsn and remove DSN_CORE_VERSION](https://github.com/XiaoMi/pegasus/pull/460), by [neverchanje](https://github.com/neverchanje)

 - - - 
# COMMITS
Last week there were 2 commits.
:hammer_and_wrench: [refactor(config): remove duplicated config items for cluster name (#462)](https://github.com/XiaoMi/pegasus/commit/91b79a79a94864968c0f62f9f9fe65e604036879) by [neverchanje](https://github.com/neverchanje)
:hammer_and_wrench: [refactor: update rdsn and remove DSN_CORE_VERSION (#460)](https://github.com/XiaoMi/pegasus/commit/bc95df0fb326d494f71ce8b503479dd251c354ff) by [neverchanje](https://github.com/neverchanje)

 - - - 
# CONTRIBUTORS
Last week there was 1 contributor.
:bust_in_silhouette: [neverchanje](https://github.com/neverchanje)

 - - - 
# STARGAZERS
Last week there were 2 stagazers.
:star: [tangwz](https://github.com/tangwz)
:star: [filediscribor](https://github.com/filediscribor)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/464/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/465,https://api.github.com/repos/apache/incubator-pegasus/issues/465,incubator-pegasus,558659033,465,"Weekly Digest (26 January, 2020 - 2 February, 2020)",weekly-digest,,,,CLOSED,2020-02-02T07:59:25Z,2020-02-16T15:58:11Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week, no issues were created.

 - - - 
# PULL REQUESTS
Last week, 1 pull request was created, updated or merged.
## UPDATED PULL REQUEST
Last week, 1 pull request was updated.
:yellow_heart: #444 [feat(collector): add statistics for partition hotspot](https://github.com/XiaoMi/pegasus/pull/444), by [Smityz](https://github.com/Smityz)

 - - - 
# COMMITS
Last week there were no commits.

 - - - 
# CONTRIBUTORS
Last week there were no contributors.

 - - - 
# STARGAZERS
Last week there were 3 stagazers.
:star: [smartbruce](https://github.com/smartbruce)
:star: [allansrc](https://github.com/allansrc)
:star: [LiShuMing](https://github.com/LiShuMing)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/465/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/468,https://api.github.com/repos/apache/incubator-pegasus/issues/468,incubator-pegasus,560378312,468,Use original rocksDB,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2020-02-05T13:39:20Z,2020-12-24T15:42:20Z,"The rocksDB library Pegasus now uses has some special modifications, we'd better remove these changes and use original rocksDB, and then we can upgrade the dependent rocksDB library freely.

## Modifications
Pegasus-rocksdb has the following modifications:

1.  set/get LastFlushedDecree
2. get PegasusDataVersion
3. get LastManualCompactFinishTime and update internally
4. Add a new CreateCheckpointQuick, this function 

## How to remove these modifications?
For 1, 2 and 3, these values are store in manifest now, we can store these meta infomation in another column family instead.
For 4, CreateCheckpointQuick has the same logic with CreateCheckpoint, and extractly return the last decree, we can CreateCheckpoint and then read the last decree from the check point instead.

## Steps
1.
- Upgrade rocksdb to the latest stable version. We will use some new featue like `atomic_flush`, `read skip memtable` in the following steps.

2. 
- Store `last_decree`, `data_version`, `last_manual_compact_finish_time` both in manifest and another column family.
- Read these values from manifest.

3. 
- Store and reads these values from the other column family.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/468/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/468,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU4Mjc1NTkzNw==,incubator-pegasus,582755937,468,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2020-02-06T06:21:49Z,2020-02-06T06:21:49Z,"We need to flush data CF and meta CF atomically, it's support since [rocksdb 5.18.3](https://github.com/facebook/rocksdb/releases/tag/v5.18.3), doc https://github.com/facebook/rocksdb/wiki/Atomic-flush","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU4Mjc1NTkzNw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/468,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc1MDA0MDc1NQ==,incubator-pegasus,750040755,468,NA,l2dy,14329097,Zero King,,NA,2020-12-23T09:37:19Z,2020-12-23T09:37:19Z,"FWIW, would this make Pegasus compatible with ByteDance's [terarkdb](https://github.com/bytedance/terarkdb)?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc1MDA0MDc1NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/468,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc1MDM1NDMyOQ==,incubator-pegasus,750354329,468,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2020-12-23T15:39:30Z,2020-12-23T15:39:30Z,"> FWIW, would this make Pegasus compatible with ByteDance's [terarkdb](https://github.com/bytedance/terarkdb)?

In theory, it's compatible if terarkdb's APIs are compatible with rocksdb. terarkdb seems interesting to adapt to Pegasus.
And in fact, we're planing to use https://github.com/tikv/rocksdb/tree/titan-5.15 to reduce write amplification in the future.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc1MDM1NDMyOQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/471,https://api.github.com/repos/apache/incubator-pegasus/issues/471,incubator-pegasus,562098921,471,"Weekly Digest (2 February, 2020 - 9 February, 2020)",weekly-digest,,,,CLOSED,2020-02-09T00:24:09Z,2020-02-16T15:58:00Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 5 issues were created.
Of these, 4 issues have been closed and 1 issues are still open.
## OPEN ISSUES
:green_heart: #468 [Use original rocksDB](https://github.com/XiaoMi/pegasus/issues/468), by [acelyc111](https://github.com/acelyc111)
## CLOSED ISSUES
:heart: #470 [feat(shell): add dependency argh to parse options and arguments](https://github.com/XiaoMi/pegasus/pull/470), by [neverchanje](https://github.com/neverchanje)
:heart: #469 [fix(rdsn): update rdsn and adapt to time_utils refactoring](https://github.com/XiaoMi/pegasus/pull/469), by [neverchanje](https://github.com/neverchanje)
:heart: #467 [refactor: refactor rocksdb options usage](https://github.com/XiaoMi/pegasus/pull/467), by [acelyc111](https://github.com/acelyc111)
:heart: #466 [feat(dup): implement duplication related shell commands](https://github.com/XiaoMi/pegasus/pull/466), by [neverchanje](https://github.com/neverchanje)
## NOISY ISSUE
:speaker: #468 [Use original rocksDB](https://github.com/XiaoMi/pegasus/issues/468), by [acelyc111](https://github.com/acelyc111)
It received 1 comments.

 - - - 
# PULL REQUESTS
Last week, 7 pull requests were created, updated or merged.
## UPDATED PULL REQUEST
Last week, 2 pull requests were updated.
:yellow_heart: #459 [feat(dup): write pegasus value in new data version for duplication](https://github.com/XiaoMi/pegasus/pull/459), by [neverchanje](https://github.com/neverchanje)
:yellow_heart: #444 [feat(collector): add statistics for partition hotspot](https://github.com/XiaoMi/pegasus/pull/444), by [Smityz](https://github.com/Smityz)
## MERGED PULL REQUEST
Last week, 5 pull requests were merged.
:purple_heart: #470 [feat(shell): add dependency argh to parse options and arguments](https://github.com/XiaoMi/pegasus/pull/470), by [neverchanje](https://github.com/neverchanje)
:purple_heart: #469 [fix(rdsn): update rdsn and adapt to time_utils refactoring](https://github.com/XiaoMi/pegasus/pull/469), by [neverchanje](https://github.com/neverchanje)
:purple_heart: #467 [refactor: refactor rocksdb options usage](https://github.com/XiaoMi/pegasus/pull/467), by [acelyc111](https://github.com/acelyc111)
:purple_heart: #466 [feat(dup): implement duplication related shell commands](https://github.com/XiaoMi/pegasus/pull/466), by [neverchanje](https://github.com/neverchanje)
:purple_heart: #463 [refactor: refactor the code about prometheus in pegasus_counter_reporter ](https://github.com/XiaoMi/pegasus/pull/463), by [levy5307](https://github.com/levy5307)

 - - - 
# COMMITS
Last week there were 5 commits.
:hammer_and_wrench: [feat(dup): implement duplication related shell commands (#466)](https://github.com/XiaoMi/pegasus/commit/2ee37f36cef7d0900c78374c19069f44f6e2187c) by [neverchanje](https://github.com/neverchanje)
:hammer_and_wrench: [feat(shell): add dependency argh to parse options and arguments (#470)](https://github.com/XiaoMi/pegasus/commit/c8e21e0eeb183ec66ff603ef95dd99d5030e2876) by [neverchanje](https://github.com/neverchanje)
:hammer_and_wrench: [fix(rdsn): update rdsn and adapt to time_utils refactoring (#469)](https://github.com/XiaoMi/pegasus/commit/9e41a0f9e9e8aef1b33c53ec0709b6c494c142f2) by [neverchanje](https://github.com/neverchanje)
:hammer_and_wrench: [refactor: refactor rocksdb options usage (#467)](https://github.com/XiaoMi/pegasus/commit/50858da87540019863def3fdce27d30714a34040) by [acelyc111](https://github.com/acelyc111)
:hammer_and_wrench: [refactor: refactor the code about prometheus in pegasus_counter_reporter  (#463)](https://github.com/XiaoMi/pegasus/commit/350488ba04cc05f6f6079db386b8db56d25fb1ba) by [levy5307](https://github.com/levy5307)

 - - - 
# CONTRIBUTORS
Last week there were 3 contributors.
:bust_in_silhouette: [neverchanje](https://github.com/neverchanje)
:bust_in_silhouette: [acelyc111](https://github.com/acelyc111)
:bust_in_silhouette: [levy5307](https://github.com/levy5307)

 - - - 
# STARGAZERS
Last week there were no stargazers.

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/471/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/475,https://api.github.com/repos/apache/incubator-pegasus/issues/475,incubator-pegasus,565829002,475,"Weekly Digest (9 February, 2020 - 16 February, 2020)",weekly-digest,,,,CLOSED,2020-02-16T00:24:10Z,2020-02-16T15:57:52Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 3 issues were created.
Of these, 1 issues have been closed and 2 issues are still open.
## OPEN ISSUES
:green_heart: #474 [feat(split): add partition_version [WIP]](https://github.com/XiaoMi/pegasus/pull/474), by [hycdong](https://github.com/hycdong)
:green_heart: #473 [feat(rocksdb): Bump rocksdb to v6.6.4 [WIP]](https://github.com/XiaoMi/pegasus/pull/473), by [acelyc111](https://github.com/acelyc111)
## CLOSED ISSUES
:heart: #472 [docs(community): add issue template for feature-request](https://github.com/XiaoMi/pegasus/pull/472), by [neverchanje](https://github.com/neverchanje)

 - - - 
# PULL REQUESTS
Last week, 5 pull requests were created, updated or merged.
## UPDATED PULL REQUEST
Last week, 3 pull requests were updated.
:yellow_heart: #474 [feat(split): add partition_version [WIP]](https://github.com/XiaoMi/pegasus/pull/474), by [hycdong](https://github.com/hycdong)
:yellow_heart: #473 [feat(rocksdb): Bump rocksdb to v6.6.4 [WIP]](https://github.com/XiaoMi/pegasus/pull/473), by [acelyc111](https://github.com/acelyc111)
:yellow_heart: #444 [feat(collector): add statistics for partition hotspot](https://github.com/XiaoMi/pegasus/pull/444), by [Smityz](https://github.com/Smityz)
## MERGED PULL REQUEST
Last week, 2 pull requests were merged.
:purple_heart: #472 [docs(community): add issue template for feature-request](https://github.com/XiaoMi/pegasus/pull/472), by [neverchanje](https://github.com/neverchanje)
:purple_heart: #459 [feat(dup): write pegasus value in new data version for duplication](https://github.com/XiaoMi/pegasus/pull/459), by [neverchanje](https://github.com/neverchanje)

 - - - 
# COMMITS
Last week there were 2 commits.
:hammer_and_wrench: [docs(community): add issue template for feature-request (#472)](https://github.com/XiaoMi/pegasus/commit/ce8f53fa22beadb9da76f1e7712ae41bd0dd60b5) by [neverchanje](https://github.com/neverchanje)
:hammer_and_wrench: [feat(dup): write pegasus value in new data version for duplication (#459)](https://github.com/XiaoMi/pegasus/commit/9d311dea0f331d2ef412797e8f6950d495165a7c) by [neverchanje](https://github.com/neverchanje)

 - - - 
# CONTRIBUTORS
Last week there was 1 contributor.
:bust_in_silhouette: [neverchanje](https://github.com/neverchanje)

 - - - 
# STARGAZERS
Last week there was 1 stargazer.
:star: [JoseCanova](https://github.com/JoseCanova)
You are the star! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/475/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/476,https://api.github.com/repos/apache/incubator-pegasus/issues/476,incubator-pegasus,565874549,476,在info_collector中，table name的命名方式可能会存在问题,Smityz,22953824,Smilencer,smityz@qq.com,CLOSED,2020-02-16T09:12:46Z,2021-08-27T03:34:49Z,"这里提个issue吧，table name为\_all_会有问题
_Originally posted by @acelyc111 in https://github.com/XiaoMi/pegasus/pull/444_

---
https://github.com/XiaoMi/pegasus/pull/444/files/9b9b66b88268909d91802a10deec2436a344c37e#diff-fec65a3de73dfc060443a5510aa60c32R146
在这个地方collector为了统计整个app_stat的使用情况，将聚合的数据命名为了_all_，但当存在表名本身为_all_的时候就会出现问题。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/476/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/477,https://api.github.com/repos/apache/incubator-pegasus/issues/477,incubator-pegasus,566625809,477,流量检测问题算法的改进,Smityz,22953824,Smilencer,smityz@qq.com,CLOSED,2020-02-18T03:39:57Z,2020-03-06T10:34:26Z,"之前在 [PR444](https://github.com/XiaoMi/pegasus/pull/444) 中的算法原理为，找出当前app中partition流量的最小值为 `qps_min`，此时partition热点数值为 `当前partition_qps/max(1,qps_min)`
实验结果如图所示：
![Lark20200218-105412](https://user-images.githubusercontent.com/22953824/74699939-0941a080-523d-11ea-8382-f96cf4809af0.png)
可见在开头和结尾时，有两个异常的凸起。
造成凸起的原因是：当读写刚刚进行时，某个partition没有流量，导致该app中 `qps_min`直接取到1，而分子是正常的流量，所以导致整体数值偏大。而当各个partition都有正常的流量后，分母`qps_min`变大，虽然分子中热点paritition的数值也相应较大，但是算下来热点数值并不如初期异常的大，所以会出现开始和结束时的异常凸起。
为了避免这种情况的产生，我调研了新的算法：三倍标准差
算法流程如下：
1. 取出当前记录的有效历史数据（所有partition的不为0的历史qps）
2. 计算标准差与平均值
3. 热点值为当前partition的qps相对于均值的差值除以标准差
4. 在热点值大于等于3的情况下，我们可以认为存在热点数据
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/477/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/477,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU4OTU0NDQ4OA==,incubator-pegasus,589544488,477,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2020-02-21T08:07:59Z,2020-02-21T08:07:59Z,"> 之前在 [PR444](https://github.com/XiaoMi/pegasus/pull/444) 中的算法原理为

github的地址可以自动转换为超链接，像下面这样：
https://github.com/XiaoMi/pegasus/pull/444
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU4OTU0NDQ4OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/482,https://api.github.com/repos/apache/incubator-pegasus/issues/482,incubator-pegasus,567715644,482,refactor(http): refactoring on http server,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2020-02-19T17:24:24Z,2021-08-27T03:44:24Z,"We have implemented many HTTP APIs so far, some of the codes repeat each time a new API is added.

- **argument validation**. For example, `/meta/app?name=<app_name>` requires ""name"" to be provided, if not, it replies with ""bad_request"". This case is general. We can eliminate the following codes by filtering illegal requests in `http_service::call` before the handler.

```cpp
void meta_http_service::get_app_handler(const http_request &req, http_response &resp)
{
    std::string app_name;
    bool detailed = false;
    for (const auto &p : req.query_args) {
        if (p.first == ""name"") {
            app_name = p.second;
        } else if (p.first == ""detail"") {
            detailed = true;
        } else {
            resp.status_code = http_status_code::bad_request;
            return;
        }
    }
```

- Thrift-struct conversion to human-readable JSON. Currently `table_printer` is the general-used utility for this purpose. However `table_printer` is designed to support both ""table"" and ""JSON"" format, for JSON format, we should use a dedicated JSON library, like rapidjson, and [nlohmann/json](https://github.com/nlohmann/json).

- `redirect_if_not_primary`. We add primary-meta-check in every handler. We can eliminate them by prepending this call before `meta_http_service::call`.

```cpp
void meta_http_service::get_app_envs_handler(const http_request &req, http_response &resp)
{
    if (!redirect_if_not_primary(req, resp))
        return;
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/482/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/483,https://api.github.com/repos/apache/incubator-pegasus/issues/483,incubator-pegasus,568013356,483,The cold backup task cannot be performed at the exact time,foreverneverer,23136769,Jiashuo,js982986555@live.com,OPEN,2020-02-20T02:42:19Z,2020-02-20T02:44:00Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
Create a 23:59 cold backup task.

2. What did you expect to see?
It shoud be executed in 23:59.

3. What did you see instead?
It was executed in 23:00

4. What version of Pegasus are you using?
Pegasus Server 1.12.1 (694cbd544436f03d34bfbfcbca0cd9b8f397197e) Release

5. Related screenshots
![lb](https://user-images.githubusercontent.com/23136769/74895779-8b60cf00-53cd-11ea-95fa-2da69c8096a0.png)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/483/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/484,https://api.github.com/repos/apache/incubator-pegasus/issues/484,incubator-pegasus,569413598,484,"Weekly Digest (16 February, 2020 - 23 February, 2020)",weekly-digest,,,,OPEN,2020-02-23T00:24:09Z,2020-02-23T00:24:09Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 8 issues were created.
Of these, 3 issues have been closed and 5 issues are still open.
## OPEN ISSUES
:green_heart: #483 [The cold backup task cannot be performed at the exact time](https://github.com/XiaoMi/pegasus/issues/483), by [Shuo-Jia](https://github.com/Shuo-Jia)
:green_heart: #482 [refactor(http): refactoring on http server](https://github.com/XiaoMi/pegasus/issues/482), by [neverchanje](https://github.com/neverchanje)
:green_heart: #479 [feat(hotspot): new algorithm of hotspot detection](https://github.com/XiaoMi/pegasus/pull/479), by [Smityz](https://github.com/Smityz)
:green_heart: #477 [流量检测问题算法的改进](https://github.com/XiaoMi/pegasus/issues/477), by [Smityz](https://github.com/Smityz)
:green_heart: #476 [在info_collector中，table name的命名方式可能会存在问题](https://github.com/XiaoMi/pegasus/issues/476), by [Smityz](https://github.com/Smityz)
## CLOSED ISSUES
:heart: #481 [fix: use db when it has been opened](https://github.com/XiaoMi/pegasus/pull/481), by [acelyc111](https://github.com/acelyc111)
:heart: #480 [fix: add libraries missing for pegasus_client_shared](https://github.com/XiaoMi/pegasus/pull/480), by [neverchanje](https://github.com/neverchanje)
:heart: #478 [fix(asan): heap-use-after-free in pegasus_write_service.cpp](https://github.com/XiaoMi/pegasus/pull/478), by [Shuo-Jia](https://github.com/Shuo-Jia)
## NOISY ISSUE
:speaker: #478 [fix(asan): heap-use-after-free in pegasus_write_service.cpp](https://github.com/XiaoMi/pegasus/pull/478), by [Shuo-Jia](https://github.com/Shuo-Jia)
It received 2 comments.

 - - - 
# PULL REQUESTS
Last week, 7 pull requests were created, updated or merged.
## UPDATED PULL REQUEST
Last week, 2 pull requests were updated.
:yellow_heart: #479 [feat(hotspot): new algorithm of hotspot detection](https://github.com/XiaoMi/pegasus/pull/479), by [Smityz](https://github.com/Smityz)
:yellow_heart: #473 [feat(rocksdb): Bump rocksdb to v6.6.4](https://github.com/XiaoMi/pegasus/pull/473), by [acelyc111](https://github.com/acelyc111)
## MERGED PULL REQUEST
Last week, 5 pull requests were merged.
:purple_heart: #481 [fix: use db when it has been opened](https://github.com/XiaoMi/pegasus/pull/481), by [acelyc111](https://github.com/acelyc111)
:purple_heart: #480 [fix: add libraries missing for pegasus_client_shared](https://github.com/XiaoMi/pegasus/pull/480), by [neverchanje](https://github.com/neverchanje)
:purple_heart: #478 [fix(asan): heap-use-after-free in pegasus_write_service.cpp](https://github.com/XiaoMi/pegasus/pull/478), by [Shuo-Jia](https://github.com/Shuo-Jia)
:purple_heart: #474 [feat(split): add partition_version](https://github.com/XiaoMi/pegasus/pull/474), by [hycdong](https://github.com/hycdong)
:purple_heart: #444 [feat(collector): add statistics for partition hotspot](https://github.com/XiaoMi/pegasus/pull/444), by [Smityz](https://github.com/Smityz)

 - - - 
# COMMITS
Last week there were 5 commits.
:hammer_and_wrench: [feat(split): add partition_version (#474)](https://github.com/XiaoMi/pegasus/commit/b7492caceae817cfdb50a1a71d42de9d16c4a234) by [hycdong](https://github.com/hycdong)
:hammer_and_wrench: [fix(asan): heap-use-after-free in pegasus_write_service.cpp (#478)](https://github.com/XiaoMi/pegasus/commit/6b2a270345b2fd4eb4a4787606690cba187ad0c3) by [Shuo-Jia](https://github.com/Shuo-Jia)
:hammer_and_wrench: [fix: use db when it has been opened (#481)](https://github.com/XiaoMi/pegasus/commit/b8b0c8f7ed71c159d456050b9f11d81695635b26) by [acelyc111](https://github.com/acelyc111)
:hammer_and_wrench: [fix: add libraries missing for pegasus_client_lib.so (#480)](https://github.com/XiaoMi/pegasus/commit/b9d632a6839eca5f8f8d30e7fbd9ce07bec5e4d3) by [neverchanje](https://github.com/neverchanje)
:hammer_and_wrench: [feat(collector): add statistics for partition hotspot (#444)](https://github.com/XiaoMi/pegasus/commit/695b366dc8605d4b27b9ff6800f2ff066ca56419) by [Smityz](https://github.com/Smityz)

 - - - 
# CONTRIBUTORS
Last week there were 5 contributors.
:bust_in_silhouette: [hycdong](https://github.com/hycdong)
:bust_in_silhouette: [Shuo-Jia](https://github.com/Shuo-Jia)
:bust_in_silhouette: [acelyc111](https://github.com/acelyc111)
:bust_in_silhouette: [neverchanje](https://github.com/neverchanje)
:bust_in_silhouette: [Smityz](https://github.com/Smityz)

 - - - 
# STARGAZERS
Last week there were 3 stagazers.
:star: [qietingfengling](https://github.com/qietingfengling)
:star: [qzhha](https://github.com/qzhha)
:star: [winter-txieyyue](https://github.com/winter-txieyyue)
You all are the stars! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/484/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/486,https://api.github.com/repos/apache/incubator-pegasus/issues/486,incubator-pegasus,571896559,486,Limit long-time running Scan/MultiGet operators,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,CLOSED,2020-02-27T08:09:21Z,2021-06-07T02:11:08Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
Sometimes Scan/MultiGet operators may run for long time because of too many expired or filtered data, which may cause CPU/IO busy and make server unavailable. We should limit these long-time running Scan/MultiGet operators, for example:
* limit max rows or bytes to scan (including expired and filtered data) in one request
* limit max running time to process in one request

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->

**Describe alternatives you've considered:**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->

**Teachability, Documentation, Adoption, Migration Strategy:**
<!-- If you can, explain some scenarios how users might use this, situations it would be helpful in. Any API designs, mockups, or diagrams are also helpful. -->
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/486/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/486,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU5ODU3NDA4MQ==,incubator-pegasus,598574081,486,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2020-03-13T06:37:26Z,2020-03-13T06:37:26Z,"Pegasus server:
- muliget
  1. add max_iterator_count check, including expire_count and filter_count
  2. add max_iterator_size check
  3. add max_execute_time check
if multiget operation exceed check, response.err = Incomplete
- scan
   1. add max_iterator_count restrict for one scanner, including expire_count and filter_count
   2. add max_execute_time check
if scanoperation exceed time limit, response.err = Incomplete
- sortkey_count
   1. add max_execute_time check
if sortkey_count operation exceed check, return -1
Options ""max_execute_time "" can also set by app_envs for each table and if ""max_execute_time = 0"" means no time limit.

Pegasus java client:
- scan: not throw exception if got Incomplete error, and stop asyncNext

Pegasus C++ client & shell tool:
- handle sortkey_count = -1
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU5ODU3NDA4MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/487,https://api.github.com/repos/apache/incubator-pegasus/issues/487,incubator-pegasus,572612687,487,Coredump in redis proxy using hostname in config file,levy5307,22141103,赵立伟,zlw5307@163.com,OPEN,2020-02-28T08:47:54Z,2020-02-28T08:48:14Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
If possible, provide a recipe for reproducing the error.
To start a redis proxy in tjwqtst-staging cluster, set hostname to host.x in config file. When bootstrap it, the application  coredump.
2. What did you expect to see?
runs well
3. What did you see instead?
coredump
4. What version of Pegasus are you using?
v1.12.2","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/487/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/488,https://api.github.com/repos/apache/incubator-pegasus/issues/488,incubator-pegasus,573443382,488,"Weekly Digest (23 February, 2020 - 1 March, 2020)",weekly-digest,,,,CLOSED,2020-03-01T00:24:09Z,2021-05-31T09:49:20Z,"Here's the **Weekly Digest** for [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus):

 - - - 
# ISSUES
Last week 3 issues were created.
Of these, 0 issues have been closed and 3 issues are still open.
## OPEN ISSUES
:green_heart: #487 [Coredump in redis proxy using hostname in config file](https://github.com/XiaoMi/pegasus/issues/487), by [levy5307](https://github.com/levy5307)
:green_heart: #486 [Limit long-time running Scan/MultiGet operators](https://github.com/XiaoMi/pegasus/issues/486), by [qinzuoyan](https://github.com/qinzuoyan)
:green_heart: #485 [feat(shell): mlog_dump support parsing check_and_set](https://github.com/XiaoMi/pegasus/pull/485), by [neverchanje](https://github.com/neverchanje)

 - - - 
# PULL REQUESTS
Last week, 2 pull requests were created, updated or merged.
## UPDATED PULL REQUEST
Last week, 2 pull requests were updated.
:yellow_heart: #485 [feat(shell): mlog_dump support parsing check_and_set](https://github.com/XiaoMi/pegasus/pull/485), by [neverchanje](https://github.com/neverchanje)
:yellow_heart: #479 [feat(hotspot): new algorithm of hotspot detection](https://github.com/XiaoMi/pegasus/pull/479), by [Smityz](https://github.com/Smityz)

 - - - 
# COMMITS
Last week there were no commits.

 - - - 
# CONTRIBUTORS
Last week there were no contributors.

 - - - 
# STARGAZERS
Last week there was 1 stargazer.
:star: [AckClinkz](https://github.com/AckClinkz)
You are the star! :star2:

 - - - 
# RELEASES
Last week there were no releases.

 - - - 

That's all for last week, please <kbd>:eyes: **Watch**</kbd> and <kbd>:star: **Star**</kbd> the repository [*XiaoMi/pegasus*](https://github.com/XiaoMi/pegasus) to receive next weekly updates. :smiley:

*You can also [view all Weekly Digests by clicking here](https://github.com/XiaoMi/pegasus/issues?q=is:open+is:issue+label:weekly-digest).* 

> Your [**Weekly Digest**](https://github.com/apps/weekly-digest) bot. :calendar:
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/488/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/490,https://api.github.com/repos/apache/incubator-pegasus/issues/490,incubator-pegasus,574451133,490,Forbid large-size-value writes to Pegasus,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2020-03-03T06:42:19Z,2021-02-22T02:27:45Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**

large-size-value writes will lead to the instability of Pegasus. We encounter many cases where the client continuously wrote >=1MB value (though in quite low qps) which caused high tail latency of other tables, even caused the program somehow coredump.

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->

Support a configuration (like 'max_allowed_write_size') to limit write size. If the value is larger than 1MB, pegasus server replies with ERR_INVALID_ARGUMENT or other suitable error code.

The better measure is that we can also add limit on java client to avoid invalid request.
related issue https://github.com/XiaoMi/pegasus-java-client/issues/94

**Describe alternatives you've considered:**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->

**Teachability, Documentation, Adoption, Migration Strategy:**
<!-- If you can, explain some scenarios how users might use this, situations it would be helpful in. Any API designs, mockups, or diagrams are also helpful. -->
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/490/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/492,https://api.github.com/repos/apache/incubator-pegasus/issues/492,incubator-pegasus,576093860,492,bug in restore of cold backup,levy5307,22141103,赵立伟,zlw5307@163.com,CLOSED,2020-03-05T09:01:28Z,2020-06-12T06:42:52Z,"## Bug Report

I want to migrate a app between two clusters. So I use cold backup to backup the data of this app, and then restore it. But some accidents happened so the progress of restore is corrupted. I find this in our log file:
```
E2020-03-05 01:53:10.540 (1583344390540076116 23e46) replica.rep_long1.0404000e00000001: replica_restore.cpp:112:verify_checkpoint(): 3.17@{ip}:{port}: file(/home/work/ssd1/pegasus/c6cloudsrv-aispeech/replica/reps/3.17.pegasus/restore.speech_policy.1583341484595/096712.sst) under checkpoint is damaged
E2020-03-05 01:53:10.540 (1583344390540168632 23e46) replica.rep_long1.0404000e00000001: replica_restore.cpp:293:download_checkpoint(): 3.17@{ip}:{port}: checkpoint is damaged, chkpt = /home/work/ssd1/pegasus/c6cloudsrv-aispeech/replica/reps/3.17.pegasus/restore.speech_policy.1583341484595
E2020-03-05 01:53:10.540 (1583344390540440823 23e46) replica.rep_long1.0404000e00000001: replica_init.cpp:92:newr(): try to restore replica 3.17@{ip}:{port} failed, error(ERR_CORRUPTION)
```
But in pegasus shell, it shows 100% proceed, and the status of each partition is ok.
```
>>> query_restore_status 3 -d
pid           progress(%)   restore_status
0             100           ok            
1             100           ok            
2             100           ok            
3             100           ok            
4             100           ok            
5             100           ok            
6             100           ok            
7             100           ok            
8             100           ok            
9             100           ok            
10            100           ok            
11            100           ok            
12            100           ok            
13            100           ok            
14            100           ok            
15            100           ok            
16            100           ok            
17            100           ok            
18            100           ok            
19            100           ok            
20            100           ok            
21            100           ok            
22            100           ok            
23            100           ok            
24            100           ok            
25            100           ok            
26            100           ok            
27            100           ok            
28            100           ok            
29            100           ok            
30            100           ok            
31            100           ok            
32            100           ok            
33            100           ok            
34            100           ok            
35            100           ok            
36            100           ok            
37            100           ok            
38            100           ok            
39            100           ok            
40            100           ok            
41            100           ok            
42            100           ok            
43            100           ok            
44            100           ok            
45            100           ok            
46            100           ok            
47            100           ok            
48            100           ok            
49            100           ok            
50            100           ok            
51            100           ok            
52            100           ok            
53            100           ok            
54            100           ok            
55            100           ok            
56            100           ok            
57            100           ok            
58            100           ok            
59            100           ok            
60            100           ok            
61            100           ok            
62            100           ok            
63            100           ok            
64            100           ok            
65            100           ok            
66            100           ok            
67            100           ok            
68            100           ok            
69            100           ok            
70            100           ok            
71            100           ok            
72            100           ok            
73            100           ok            
74            100           ok            
75            100           ok            
76            100           ok            
77            100           ok            
78            100           ok            
79            100           ok            
80            100           ok            
81            100           ok            
82            100           ok            
83            100           ok            
84            100           ok            
85            100           ok            
86            100           ok            
87            100           ok            
88            100           ok            
89            100           ok            
90            100           ok            
91            100           ok            
92            100           ok            
93            100           ok            
94            100           ok            
95            100           ok            
96            100           ok            
97            100           ok            
98            100           ok            
99            100           ok            
100           100           ok            
101           100           ok            
102           100           ok            
103           100           ok            
104           100           ok            
105           100           ok            
106           100           ok            
107           100           ok            
108           100           ok            
109           100           ok            
110           100           ok            
111           100           ok            
112           100           ok            
113           100           ok            
114           100           ok            
115           100           ok            
116           100           ok            
117           100           ok            
118           100           ok            
119           100           ok            
120           100           ok            
121           100           ok            
122           100           ok            
123           100           ok            
124           100           ok            
125           100           ok            
126           100           ok            
127           100           ok            

the overall progress of restore is 100%

annotations:
    ok : mean restore complete
    ongoing : mean restore is under going
    skip : data on cold backup media is damaged, but skip the damaged partition
    unknown : invalid, should login server and check it
```

version: 1.12.1","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/492/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/492,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY0MzA5NjYzOQ==,incubator-pegasus,643096639,492,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2020-06-12T06:42:49Z,2020-06-12T06:42:49Z,fixed in https://github.com/XiaoMi/rdsn/pull/492,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY0MzA5NjYzOQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/493,https://api.github.com/repos/apache/incubator-pegasus/issues/493,incubator-pegasus,576780017,493,problem in files which are generated by cold backup,levy5307,22141103,赵立伟,zlw5307@163.com,OPEN,2020-03-06T08:41:29Z,2020-03-24T10:59:25Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
If possible, provide a recipe for reproducing the error.

I want to migrate a app between two clusters. So I use cold backup to backup the data of one app. But there is a problem in files which are generated by cold backup: the md5 which is mantained by `backup_metada` that is corresponding with a file is different with the md5 the file generates.
In backup_metada, the md5 is `9f0a038724ffb14f0e2636b0a02c5904`. But using linux command md5sum, the value of md5 which was produce by file `091826.sst` is `1e7df56297989095d467ea8bc276a6ad`

Here are these two files:
https://github.com/levy5307/pegasus-problem/tree/master/cold-backup

4. What version of Pegasus are you using?
v1.12.1","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/493/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/495,https://api.github.com/repos/apache/incubator-pegasus/issues/495,incubator-pegasus,578977673,495,Support hotspot key detection in Pegasus,Smityz,22953824,Smilencer,smityz@qq.com,CLOSED,2020-03-11T03:03:13Z,2021-08-27T03:34:34Z,"# Support hotspot key detection in Pegasus
Related PR https://github.com/XiaoMi/pegasus/pull/502
# 1 Background

At present, Pegasus lists the monitoring values for hotspot detection primitively, using manual methods for fault detection and troubleshooting, which is cumbersome for operation and maintenance. To make the data more visually presented, we have added a hotspot detection module https://github.com/XiaoMi/pegasus/pull/444 and https://github.com/XiaoMi/pegasus/pull/479, which scores relevant values of hotspot issues and feeds them back to falcon, reducing the burden of operation and maintenance. And in this Issue https://github.com/XiaoMi/pegasus/issues/495, I want to discuss the function of hotspot key detection. 

This function is to find the possible hotspot key in the partition which hotspot detection algorithm recorded, and feed it back to the Ops, for timely troubleshooting.

At the same time, this work is also one of the solutions for hot data reading and writing problems.

# 2 Design

## 2.1 Timing diagram

![hotspot key detection](https://user-images.githubusercontent.com/22953824/76389697-d22b5e80-63a6-11ea-9d40-50007aa595f5.png)

## 2.2 How to start capturing data

According to https://github.com/XiaoMi/pegasus/pull/479, we can get a ""hotspot_value"" of each partition. We can set a threshold and if a partition's hotspot_value exceeded the threshold 3 times, the collector will send an RPC to the replica server.

### 2.2.1 First step

`hotspot_calculator`(in the collector) will check every partition's hotspot_value given by https://github.com/XiaoMi/pegasus/pull/479, if it is higher than `THRESHOLD_OF_HOTSPOT_PARTITION_VALUE` we set (normally it would be 4), it will be recorded in ``global_read_count/global_write_count``. This function will be started every time hotspot_value updated, which is carried out once a minute.

### 2.2.2 Second step

`hotspot_calculator`(in the collector) will check ``global_read_count/global_write_count`` status. When ``global_read_count/global_write_count`` is higher than the `THRESHOLD_OF_SEND_RPC_TO_DETECT_HOTKEY`, which means these partitions are abnormal for threshold minutes, we will send an RPC to detect hotkey. 

## 2.3 How to capture data traffic

There are two points we need to pay attention to **Thread-safety** and **low resource occupation**
To reduce the load, we introduce a two-level model to filter data.

### 2.3.1 performance pre-test
According to the performance test, https://github.com/XiaoMi/pegasus/issues/495#issuecomment-601220844. I found that shunt data flow before try_lock can significantly improve efficiency. Use random sampling can further reduce latency.
### 2.3.2 RPC
Ref https://github.com/XiaoMi/pegasus/issues/495#issuecomment-599304060

### 2.3.3 hotkey_collector

`hotkey_collector` is used to capture and analyze data for one partition,  it is running on the replica.`hotkey_collector` has four statuses:

- **stop**: hotkey_collector is stopped, all its data has been cleared, we can reuse it when needed.
- **coarse_level**: hotkey_collector is running, it will collect the hash value of data flow, without its specific content. If it finds the hot hash bucket successfully, its status will turn to fine_level. Otherwise, it will keep running.
- **fine_level**: hotkey_collector is running, it will sampling survey the specific content of data flow in the hottest hash bucket, including hash_key and sort_key. If it finds the hotkey successfully, its status will turn to finish. Otherwise, it will keep running.
- **finish**: hotkey_collector is waiting for hotkey_manager to store the result, after recording the result and send it back with the RPC, its status will turn to stop.

```cpp
void capture_data(data)
{
    if (collector_status == coarse)
        coarse_capture_data(data);
    if (collector_status == fine)
        fine_capture_data(data);
}

void coarse_capture_data(data)
{
    hash_table[hash(data)]++;
}

void fine_capture_data(data)
{
    queue_number = rand(0-10); // Random range can be adjusted according to the actual situation
    if (mutex[queue_number].try_lock()){
        queue[queue_number].push(data); // When analyse data, we should merge these queue
    }
}

void analyse_data()
{
    if (collector_status == coarse)
        if (analyse_coarse_data_successful)
            collector_status = fine;
    if (collector_status == fine)
        if (analyse_fine_data_successful)
            collector_status = finish;
    if (collector_status == finish)
        if (send_RPC_back_successful)
            clear_data;
            collector_status = stop;
    if (time_out)
        clear_data;
        collector_status = stop;
}

```


`capture_data()` and `analyse_data()` have different frequencies, separate two functions contribute to improving efficiency.
`capture_data()` will be executed in https://github.com/XiaoMi/pegasus/blob/b7492caceae817cfdb50a1a71d42de9d16c4a234/src/server/pegasus_server_impl.cpp#L573 and https://github.com/XiaoMi/pegasus/blob/b7492caceae817cfdb50a1a71d42de9d16c4a234/src/server/pegasus_server_impl.cpp#L562

`analyse_data()` is a timing task.

## 2.4 How to stop capturing data traffic

It can be terminated by timed out or by manual(send a RPC)

## 2.5 How to notify the result of hotkey detection

By logging in `derror()`. We can search the specific error logs on the ""hot"" server or the collector.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/495/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/495,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU5OTMwNDA2MA==,incubator-pegasus,599304060,495,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-03-16T01:59:43Z,2020-03-16T01:59:43Z,"> the collector would send an RPC to the replica server. 

We can name this RPC to `hotkey_detect_rpc`. You should define the structure of the request as well as response, and include them in your proposal. Because the API structures are highly related to your design.

```thrift
struct hotkey_detect_request {
  optional gpid partition;
}

struct hotkey_detect_response {
  optional int err;
  optional blob hashkey;
}
```

> It can be terminated by timed out or by manual(send a RPC)

We can name it `stop_hotkey_detect_rpc`

```thrift
struct stop_hotkey_detect_request {
  optional gpid partition;
}

struct stop_hotkey_detect_response {
  optional int err;
}
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU5OTMwNDA2MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/495,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU5OTMwODg1Mw==,incubator-pegasus,599308853,495,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-03-16T02:24:45Z,2020-03-16T02:24:45Z,">```
>    for (partition : suspected_read_partition){
>        if (_read_points[partition]>THRESHOLD_1){
>            global_read_count[partition]++;
>            if (global_read_count[partition]>THRESHOLD_2){
>                suspected_read_partition.push_back(partition);
>           }
>        }
>    }
>```

What is THRESHOLD_1 and THRESHOLD_2?

> `if (suspected_partition in read_watch_list){`

What is read_watch_list?

I recommend that you can describe the data structures first, the pseudocodes like functions or classes should not be our first concern. Of course, it will be fine to include them, you can give a highly-descriptive function name without implementation, rather than give something like ""excpetion_read_check"". I don't really know what it means.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU5OTMwODg1Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/495,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU5OTkxNzk0NQ==,incubator-pegasus,599917945,495,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2020-03-17T07:28:32Z,2020-03-17T07:28:32Z,"* When you find suspected hot spot partition, you will re-hash the key of the partiton. But It should be care: if the partition is indeed hot spot and only one repeat key, re-hash would't achieve goals that you want decrease the calc time. So you may still consider the `proportional sampling` before re-hash
* You need consider the thread security and the efficiency when different thread calc one key count as we discussed before","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDU5OTkxNzk0NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/495,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYwMTIyMDg0NA==,incubator-pegasus,601220844,495,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2020-03-19T14:47:27Z,2020-03-19T14:47:27Z,"## Update
#### 2.2.1 How to start capturing data
**benchmark**
For the reason to further explore the implementation of the scheme, I did a benchmark to test the different ways to capture and analyse data. The two main ideas are optimizing `try_lock` and using `atomic`. And here is the result of the benchmark:

```
Environment:
GCC/OS: g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CPU: Intel(R) Core(TM) i5-8250U CPU @ 1.60GHz
RAM: 8192 MB
```

![benchmark](https://user-images.githubusercontent.com/22953824/77076515-e4ed0580-6a2e-11ea-97d6-2150dacbefe7.png)

  | test1 | test2 | test3 | avg | qps
-- | -- | -- | -- | -- | --
lock_free(normal  data) | 7933.03ms | 7958.41ms | 8013.13ms | 7968.19ms | 50,200,803.21
lock_free(hotspot  data) | 7889.16ms | 7902.19ms | 7908.76ms | 7900.04ms | 50,632,911.39
atomic(normal data) | 17957.5ms | 17575.1ms | 18112.1ms | 17881.57ms | 22,370,113.52
atomic(hotspot data) | 20764.7ms | 20985.3ms | 20280ms | 20676.67ms | 19,346,101.76
test_try_lock(normal data) | 65484.2ms/71549112 | 71451.2ms/62879825 | 56493ms/88043207 | 64476.13ms/74157381 | 1,150,154.80
test_try_lock(hotspot data) | 65778.3ms/72520846 | 73553.3ms/64252630 | 58553.3ms/81606050 | 65961.63ms/72793175 | 1,103,579.01
test_try_lock_random(normal  data) | 29997.5ms/42620480 | 35545ms/38234607 | 24183.2ms/24459466 | 29908.56ms/35104851 | 1,173,739.25
test_try_lock_random(hotspot  data) | 29400.7ms/41056169 | 29335.1ms/40157626 | 26746.5ms/33302722 | 28494.10ms/38172172 | 1,339,651.78
test_try_lock_split(normal  data) | 65504.8ms/209902105 | 65157.2ms/220578017 | 64537.7ms/220200261 | 65066.56ms/216893461 | 3,333,409.06
test_try_lock_split(hotspot  data) | 75059.9ms/212746057 | 71759.6ms/208519921 | 74664.2ms/211368797 | 73827.90ms/210878258 | 2,856,349.13
test_try_lock_split_random(normal  data) | 32150.1ms/109737265 | 32817.8ms/105833686 | 32475ms/107312941 | 32480.96ms/107627964 | 3,313,570.90
test_try_lock_split_random((hotspot  data) | 31861ms/69960417 | 27350.5ms/60935536 | 27007.9ms/72499596 | 28739.8ms/67798516 | 2,359,046.20

Test code: https://github.com/Smityz/multithreading

**Result analysis**
According to the results we can know, we should use `atomic` as much as possible, which has far more advance to `lock`. So, we can use `atomic` in coarse_level_capture.
But it is difficult to use `atomic` in fine_level_capture, since there is no atomic_map we can use. So I suggest using try_lock_split in fine_level_capture, and if we use randomize to limit data flow, it can also accelerate operation.
By the way `rand()` in <stdlib.h> is deprecated for it's performance.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYwMTIyMDg0NA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/495,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYwMjM3Mjc4Mw==,incubator-pegasus,602372783,495,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2020-03-23T04:00:51Z,2020-03-23T04:00:51Z,"Maybe you can use atomic\<T\> in fine_level_capture, and define a struct, which has a map member  ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYwMjM3Mjc4Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/495,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYwMjQ1NzE1OQ==,incubator-pegasus,602457159,495,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2020-03-23T08:39:10Z,2020-03-23T08:39:10Z,"When either coarse_level or fine_level run a period of time but no hot sub-partition or hot key detected, you should turn off `hotkey_collector`, it's costly to run it all the time.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYwMjQ1NzE1OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/495,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYwMzE1NTU2MA==,incubator-pegasus,603155560,495,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2020-03-24T10:26:06Z,2020-03-24T10:26:06Z,"> > the collector would send an RPC to the replica server.
> 
> We can name this RPC to `hotkey_detect_rpc`. You should define the structure of the request as well as response, and include them in your proposal. Because the API structures are highly related to your design.
> 
> ```thrift
> struct hotkey_detect_request {
>   optional gpid partition;
> }
> 
> struct hotkey_detect_response {
>   optional int err;
>   optional blob hashkey;
> }
> ```
> 
> > It can be terminated by timed out or by manual(send a RPC)
> 
> We can name it `stop_hotkey_detect_rpc`
> 
> ```thrift
> struct stop_hotkey_detect_request {
>   optional gpid partition;
> }
> 
> struct stop_hotkey_detect_response {
>   optional int err;
> }
> ```

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYwMzE1NTU2MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/495,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYxOTEwMzU0NQ==,incubator-pegasus,619103545,495,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2020-04-24T16:07:48Z,2020-04-24T16:07:48Z,"## A new idea of capture hotkey string(capture_fine_data)

In the old design, we use try_lock and random selection to reduce the probability of lockup. It's a good idea but still has a lock. In the new design, we will have an exciting lock-free method to capture the huge data flow.

### Thread structure

```
+------------------------------+      +------------------------------+
|   THREAD_POOL_LOCAL_APP      |      |   THREAD_POOL_LOCAL_APP      |
|         thread_1             |      |         thread_2             |
|                              |      |                              |
|    +----------------+        |      |    +----------------+        |
|    |                |        |      |    |                |        |
|    |    on_get()    |        |      |    |    on_get()    |        |
|    |                |        |      |    |                |        |
|    +-------+--------+        |      |    +-------+--------+        |
|            |                 |      |            |                 |
|            | push            |      |            | push            |
|            |(parallel)       |      |            |(parallel)       |
|            v                 |      |            v                 |
|       +----+----+            |      |       +----+----+            |
|       |         |            |      |       |         |            |
|       |  data   |            |      |       |  data   |            |
|       +---------+            |      |       +---------+            |
|       |         |            |      |       |         |            |
|       |  data   |            |      |       |  data   |            |
|       +---------+ read-write |      |       +---------+ read-write |
|       |         |  queue[1]  |      |       |         |  queue[2]  |
|       |  data   |            |      |       |  data   |            |
|       +---------+            |      |       +---------+            |
|       |         |            |      |       |         |            |
|       |         |            |      |       |         |            |
|       |         |            |      |       |         |            |
|       |         |            |      |       |         |            |
|       +----+----+            |      |       +----+----+            |
|            |                 |      |            |                 |
|            | pop             |      |            | pop             |
|            |(order by order) |      |            |(order by order) |
|            |                 |      |            |                 |
|            |                 |      |            |                 |
+------------------------------+      +------------------------------+
             |                                     |
             |                                     |
+---------------------------------------------------------------------+
|            |          THREAD_POOL_DEFAULT        |                  |
|            |                                     |                  |
|            v                                     v                  |
|          +-+-------------------------------------+---+              |
|          | key1| key2| key3|                         |              |
|          +-------------------------------------------+              |
|          |count|count|count|                         |              |
|          +-------------------------------------------+              |
|                         count_hash_map                              |
|                                                                     |
+---------------------------------------------------------------------+


```

### How to allocate queue to the thread

By CAS we can handle this easily

```C++
int find_queue(int threadID)
{
    int t_size = size.load(memory_order_seq_cst);
    for (int i=0;i<t_size;i++){
       if (v[i]==threadID) return i;
    }
    while (!size.compare_exchange_weak(t_size, t_size + 1));
        v[t_size] = threadID;
    return t_size;
}
```

### How to ensure the thread-safety of queues

We can use the producer-consumer queue to ensure thread safety meanwhile keeping high performance.

https://github.com/cameron314/readerwriterqueue is recommended.

![img](https://moodycamel.com/blog/2014/enqueue8.png)

![img](https://moodycamel.com/blog/2014/dequeue8.png)

We can also use `boost::lockfree:queue` or `folly::ProducerConsumerQueue` to instead.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYxOTEwMzU0NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/496,https://api.github.com/repos/apache/incubator-pegasus/issues/496,incubator-pegasus,579787635,496,Monitor and optimize options for bloom filter,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2020-03-12T08:59:55Z,2020-04-24T11:09:38Z,"## Feature Request

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->
We use 10 bits per key for bloom filter, it provide 1% false positive rate, but some user might need a lower FP rate, like 0.1%. We need to do some benchmark on it to check whether it would introduce some unacceptable memory and disk space waste.
And also, we need to monitor more metrics of BF.
https://github.com/facebook/rocksdb/wiki/RocksDB-Bloom-Filter
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/496/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/506,https://api.github.com/repos/apache/incubator-pegasus/issues/506,incubator-pegasus,590066844,506,Release 1.12.3,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2020-03-30T07:44:38Z,2020-04-26T06:36:31Z,"# 1.12.3-RC1

## rDSN

| PR (48 TOTAL) |                                          TITLE                                          |
|----------------------------|-----------------------------------------------------------------------------------------|
| XiaoMi/rdsn#430            | refactor(backup): delay the removal of checkpoint files produced by cold backup         |
| XiaoMi/rdsn#431            | refactor: move log-block-writing-related codes from mutation_log to log_block           |
| XiaoMi/rdsn#429            | feat(dup): support multiple fail modes for duplication                                  |
| XiaoMi/rdsn#427            | refactor: move log_block class from mutation_log.h to separated file                    |
| XiaoMi/rdsn#428            | fix(backup): use https to access fds, instead of http                                   |
| XiaoMi/rdsn#425            | fix(dup): multiple fixes on replica duplication                                         |
| XiaoMi/rdsn#424            | feat(dup): add dsn_replica_dup_test to CI testing and fix http outputs                  |
| XiaoMi/rdsn#421            | feat: add a new app_env to limit scan time                                              |
| XiaoMi/rdsn#423            | feat(dup): rename change_dup_status to modify_dup                                       |
| XiaoMi/rdsn#422            | fix: fix memory leak in tls_transient_memory_t                                          |
| XiaoMi/rdsn#416            | feat: add query_disk_info api for shell command                                         |
| XiaoMi/rdsn#411            | feat(dup): handle non-idempotent writes during duplication                              |
| XiaoMi/rdsn#409            | feat: add query_disk_info support for replica_server                                    |
| XiaoMi/rdsn#420            | refactor: delete unused log in replica::on_config_sync                                  |
| XiaoMi/rdsn#399            | feat(split): re-implement function split_replica_exec                                   |
| XiaoMi/rdsn#415            | feat(dup): support replica http api for duplication state query                         |
| XiaoMi/rdsn#414            | feat: forbid large-size-value writes to Pegasus                                         |
| XiaoMi/rdsn#390            | feat(split): child notify parent catch up                                               |
| XiaoMi/rdsn#401            | feat(dup): support HTTP API to inspect duplication state                                |
| XiaoMi/rdsn#403            | refactor(utils): make logger decoupled from dsn runtime                                 |
| XiaoMi/rdsn#406            | fix: weak constraint of ipv4 and incorrect code logic in hostname_from_ip               |
| XiaoMi/rdsn#407            | feat(dup): respond with hint when add_dup failed                                        |
| XiaoMi/rdsn#394            | feat(split): add partition_version                                                      |
| XiaoMi/rdsn#393            | feat(dup): add metrics for duplication                                                  |
| XiaoMi/rdsn#398            | refactor: make clock decoupled from rdsn runtime                                        |
| XiaoMi/rdsn#319            | feat(split): child replica apply private logs, in-memory mutations and catch up parent  |
| XiaoMi/rdsn#395            | fix(utils): fix bug in time_utils                                                       |
| XiaoMi/rdsn#388            | refactor(util): make time_utils decoupled from utils.cpp                                |
| XiaoMi/rdsn#387            | fix(thrift): add start_time_ns for structure mutation_update in thrift file             |
| XiaoMi/rdsn#384            | fix(lsan): memory leak in perf_counter_atomic.h                                         |
| XiaoMi/rdsn#353            | feat: add check for app envs                                                            |
| XiaoMi/rdsn#380            | refactor(network): refine codes and comments                                            |
| XiaoMi/rdsn#381            | fix(asan): fix heap-use-after-free in replica_split_test                                |
| XiaoMi/rdsn#379            | refactor: remove useless files (version.h, coredump.win.cpp, ReleaseNote)               |
| XiaoMi/rdsn#377            | fix(asan): heap-buffer-overflow in meta_test_base.h                                     |
| XiaoMi/rdsn#375            | fix(asan): heap-use-after-free in simple_lb_cure_test.cpp                               |
| XiaoMi/rdsn#374            | feat(dup): add is_duplicating API on replication_app_base                               |
| XiaoMi/rdsn#371            | refactor: remove aspects for dsn runtime                                                |
| XiaoMi/rdsn#370            | feat(dup): implement load_mutation                                                      |
| XiaoMi/rdsn#355            | feat(dup): preserve data consistency during replica learn                               |
| XiaoMi/rdsn#365            | refactor: move load balance related tests under another test suite                      |
| XiaoMi/rdsn#361            | fix(asan): pointer used after free in autoref_ptr_test                                  |
| XiaoMi/rdsn#363            | fix(asan): global-buffer-overflow in string_conv_test                                   |
| XiaoMi/rdsn#362            | refactor: remove unused function in greedy_load_balancer                                |
| XiaoMi/rdsn#360            | fix: sanitizer options don't take effect                                                |
| XiaoMi/rdsn#359            | build: add sanitizer support                                                            |
| XiaoMi/rdsn#356            | feat(dup): implement is_duplicating                                                     |
| XiaoMi/rdsn#348            | refactor: refine replica restart by adding func get_replay_start_decree                 |
| XiaoMi/rdsn#320            | feat(dup): protect private log from missing when duplication is enabled                 |

## Pegasus

| PR (25 TOTAL) |                                     TITLE                                     |
|----------------------------|-------------------------------------------------------------------------------|
| XiaoMi/pegasus#503         | refactor: simplify logging in pegasus_server_impl                             |
| XiaoMi/pegasus#498         | feat: add shell command for query_disk_info                                   |
| XiaoMi/pegasus#479         | feat(hotspot): new algorithm of hotspot detection                             |
| XiaoMi/pegasus#489         | fix: fix bug in del function of redis_proxy                                   |
| XiaoMi/pegasus#491         | fix: fix the invalid IP address in src/server/test/config.ini                 |
| XiaoMi/pegasus#485         | feat(shell): mlog_dump support parsing check_and_set                          |
| XiaoMi/pegasus#474         | feat(split): add partition_version                                            |
| XiaoMi/pegasus#478         | fix(asan): heap-use-after-free in pegasus_write_service.cpp                   |
| XiaoMi/pegasus#481         | fix: use db when it has been opened                                           |
| XiaoMi/pegasus#480         | fix: add libraries missing for pegasus_client_lib.so                          |
| XiaoMi/pegasus#444         | feat(collector): add statistics for partition hotspot                         |
| XiaoMi/pegasus#472         | docs(community): add issue template for feature-request                       |
| XiaoMi/pegasus#466         | feat(dup): implement duplication related shell commands                       |
| XiaoMi/pegasus#470         | feat(shell): add dependency argh to parse options and arguments               |
| XiaoMi/pegasus#469         | fix(rdsn): update rdsn and adapt to time_utils refactoring                    |
| XiaoMi/pegasus#467         | refactor: refactor rocksdb options usage                                      |
| XiaoMi/pegasus#463         | refactor: refactor the code about prometheus in pegasus_counter_reporter      |
| XiaoMi/pegasus#462         | refactor(config): remove duplicated config items for cluster name             |
| XiaoMi/pegasus#460         | refactor: update rdsn and remove DSN_CORE_VERSION                             |
| XiaoMi/pegasus#456         | feat(dup): implement server handling of duplicate rpc (part 1)                |
| XiaoMi/pegasus#450         | feat(rocksdb): Select the option of Direct-IO in Rocksdb                      |
| XiaoMi/pegasus#455         | fix: fix the bug that threads don't stop when pegasus_io_service is released  |
| XiaoMi/pegasus#399         | feat(dup): implement pegasus_mutation_duplicator                              |
| XiaoMi/pegasus#453         | fix: retry starting zookeeper if it failed on onebox environment              |
| XiaoMi/pegasus#446         | build: add sanitizer support                                                  |

## New Perf-Counters

- `replica*eon.replica_stub*dup.log_read_bytes_rate` (XiaoMi/rdsn#393)
- `replica*eon.replica_stub*dup.log_read_mutations_rate` (XiaoMi/rdsn#393)
- `replica*eon.replica_stub*dup.shipped_bytes_rate` (XiaoMi/rdsn#393)
- `replica*eon.replica_stub*dup.confirmed_rate` (XiaoMi/rdsn#393)
- `replica*eon.replica_stub*dup.pending_mutations_count` (XiaoMi/rdsn#393)
- `replica*eon.replica_stub*dup.time_lag(ms)` (XiaoMi/rdsn#393)
- `replica*eon.replica_stub*dup.load_file_failed_count` (https://github.com/XiaoMi/rdsn/pull/425)
- `replica*eon.replica*dup.disabled_non_idempotent_write_count@<app_name>` (https://github.com/XiaoMi/rdsn/pull/411)
- `replica*eon.replica_stub*recent_write_size_exceed_threshold_count` (https://github.com/XiaoMi/rdsn/pull/414)
- `replica*app.pegasus*dup_shipped_ops@<gpid>` (https://github.com/XiaoMi/pegasus/pull/399)
- `replica*app.pegasus*dup_failed_shipping_ops@<gpid>` (https://github.com/XiaoMi/pegasus/pull/399)
- `app.pegasus*app.stat.hotspots@<app_name>.<partition_index>` (https://github.com/XiaoMi/pegasus/pull/444)

## New HTTP APIs

- `http://0.0.0.0:34602/meta/app/duplication?name=temp` (https://github.com/XiaoMi/rdsn/pull/401, https://github.com/XiaoMi/rdsn/pull/424)
- `http://0.0.0.0:34801/replica/duplication?appid=2` (XiaoMi/rdsn#415)

## New App Env

- `replica.rocksdb_iteration_threshold_time_ms` (https://github.com/XiaoMi/rdsn/pull/421)

## Configuration Changes

```diff
[replication]
- allow_non_idempotent_write = false
+ max_allowed_write_size = 1048576 # default = 1MB

[pegasus.server]
+ rocksdb_use_direct_reads = true
+ rocksdb_use_direct_io_for_flush_and_compaction = true
+ rocksdb_compaction_readahead_size = 2097152
+ rocksdb_writable_file_max_buffer_size = 1048576
- perf_counter_cluster_name = %{cluster.name}
- perf_counter_enable_falcon = false
- perf_counter_enable_prometheus = false
+ perf_counter_sink = <falcon | prometheus>

[pegasus.collector]
- cluster = %{cluster.name}
+ hotspot_detect_algorithm = <hotspot_algo_qps_variance | hotspot_algo_qps_skew> 

[replication]
+ cluster_name = %{cluster.name}
```

Blame list:

- [replication] allow_non_idempotent_write (XiaoMi/rdsn#411)
- [replication] max_allowed_write_size (https://github.com/XiaoMi/rdsn/pull/414)
- [pegasus.server] rocksdb_use_direct_reads, rocksdb_use_direct_io_for_flush_and_compaction, rocksdb_compaction_readahead_size, rocksdb_writable_file_max_buffer_size (https://github.com/XiaoMi/pegasus/pull/450)
- [pegasus.collector] cluster (https://github.com/XiaoMi/pegasus/pull/462)
- [pegasus.server] perf_counter_cluster_name (https://github.com/XiaoMi/pegasus/pull/462)
- [replication] cluster_name (https://github.com/XiaoMi/pegasus/pull/462)
- [pegasus.server] perf_counter_enable_falcon, perf_counter_enable_prometheus, perf_counter_sink (https://github.com/XiaoMi/pegasus/pull/463)
- [pegasus.collector] hotspot_detect_algorithm (https://github.com/XiaoMi/pegasus/pull/479)

## New limitation of requests

Like [aws dynamodb limits](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html#limits-partition-sort-keys), we imposed read/write limitation to protect the service of Pegasus in XiaoMi. All of the limits can be reconfigured:

```
  max_allowed_write_size = 1048576 # default = 1MB
```

Please remember to check if any requests to your service exceed the limits.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/506/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/506,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYxMTkxOTUyNw==,incubator-pegasus,611919527,506,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-04-10T07:39:24Z,2020-04-10T07:39:24Z,"# 1.12.3-RC2

## rDSN

| PR (1 RELEASED) |                                      TITLE                                       |
|--------------------------|----------------------------------------------------------------------------------|
| XiaoMi/rdsn#430          | refactor(backup): delay the removal of checkpoint files produced by cold backup  |

## Pegasus

| PR (6 RELEASED) |                                         TITLE                                         |
|--------------------------|---------------------------------------------------------------------------------------|
| XiaoMi/pegasus#511       | fix: node qps sum inconsistent with app qps sum                                       |
| XiaoMi/pegasus#512       | fix(collector): throughput statistics isn’t right                                     |
| XiaoMi/pegasus#500       | feat: limit long time rocksdb iteration operation                                     |
| XiaoMi/pegasus#505       | feat(collector): add throughput statistic for table                                   |
| XiaoMi/pegasus#509       | fix(asan): global-buffer-overflow in function escape_sds_argv of data_operations.cpp  |
|XiaoMi/pegasus#508 | fix(shell): disk_replica printer bug |

## New perf-counter

On replica-server

* `replica*app.pegasus*get_bytes` (XiaoMi/pegasus#505)
* `replica*app.pegasus*multi_get_bytes` (XiaoMi/pegasus#505)
* `replica*app.pegasus*scan_bytes` (XiaoMi/pegasus#505)
* `replica*app.pegasus*put_bytes` (XiaoMi/pegasus#505)
* `replica*app.pegasus*multi_put_bytes` (XiaoMi/pegasus#505)
* `replica*app.pegasus*check_and_set_bytes` (XiaoMi/pegasus#505)
* `replica*app.pegasus*check_and_mutate_bytes` (XiaoMi/pegasus#505)

On collector

* `collector*app.pegasus*app.stat.get_bytes #<app_name>` (XiaoMi/pegasus#505)
* `collector*app.pegasus*app.stat.multi_get_bytes#<app_name>` (XiaoMi/pegasus#505)
* `collector*app.pegasus*app.stat.scan_bytes#<app_name>` (XiaoMi/pegasus#505)
* `collector*app.pegasus*app.stat.put_bytes#<app_name>` (XiaoMi/pegasus#505)
* `collector*app.pegasus*app.stat.multi_put_bytes#<app_name>` (XiaoMi/pegasus#505)
* `collector*app.pegasus*app.stat.check_and_set_bytes#<app_name>` (XiaoMi/pegasus#505)
* `collector*app.pegasus*app.stat.check_and_mutate_bytes#<app_name>` (XiaoMi/pegasus#505)

## New configuration

```diff
[replication]
+cold_backup_checkpoint_reserve_minutes = 10

[pegasus.server]
# cluster level restriction {3000, 30MB, 1000, 30s}
+rocksdb_multi_get_max_iteration_count = 3000
+rocksdb_multi_get_max_iteration_size = 31457280
+rocksdb_max_iteration_count = 1000
+rocksdb_iteration_threshold_time_ms = 30000
```

Blame list:

- [replication] cold_backup_checkpoint_reserve_minutes (XiaoMi/rdsn#430)
- [pegasus.server] rocksdb_multi_get_max_iteration_count, rocksdb_multi_get_max_iteration_size, rocksdb_max_iteration_count, rocksdb_iteration_threshold_time_ms (https://github.com/XiaoMi/pegasus/pull/500)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYxMTkxOTUyNw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/506,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYxMzMyMzk2MA==,incubator-pegasus,613323960,506,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-04-14T09:14:59Z,2020-04-14T09:14:59Z,"# 1.12.3-RC3

## rDSN

|  PR (1 TOTAL)   |                                 TITLE                                  |
|-----------------|------------------------------------------------------------------------|
| XiaoMi/rdsn#435 | feat: tcmalloc memory release improvements                             |

## Pegasus

|    PR (1 TOTAL)    |                                    TITLE                                    |
|--------------------|-----------------------------------------------------------------------------|
| XiaoMi/pegasus#515 | fix: fix the bug in restore_test                                            |

## New perf-counter
- `replica*eon.replica_stub*tcmalloc.release.memory.size` (XiaoMi/rdsn#435)

## New remote command
command help: `replica.release-tcmalloc-memory <true|false>`
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYxMzMyMzk2MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/507,https://api.github.com/repos/apache/incubator-pegasus/issues/507,incubator-pegasus,591595715,507,feat(coldbackup): add flow controlling for coldbackup upload & download,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2020-04-01T03:40:40Z,2020-06-12T06:45:19Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->

**Describe alternatives you've considered:**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->

**Teachability, Documentation, Adoption, Migration Strategy:**
<!-- If you can, explain some scenarios how users might use this, situations it would be helpful in. Any API designs, mockups, or diagrams are also helpful. -->
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/507/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/507,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY0MzA5NzY1Mw==,incubator-pegasus,643097653,507,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2020-06-12T06:45:18Z,2020-06-12T06:45:18Z,It's done in https://github.com/XiaoMi/rdsn/pull/443,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY0MzA5NzY1Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/510,https://api.github.com/repos/apache/incubator-pegasus/issues/510,incubator-pegasus,594207371,510,bug: append log failed,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,OPEN,2020-04-05T01:52:30Z,2022-07-21T06:30:24Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
If possible, provide a recipe for reproducing the error.

Normal setup.

2. What did you expect to see?

No coredump.

3. What did you see instead?

Coredump:

```
#0  0x00007f2d945181d7 in raise () from /lib64/libc.so.6
#1  0x00007f2d945198c8 in abort () from /lib64/libc.so.6
#2  0x00007f2d984a87be in dsn_coredump () at /home/wutao1/pegasus-release/rdsn/src/core/core/service_api_c.cpp:76
#3  0x00007f2d983b2bde in dsn::replication::replica_stub::handle_log_failure (this=<optimized out>, err=...)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_stub.cpp:1919
#4  0x00007f2d98361215 in dsn::replication::replica::on_append_log_completed (this=0x13c176500, mu=..., err=..., size=<optimized out>)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_2pc.cpp:455
#5  0x00007f2d984bcd08 in operator() (__args#1=<optimized out>, __args#0=..., this=<optimized out>) at /home/wutao1/app/include/c++/4.8.2/functional:2464
#6  dsn::aio_task::exec (this=<optimized out>) at /home/wutao1/pegasus-release/rdsn/include/dsn/tool-api/task.h:600
#7  0x00007f2d984ba8a9 in dsn::task::exec_internal (this=this@entry=0x3c26170b2) at /home/wutao1/pegasus-release/rdsn/src/core/core/task.cpp:180
#8  0x00007f2d984cea6d in dsn::task_worker::loop (this=0x34a5b80) at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:211
#9  0x00007f2d984cec39 in dsn::task_worker::run_internal (this=0x34a5b80) at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:191
#10 0x00007f2d94e70600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#11 0x00007f2d959d5dc5 in start_thread () from /lib64/libpthread.so.0
#12 0x00007f2d945da73d in clone () from /lib64/libc.so.6
```

```
(gdb) p *this
    _gpid = {
      _value = {
        u = {
          app_id = 3, 
          partition_index = 107
        }, 
        value = 459561500675
      }
    }, 
(gdb) p *(mutation_log_private*)(0x194727dc0)
/home/work/ssd1/pegasus/c3srv-offlinetags/replica/reps/3.107.pegasus/plog
```

Error logs:

The real cause is the disk was unable to write.

```
E2020-04-05 07:06:13.554 (1586041573554689557 a436) replica.rep_long5.0401000000000da7: native_aio_provider.linux.cpp:210:aio_internal(): io_submit error, ret = -11
E2020-04-05 07:06:13.555 (1586041573555319655 a436) replica.rep_long5.0401000000000da7: native_aio_provider.linux.cpp:210:aio_internal(): io_submit error, ret = -11
E2020-04-05 07:06:13.555 (1586041573555345754 a3e3) replica.default4.0406000500101e33: mutation_log.cpp:459:operator()(): write private log failed, err = ERR_FILE_OPERATION_FAILED
E2020-04-05 07:06:13.555 (1586041573555383230 a436) replica.rep_long5.0401000000000da7: native_aio_provider.linux.cpp:210:aio_internal(): io_submit error, ret = -11
E2020-04-05 07:06:13.555 (1586041573555411969 a3e3) replica.default4.0406000500101e34: mutation_log.cpp:459:operator()(): write private log failed, err = ERR_FILE_OPERATION_FAILED
E2020-04-05 07:06:13.555 (1586041573555457803 a436) replica.rep_long5.0401000000000da7: native_aio_provider.linux.cpp:210:aio_internal(): io_submit error, ret = -11
E2020-04-05 07:06:13.555 (1586041573555486127 a3e2) replica.default3.0406000500101e35: mutation_log.cpp:459:operator()(): write private log failed, err = ERR_FILE_OPERATION_FAILED
E2020-04-05 07:06:13.555 (1586041573555533959 a436) replica.rep_long5.0401000000000da7: native_aio_provider.linux.cpp:210:aio_internal(): io_submit error, ret = -11
E2020-04-05 07:06:13.555 (1586041573555568369 a3e5) replica.default6.0406000500101e36: mutation_log.cpp:459:operator()(): write private log failed, err = ERR_FILE_OPERATION_FAILED
E2020-04-05 07:06:13.555 (1586041573555589761 a436) replica.rep_long5.0401000000000da7: native_aio_provider.linux.cpp:210:aio_internal(): io_submit error, ret = -11
E2020-04-05 07:06:13.555 (1586041573555607435 a3e5) replica.default6.0406000500101e37: mutation_log.cpp:459:operator()(): write private log failed, err = ERR_FILE_OPERATION_FAILED
```

Error code `-11` means EAGAIN.

Then the recovery failed, as the log shows, the bad replica was moved to `'/home/work/ssd7/`, but since the data was written failed, the log replay cannot continue.

```
W2020-04-05 07:09:08.832 (1586041748832989300 1ea4d) replica.default0.0000ea1200010001: replica_stub.cpp:484:initialize(): init_replica: {replica_dir_op} succeed to move directory '/home/work/ssd7/pegasus/c3srv-offlinetags/replica/reps/3.97.pegasus' to '/home/work/ssd7/pegasus/c3srv-offlinetags/replica/reps/3.97.pegasus.1586041748832942.err'
```

4. What version of Pegasus are you using?

> pegasus-server-1.12.1-694cbd5-glibc2.12-release

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/510/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/510,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDg2Njk0NjAyMg==,incubator-pegasus,866946022,510,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2021-06-23T15:38:54Z,2021-06-23T15:38:54Z,Has this bug been fixed?,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDg2Njk0NjAyMg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/510,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDg2Njk0Njg2Ng==,incubator-pegasus,866946866,510,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2021-06-23T15:40:08Z,2021-06-23T15:40:08Z,Related issue: https://github.com/apache/incubator-pegasus/issues/287,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDg2Njk0Njg2Ng==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/510,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5G_qVG,incubator-pegasus,1191093574,510,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-07-21T06:30:23Z,2022-07-21T06:30:23Z,https://github.com/XiaoMi/rdsn/pull/818,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5G_qVG/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/513,https://api.github.com/repos/apache/incubator-pegasus/issues/513,incubator-pegasus,598220636,513,fix: reduce execution time of some time-consuming tests,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,OPEN,2020-04-11T08:36:12Z,2020-04-11T08:36:12Z,"Here we list the time-consuming unit tests. Propose a PR if you have an idea to fix any one of them:

- [ ] src/function_test/test_recovery

- [ ] src/function_test/test_restore

- [ ] src/function_test/test_ttl

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/513/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/516,https://api.github.com/repos/apache/incubator-pegasus/issues/516,incubator-pegasus,599267815,516,Bug: coredump in function last_prepared_decree(),hycdong,17868458,HeYuchen,377710264@qq.com,OPEN,2020-04-14T03:22:31Z,2020-04-14T03:49:36Z,"## Bug Report
What version of Pegasus are you using?
`Pegasus Server 1.12.1 (694cbd544436f03d34bfbfcbca0cd9b8f397197e)`

Coredump Stack
```
(gdb) bt
#0  0x00007f571187e1d7 in raise () from /lib64/libc.so.6
#1  0x00007f571187f8c8 in abort () from /lib64/libc.so.6
#2  0x00007f5711877146 in __assert_fail_base () from /lib64/libc.so.6
#3  0x00007f57118771f2 in __assert_fail () from /lib64/libc.so.6
#4  0x00007f57155dc98f in dsn::ref_counter::release_ref (this=<optimized out>)
    at /home/wutao1/pegasus-release/rdsn/include/dsn/utility/autoref_ptr.h:71
#5  0x00007f57156bef1c in release_ref (this=<optimized out>)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica.cpp:338
#6  ~ref_ptr (this=0x7f56cc5e2fc0, __in_chrg=<optimized out>)
    at /home/wutao1/pegasus-release/rdsn/include/dsn/utility/autoref_ptr.h:142
#7  dsn::replication::replica::last_prepared_decree (this=this@entry=0x14187200)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica.cpp:336
#8  0x00007f57156ff742 in dsn::replication::replica::on_copy_remote_state_completed(dsn::error_code, unsigned long, unsigned long, dsn::replication::learn_request&&, dsn::replication::learn_response&&) (this=0x14187200, err=..., 
    size=0, copy_start_time=152, 
    req=<unknown type in /home/work/app/pegasus/c3srv-music/replica/package/bin/libdsn_replica_server.so, CU 0xe06a8c, DIE 0xef234a>, 
    resp=<unknown type in /home/work/app/pegasus/c3srv-music/replica/package/bin/libdsn_replica_server.so, CU 0xe06a8c, DIE 0xef234f>) at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_learn.cpp:913
#9  0x00007f57158208a9 in dsn::task::exec_internal (this=this@entry=0x789b277af)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/task.cpp:180
---Type <return> to continue, or q <return> to quit---
#10 0x00007f5715834a6d in dsn::task_worker::loop (this=0x1da73f0)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:211
#11 0x00007f5715834c39 in dsn::task_worker::run_internal (this=0x1da73f0)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:191
#12 0x00007f57121d6600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#13 0x00007f5712d3bdc5 in start_thread () from /lib64/libpthread.so.0
#14 0x00007f571194073d in clone () from /lib64/libc.so.6
```

Information from frame 7:
replica gpid is 10.55
mutation is 10.55.34.194880435
start decree is 194880434

Information from frame 6:
autoref_ptr is dsn::ref_ptr<dsn::replication::mutation> * const
mutation is 10.55.34.194880435

Corresponding code:
https://github.com/XiaoMi/rdsn/blob/4b14d3adeec056736d621645c15d2ef0e495b97e/src/dist/replication/lib/replica.cpp#L325-L338

https://github.com/XiaoMi/rdsn/blob/4b14d3adeec056736d621645c15d2ef0e495b97e/include/dsn/utility/autoref_ptr.h#L68-L71

This bug seems to caused by trying to release reference the mutation structure who has already been released.


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/516/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/517,https://api.github.com/repos/apache/incubator-pegasus/issues/517,incubator-pegasus,599425290,517,Rate Limiter,levy5307,22141103,赵立伟,zlw5307@163.com,CLOSED,2020-04-14T09:25:07Z,2020-05-06T07:14:39Z,"## Rate Limiter

In general, a rate is a simple count of occurrences over time. However, there are several different techniques for measuring and limiting rates.

### Fixed Window 
In a fixed window algorithm, a window size of n seconds (typically using human-friendly values, such as 60 or 3600 seconds) is used to track the rate. Each incoming request increments the counter for the window. If the counter exceeds a threshold, the request is discarded. The windows are typically defined by the floor of the current timestamp, so 12:00:03 with a 60 second window length, would be in the 12:00:00 window.

The advantage of this algorithm is that it ensures more recent requests gets processed without being starved by old requests. However, a single burst of traffic that occurs near the boundary of a window can result in twice the rate of requests being processed, because it will allow requests for both the current and next windows within a short time. Additionally, if many consumers wait for a reset window, for example at the top of the hour, then they may stampede your API at the same time.

### Sliding window
This is a hybrid approach that combines the low processing cost of the fixed window algorithm, and the improved boundary conditions of the sliding log. Like the fixed window algorithm, we track a counter for each fixed window. Next, we account for a weighted value of the previous window’s request rate based on the current timestamp to smooth out bursts of traffic. For example, if the current window is 25% through, then we weight the previous window’s count by 75%. The relatively small number of data points needed to track per key allows us to scale and distribute across large clusters.

### Token Bucket
A token bucket maintains a rolling and accumulating budget of usage as a balance of tokens. This technique recognizes that not all inputs to a service correspond 1:1 with requests. A token bucket adds tokens at some rate. When a service request is made, the service attempts to withdraw a token (decrementing the token count) to fulfill the request. If there are no tokens in the bucket, the service has reached its limit and responds with backpressure. For example, in a GraphQL service, a single request might result in multiple operations that are composed into a result. These operations may each take one token. This way, the service can keep track of the capacity that it needs to limit the use of, rather than tie the rate-limiting technique directly to requests

### Leaky Bucket
A leaky bucket is similar to a token bucket, but the rate is limited by the amount that can drip or leak out of the bucket. This technique recognizes that the system has some degree of finite capacity to hold a request until the service can act on it; any extra simply spills over the edge and is discarded. This notion of buffer capacity (but not necessarily the use of leaky buckets) also applies to components adjacent to your service, such as load balancers and disk I/O buffers.

### Conclusion

With twice the rate of requests being processed occured near the boundary of a window, fixed window is passed.

Sliding Windows are a variant of fixed Windows. The more Windows you have, the higher the accuracy, but the more resources you consume. The fewer Windows you have, the less resources you consume, but the less accurate you get.

So We mainly consider the following two ways：token bucket and leaky bucket.

***Token Bucket vs Leaky Bucket***

- LB discards packets; TB does not. TB discards tokens.

- LB sends packets at an average rate. TB allows for large bursts to be sent faster by speeding up the output.

- TB allows saving up tokens (permissions) to send large bursts. LB does not allow saving

- With LB, when there are a large number of burst requests in a short period of time, each request has to wait in the queue for a while before it can be answered, even if the server is not under any load.

Because LB does not allow saving tokens to send large bursts. So if we have a large request, which is larger than the size of bucket, the request will not be processed.

So, I recommend choosing the token bucket algorithm.

### Reference
https://konghq.com/blog/how-to-design-a-scalable-rate-limiting-algorithm/
https://cloud.google.com/solutions/rate-limiting-strategies-techniques
https://www.infoq.cn/article/Qg2tX8fyw5Vt-f3HH673
https://www.slideshare.net/vimal25792/leaky-bucket-tocken-buckettraffic-shaping","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/517/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/518,https://api.github.com/repos/apache/incubator-pegasus/issues/518,incubator-pegasus,599553100,518,Bug: creating `temp` table in onebox causes the collector segment fault,Smityz,22953824,Smilencer,smityz@qq.com,OPEN,2020-04-14T12:58:34Z,2020-04-14T13:11:30Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
```bash
./run.sh start_onebox -c
# repeat the following operation
drop temp
create temp -p 8 -r 3
```
and then collector core dump
```bash
(gdb) where
#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
#1  0x00007fc93906b801 in __GI_abort () at abort.c:79
#2  0x00007fc93bf72ece in dsn_coredump () at /home/smilencer/Code/pegasus/rdsn/src/core/core/service_api_c.cpp:76
#3  0x00005596bacaa028 in dsn::replication::partition_resolver_simple::query_config_reply (this=0x5596bc2d8600, err=..., request=<optimized out>, response=<optimized out>, partition_index=1)
    at /home/smilencer/Code/pegasus/rdsn/src/dist/replication/client/partition_resolver_simple.cpp:269
#4  0x00007fc93bf88c49 in std::function<void (dsn::error_code, dsn::message_ex*, dsn::message_ex*)>::operator()(dsn::error_code, dsn::message_ex*, dsn::message_ex*) const (__args#2=<optimized out>, __args#1=<optimized out>, 
    __args#0=..., this=<optimized out>) at /usr/include/c++/7/bits/std_function.h:706
#5  dsn::rpc_response_task::exec (this=<optimized out>) at /home/smilencer/Code/pegasus/rdsn/include/dsn/tool-api/task.h:480
#6  0x00007fc93bf852e9 in dsn::task::exec_internal (this=0x5596bce46240) at /home/smilencer/Code/pegasus/rdsn/src/core/core/task.cpp:180
#7  0x00007fc93bf9d2ca in dsn::task_worker::loop (this=0x5596bc31e300) at /home/smilencer/Code/pegasus/rdsn/src/core/core/task_worker.cpp:211
#8  0x00007fc93bf9d4e9 in dsn::task_worker::run_internal (this=0x5596bc31e300) at /home/smilencer/Code/pegasus/rdsn/src/core/core/task_worker.cpp:191
#9  0x00007fc939a8f6df in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#10 0x00007fc93a61e6db in start_thread (arg=0x7fc92c2d1700) at pthread_create.c:463
#11 0x00007fc93914c88f in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95
(gdb) f 3
#3  0x00005596bacaa028 in dsn::replication::partition_resolver_simple::query_config_reply (this=0x5596bc2d8600, err=..., request=<optimized out>, response=<optimized out>, partition_index=1)
    at /home/smilencer/Code/pegasus/rdsn/src/dist/replication/client/partition_resolver_simple.cpp:269
269                     dassert(false,
```

2. What did you expect to see?

3. What did you see instead?

4. What version of Pegasus are you using?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/518/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/531,https://api.github.com/repos/apache/incubator-pegasus/issues/531,incubator-pegasus,613900640,531,并发批量请求服务器，服务端处理都在1ms以内返回，但客户端等请求都返回到回调处理时总共经历了6~8ms。,mzss0,20411576,,,OPEN,2020-05-07T09:14:57Z,2020-06-03T04:39:49Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
If possible, provide a recipe for reproducing the error.
万兆网卡。空数据(1000~5000)async_multi_get,c++接口测试
2. What did you expect to see?
期望在服务端请求时延(1ms以内)内返回，或者2倍左右
3. What did you see instead?
总请求耗时5ms以上
4. What version of Pegasus are you using?
v1.12","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/531/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/531,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYyNTY0MjkxMQ==,incubator-pegasus,625642911,531,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-05-08T05:46:35Z,2020-05-08T05:46:35Z,"> 服务端处理都在1ms以内返回

请问服务端1ms耗时是如何看出的?

> 客户端等请求都返回到回调处理时总共经历了6~8ms

你可以给一下服务端的总吞吐, 并实验看是否吞吐下降后, 延迟也会下降.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYyNTY0MjkxMQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/531,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYyOTY1MDExMQ==,incubator-pegasus,629650111,531,NA,mzss0,20411576,,,NA,2020-05-16T13:58:40Z,2020-05-16T13:58:40Z,"> > 服务端处理都在1ms以内返回
> 
> 
> 
> 请问服务端1ms耗时是如何看出的?
> 
> 从tcpdump抓包或者服务端记录都能看出来
> 
> > 客户端等请求都返回到回调处理时总共经历了6~8ms
> 
> 这个计时是客户端程序从发起到接收所有时间
> 
> 你可以给一下服务端的总吞吐, 并实验看是否吞吐下降后, 延迟也会下降.
> 单条批量测试都能看见，从总吞吐看是一致的




> 这边处理的是只读请求，数据库数据量不论大小都有这个现象。表设计是100分区，客户端请求5000条空数据，数据随机分布所有分区，然后并行请求这100个mget。就能复现上述情况

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYyOTY1MDExMQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/531,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYyOTkyNDU2OQ==,incubator-pegasus,629924569,531,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-05-18T03:22:46Z,2020-05-18T03:22:46Z,"""服务端记录"" 是指的哪个记录? 

""客户端请求5000条空数据，数据随机分布所有分区，然后并行请求这100个mget""

刚开始客户端建立TCP连接需要时间, 偶尔的TCP预热也需要时间, 一个比较准确的benchmark还是参照我们在https://pegasus-kv.github.io/overview/benchmark的测试结果. 首先运行时间要足够长, 其次应该有明确一些的延迟数值, 我们后续也会提供更细粒度延迟的统计.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYyOTkyNDU2OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/531,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYyOTkzMDg5OQ==,incubator-pegasus,629930899,531,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2020-05-18T03:52:20Z,2020-05-18T03:52:20Z,"@mzss0 非常感谢你的反馈，以帮助我们提升性能。

是否可以使用async_get来试试？看看服务端返回时间和客户端感知时间的差距是不是还是很大，以帮助我们定位是不是客户端的问题。

另外可以不开启并发，而是只串行发送，看看是不是还有相同问题。
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYyOTkzMDg5OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/531,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMDA0NzI1Nw==,incubator-pegasus,630047257,531,NA,mzss0,20411576,,,NA,2020-05-18T09:02:15Z,2020-05-18T09:02:15Z,"> ""服务端记录"" 是指的哪个记录? 
> 
> serverstat命令看到的服务端信息
> 
> ""客户端请求5000条空数据，数据随机分布所有分区，然后并行请求这100个mget""
> 
> 这个预热我做了处理，处理后第一次很明显没有过长耗时，处理方法就是获取客户端以后随便请求了一个不存在的key
> 
> 刚开始客户端建立TCP连接需要时间, 偶尔的TCP预热也需要时间, 一个比较准确的benchmark还是参照我们在https://pegasus-kv.github.io/overview/benchmark的测试结果. 首先运行时间要足够长, 其次应该有明确一些的延迟数值, 我们后续也会提供更细粒度延迟的统计.

benchmark的测试场景跟我们所用的不一样。一开始也是采用这种方式去测试的。后面我们单独做了压测和点测，然后有了上述问题。。

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMDA0NzI1Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/531,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMDA1MTUwOQ==,incubator-pegasus,630051509,531,NA,mzss0,20411576,,,NA,2020-05-18T09:09:50Z,2020-05-18T09:09:50Z,"> @mzss0 非常感谢你的反馈，以帮助我们提升性能。
> 
> 
> 
> 是否可以使用async_get来试试？看看服务端返回时间和客户端感知时间的差距是不是还是很大，以帮助我们定位是不是客户端的问题。
> 
> 这个我们定位处理过，也做过优化，客户端和服务端的map结构在mget里是极耗时的，我们尝试修改了服务端，让请求数据有序返回，即使有空的也在对应位置加了空数据，保证一一对应，然后mget采用vector。提升了这边场景下的性能。我这边的计时统计是在rrdbclient的回调里，可以很明显看到这个客户端处理多了些时间
> 
> 另外可以不开启并发，而是只串行发送，看看是不是还有相同问题。
> 
> 单发不会有这个问题，但是不符合这边使用场景。。

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMDA1MTUwOQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/531,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMDA3NjE1Mw==,incubator-pegasus,630076153,531,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2020-05-18T09:54:30Z,2020-05-18T09:54:30Z,@mzss0 上面你的意思是map数据结构导致了性能下降？那修改后对性能有多大提升？,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMDA3NjE1Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/531,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMDEwNjg4MA==,incubator-pegasus,630106880,531,NA,mzss0,20411576,,,NA,2020-05-18T10:59:12Z,2020-05-18T10:59:12Z,"> @mzss0 上面你的意思是map数据结构导致了性能下降？那修改后对性能有多大提升？

是用vector替换set，map。这个跟测试场景有关系，可以抛开rpc调用单独测试mget前的key处理和回调里面的数据整合。这边测试key的数据量是5000，分区是256

a，用原pegasus的asyncmultiget接口，通过多个set存多个分区的sortkey和rrdbclient回调返回的整合
b，去掉set结构用两个vector处理sortkey和返回定位，vector的size可以在初始化的时候预分配

b的耗时是a的一半以内，量大的话用多线程可以压缩更多，这一点map和set都做不到。

分区的数量后面做了调整，经测试不同分区数在上述场景下的点测时延效果，随着分区数的增加，是有个先降后升的。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMDEwNjg4MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/531,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzNjU2OTk4Nw==,incubator-pegasus,636569987,531,NA,mzss0,20411576,,,NA,2020-06-01T01:40:09Z,2020-06-01T01:40:09Z,上面提到的问题这边查看代码和测试验证已经有了结论了，一是创建连接的时候阻塞了其他并行的读操作，二是异步mget从开始调用到最后调用rpc请求都是串行的。这边的预热没有创建连接，只是从meta拿了config配置,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzNjU2OTk4Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/531,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzNjYxMTU4OQ==,incubator-pegasus,636611589,531,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-06-01T04:52:12Z,2020-06-01T04:52:12Z,"> ""一是创建连接的时候阻塞了其他并行的读操作""

你意思是C++客户端内部在连接建立的时候阻塞读? 如果是的话我们可以看一下具体实现.

> 二是异步mget从开始调用到最后调用rpc请求都是串行的

没理解这句话的意思.

> 这边的预热没有创建连接，只是从meta拿了config配置

是的, 我们C++客户端暂时没支持连接预热, 我们只在java客户端支持了: https://pegasus-kv.github.io/clients/java-client#%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%BF%9E%E6%8E%A5%E9%A2%84%E7%83%ADwarm-up","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzNjYxMTU4OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/531,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzNzkyMTM1NQ==,incubator-pegasus,637921355,531,NA,mzss0,20411576,,,NA,2020-06-03T02:51:28Z,2020-06-03T02:51:28Z,"> > ""一是创建连接的时候阻塞了其他并行的读操作""
> 
> 
> 
> 你意思是C++客户端内部在连接建立的时候阻塞读? 如果是的话我们可以看一下具体实现.
> 
> 
> 
> > 二是异步mget从开始调用到最后调用rpc请求都是串行的
> 
> 
> 
> 没理解这句话的意思.
> 
> 
> 
> > 这边的预热没有创建连接，只是从meta拿了config配置
> 
> 
> 
> 是的, 我们C++客户端暂时没支持连接预热, 我们只在java客户端支持了: https://pegasus-kv.github.io/clients/java-client#%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%BF%9E%E6%8E%A5%E9%A2%84%E7%83%ADwarm-up

1，rdsn网络那一块，network.cpp的send_message函数，有读写锁

2，这边的场景每次mget请求会触发多个分区的mget请求，这一部分请求只在发送rpc请求异步等待的时候是并行的，从收到请求到发送是串行。

其实我这边还有一个疑问，在运行测试的时候，定位到address这个类的创建会耗时，具体应该是编译器做了什么操作，因为这个类没有实现默认构造函数，也可能是编译优化做了什么。从观察来说，就是在calltask函数到调用dsnrpccall之前，不确定是哪里有阻塞，而且只在前面几个请求出现，并行情况下多了5ms左右","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzNzkyMTM1NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/536,https://api.github.com/repos/apache/incubator-pegasus/issues/536,incubator-pegasus,617996100,536,Release 2.0.0,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2020-05-14T07:34:45Z,2020-08-04T11:35:49Z,"# 2.0.0

## rDSN

| PR (34 TOTAL) |                                                 TITLE                                                 |
|--------------------------|-------------------------------------------------------------------------------------------------------|
| XiaoMi/rdsn#468          | fix: fix bug in hpc_task_queue                                                                        |             12.17 | v2.0.0-RC2 |
| XiaoMi/rdsn#462          | improvement(third-party): Update the concurrentqueue to a stable release version                      |             12.17 | v2.0.0-RC2 |
| XiaoMi/rdsn#472          | fix(dup): reject add_dup if remote address incorrect                                                  |             13.00 | v2.0.0-RC2 |
| XiaoMi/rdsn#474          | refactor: use rpc_holder to reimplement on_query_configuration_by_index                               |             13.00 | v2.0.0-RC2 |
| XiaoMi/rdsn#470          | fix: fix memory leak in dsn_message_parser                                                            |             13.00 | v2.0.0-RC2 |
| XiaoMi/rdsn#461          | feat: add rate limit for learning and support remote-command                                          |             13.00 | v2.0.0-RC2 |
| XiaoMi/rdsn#458          | refactor: change large write size request forbiden log                                                |             13.03 | v2.0.0-RC2 |
| XiaoMi/rdsn#459          | fix: fix the bug in restore                                                                           |             28.04 | v2.0.0-RC1 |
| XiaoMi/rdsn#457          | feat(bulk-load): meta server send bulk load request                                                   |             28.20 | v2.0.0-RC1 |
| XiaoMi/rdsn#454          | feat(bulk-load): meta server start bulk load                                                          |             32.23 | v2.0.0-RC1 |
| XiaoMi/rdsn#443          | feat(cold-backup): add rate limit for fds                                                             |             32.92 | v2.0.0-RC1 |
| XiaoMi/rdsn#456          | feat: update rpc_holder                                                                               |             32.98 | v2.0.0-RC1 |
| XiaoMi/rdsn#455          | fix: fix bug in local_service                                                                         |             33.24 | v2.0.0-RC1 |
| XiaoMi/rdsn#452          | fix: fix memory leak in meta_load_balance_test                                                        |             40.95 | v2.0.0-RC1 |
| XiaoMi/rdsn#445          | feat(bulk-load): add meta_bulk_load_service and some structures                                       |             40.99 | v2.0.0-RC1 |
| XiaoMi/rdsn#450          | feat(dup): optimize time-lag by reducing repeat delay                                                 |             41.13 | v2.0.0-RC1 |
| XiaoMi/rdsn#451          | fix: fix memory leak in meta_state_service_simple                                                     |             41.21 | v2.0.0-RC1 |
| XiaoMi/rdsn#275          | refactor(util): use flags to define configurations                                                    |             41.98 | v2.0.0-RC1 |
| XiaoMi/rdsn#448          | fix(dup): don't GC by valid_start_offset during duplication & add app_name to replica_base            |             43.25 | v2.0.0-RC1 |
| XiaoMi/rdsn#433          | feat: update the way to get heap profile                                                              |             43.28 | v2.0.0-RC1 |
| XiaoMi/rdsn#449          | fix: fix memory leak in meta test                                                                     |             43.89 | v2.0.0-RC1 |
| XiaoMi/rdsn#441          | feat(hotkey): add replication.codes about hotkey detect                                               |             44.05 | v2.0.0-RC1 |
| XiaoMi/rdsn#447          | fix(asan): memory leak in local_service.cpp                                                           |             45.05 | v2.0.0-RC1 |
| XiaoMi/rdsn#442          | feat(util): add restore_read function in rpc_message                                                  |             46.83 | v2.0.0-RC1 |
| XiaoMi/rdsn#391          | feat(split): register child partition                                                                 |             47.01 | v2.0.0-RC1 |
| XiaoMi/rdsn#446          | fix(asan): heap-use-after-free caused by using string_view in fail_point                              |             47.70 | v2.0.0-RC1 |
| XiaoMi/rdsn#418          | feat: append mlog in fixed-size blocks using log_appender                                             |             55.05 | v2.0.0-RC1 |
| XiaoMi/rdsn#436          | refactor: simplify mutation_log write_pending_mutations                                               |             58.19 | v2.0.0-RC1 |
| XiaoMi/rdsn#434          | refactor(backup): move collect_backup_info to replica_backup_manager                                  |             63.12 | v2.0.0-RC1 |
| XiaoMi/rdsn#432          | refactor(backup): make backup clear decoupled from on_cold_backup                                     |             64.20 | v2.0.0-RC1 |
| XiaoMi/rdsn#419          | feat: add perf-counter for backup request                                                             |             83.03 | v2.0.0-RC1 |
| XiaoMi/rdsn#408          | feat: refine mlog_dump output                                                                         |             97.78 | v2.0.0-RC1 |
| XiaoMi/rdsn#255          | refactor(rpc): refactor request meta & add support for backup request                                 |            124.05 | v2.0.0-RC1 |

## Pegasus

| PR (28 TOTAL) |                                           TITLE                                           |
|--------------------------|-------------------------------------------------------------------------------------------|
| XiaoMi/pegasus#539       | docs: update CentOS build dependencies                                                    |             13.00 | v2.0.0-RC2 |
| XiaoMi/pegasus#538       | fix(shell/sds): NULL check before memset                                                  |             13.00 | v2.0.0-RC2 |
| XiaoMi/pegasus#533       | feat: overload dump_write_request of replication_app_base.h                               |             13.00 | v2.0.0-RC2 |
| XiaoMi/pegasus#537       | fix: fix bug in table_hotspot_policy                                                      |             13.03 | v2.0.0-RC2 |
| XiaoMi/pegasus#534       | fix: fix bug in local_service                                                             |             28.26 | v2.0.0-RC1 |
| XiaoMi/pegasus#532       | feat(rocksdb): Support to config meta data read source                                    |             29.73 | v2.0.0-RC1 |
| XiaoMi/pegasus#530       | improvement(cu): set ttl on capacity unit data                                            |             30.08 | v2.0.0-RC1 |
| XiaoMi/pegasus#528       | improvement(lb): add add_node_list.sh to add nodes with copy_pri after all copy_sec done  |             31.91 | v2.0.0-RC1 |
| XiaoMi/pegasus#529       | improvement: add logging on rocksdb write stalls                                          |             33.17 | v2.0.0-RC1 |
| XiaoMi/pegasus#526       | feat(dup): add metric for time lag between master&slave                                   |             41.19 | v2.0.0-RC1 |
| XiaoMi/pegasus#525       | fix: bind prometheus exposer to 0.0.0.0                                                   |             43.90 | v2.0.0-RC1 |
| XiaoMi/pegasus#524       | refactor(server_impl): Separate constructor to an independent file                        |             44.95 | v2.0.0-RC1 |
| XiaoMi/pegasus#522       | feat(rocksdb): Support more configurable items for bloom filter                           |             44.99 | v2.0.0-RC1 |
| XiaoMi/pegasus#520       | feat(dup): support shell set fail_mode and collector duplication ops                      |             45.19 | v2.0.0-RC1 |
| XiaoMi/pegasus#521       | feat(metrics): Add bloom filter related metrics                                           |             46.87 | v2.0.0-RC1 |
| XiaoMi/pegasus#519       | feat(shell): count_data return estimate count by default                                  |             54.76 | v2.0.0-RC1 |
| XiaoMi/pegasus#514       | fix(collector): no validate the app_name after parse_app_perf_counter_name                |             57.68 | v2.0.0-RC1 |
| XiaoMi/pegasus#504       | fix: include key size while calculating capacity units                                    |             63.19 | v2.0.0-RC1 |
| XiaoMi/pegasus#494       | feat(rocksdb): write meta info both in manifest and meta CF                               |             76.28 | v2.0.0-RC1 |
| XiaoMi/pegasus#501       | feat: statistics backup request qps in info collector                                     |             79.19 | v2.0.0-RC1 |
| XiaoMi/pegasus#499       | feat: support of getting backup request perf-counter in command_helper                    |             79.27 | v2.0.0-RC1 |
| XiaoMi/pegasus#473       | feat(rocksdb): Bump rocksdb to v6.6.4                                                     |             96.04 | v2.0.0-RC1 |
| XiaoMi/pegasus#459       | feat(dup): write pegasus value in new data version for duplication                        |            120.19 | v2.0.0-RC1 |
| XiaoMi/pegasus#436       | chore: upgrade dev version to 1.13.SNAPSHOT                                               |            188.96 | v2.0.0-RC1 |


## Incompatitable Modifications

| | |
|--|--|
| XiaoMi/pegasus#459         | feat(dup): write pegasus value in new data version for duplication                        |
| XiaoMi/pegasus#473         | feat(rocksdb): Bump rocksdb to v6.6.4                                                     |
| XiaoMi/rdsn#255            | refactor(rpc): refactor request meta & add support for backup request                       |

## New perf-counter

- `collector*app.pegasus*app.stat.backup_request_qps #<app_name>` https://github.com/XiaoMi/pegasus/pull/501

- `replica*app.pegasus*rdb.bf_seek_total@<gpid>` https://github.com/XiaoMi/pegasus/pull/521
- `replica*app.pegasus*rdb.bf_seek_negatives<gpid>` https://github.com/XiaoMi/pegasus/pull/521
- `replica*app.pegasus*rdb.bf_point_positive_true<gpid>` https://github.com/XiaoMi/pegasus/pull/521
- `replica*app.pegasus*rdb.bf_point_positive_total<gpid>`  https://github.com/XiaoMi/pegasus/pull/521
- `replica*app.pegasus*rdb.bf_point_negatives<gpid>` https://github.com/XiaoMi/pegasus/pull/521
- `collector*app.pegasus*app.stat.rdb_bf_seek_negatives_rate#<app_name>` https://github.com/XiaoMi/pegasus/pull/521
- `collector*app.pegasus*app.stat.rdb_bf_point_negatives_rate#<app_name>` https://github.com/XiaoMi/pegasus/pull/521
- `collector*app.pegasus*app.stat.rdb_bf_point_false_positive_rate#<app_name>` https://github.com/XiaoMi/pegasus/pull/521
- `replica*eon.replica*backup_request_qps@<app_name>` https://github.com/XiaoMi/rdsn/pull/419
- `collector*app.pegasus*app.stat.duplicate_qps#<app_name>` XiaoMi/pegasus#520
- `collector*app.pegasus*app.stat.dup_shipped_ops#<app_name>` XiaoMi/pegasus#520
- `collector*app.pegasus*app.stat.dup_failed_shipping_ops#<app_name>` XiaoMi/pegasus#520
- `replica*app.pegasus*dup.time_lag_ms@<app_name>` XiaoMi/pegasus#526
- `replica*app.pegasus*dup.lagging_writes@<app_name>` XiaoMi/pegasus#526

## New configuration

```diff
[pegasus.server]
+rocksdb_bloom_filter_bits_per_key = 10
+rocksdb_format_version = 2
+dup_lagging_write_threshold_ms = 10000
+get_meta_store_type = manifest

[replication]
+ fds_write_limit_rate = 100
+ fds_read_limit_rate = 100

[nfs]
+ max_copy_rate_megabytes = 500
```

- `[pegasus.server] rocksdb_bloom_filter_bits_per_key, rocksdb_format_version` https://github.com/XiaoMi/pegasus/pull/522
- `[pegasus.server] get_meta_store_type` https://github.com/XiaoMi/pegasus/pull/532
- `[nfs]` max_copy_rate_megabytes XiaoMi/rdsn#461
- `[pegasus.server] dup_lagging_write_threshold_ms` XiaoMi/pegasus#526
- `[replication] fds_write_limit_rate, fds_read_limit_rate` XiaoMi/rdsn#443
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/536/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/536,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYyODU3MTk3NQ==,incubator-pegasus,628571975,536,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-05-14T11:29:24Z,2020-05-14T11:29:24Z,"# Release Note

**NOTE: 2.0.0 is backward-compatible only, which means servers upgraded to this version can't rollback to previous versions.**

The following are the highlights in this release:

## Duplication

Duplication is the solution of Pegasus for intra-cluster data copying in real-time. We currently limit our master-master duplication for 'PUT' and 'MULTI_PUT' only. See this document for more details:
https://pegasus-kv.github.io/administration/duplication.

## Backup Request

Backup Request is a way to eliminate tail latency by sacrificing minor data consistency, fallback reading from a random secondary when the primary read failed to finish at the expected time.
See the discussion here: https://github.com/XiaoMi/pegasus/issues/251.

## RocksDB Meta CF

Pegasus currently has a hacked version of RocksDB that stores a few metadata in the manifest file, which makes our RocksDB incompatible with the official version. In this version, we exploit an additional column family (called 'Meta CF') to store those metadata.

To finally get rid of the legacy RocksDB, you must first upgrade the ReplicaServer to 2.0.0.

## Bloom Filter Optimization

This time we support metrics for the utilization of bloom filters in Pegasus. And for critical scenarios, we provide configurations for performance tuning on bloom filters.
See https://github.com/XiaoMi/pegasus/pull/522, https://github.com/XiaoMi/pegasus/pull/521.

## Cold-Backup FDS Limit

This feature adds throttling on download and upload during cold-backup.
See https://github.com/XiaoMi/rdsn/pull/443.

## Adding Node Optimization

We previously suffer from the effect brought by data migration when adding one or more nodes into a cluster. In some latency-critical scenarios (mostly focus on read-latency) this (3~10 times increase in latency) usually implies the service briefly unavailable.

In 2.0.0 we support a strategy that the new nodes do not serve read requests until most migrations are done. Although the new nodes still participate in write-2PC and the overall migration workload doesn't decrease, the read latency significantly improved thanks to this job.

Be aware that this feature requires merely pegasus-tools to be 2.0.0, you don't have to upgrade the server to 2.0.0. See https://github.com/XiaoMi/pegasus/pull/528.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYyODU3MTk3NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/540,https://api.github.com/repos/apache/incubator-pegasus/issues/540,incubator-pegasus,622488289,540,GCC 8.3.1编译错误,JiajieSun,9063755,,,CLOSED,2020-05-21T13:09:20Z,2023-05-16T16:42:47Z,"gcc8.3.1检查更加严格，导致编译出现了一些错误。请看下是否需要修正

[root@localhost pegasus]# gcc -v
gcc version 8.3.1 20190311 (Red Hat 8.3.1-3) (GCC) 

1. Rdsn部分
1.1 ./rdsn/include/dsn/utility/time_utils.h
    sprintf(str,
            ""%04u-%02u-%02u %02u:%02u:%02u.%03u"",
            static_cast<uint32_t>((ret->tm_year + 1900) % 10000),
            static_cast<uint32_t>((ret->tm_mon + 1) % 100),
            static_cast<uint32_t>(ret->tm_mday % 100),
            static_cast<uint32_t>(ret->tm_hour % 100),
            static_cast<uint32_t>(ret->tm_min % 100),
            static_cast<uint32_t>(ret->tm_sec % 100),
            static_cast<uint32_t>(ts_ms % 1000));

1.2 ./rdsn/src/dist/replication/meta_server/meta_split_service.cpp
“zauto_write_lock(app_lock()); -> ""zauto_write_lock l(app_lock());""

1.3 ./rdsn/src/core/core/rpc_message.cpp
""memset(msg->header, 0, sizeof(message_header));"" -> ""memset(static_cast<void *>(msg->header), 0, sizeof(message_header));""


2. Third Party部分，有些最新的repo已经修正了8.3.1的错误，比如说rapidjson。下面列取了有编译问题的文件
2.1 ./rdsn/thirdparty/src/zookeeper-3.4.10/src/c/src/zookeeper.c,
 ""static char buf[128];"" -> ""static char buf[512];""

2.2 ./rdsn/thirdparty/output/include/s2/util/coding/coder.h
""std::memset(array, 0, num_decoders * sizeof(Decoder));"" -> ""std::memset(static_cast<void *>array, 0, num_decoders * sizeof(Decoder));""

2.3 ./rdsn/thirdparty/output/include/rapidjson/document.h
""std::memcpy(e, values, count * sizeof(GenericValue));"" -> ""std::memcpy(static_cast<void*>(e), values, count * sizeof(GenericValue));""
""std::memcpy(m, members, count * sizeof(Member));"" -> ""std::memcpy(static_cast<void*>(m), members, count * sizeof(Member));""


3. 此外moodycamel的新版本的namespace发生了变化, ./rdsn/src/core/tools/hpc/hpc_task_queue.h
""moodycamel::details::mpmc_sema::LightweightSemaphore _sema;"" -> ""moodycamel::LightweightSemaphore _sema;""","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/540/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/540,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMjEwNTU0Nw==,incubator-pegasus,632105547,540,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2020-05-21T14:05:53Z,2020-05-21T14:05:53Z,"@JiajieSun 感谢提出问题！
其中
> 此外moodycamel的新版本的namespace发生了变化, ./rdsn/src/core/tools/hpc/hpc_task_queue.h
""moodycamel::details::mpmc_sema::LightweightSemaphore _sema;"" -> ""moodycamel::LightweightSemaphore _sema;""

已经修复了https://github.com/XiaoMi/rdsn/pull/468
其他问题可以帮忙修复下吗？欢迎贡献代码","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMjEwNTU0Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/540,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMjQyMzE3MQ==,incubator-pegasus,632423171,540,NA,JiajieSun,9063755,,,NA,2020-05-22T01:08:32Z,2020-05-22T01:08:32Z,请教一下，代码提交之前需要跑哪些测试，以确保基本的质量？,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMjQyMzE3MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/540,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMjQ1NjIyMQ==,incubator-pegasus,632456221,540,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-05-22T03:19:50Z,2020-05-22T03:19:50Z,"> 请教一下，代码提交之前需要跑哪些测试，以确保基本的质量？

- Run ./scripts/format_files.sh if you modified any codes in Pegasus
- Run ./rdsn/scripts/linux/run-clang-format.sh in rDSN if any
- Run ./run.sh test to pass CI locally. We use travis-ci also.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMjQ1NjIyMQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/540,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMzQwMDY0OQ==,incubator-pegasus,633400649,540,NA,JiajieSun,9063755,,,NA,2020-05-25T06:25:32Z,2020-05-25T06:25:32Z,"第三方库似乎都维护在小米购买的阿里云上，所以我应该无法修复第三方库连接问题（除非重新指回官方第三方库）。还请小米的小伙伴帮忙修复下这些问题，谢谢。 ：）

比如说ZK，注释是从apache下载：
zookeeper c client
from: http://ftp.jaist.ac.jp/pub/apache/zookeeper/zookeeper-3.4.10/zookeeper-3.4.10.tar.gz

实际下载地址：
http://pegasus-thirdparties.oss-cn-beijing.aliyuncs.com/zookeeper-3.4.10.tar.gz","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMzQwMDY0OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/540,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMzc5NTM0MA==,incubator-pegasus,633795340,540,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2020-05-26T04:00:55Z,2020-05-26T04:00:55Z,"我们这么做主要是为了加速第三方包的下载速度，有些包在国内从官网下载太慢了。
@JiajieSun 可以说下需要更新的包的官方链接，我们先传到阿里云。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMzc5NTM0MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/540,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMzg0Mzc0NQ==,incubator-pegasus,633843745,540,NA,qinzuoyan,528445,QinZuoyan,qinzuoyan@gmail.com,NA,2020-05-26T06:54:24Z,2020-05-26T06:54:24Z,建议在run.sh build里面加一个选项，可以选择是用官方下载路径，还是阿里云下载路径,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDYzMzg0Mzc0NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/540,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5cY1TP,incubator-pegasus,1550013647,540,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-05-16T16:40:47Z,2023-05-16T16:40:47Z,"The master branch will be built everyday on various of plats including gcc 7.5.0, 9.4.0, I guess the build failure has been fixed on gcc 8.3.1.

I'll close this issue, you can reopen it if you find the master branch build it failed, thanks.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5cY1TP/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/547,https://api.github.com/repos/apache/incubator-pegasus/issues/547,incubator-pegasus,635950386,547,Release 1.12.4,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2020-06-10T05:53:17Z,2020-06-16T03:12:46Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/547/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/552,https://api.github.com/repos/apache/incubator-pegasus/issues/552,incubator-pegasus,639378851,552,share log calculated size is unreasonable or the shared log may be damaged,foreverneverer,23136769,Jiashuo,js982986555@live.com,OPEN,2020-06-16T05:22:18Z,2020-06-23T06:51:12Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?

* One replica-server was down, we manually re-added it into the cluster.
* Run the following commands to add the node:

  - `remote-command -t meta-server meta.lb.only_move_primary true`
  - `set_meta_level lively `

2. What did you expect to see?
The node server can be restarted and no any error

3. What did you see instead?
* the perfcounter report the `shared log too large 25853163(MB) > 50000`
* the log show error  as soon as when the node server restart:
```
mutation_log.cpp:2057:read_next_log_block(): read data block body failed, size = 328 vs 676, err = ERR_HANDLE_EOF
replica_stub.cpp:552:initialize():some shared log state must be lost, smax(1301076891) vs pmax(1301079680)
replica_stub.cpp:565:initialize(): logs are not complete for some replicas, which means that shared log is truncated, mark all replicas as inactive
```

4. What version of Pegasus are you using?
pegasus-server-1.12.3-a948e89-glibc2.12-release.tar.gz

5. Suggestion
* suggest `dessart` instead of `derror` if the shared log is damaged when restart the node server
* cleanup the node and then restart
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/552/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/552,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY0NzkyNjQ1MA==,incubator-pegasus,647926450,552,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-06-23T05:57:46Z,2020-06-23T05:57:46Z,"> What did you expect to see?
> The node server can be restarted and no any error.
>
> What did you see instead?
> the perfcounter report the shared log too large 25853163(MB) > 50000
> the log show error as soon as when the node server restart:

So what's the next result of a too-large-shared-log? Did it make the cluster unable to serve anymore? Or was the replica-server unable to restart? ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY0NzkyNjQ1MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/552,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY0Nzk0NjA5Mw==,incubator-pegasus,647946093,552,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-06-23T06:51:11Z,2020-06-23T06:51:11Z,"I think we can consider mocking such case in replica's UT, by intentionally append some mutations only to the plog, without appending to the slog. Let the server restart then, and see what happens.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY0Nzk0NjA5Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/555,https://api.github.com/repos/apache/incubator-pegasus/issues/555,incubator-pegasus,646959920,555,unit test lost_log.slog failed,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2020-06-28T16:32:02Z,2020-06-30T10:01:16Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
`./run.sh test`

2. What did you expect to see?
All tests should passed

3. What did you see instead?
lost_log.slog test failed, one of replica server core dumped with output:
```
E2020-06-29 00:10:00.681 (1593360600681081538 6f00) replica.default0.00006eb700010001: mutation_log.cpp:1999:read_next_log_block(): read data block body failed, size = 77 vs 613, err = ERR_HANDLE_EOF
F2020-06-29 00:10:00.687 (1593360600687823243 6f00) replica.default0.00006eb700010001: replica_stub.cpp:604:initialize(): assertion expression: smax == pmax
F2020-06-29 00:10:00.687 (1593360600687852372 6f00) replica.default0.00006eb700010001: replica_stub.cpp:604:initialize(): 2.1@10.132.7.51:34801: some shared log state must be lost, smax(2697) vs pmax(22252)
```

4. What version of Pegasus are you using?
The latest version(cdda746e05bbbae6b49d36eb7afce611d4f5ea25, rdsn: c9944f3901dffc9e2a2bd5cd80117e5111eb558e)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/555/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/555,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY1MTY5Mzk5Mg==,incubator-pegasus,651693992,555,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2020-06-30T10:01:09Z,2020-06-30T10:01:09Z,Resolved in https://github.com/XiaoMi/rdsn/pull/519,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY1MTY5Mzk5Mg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/558,https://api.github.com/repos/apache/incubator-pegasus/issues/558,incubator-pegasus,657178811,558,从Rocksdb导入数据无法转移,mzss0,20411576,,,CLOSED,2020-07-15T08:49:55Z,2020-07-22T03:54:20Z,"## General Question

Before asking a question, make sure you have:

- Searched open and closed [GitHub issues](https://github.com/XiaoMi/pegasus/issues)
- Read the documentation:
  - [Pegasus Doc](https://pegasus-kv.github.io)


通过rocksdb导入sst文件成功了并且能从pegasus查到，但是新的replica不能通过learn获取到rocksdb导入的数据

唯一尝试成功转移的方法是调用远程kill命令，这个命令在每个分片replica重启后会生成checkpoint。但是我直接生成checkpoint并不能转移。我看了learn相关文档和代码，试了flush mem等很多操作，也没成功。请问要怎么绕开kill来实现这个功能？","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/558/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/558,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY1ODY0NzgyMw==,incubator-pegasus,658647823,558,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2020-07-15T09:09:50Z,2020-07-15T09:09:50Z,"""通过rocksdb导入sst文件"" 这个你是怎么做的呢？
这个功能 [bulkload](https://pegasus-kv.github.io/2020/02/18/bulk-load-design.html) 我们还在开发中","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY1ODY0NzgyMw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/558,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY1ODY1NzYxNw==,incubator-pegasus,658657617,558,NA,mzss0,20411576,,,NA,2020-07-15T09:28:29Z,2020-07-15T09:28:29Z,"> ""通过rocksdb导入sst文件"" 这个你是怎么做的呢？
> 这个功能 [bulkload](https://pegasus-kv.github.io/2020/02/18/bulk-load-design.html) 我们还在开发中

就是rocksdb的一个方法，可以导入数据，跟你们那个文档一样。具体就是弄了一个jni生成sst，pegasus规则的，然后就导进去，能查数据。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY1ODY1NzYxNw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/558,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY1ODY3NTI0MA==,incubator-pegasus,658675240,558,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2020-07-15T10:01:39Z,2020-07-15T10:01:39Z,在当前版本的Pegasus实现中，新数据的写入会更新decree，并记录到rocksdb的manifest中。这是一个我们在原生rocksdb上新增的字段。我怀疑ingest sst的时候并没有更新decree，其他replica自然也learn不到这部分新增的数据。,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY1ODY3NTI0MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/558,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY1ODcyMjQ2NQ==,incubator-pegasus,658722465,558,NA,mzss0,20411576,,,NA,2020-07-15T11:50:07Z,2020-07-15T11:50:07Z,"> 在当前版本的Pegasus实现中，新数据的写入会更新decree，并记录到rocksdb的manifest中。这是一个我们在原生rocksdb上新增的字段。我怀疑ingest sst的时候并没有更新decree，其他replica自然也learn不到这部分新增的数据。

有更新这个decree的参考嘛，我试过直接更新lastcommitdecree，就是直接做一个＋＋，就像写入的时候一样，但是会coredump

还有能解答一下为什么用远程命令kill之后产生的checkpoint能正常转移而我直接生成的不行，这里面能加代码实现功能吗","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY1ODcyMjQ2NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/558,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY2MDc2OTgyMA==,incubator-pegasus,660769820,558,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2020-07-20T02:44:18Z,2020-07-20T02:44:18Z,"> > 在当前版本的Pegasus实现中，新数据的写入会更新decree，并记录到rocksdb的manifest中。这是一个我们在原生rocksdb上新增的字段。我怀疑ingest sst的时候并没有更新decree，其他replica自然也learn不到这部分新增的数据。
> 
> 有更新这个decree的参考嘛，我试过直接更新lastcommitdecree，就是直接做一个＋＋，就像写入的时候一样，但是会coredump
> 
> 还有能解答一下为什么用远程命令kill之后产生的checkpoint能正常转移而我直接生成的不行，这里面能加代码实现功能吗

没有看到具体的代码，还真不清楚具体是哪里的问题。
你可以等等我们官方支持的bulk load，1~2周内就会发RC版本了。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY2MDc2OTgyMA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/562,https://api.github.com/repos/apache/incubator-pegasus/issues/562,incubator-pegasus,662595021,562,ubuntu 18.04  run.sh build -c error,totalo,29777558,,totalo@apache.org,CLOSED,2020-07-21T05:09:55Z,2020-07-21T05:57:29Z,"ubuntu 18.04  run.sh build -c error

`./autogen.sh: 18: ./autogen.sh: aclocal: not found
./build-thirdparty.sh: line 145: ./configure: No such file or directory
make: *** No targets specified and no makefile found.  Stop.
build libevent failed
ERROR: build rdsn failed`","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/562/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/562,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY2MTYzOTYxNA==,incubator-pegasus,661639614,562,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2020-07-21T05:20:35Z,2020-07-21T05:20:35Z,Have you installed all dependent libs and tools? https://pegasus-kv.github.io/overview/compilation,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY2MTYzOTYxNA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/562,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY2MTY0NDcyNw==,incubator-pegasus,661644727,562,NA,totalo,29777558,,totalo@apache.org,NA,2020-07-21T05:37:39Z,2020-07-21T05:37:39Z,"> dependent

Yeah! I think i've solved it! I checked the version and install them again,but a new error is here.


```
/usr/bin/x86_64-linux-gnu-ld: cannot find -lgtest
collect2: error: ld returned 1 exit status
base/test/CMakeFiles/base_test.dir/build.make:126: recipe for target 'base/test/base_test' failed
make[2]: *** [base/test/base_test] Error 1
CMakeFiles/Makefile2:195: recipe for target 'base/test/CMakeFiles/base_test.dir/all' failed
make[1]: *** [base/test/CMakeFiles/base_test.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs..
```

```
Makefile:129: recipe for target 'all' failed
make: *** [all] Error 2
ERROR: build pegasus failed
ERROR: build pegasus failed
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY2MTY0NDcyNw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/562,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY2MTY1MTA4Mg==,incubator-pegasus,661651082,562,NA,totalo,29777558,,totalo@apache.org,NA,2020-07-21T05:57:26Z,2020-07-21T05:57:26Z,All right !  I've build success! it's seem to be loss gtest.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY2MTY1MTA4Mg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/564,https://api.github.com/repos/apache/incubator-pegasus/issues/564,incubator-pegasus,663397139,564,meta server init failed,totalo,29777558,,totalo@apache.org,CLOSED,2020-07-22T00:57:08Z,2020-07-22T11:19:57Z,"hi, When I use `./run.sh start_onebox to start onebox`, all three mete servers init failed, and the error log is as follows:

```
W2020-07-22 08:51:08.790 (1595379068790777610 668a)   meta.io-thrd.26250: rpc_engine.cpp:597:on_recv_request(): recv message with unhandled rpc name RPC_FD_FAILURE_DETECTOR_PING from 10.232.21.160:34802, trace_id = 37a1e84eda2df0fb
E2020-07-22 08:51:09.678 (1595379069678368368 668d)   meta.default0.0000665500010001: meta_service.cpp:98:remote_storage_initialize(): init meta_state_service failed, err = ERR_TIMEOUT
E2020-07-22 08:51:09.678 (1595379069678483477 668d)   meta.default0.0000665500010001: meta_service.cpp:214:start(): init remote storage failed, err = ERR_TIMEOUT
F2020-07-22 08:51:09.678 (1595379069678505527 668d)   meta.default0.0000665500010001: tool_api.cpp:61:exec(): assertion expression: err == ERR_OK
F2020-07-22 08:51:09.678 (1595379069678529172 668d)   meta.default0.0000665500010001: tool_api.cpp:61:exec(): start app failed, err = ERR_TIMEOU
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/564/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/564,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY2MjE5MjkzOQ==,incubator-pegasus,662192939,564,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2020-07-22T01:41:31Z,2020-07-22T01:41:31Z,"Seems zookeeper is not well initialized, what's the output of `./run.sh start_onebox`?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY2MjE5MjkzOQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/564,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY2MjI0MTE3OA==,incubator-pegasus,662241178,564,NA,totalo,29777558,,totalo@apache.org,NA,2020-07-22T04:57:39Z,2020-07-22T04:57:39Z,"```
gzip: stdin: not in gzip format
tar: Child returned status 1
tar: Error is not recoverable: exiting now
ERROR: decompress zookeeper failed
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY2MjI0MTE3OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/564,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY2MjM5NzEwNA==,incubator-pegasus,662397104,564,NA,totalo,29777558,,totalo@apache.org,NA,2020-07-22T11:19:53Z,2020-07-22T11:19:53Z,This problem is due to a problem with the zookeeper download link of the v1.11.4 version. I downloaded the corresponding version of zookeeper myself and the problem was solved,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY2MjM5NzEwNA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/567,https://api.github.com/repos/apache/incubator-pegasus/issues/567,incubator-pegasus,667593977,567,Fix time-consuming CI tests,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,OPEN,2020-07-29T06:42:24Z,2020-10-26T08:13:27Z,"Here I list the most time-consuming unit tests in rDSN:

- [ ] meta_load_balance_test.simple_lb_cure_test (14650 ms)
- [ ] meta.data_definition (40765 ms)
- [ ] meta.adjust_dropped_size (11367 ms)
- [ ] meta.state_sync (39060 ms)
- [ ] meta.update_configuration (14376 ms)
- [ ] meta.balancer_validator (60094 ms)
- [ ] fd.worker_died_when_switch_master (12001 ms)
- [ ] fd.switch_new_master_suddenly (18002 ms)
- [ ] fd.dummy_connect_disconnect (11002 ms)
- [x] tools_common.asio_network_provider_connection_threshold (70017 ms)

All of them spend in total 5min. It's unacceptable to have these tests taking so long. Welcome if anyone could help me to fix any of them.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/567/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/571,https://api.github.com/repos/apache/incubator-pegasus/issues/571,incubator-pegasus,675946838,571,Load options from file when open an exist DB,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2020-08-10T08:03:39Z,2021-06-01T07:03:10Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
Replicas may transient lost from consistent protocol and then reopen on the same ReplicaServer shortly, because of network or some other problems.
When replicas reopen rocksdb, its **usage scenario** will reset to **NORMAL** and set to **actual scenario** of this table after ENVS synced from MetaServer. It will cause flushs and compactions when call rocksdb's `SetOptions()`, we'd better avoid it.

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->
Avoid reset usage scenario and DB options, for example:
- store usage scenario into meta column family
- load options from file when open an exist DB (using `rocksdb::LoadLatestOptions`)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/571/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/574,https://api.github.com/repos/apache/incubator-pegasus/issues/574,incubator-pegasus,677456984,574,Using Chaos Mesh to enhance pegasus's stability,zhouqiang-cl,23521459,zhouqiang,zhouqiang.cl@gmail.com,OPEN,2020-08-12T07:17:06Z,2020-08-14T02:29:48Z,"Hello, I am [Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) maintainer. [Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) is a open source chaos engineering platform for Kubernetes.

As you may have known, chaos engineering is a promising method that can guarantee system resilience. We have been practicing chaos engineering for long on databases and have found many issues in various fault scernarios (for example data consistency issue caused by excessive recovery time). Some of the issue records at https://github.com/orgs/chaos-mesh/projects/1. There are also [many adopters]( https://github.com/chaos-mesh/chaos-mesh#adopters) who are using [Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) to test their systems, such as PingCAP, Daily motion, Xpeng Motors, Meituan-Dianping, Apache pulsar, etc.

That said, I would like to invite you to use [Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) to enhance pegasus's stability. You can directly use [Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) or integrate it with your test platform. [Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) supports rich injection methods, such as pod/container kill, network partition, network interruption, CPU/memory usage, time rollback, disk failure, etc.
Welcome to use it to enhance the stability of pegasus. If you have any questions,  submit an issue at https://github.com/chaos-mesh/chaos-mesh, or send me an email zhouqiang@pingcap.com


Hope this helps and look forward to further communications. ❤️","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/574/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/574,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY3MzI3Njk1Nw==,incubator-pegasus,673276957,574,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-08-13T05:53:23Z,2020-08-13T05:53:23Z,"@zhouqiang-cl You guys are pretty nice :) We actually have an objective to integrate Chaos Mesh in our system. I think this is a very practical and innovative tool to improve our testing process.

Nevertheless, there're some problems the Pegasus needs to solve out first. The prerequisite for Chaos Mesh is a Pegasus deployment automated inside k8s. But this is not trivial since Pegasus is a stateful service. We do not have ""tidb-operator""-like stuff.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY3MzI3Njk1Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/574,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY3Mzg0OTE0Mw==,incubator-pegasus,673849143,574,NA,zhouqiang-cl,23521459,zhouqiang,zhouqiang.cl@gmail.com,NA,2020-08-14T02:29:48Z,2020-08-14T02:29:48Z,"> @zhouqiang-cl You guys are pretty nice :) We actually have an objective to integrate Chaos Mesh in our system. I think this is a very practical and innovative tool to improve our testing process.
> 
> Nevertheless, there're some problems the Pegasus needs to solve out first. The prerequisite for Chaos Mesh is a Pegasus deployment automated inside k8s. But this is not trivial since Pegasus is a stateful service. We do not have ""tidb-operator""-like stuff.

Sorry for the late reply. 
Very glad that you have the interest,  If you have any problem in using Chaos Mesh, we are very pleasure if we can help

Waiting for your great news  ❤️","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY3Mzg0OTE0Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/577,https://api.github.com/repos/apache/incubator-pegasus/issues/577,incubator-pegasus,680070384,577,Release 2.1.0,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2020-08-17T09:03:36Z,2021-03-01T03:59:16Z,"## 2.1.0-RC1
### rdsn
| PR (107 TOTAL)            |                                                      TITLE                                                       |
|----------------------------|------------------------------------------------------------------------------------------------------------------|
| XiaoMi/rdsn#587            | fix(bulk-load): fix bug during ingestion                                                                         |
| XiaoMi/rdsn#584            | refactor: make read batch size of fds configurable                                                               |
| XiaoMi/rdsn#583            | fix(bulk_load): fix bug of clear_bulk_load_states_if_needed                                                      |
| XiaoMi/rdsn#581            | fix(bulk-load): fix bug about concurrent downloading count                                                       |
| XiaoMi/rdsn#580            | fix: remote-command max_copy_rate_megabytes failed when set ""DEFAULT""                                            |
| XiaoMi/rdsn#579            | fix(bulk-load): fix bug while check_all_partitions                                                               |
| XiaoMi/rdsn#578            | fix: fix building errors by clang-9                                                                              |
| XiaoMi/rdsn#577            | refactor: reduce compilation size for building each meta test                                                    |
| XiaoMi/rdsn#576            | fix(bulk-load): fix bug while check app bulk load states                                                         |
| XiaoMi/rdsn#574            | build(ci): refine build scripts                                                                                  |
| XiaoMi/rdsn#572            | fix(bulk-load): fix bug while rolling back to downloading                                                        |
| XiaoMi/rdsn#571            | feat(bulk-load): ignore bulk loading app while load balance                                                      |
| XiaoMi/rdsn#570            | fix(bulk-load): update cleanup download task                                                                     |
| XiaoMi/rdsn#568            | perf(slog): Use a special thread pool for slog write callback                                                    |
| XiaoMi/rdsn#567            | feat(bulk-load): add perf-counter                                                                                |
| XiaoMi/rdsn#566            | refactor: move runtime module from src/core/core to src/runtime                                                  |
| XiaoMi/rdsn#565            | feat(bulk-load): add bulk load ddl client interface                                                              |
| XiaoMi/rdsn#564            | feat(bulk-load): check app bulk load states while sync app from remote storage                                   |
| XiaoMi/rdsn#563            | feat(bulk-load): continue bulk load while meta server start part2                                                |
| XiaoMi/rdsn#562            | refactor: use DSN_REGISTER_COMPONENT_PROVIDER to register logger                                                 |
| XiaoMi/rdsn#560            | fix: cleaup lock in ut abnormal_api_call                                                                         |
| XiaoMi/rdsn#559            | feat(bulk-load): continue bulk load while meta server start part1                                                |
| XiaoMi/rdsn#558            | refactor: move tools module from src/dist/replication/tool_lib to src/tools                                      |
| XiaoMi/rdsn#557            | refactor: change tests module name to test                                                                       |
| XiaoMi/rdsn#556            | feat(bulk-load): add query bulk load                                                                             |
| XiaoMi/rdsn#555            | refactor: move replica module from src/dist/replication/lib to src/replica                                       |
| XiaoMi/rdsn#554            | refactor: move meta module from src/dist/replication/meta_server to src/meta                                     |
| XiaoMi/rdsn#553            | feat(bulk-load): sync bulk load from remote storage                                                              |
| XiaoMi/rdsn#552            | fix: fix compile thrift error in compile_thrift.py                                                               |
| XiaoMi/rdsn#551            | fix: compile thrift error                                                                                        |
| XiaoMi/rdsn#550            | refactor: move common module from src/dist/replication/common to src/common                                      |
| XiaoMi/rdsn#549            | refactor: move zookeeper module from src/dist/replication to src/zookeeper                                       |
| XiaoMi/rdsn#548            | feat(bulk-load): small update for meta bulk load service                                                         |
| XiaoMi/rdsn#547            | refactor: combine src/dist/replication/client and src/dist/replication/ddl_lib into src/client                   |
| XiaoMi/rdsn#546            | refactor: delete replication_checker which is unused                                                             |
| XiaoMi/rdsn#545            | refactor: make throttle controller decoupled from dsn_runtime                                                    |
| XiaoMi/rdsn#543            | feat(bulk-load): replica validate status                                                                         |
| XiaoMi/rdsn#541            | refactor: move perf_counter module from src/core/perf_counter to src/perf_counter                                |
| XiaoMi/rdsn#540            | refactor: combine src/dist/failure_detector and src/dist/failure_detector_multimaster into src/failure_detector  |
| XiaoMi/rdsn#539            | refactor: move remote_cmd module from src/dist/cmd to src/remote_cmd                                             |
| XiaoMi/rdsn#538            | refactor: move nfs module from src/dist/nfs to src/nfs                                                           |
| XiaoMi/rdsn#537            | refactor: move http module from src/dist/http to src/http                                                        |
| XiaoMi/rdsn#536            | refactor: move block_service module from src/dist/block_service to src/block_service                             |
| XiaoMi/rdsn#535            | feat(bulk-load): handle recoverable errors during bulk load                                                      |
| XiaoMi/rdsn#534            | feat(bulk-load): replica handle errors during bulk load                                                          |
| XiaoMi/rdsn#533            | refactor: move aio module from src/core/aio to src/aio                                                           |
| XiaoMi/rdsn#532            | handle bulk load failed and app unavailable                                                                      |
| XiaoMi/rdsn#531            | feat(bulk-load): cancel bulk load                                                                                |
| XiaoMi/rdsn#530            | refactor: make simple logger decoupled from dsn_runtime                                                          |
| XiaoMi/rdsn#529            | refactor: decouple sys_exit_type with dsn_runtime                                                                |
| XiaoMi/rdsn#528            | refactor(aio): simplify disk_engine interfaces                                                                   |
| XiaoMi/rdsn#526            | fix(scripts): change the download source of zookeeper                                                            |
| XiaoMi/rdsn#525            | build: use regex to find rocksdb depend module path                                                              |
| XiaoMi/rdsn#524            | feat(bulk-load): restart bulk load                                                                               |
| XiaoMi/rdsn#523            | feat(thirdparty): Update rocksdb to rocksdb-6.6.4-compatible                                                     |
| XiaoMi/rdsn#522            | refactor: make command manager decoupled from dsn_runtime                                                        |
| XiaoMi/rdsn#521            | refactor(log): separate log_file from mutation_log                                                               |
| XiaoMi/rdsn#519            | fix: revert ""dassert if shared log is damaged when replica server init"" commit                                   |
| XiaoMi/rdsn#518            | add pause bulk load                                                                                              |
| XiaoMi/rdsn#517            | refactor: make utils decoupled from dsn_runtime                                                                  |
| XiaoMi/rdsn#516            | refactor(aio): refine code style of aio module                                                                   |
| XiaoMi/rdsn#515            | refactor: move task to a seperated directory                                                                     |
| XiaoMi/rdsn#514            | feat(bulk-load): pause bulk load part1 - add control_bulk_load rpc and start to pause bulk load                  |
| XiaoMi/rdsn#513            | fix: dassert if shared log is damaged when replica server init                                                   |
| XiaoMi/rdsn#512            | refactor: remove the unused struct io_engine                                                                     |
| XiaoMi/rdsn#510            | feat(utils): add latency trace tool                                                                              |
| XiaoMi/rdsn#508            | feat(bulk-load): bulk load succeed part2 - meta handle bulk load succeed                                         |
| XiaoMi/rdsn#507            | refactor: use DSN_REGISTER_COMPONENT_PROVIDER to register aio_provider                                           |
| XiaoMi/rdsn#505            | refactor: add a interface of rpc_holder in storage_serverlet                                     |
| XiaoMi/rdsn#504            | refactor: move rpc module to a separated directory                                                               |
| XiaoMi/rdsn#502            | feat(bulk-load): bulk load succeed part1 - replica handle bulk load succeed                                      |
| XiaoMi/rdsn#501            | refactor: use check_status to reimplement some rpc interfaces of meta_service                                    |
| XiaoMi/rdsn#500            | feat(bulk-load): bulk load ingestion part6 - meta handle bulk_load_response during ingestion                     |
| XiaoMi/rdsn#498            | feat: support adding ignored apps for load balancer                                                              |
| XiaoMi/rdsn#497            | fix: fix RPC_CHECK_STATUS incorrect usage in meta_service                                                        |
| XiaoMi/rdsn#496            | feat(bulk-load): bulk load ingestion part5 - replica handle bulk load request during ingestion                   |
| XiaoMi/rdsn#495            | feat: add perf counter for group check failure                                                                   |
| XiaoMi/rdsn#494            | refactor: use flag api to load the config in block_service module                                                |
| XiaoMi/rdsn#493            | feat(bulk-load): bulk load ingestion part4 - meta handle ingestion response                                      |
| XiaoMi/rdsn#492            | refactor: refactor restore progress                                                                              |
| XiaoMi/rdsn#491            | refactor: move function verify_file to file_system                                                               |
| XiaoMi/rdsn#490            | feat(bulk-load): bulk load ingestion part2 - replica handle ingestion request                                    |
| XiaoMi/rdsn#489            | refactor: use rpc holder to reimplement rpc interface of replica server                                          |
| XiaoMi/rdsn#488            | refactor: use flag api to load the config in nfs module                                                          |
| XiaoMi/rdsn#487            | refactor: use rpc holder to reimplement rpc interface of meta server                                             |
| XiaoMi/rdsn#486            | feat(bulk-load): bulk load ingestion part1 - meta start ingestion                                                |
| XiaoMi/rdsn#484            | build: fix build failure when compiling pegasus                                                                  |
| XiaoMi/rdsn#483            | build: add rocksdb as thirdparty                                                                                 |
| XiaoMi/rdsn#482            | feat(bulk-load): bulk load download part5 - meta handle download status and progress                             |
| XiaoMi/rdsn#481            | fix(dup): convert type of failure perf-counter to VOLATILE_NUMBER                                                |
| XiaoMi/rdsn#480            | refactor: refactor block service manager                                                                         |
| XiaoMi/rdsn#479            | feat(bulk-load): bulk load download part4 - replica report download status and progress                          |
| XiaoMi/rdsn#478            | fix: fix memory leak in unit test netprovider.cpp                                                                |
| XiaoMi/rdsn#477            | fix: fix memory leak in asio_udp_provider                                                                        |
| XiaoMi/rdsn#476            | fix: use reference instead of value                                                                              |
| XiaoMi/rdsn#475            | feat(bulk-load): bulk load download part3 - replica parse metadata, verify files and update progress             |
| XiaoMi/rdsn#473            | refactor: simplify remote-command rpc                                                                            |
| XiaoMi/rdsn#471            | feat(bulk-load): bulk load download part2 - replica download files                                               |
| XiaoMi/rdsn#469            | fix: fix bug in download-thirdparty                                                                              |
| XiaoMi/rdsn#467            | refactor(bulk-load): add replica_bulk_loader class                                                               |
| XiaoMi/rdsn#466            | refactor: make aio decoupled from dsn_runtime                                                                    |
| XiaoMi/rdsn#465            | feat(bulk-load): bulk load download part1 - replica start download                                               |
| XiaoMi/rdsn#464            | feat(task): add an interface to query thread info                                                                |
| XiaoMi/rdsn#463            | feat(bulk-load): meta handle bulk_load_response                                                                  |
| XiaoMi/rdsn#460            | feat(bulk-load): group bulk load request in replica group                                                        |
| XiaoMi/rdsn#453            | feat(thirdparty): Introduce readerwriterqueue util                                                               |
| XiaoMi/rdsn#438            | build(deps): use cmake ExternalProject to manage dependencies                                                    |

### pegasus
| PR (26 TOTAL)     |                                     TITLE                                     |
|-------------------------|-------------------------------------------------------------------------------|
| XiaoMi/pegasus#600     | refactor: update license statement             |
| XiaoMi/pegasus#594     | fix: fix db is not nullptr when release_db          |
| XiaoMi/pegasus#593     | fix: Use default options to open db when latest option file has incompatible db options   |
| XiaoMi/pegasus#589     | chore: add disclaimer for not compliant with asf policy                  |
| XiaoMi/pegasus#587     | fix: Load options from file when open an exist DB             |
| XiaoMi/pegasus#573     | fix: change the construction parameters of hotspot calculator                 |
| XiaoMi/pegasus#572     | fix: delete outdated algorithm of table_hotspot_policy                        |
| XiaoMi/pegasus#569     | fix: update pack scripts to remove boost paths                                |
| XiaoMi/pegasus#568     | feat(bulk-load): add bulk load function test                                |
| XiaoMi/pegasus#566     | perf(slog): add new pool for slog callback                                    |
| XiaoMi/pegasus#565     | feat(bulk-load): add query bulk load shell command                            |
| XiaoMi/pegasus#563     | fix: modify pack_server.sh to copy libdsn_utils.so                            |
| XiaoMi/pegasus#561     | feat(bulk-load): add bulk load shell command                                  |
| XiaoMi/pegasus#559     | refactor: add the dependend lib which name is dsn_utils                       |
| XiaoMi/pegasus#557     | refactor: modify the dependent lib names which are generated by rdsn          |
| XiaoMi/pegasus#556     | feat(rocksdb): Remove all calls of Pegasus introduced APIs on RocksDB         |
| XiaoMi/pegasus#554     | refactor: use rpc_holder to receive RPC in pegasus_server_impl                |
| XiaoMi/pegasus#553     | fix: fix incorrect log message in info_collector::on_app_stat                 |
| XiaoMi/pegasus#551     | refactor: refactor rrdb_server into pegasus _read_service                     |
| XiaoMi/pegasus#550     | refactor: use flag api to define configs in pegasus_reporter                  |
| XiaoMi/pegasus#546     | feat(bulk-load): bulk load ingestion part3 - execute ingestion                |
| XiaoMi/pegasus#545     | build: remove rocksdb as a submodule                                          |
| XiaoMi/pegasus#544     | refactor: adapt to rdsn dist cmd refactoring                                  |
| XiaoMi/pegasus#543     | feat(rocksdb): Adapt rate limiter to prevent bust writes and huge compaction  |
| XiaoMi/pegasus#542     | fix: fix bug in function_test/test_restore                                    |
| XiaoMi/pegasus#535     | feat(rocksdb): Use original CreateCheckpoint and read meta data from meta CF  |

## Incompatitable Modifications
|          PR                        |                                     TITLE                                     |
|--------------------------------|-----------------------------------------------------------------------------------------|
| XiaoMi/rdsn#486           | feat(bulk-load): bulk load ingestion part1 - meta start ingestion                          |
| XiaoMi/rdsn#523           | feat(thirdparty): Update rocksdb to rocksdb-6.6.4-compatible         |
| #535                              | feat(rocksdb): Use original CreateCheckpoint and read meta data from meta CF         |
| #556                              | feat(rocksdb): Remove all calls of Pegasus introduced APIs on RocksDB         |


## New perf-counter
- `replica*eon.replica_stub*replicas.recent.group.check.fail.count`   XiaoMi/rdsn#495

- `replica*eon.replica_stub*bulk.load.running.count`   XiaoMi/rdsn#567

- `replica*eon.replica_stub*bulk.load.downloading.count`  XiaoMi/rdsn#567

- `replica*eon.replica_stub*bulk.load.ingestion.count`  XiaoMi/rdsn#567

- `replica*eon.replica_stub*bulk.load.succeed.count`  XiaoMi/rdsn#567

- `replica*eon.replica_stub*bulk.load.failed.count`  XiaoMi/rdsn#567

- `replica*eon.replica_stub*bulk.load.ingestion.reject.write.count`  XiaoMi/rdsn#567

- `replica*eon.replica_stub*bulk.load.download.file.success.count`  XiaoMi/rdsn#567

- `replica*eon.replica_stub*bulk.load.download.file.fail.count`  XiaoMi/rdsn#567

- `replica*eon.replica_stub*bulk.load.download.file.size`  XiaoMi/rdsn#567

- `replica*eon.replica_stub*bulk.load.max.ingestion.duration.time.ms`  XiaoMi/rdsn#567

- `replica*eon.replica_stub*bulk.load.max.duration.time.ms`  XiaoMi/rdsn#567

- `replica*app.pegasus*rdb.write_limiter_rate_bytes` #543 

## Configuration update
```diff
[apps.replica]
+  pools = ...THREAD_POOL_INGESTION, THREAD_POOL_SLOG

+[threadpool.THREAD_POOL_INGESTION]
+  name = ingestion
+  partitioned = false
+  worker_priority = THREAD_xPRIORITY_NORMAL
+  worker_count = 24

+[threadpool.THREAD_POOL_SLOG]
+ name = slog
+ worker_count = 1

[replication]
+ bulk_load_provider_root = bulk_load_root
+ max_concurrent_bulk_load_downloading_count = 5
+ enable_latency_tracer = false
+ fds_read_batch_size = 100

[pegasus.server]
+ rocksdb_limiter_max_write_megabytes_per_sec = 500
+ rocksdb_limiter_enable_auto_tune = false
- get_meta_store_type = metacf
```
- `[pegasus.server] rocksdb_limiter_max_write_megabytes_per_sec` #543 

- `[pegasus.server] rocksdb_limiter_enable_auto_tune` #543 

- `[replication] bulk_load_provider_root` #546 

- `[replication] max_concurrent_bulk_load_downloading_count` #546  XiaoMi/rdsn#465

- `[threadpool.THREAD_POOL_INGESTION]`  #546   XiaoMi/rdsn#490

- `[replication] enable_latency_tracer`  XiaoMi/rdsn#510

- `[threadpool.THREAD_POOL_SLOG]`  #566  XiaoMi/rdsn#568

- `[replication] fds_read_batch_size` XiaoMi/rdsn#584

- remove `[pegasus.server] get_meta_store_type` #556 
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/577/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/578,https://api.github.com/repos/apache/incubator-pegasus/issues/578,incubator-pegasus,681600180,578,use json result to get target value replace complex regex to filter in scripts,foreverneverer,23136769,Jiashuo,js982986555@live.com,OPEN,2020-08-19T07:02:32Z,2020-08-19T07:02:32Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
Now in scripts file, we usually use complex regex to filter the shell command result, just as:
https://github.com/apache/incubator-pegasus/blob/fc1d2aa05df7bcea25116e1533a919b43f478209/scripts/pegasus_rolling_update.sh#L63
which is not elegant. Since the shell supports the json format result, we can consider using the json result:
```
{
    ""cluster_info"": {
        ""meta_servers"": ""127.0.0.1:34601,127.0.0.1:34602,127.0.0.1:34603"",
        ""primary_meta_server"": ""127.0.0.1:34603"",
        ""zookeeper_hosts"": ""127.0.0.1:22181"",
        ""zookeeper_root"": ""/pegasus/onebox/127.0.0.1"",
        ""meta_function_level"": ""steady"",
        ""balance_operation_count"": ""move_pri=0,copy_pri=0,copy_sec=0,total=0"",
        ""primary_replica_count_stddev"": ""0.00"",
        ""total_replica_count_stddev"": ""0.00""
    }
}

```
 to get the target value

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/578/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/582,https://api.github.com/repos/apache/incubator-pegasus/issues/582,incubator-pegasus,683482481,582,support new “data export” feature to replace using backup,foreverneverer,23136769,Jiashuo,js982986555@live.com,OPEN,2020-08-21T11:04:52Z,2020-08-21T11:05:29Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
Now if we want to dump/export data fastly, we need use [backup](http://pegasus.apache.org/administration/cold-backup). However, the backup feature is not friendly for dumping data, for example:
* Don't support dump immediately, we need create a task and wait excuting
* The path contain redundant sub-dir such as `policy name` etc. and don‘t support custom path

Above and some other question result in dumping data is complex and especially using [Pegasus-Spark](https://github.com/pegasus-kv/pegasus-spark) to read the dumped data.

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->
I expect the command line should be simple as follow:
```shell
# dump/export once
# target is hdfs, sub-path is optional, default can be pegasus/cluster_name/table_name
>> dump/export table_name hdfs://url sub-path
# target is fds, sub-path is optional, default can be pegasus/cluster_name/table_name
>> dump/export table_name endpoint bucket sub-path

# dump/export periodicly
>> dump/export table_name hdfs://url sub-path start_time periodic_time
```
finaly, the data path is:
```
root/pegasus/cluster/table/timestamp/partition/file.sst
```
and then, user can use [Pegasus-Spark](https://github.com/pegasus-kv/pegasus-spark) to read data directly but no need we must offer the `policy name`,`cluster_name`, `table_name`, `fds/hdfs config`.

**Describe alternatives you've considered:**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->
Since the `cold backup` code is complex, so we no need refactor it to achive the above result, but can re-implement new feature

**Teachability, Documentation, Adoption, Migration Strategy:**
<!-- If you can, explain some scenarios how users might use this, situations it would be helpful in. Any API designs, mockups, or diagrams are also helpful. -->
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/582/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/588,https://api.github.com/repos/apache/incubator-pegasus/issues/588,incubator-pegasus,686488521,588,The way to split PR 502,Smityz,22953824,Smilencer,smityz@qq.com,CLOSED,2020-08-26T17:20:38Z,2021-08-27T03:43:28Z,"## Step 1: Communication protocol
**PR: https://github.com/apache/incubator-pegasus/pull/591**
These codes construct the communication protocol hotkey collector needs. **No effect on the original function!**

## Step 2: Refactor the hot partition detection(split read/write hotkey)
**PR:https://github.com/apache/incubator-pegasus/pull/597 https://github.com/apache/incubator-pegasus/pull/592**
The original hot partition detection can not distinguish read and write hotspots, the current code adds this function. **Changed the original data structure, changed the log format, and did not bring additional performance burden!**

## Step 3: Add the ability of collector to send RPC to start hotkey detection
**PR: https://github.com/apache/incubator-pegasus/pull/601**
These codes can add the ability of the collector to send RPC to start hotkey detection. **No effect on the original function!**

## Step 4: Add hotkey collector's main codes
**PR: https://github.com/apache/incubator-pegasus/pull/603**
These codes contain the main functions of the hotkey collector, but in this PR, it won't be starting.  **No effect on the original function!**

## Step 5: Start timing task of hotkey_collector in server 
These codes integrate hotkey collector and pegasus_server_impl and start timing time in the tasking queue. **Add a timing task, but no effect on the original function!**

## Step 6: Realize the function of capturing data
These codes will implement the ability to capture data. **May affect performance and stability, so be careful!**","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/588/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/588,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY4MTMwNjM1NQ==,incubator-pegasus,681306355,588,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2020-08-27T02:30:34Z,2020-08-27T02:30:34Z,Good job! you can start your work.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY4MTMwNjM1NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/595,https://api.github.com/repos/apache/incubator-pegasus/issues/595,incubator-pegasus,694257749,595,GCC 9.2.0 编译错误,darionyaphet,4414314,yaphet,,CLOSED,2020-09-06T05:31:33Z,2020-10-19T07:14:00Z,"## Bug Report

GCC 9.2.0 编译错误 按照下列命令运行：

```
git clone https://github.com/xiaomi/pegasus.git --recursive
cd pegasus

git checkout -b v1.11.4 v1.11.4
git submodule update

./run.sh build -c
```

出现如下错误

```
src/zookeeper.c: In function 'format_endpoint_info':
src/zookeeper.c:3469:21: error: '%d' directive writing between 1 and 5 bytes into a region of size between 0 and 127 [-Werror=format-overflow=]
 3469 |     sprintf(buf,""%s:%d"",addrstr,ntohs(port));
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/595/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/595,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY4ODIwOTg2MQ==,incubator-pegasus,688209861,595,NA,vagetablechicken,24697960,HuangWei,huangwei@apache.org,NA,2020-09-07T09:49:10Z,2020-09-07T09:49:10Z,"We shouldn't change zookeeper source（thirdparty）. So you could disable `-Werror` in `Makefile.am:AM_CFLAGS = -Wall -Werror`, then rebuild thirdparty.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY4ODIwOTg2MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/595,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY4ODU5Mzg4Mw==,incubator-pegasus,688593883,595,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-09-08T03:15:19Z,2020-09-08T03:15:19Z,"BTW, 1.11.4 is an old version. We are striving to make the latest version robust and buildable. Please use the version as latest as possible.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY4ODU5Mzg4Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/595,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxMTQ5MjI0OA==,incubator-pegasus,711492248,595,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2020-10-19T03:34:34Z,2020-10-19T03:34:34Z,"If you are using Ubuntu 20.04, you can use gcc-7/g++-7","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxMTQ5MjI0OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/599,https://api.github.com/repos/apache/incubator-pegasus/issues/599,incubator-pegasus,696394634,599,Xiao statement should be removed,vongosling,635581,vongosling,vongosling@apache.org,CLOSED,2020-09-09T04:16:26Z,2020-09-11T08:25:19Z,"I have noticed that the contact info has a similar description in the readme, we'd better remove it before starting the first informal release.


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/599/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/599,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY5MDk1MjY1OA==,incubator-pegasus,690952658,599,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-09-11T08:25:18Z,2020-09-11T08:25:18Z,Thank your for your reporting. I think this issue is fixed by https://github.com/apache/incubator-pegasus/pull/600/files. Reopen this issue if there are other license problems.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY5MDk1MjY1OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/606,https://api.github.com/repos/apache/incubator-pegasus/issues/606,incubator-pegasus,707074611,606,The readme page still references lots of resources of Xiaomi,Apache9,4958168,Duo Zhang,palomino219@gmail.com,CLOSED,2020-09-23T06:08:50Z,2020-10-10T10:49:19Z,"At least in the Contact section, we should change the url of issues to the new place, and also add our mailing list to it?

And for the submodules and client libs, have we already moved them to apache or they are still under the control of Xiaomi? For former we should change the url, for latter then we need to start the process of also donating these projects?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/606/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/606,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY5ODMxNTAzNg==,incubator-pegasus,698315036,606,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-09-24T12:34:30Z,2020-09-24T12:34:30Z,"Thank you for your reminding. I've committed a PR for this https://github.com/apache/incubator-pegasus/pull/609/files.

Submodules and client libs were donated to ASF as well, as we included them in the SGA. But they are not transferred yet. That's what we certainly need to work out.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY5ODMxNTAzNg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/606,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY5OTYxOTk2MQ==,incubator-pegasus,699619961,606,NA,vongosling,635581,vongosling,vongosling@apache.org,NA,2020-09-27T11:00:12Z,2020-09-27T11:00:12Z,"It is better to use the apache domain instead of the previous Pegasus Website, while it is now directed to the site under the apache domain.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDY5OTYxOTk2MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/617,https://api.github.com/repos/apache/incubator-pegasus/issues/617,incubator-pegasus,720070751,617,thrift unmarshall bug in 1.12.3 release,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2020-10-13T09:54:24Z,2021-07-28T07:49:30Z,"## Bug Report

### Pegasus version
Pegasus Server 1.12.3 (a948e89b180b6a5c82d298d0dcc65f7bb770a8be) 

### Coredump stack
```
(gdb) bt
#0  0x00007fbb92c8c1d7 in raise () from /lib64/libc.so.6
#1  0x00007fbb92c8d8c8 in abort () from /lib64/libc.so.6
#2  0x00007fbb92c85146 in __assert_fail_base () from /lib64/libc.so.6
#3  0x00007fbb92c851f2 in __assert_fail () from /lib64/libc.so.6
#4  0x00007fbb96c0cb9f in dsn::binary_reader::read (this=0x7fbb58330d20, buffer=buffer@entry=0x70b900000 ""\340\031\367\256\005"", sz=sz@entry=16777216)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/binary_reader.cpp:80
#5  0x00000000005e30e5 in read (len=16777216, buf=0x70b900000 ""\340\031\367\256\005"", this=0x7fbb58330cc0)
    at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/cpp/serialization_helper/thrift_helper.h:66
#6  apache::thrift::transport::readAll<dsn::binary_reader_transport> (trans=..., buf=0x70b900000 ""\340\031\367\256\005"", len=16777216)
    at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/thrift/transport/TTransport.h:41
#7  0x00000000005e9702 in readAll (len=16777216, buf=<optimized out>, this=<optimized out>)
    at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/thrift/transport/TTransport.h:121
#8  apache::thrift::protocol::TBinaryProtocolT<apache::thrift::transport::TTransport, apache::thrift::protocol::TNetworkBigEndian>::readStringBody<dsn::blob_string> (
    this=this@entry=0x7fbb58330ce0, str=..., size=16777216) at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/thrift/protocol/TBinaryProtocol.tcc:445
#9  0x00000000005eb949 in readString<dsn::blob_string> (str=..., this=0x7fbb58330ce0)
    at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/thrift/protocol/TBinaryProtocol.tcc:408
#10 read (iprot=0x7fbb58330ce0, this=0x7fbb58330fb0) at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/cpp/serialization_helper/thrift_helper.h:458
#11 unmarshall_internal (value=..., iproto=0x7fbb58330ce0) at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/cpp/serialization_helper/thrift_helper.h:583
#12 unmarshall (value=..., iproto=0x7fbb58330ce0) at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/cpp/serialization_helper/thrift_helper.h:594
#13 unmarshall_base<dsn::blob> (val=..., iproto=0x7fbb58330ce0) at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/cpp/serialization_helper/thrift_helper.h:608
#14 dsn::unmarshall_thrift_internal<dsn::blob> (val=..., proto=proto@entry=0x7fbb58330ce0)
    at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/cpp/serialization_helper/thrift_helper.h:665
---Type <return> to continue, or q <return> to quit---
#15 0x00000000005efc93 in unmarshall_thrift_binary<dsn::blob> (val=..., reader=...) at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/cpp/serialization_helper/thrift_helper.h:703
#16 unmarshall<dsn::blob> (fmt=<optimized out>, value=..., reader=...) at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/cpp/serialization.h:73
#17 dsn::unmarshall<dsn::blob> (msg=msg@entry=0x2d4906b14, val=...) at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/cpp/serialization.h:101
#18 0x00000000005effe9 in operator() (r=0x2d4906b14, p=0x414fb800, __closure=0x220a138) at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/dist/replication/storage_serverlet.h:27
#19 std::_Function_handler<void (dsn::apps::rrdb_service*, dsn::message_ex*), bool dsn::replication::storage_serverlet<dsn::apps::rrdb_service>::register_async_rpc_handler<dsn::blob, dsn::apps::read_response>(dsn::task_code, char const*, void (*)(dsn::apps::rrdb_service*, dsn::blob const&, dsn::rpc_replier<dsn::apps::read_response>&))::{lambda(dsn::apps::rrdb_service*, dsn::message_ex*)#1}>::_M_invoke(std::_Any_data const&, dsn::apps::rrdb_service*, dsn::message_ex*) (__functor=..., __args#0=0x414fb800, __args#1=0x2d4906b14)
    at /home/wutao1/app/include/c++/4.8.2/functional:2071
#20 0x0000000000619a8c in operator() (__args#1=0x2d4906b14, __args#0=0x414fb800, this=<optimized out>) at /home/wutao1/app/include/c++/4.8.2/functional:2464
#21 handle_request (request=0x2d4906b14, this=0x414fb800) at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/dist/replication/storage_serverlet.h:80
#22 dsn::apps::rrdb_service::on_request (this=0x414fb800, request=0x2d4906b14) at /home/wutao1/pegasus-release/src/include/rrdb/rrdb.server.h:17
#23 0x00007fbb96ac9ab5 in dsn::replication::replica::on_client_read (this=0x2d070c00, request=request@entry=0x2d4906b14)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica.cpp:177
#24 0x00007fbb96b3641f in dsn::replication::replica_stub::on_client_read (this=0x2af6000, id=..., request=0x2d4906b14)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_stub.cpp:811
#25 0x00007fbb96c4afd9 in dsn::task::exec_internal (this=this@entry=0x2d4906cac) at /home/wutao1/pegasus-release/rdsn/src/core/core/task.cpp:180
#26 0x00007fbb96c5f22d in dsn::task_worker::loop (this=0x2b17130) at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:211
#27 0x00007fbb96c5f3f9 in dsn::task_worker::run_internal (this=0x2b17130) at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:191
#28 0x00007fbb935e4600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#29 0x00007fbb940f6dc5 in start_thread () from /lib64/libpthread.so.0
#30 0x00007fbb92d4e73d in clone () from /lib64/libc.so.6
```
**Frame 4**
```
(gdb) f 4
#4  0x00007fbb96c0cb9f in dsn::binary_reader::read (this=0x7fbb58330d20, buffer=buffer@entry=0x70b900000 ""\340\031\367\256\005"", sz=sz@entry=16777216)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/binary_reader.cpp:80
(gdb) p sz
$1 = 16777216
(gdb) p _remaining_size
$2 = 42
```
**Frame 23**
```
(gdb) f 23
#23 0x00007fbb96ac9ab5 in dsn::replication::replica::on_client_read (this=0x2d070c00, request=request@entry=0x2d4906b14)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica.cpp:177
(gdb) p *(*request).header
$6 = {hdr_type = 1413892180, hdr_version = 0, hdr_length = 192, hdr_crc32 = 0, body_length = 76, body_crc32 = 0, id = 160, trace_id = 0, 
  rpc_name = ""RPC_RRDB_RRDB_GET"", '\000' <repeats 30 times>, rpc_code = {local_code = 207, local_hash = 0}, gpid = {_value = {u = {app_id = 7, partition_index = 27}, 
      value = 115964116999}}, context = {u = {is_request = 1, is_forwarded = 0, unused = 0, serialize_format = 1, is_forward_supported = 0, reserved = 0}, context = 65}, from_address = {
    static s_invalid_address = {static s_invalid_address = <same as static member of an already seen type>, _addr = {v4 = {type = 0, padding = 0, port = 0, ip = 0}, group = {type = 0, 
          group = 0}, value = 0}}, _addr = {v4 = {type = 1, padding = 0, port = 41404, ip = 175579414}, group = {type = 1, group = 188526960923574272}, value = 754107843694297089}}, 
  client = {timeout_ms = 0, thread_hash = 55460, partition_hash = 0}, server = {error_name = '\000' <repeats 47 times>, error_code = {local_code = 0, local_hash = 0}}}
```
**Simple analysis**
The stack shows server received a read request from user client whose body_length is 76, then server would unmarshall the request body into a blob structure, however, something wrong happened during unmarshall leading this coredump. I can only know that the blob size may be calculated wrong which is 16777216, the root cause is not found right now.



","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/617/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/617,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM407zfc,incubator-pegasus,888092636,617,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2021-07-28T07:47:43Z,2021-07-28T07:47:43Z,Fixed at https://github.com/apache/incubator-pegasus/pull/790,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM407zfc/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/622,https://api.github.com/repos/apache/incubator-pegasus/issues/622,incubator-pegasus,723903043,622,v2.1 build error ,totalo,29777558,,totalo@apache.org,CLOSED,2020-10-18T04:38:33Z,2021-12-04T14:36:18Z,"common.copy lib/libboost_regex.a
...updated 14820 targets...
[ 77%] Performing install step for 'boost'
[ 77%] Completed 'boost'
[ 77%] Built target boost
make: *** [Makefile:84：all] 错误 2","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/622/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/622,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxMTExNzgyNw==,incubator-pegasus,711117827,622,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2020-10-18T05:12:09Z,2020-10-18T05:12:09Z,"> common.copy lib/libboost_regex.a
> ...updated 14820 targets...
> [ 77%] Performing install step for 'boost'
> [ 77%] Completed 'boost'
> [ 77%] Built target boost
> make: *** [Makefile:84：all] 错误 2

what is your boost gcc cmake's version","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxMTExNzgyNw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/622,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxMTExODkyNw==,incubator-pegasus,711118927,622,NA,totalo,29777558,,totalo@apache.org,NA,2020-10-18T05:25:03Z,2020-10-18T05:25:03Z,"> > common.copy lib/libboost_regex.a
> > ...updated 14820 targets...
> > [ 77%] Performing install step for 'boost'
> > [ 77%] Completed 'boost'
> > [ 77%] Built target boost
> > make: *** [Makefile:84：all] 错误 2
> 
> what is your boost gcc cmake's version

gcc --version
gcc (Ubuntu 9.3.0-10ubuntu2) 9.3.0
Copyright (C) 2019 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxMTExODkyNw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/622,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxMTExOTcyNA==,incubator-pegasus,711119724,622,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2020-10-18T05:32:11Z,2020-10-18T05:32:11Z,"gcc version is too high, recommand use 5.4.0. If you are using ubuntu20, it is also compatible with g++-7","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxMTExOTcyNA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/622,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxMTEyMjEwOA==,incubator-pegasus,711122108,622,NA,totalo,29777558,,totalo@apache.org,NA,2020-10-18T06:05:36Z,2020-10-18T06:05:36Z,"ok, THKS","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxMTEyMjEwOA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/622,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc1MTMzOTY1MA==,incubator-pegasus,751339650,622,NA,765308949,43138355,,,NA,2020-12-26T09:48:30Z,2020-12-26T09:48:30Z,"common.copy lib/libboost_regex.a
...updated 14820 targets...
[ 55%] Performing install step for 'boost'
[ 55%] Completed 'boost'
[ 55%] Built target boost
make: *** [all] Error 2
ERROR: build rdsn failed

gcc版本已经修改了，还是不行，请帮忙看下，谢谢。
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/x86_64-unknown-linux-gnu/5.4.0/lto-wrapper
Target: x86_64-unknown-linux-gnu
Configured with: ../configure --enable-checking=release --enable-languages=c,c++ --disable-multilib
Thread model: posix
gcc version 5.4.0 (GCC) 
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc1MTMzOTY1MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/622,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc1MTM0MTc4Mw==,incubator-pegasus,751341783,622,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-12-26T10:18:58Z,2020-12-26T10:18:58Z,"请用下面命令，用1线程编译试试：

```sh
cd rdsn
./run.sh build -j 1
```

使用1线程可以方便把直接的错误原因打印出来，否则你提供的信息我看不出问题。

另外，编译可以借鉴 http://pegasus.apache.org/docs/build/compile-by-docker/ ，推荐用docker编译","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc1MTM0MTc4Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/622,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc1MTM0MjM4NA==,incubator-pegasus,751342384,622,NA,765308949,43138355,,,NA,2020-12-26T10:27:56Z,2020-12-26T10:27:56Z,"CMake Error at /usr/local/share/cmake-3.19/Modules/FindPackageHandleStandardArgs.cmake:218 (message):
  Could NOT find zstd (missing: zstd_LIBRARIES zstd_INCLUDE_DIRS)
Call Stack (most recent call first):
  /usr/local/share/cmake-3.19/Modules/FindPackageHandleStandardArgs.cmake:582 (_FPHSA_FAILURE_MESSAGE)
  cmake/modules/Findzstd.cmake:17 (find_package_handle_standard_args)
  CMakeLists.txt:131 (find_package)


-- Configuring incomplete, errors occurred!
See also ""/home/pagesus/apache-pegasus-2.1.0-incubating-src/rdsn/thirdparty/build/Build/rocksdb/CMakeFiles/CMakeOutput.log"".
make[2]: *** [Stamp/rocksdb/rocksdb-configure] Error 1
make[1]: *** [CMakeFiles/rocksdb.dir/all] Error 2
make: *** [all] Error 2

请问这种情况应该怎么处理呢？","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc1MTM0MjM4NA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/622,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc1MTM0NDgzMw==,incubator-pegasus,751344833,622,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-12-26T11:03:37Z,2020-12-26T11:03:37Z,"Could NOT find zstd (missing: zstd_LIBRARIES zstd_INCLUDE_DIRS)

正如这条错误显示的，你没有安装zstd，你应该是没有正确按照文档安装依赖。http://pegasus.apache.org/docs/build/compile-from-source/","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc1MTM0NDgzMw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/622,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc1MTM0Njc0Nw==,incubator-pegasus,751346747,622,NA,765308949,43138355,,,NA,2020-12-26T11:29:43Z,2020-12-26T11:29:43Z,好的 我这边重新安装一次  谢谢,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc1MTM0Njc0Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/623,https://api.github.com/repos/apache/incubator-pegasus/issues/623,incubator-pegasus,724744089,623,Compile Error in v2.1.0-RC4 in Ubuntu 20.04,Smityz,22953824,Smilencer,smityz@qq.com,CLOSED,2020-10-19T15:41:35Z,2020-10-22T06:03:30Z,"## Bug Report

### pegasus version
Pegasus Server v2.1.0-RC4

### build error info
```
[18%] Built target pegasus.rproxylib
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/DSN_ROOT/lib/libdsn_runtime.a(kinit_context.cpp.o): in function `dsn::security::kinit_context::krb5_call_to_errors(int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)':
/home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/kinit_context.cpp:301: undefined reference to `krb5_get_error_message'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/kinit_context.cpp:303: undefined reference to `krb5_free_error_message'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/DSN_ROOT/lib/libdsn_runtime.a(kinit_context.cpp.o): in function `dsn::security::kinit_context::~kinit_context()':
/home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/kinit_context.cpp:121: undefined reference to `krb5_get_init_creds_opt_free'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/DSN_ROOT/lib/libdsn_runtime.a(kinit_context.cpp.o): in function `std::call_once<dsn::security::kinit_context::init_krb5_ctx()::{lambda()#1}>(std::once_flag&, dsn::security::kinit_context::init_krb5_ctx()::{lambda()#1}&&)::{lambda()#2}::_FUN()':
/home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/kinit_context.cpp:167: undefined reference to `krb5_init_context'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/DSN_ROOT/lib/libdsn_runtime.a(kinit_context.cpp.o): in function `dsn::security::kinit_context::parse_username_from_principal()':
/home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/kinit_context.cpp:177: undefined reference to `krb5_aname_to_localname'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/DSN_ROOT/lib/libdsn_runtime.a(kinit_context.cpp.o): in function `dsn::security::kinit_context::get_credentials()':
/home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/kinit_context.cpp:219: undefined reference to `krb5_get_init_creds_keytab'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/kinit_context.cpp:236: undefined reference to `krb5_cc_store_cred'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/kinit_context.cpp:233: undefined reference to `krb5_free_cred_contents'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/kinit_context.cpp:233: undefined reference to `krb5_free_cred_contents'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/DSN_ROOT/lib/libdsn_runtime.a(kinit_context.cpp.o): in function `dsn::security::kinit_context::kinit()':
/home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/kinit_context.cpp:134: undefined reference to `krb5_parse_name'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/kinit_context.cpp:141: undefined reference to `krb5_kt_resolve'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/kinit_context.cpp:145: undefined reference to `krb5_cc_default'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/kinit_context.cpp:149: undefined reference to `krb5_cc_initialize'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/kinit_context.cpp:153: undefined reference to `krb5_get_init_creds_opt_alloc'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/DSN_ROOT/lib/libdsn_runtime.a(sasl_init.cpp.o): in function `dsn::security::init_sasl(bool)':
/home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/sasl_init.cpp:132: undefined reference to `sasl_client_init'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/sasl_init.cpp:141: undefined reference to `sasl_server_init'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/sasl_init.cpp:145: undefined reference to `sasl_errstring'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/sasl_init.cpp:137: undefined reference to `sasl_errstring'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/DSN_ROOT/lib/libdsn_runtime.a(sasl_init.cpp.o): in function `dsn::security::sasl_set_mutex_local()':
/home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/sasl_init.cpp:121: undefined reference to `sasl_set_mutex'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/DSN_ROOT/lib/libdsn_runtime.a(sasl_wrapper.cpp.o): in function `dsn::security::sasl_err_desc(int, sasl_conn*)':
/home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/sasl_wrapper.cpp:29: undefined reference to `sasl_errdetail'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/sasl_wrapper.cpp:31: undefined reference to `sasl_errstring'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/DSN_ROOT/lib/libdsn_runtime.a(sasl_wrapper.cpp.o): in function `dsn::security::sasl_wrapper::~sasl_wrapper()':
/home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/sasl_wrapper.cpp:38: undefined reference to `sasl_dispose'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/DSN_ROOT/lib/libdsn_runtime.a(sasl_client_wrapper.cpp.o): in function `dsn::security::sasl_client_wrapper::start(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, dsn::blob const&, dsn::blob&)':
/home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/sasl_client_wrapper.cpp:52: undefined reference to `sasl_client_start'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/DSN_ROOT/lib/libdsn_runtime.a(sasl_client_wrapper.cpp.o): in function `dsn::security::sasl_client_wrapper::init()':
/home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/sasl_client_wrapper.cpp:37: undefined reference to `sasl_client_new'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/DSN_ROOT/lib/libdsn_runtime.a(sasl_client_wrapper.cpp.o): in function `dsn::security::sasl_client_wrapper::step(dsn::blob const&, dsn::blob&)':
/home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/sasl_client_wrapper.cpp:67: undefined reference to `sasl_client_step'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/DSN_ROOT/lib/libdsn_runtime.a(sasl_server_wrapper.cpp.o): in function `dsn::security::sasl_server_wrapper::start(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, dsn::blob const&, dsn::blob&)':
/home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/sasl_server_wrapper.cpp:51: undefined reference to `sasl_server_start'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/DSN_ROOT/lib/libdsn_runtime.a(sasl_server_wrapper.cpp.o): in function `dsn::security::sasl_server_wrapper::init()':
/home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/sasl_server_wrapper.cpp:37: undefined reference to `sasl_server_new'
/usr/bin/ld: /home/smilencer/Code/incubator-pegasus/DSN_ROOT/lib/libdsn_runtime.a(sasl_server_wrapper.cpp.o): in function `dsn::security::sasl_server_wrapper::step(dsn::blob const&, dsn::blob&)':
/home/smilencer/Code/incubator-pegasus/rdsn/src/runtime/security/sasl_server_wrapper.cpp:66: undefined reference to `sasl_server_step'
collect2: error: ld returned 1 exit status
make[2]: *** [base/test/CMakeFiles/base_test.dir/build.make:112: base/test/base_test] Error 1
make[1]: *** [CMakeFiles/Makefile2:544: base/test/CMakeFiles/base_test.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
[ 18%] Built target combine_lib
[ 18%] Built target pegasus_client_shared
make: *** [Makefile:130: all] Error 2
ERROR: build pegasus failed
ERROR: build pegasus failed
```
### Simple analysis
I think the document of v2.1.0-RC4 needs to be updated to add the necessary dependency.



","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/623/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/623,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxMjI2NDc2Nw==,incubator-pegasus,712264767,623,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2020-10-19T15:58:10Z,2020-10-19T15:58:10Z,"Now code and docs are in different github repos separately, we can update docs later.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxMjI2NDc2Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/623,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxMzY2OTE5Nw==,incubator-pegasus,713669197,623,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2020-10-21T15:42:05Z,2020-10-21T15:42:05Z,One way to solve that is to delete `rdsn/thirdparty/output` and `rdsn/thirdparty/build` and recompile it,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxMzY2OTE5Nw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/623,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxMzY3MjA5Mg==,incubator-pegasus,713672092,623,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2020-10-21T15:46:24Z,2020-10-21T15:46:24Z,"> One way to solve that is to delete `rdsn/thirdparty/output` and `rdsn/thirdparty/build` and recompile it

Of course, I think there is no problem of the source code, but the docs are out of date.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxMzY3MjA5Mg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/623,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxMzY3MjY1Mg==,incubator-pegasus,713672652,623,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2020-10-21T15:47:14Z,2020-10-21T15:47:14Z,@Smityz Could you plz to update the docs?,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxMzY3MjY1Mg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/623,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxNDIyNzYyNQ==,incubator-pegasus,714227625,623,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2020-10-22T05:01:26Z,2020-10-22T05:01:26Z,"There is no need to update docs, because the corresponding dependence will be installed for the first time you compile pegasus
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxNDIyNzYyNQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/626,https://api.github.com/repos/apache/incubator-pegasus/issues/626,incubator-pegasus,725275618,626,build: building pegasus via docker,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2020-10-20T07:09:46Z,2020-10-27T03:15:51Z,"Hi, this is a plan to build pegasus releases via docker.

See the Doris recommended way to compilation:
http://doris.apache.org/master/en/installing/compilation.html#developing-mirror-compilation-using-docker-recommended.

Doris provides a virtual environment in docker for building. This environment is bundled in a [docker image](https://github.com/apache/incubator-doris/blob/master/docker/Dockerfile). Before setting up the environment, the docker container should mount on the local directory containing Doris sources. Running the container will compile the sources and generate the binaries into the source directory.

This is an interesting idea to make Pegasus building far easier than what we used to do. It eliminates the troubles of the C++ environment stuffs for users who simply want to build an official release.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/626/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/626,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxNjk1MTQxNw==,incubator-pegasus,716951417,626,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-10-27T03:15:50Z,2020-10-27T03:15:50Z,"Hi, everyone.

This project maintains the building environment for Pegasus: https://github.com/pegasus-kv/pegasus-docker.
I've completed the dockerfiles and published the images to https://hub.docker.com/r/apachepegasus/build-env.
So this issue is closed. Open another issue for building failures.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxNjk1MTQxNw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/628,https://api.github.com/repos/apache/incubator-pegasus/issues/628,incubator-pegasus,727530378,628,The hash method in hotkey coarse detection can be fast,Smityz,22953824,Smilencer,smityz@qq.com,OPEN,2020-10-22T16:12:43Z,2020-11-15T12:19:34Z,"`boost::hash_range` is used as the common method to calculate the hash value in hotkey_detection https://github.com/apache/incubator-pegasus/pull/624
After testing we find some faster algorithms to replace it.

One way is to use [xxhash](https://github.com/Cyan4973/xxHash), the other way is to use `boost::hash`.
We did a benchmark to evaluate the performance of the algorithm: set a random fixed-length string set, and loop to hash them. Here is a rough test result.

```shell
smilencer@smilencer-OptiPlex-7040 ~/Code/test_code/test_blob_xxhash $ ./hash_test
xxhash used time: 50993 us
boost_hash used time: 333000 us
smilencer@smilencer-OptiPlex-7040 ~/Code/test_code/test_blob_xxhash $ ./hash_test
xxhash used time: 78303 us
boost_hash used time: 331219 us
smilencer@smilencer-OptiPlex-7040 ~/Code/test_code/test_blob_xxhash $ ./hash_test
xxhash used time: 77513 us
boost_hash used time: 342499 us
smilencer@smilencer-OptiPlex-7040 ~/Code/test_code/test_blob_xxhash $ ./hash_test
xxhash used time: 79580 us
boost_hash used time: 333332 us
smilencer@smilencer-OptiPlex-7040 ~/Code/test_code/test_blob_xxhash $ ./hash_test
xxhash used time: 76561 us
boost_hash used time: 335781 us
smilencer@smilencer-OptiPlex-7040 ~/Code/test_code/test_blob_xxhash $ ./hash_test
xxhash used time: 81304 us
boost_hash used time: 335682 us
smilencer@smilencer-OptiPlex-7040 ~/Code/test_code/test_blob_xxhash $ ./hash_test
xxhash used time: 75173 us
boost_hash used time: 331603 us
smilencer@smilencer-OptiPlex-7040 ~/Code/test_code/test_blob_xxhash $ ./hash_test
xxhash used time: 81142 us
boost_hash used time: 335251 us
smilencer@smilencer-OptiPlex-7040 ~/Code/test_code/test_blob_xxhash $ ./hash_test
xxhash used time: 76805 us
boost_hash used time: 332577 us
smilencer@smilencer-OptiPlex-7040 ~/Code/test_code/test_blob_xxhash $ ./hash_test
xxhash used time: 77515 us
boost_hash used time: 333144 us

```
I plan to replace it in future PR. `xxhash` is really fast.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/628/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/630,https://api.github.com/repos/apache/incubator-pegasus/issues/630,incubator-pegasus,729410440,630,Add a util to manage info synchronize in mutithreads,Smityz,22953824,Smilencer,smityz@qq.com,CLOSED,2020-10-26T09:43:51Z,2020-10-27T08:34:01Z,"Reference to my [StackOverflowow question](https://stackoverflow.com/questions/64549506/what-to-do-if-the-object-is-destroyed-during-the-call-in-c)
I think we can add util to solve some synchronize problem in a lock-free way.
demo:
```cpp
#include <bits/stdc++.h>
#include <iostream>
#include <utility>
#include <thread>
#include <chrono>
#include <functional>
#include <atomic>
#include <unistd.h>
// #include <bits/shared_ptr_atomic.h>

using namespace std;

class bar
{
public:
    explicit bar(int x) : num(x), a(3), b(2)
    {
    }
    int get_num()
    {
        a += b;
        usleep(10);
        b += a;
        usleep(10);
        b += a + b;
        if (abs(a) > 100)
            a = 0;
        if (abs(b) > 100)
            b = 0;
        return num;
    }
    ~bar()
    {
        printf(""bar:%d is destroyed\n"", num);
    }

private:
    int num, a, b;
};

vector<int> v;

shared_ptr<bar> ptr_store;

int st;

void get_func()
{
    while (1)
        for (int i = 0; i <= 1000; i++)
        {
            printf(""get_num:%d\n"", ptr_store->get_num());
        }
}

void set_func()
{
    while (1)
        for (int i = 0; i < 10000; i++)
        {
            usleep(10);
            //atomic_exchange(&ptr_store, make_shared<bar>(i));
            ptr_store = make_shared<bar>(i);
        }
}

int main()
{
    ptr_store = make_shared<bar>(-1);
    std::thread t1(get_func);
    std::thread t2(set_func);
    t1.join();
    t2.join();
}

```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/630/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/630,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxNzA3NzU4Mg==,incubator-pegasus,717077582,630,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2020-10-27T08:33:59Z,2020-10-27T08:33:59Z,"It will make some mistakes, forget it","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcxNzA3NzU4Mg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/635,https://api.github.com/repos/apache/incubator-pegasus/issues/635,incubator-pegasus,734000876,635,I don't know the environment that Pegasus compiles depends on,Smityz,22953824,Smilencer,smityz@qq.com,CLOSED,2020-11-01T16:03:30Z,2020-11-05T07:56:28Z,"I can't  access to https://pegasus.apache.org/docs/installation/ to get the dependency version requirements
BTW, according to [document history version](https://github.com/pegasus-kv/pegasus-kv.github.io/commit/2f89a06b3eec82140b7b9ffbc4b7b2f332c31f30#diff-bdaaeb844c24423ef50a25e96b33974c405c21b5ec9fb0bca4d00bab4fe2b063), the minimum requirements of cmake is 3.5.2, but it is too old to extract tarball filename from url, I suggest to update it.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/635/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/635,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcyMDE4MjMyMA==,incubator-pegasus,720182320,635,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-11-02T00:47:55Z,2020-11-02T00:47:55Z,Links were changed. http://pegasus.apache.org/docs/build/compile-from-source/. We have no guarantee for keeping the url unchanged. You should find the doc from the sidebar in http://pegasus.apache.org/docs. Or just search using the searchbar on top of each page.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcyMDE4MjMyMA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/635,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcyMDE5MzAxNw==,incubator-pegasus,720193017,635,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2020-11-02T01:43:10Z,2020-11-02T01:43:10Z,I still can't find the requirements of GCC/CMake/boost,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcyMDE5MzAxNw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/635,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcyMDIyMzgyNg==,incubator-pegasus,720223826,635,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-11-02T04:02:06Z,2020-11-02T04:02:06Z,"Ok. I've updated the requirements of gcc & cmake on the doc, please check it out.
There's no requirement to install boost via `apt-get/yum` since we manage it through cmake ExternalProject. https://github.com/XiaoMi/rdsn/blob/master/thirdparty/CMakeLists.txt#L29. You don't need to install it yourself.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcyMDIyMzgyNg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/636,https://api.github.com/repos/apache/incubator-pegasus/issues/636,incubator-pegasus,736596619,636,the document maybe have error,shenxingwuying,6360122,,,CLOSED,2020-11-05T04:15:57Z,2020-11-07T02:21:35Z,"## General Question

1、 The pegasus doc maybe error.
http://pegasus.apache.org/overview/benchmark/
The latency is ms? That's not reasonable. I guess the author of doc make a mistake.

![image](https://user-images.githubusercontent.com/6360122/98196833-23a0e300-1f60-11eb-841a-7b8db6dca0b0.png)

2、Some url links are not found, eg:
![image](https://user-images.githubusercontent.com/6360122/98196924-6b276f00-1f60-11eb-83ce-31ef04ddf8c8.png)

At http://pegasus.apache.org/overview/， 
![image](https://user-images.githubusercontent.com/6360122/98197001-99a54a00-1f60-11eb-9964-6e73490f1d55.png)   Page Not Found","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/636/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/636,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcyMjQyMjA3OA==,incubator-pegasus,722422078,636,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2020-11-05T14:43:47Z,2020-11-05T14:43:47Z,"Thank you for finding the mistakes! I've now fixed them already, except for the incorrect benchmark results.
I'm not the author of the benchmark. @Shuo-Jia will handle this.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcyMjQyMjA3OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/636,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcyMjkxNTYyMA==,incubator-pegasus,722915620,636,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2020-11-06T07:02:23Z,2020-11-06T07:02:23Z,"> Thank you for finding the mistakes! I've now fixed them already, except for the incorrect benchmark results.
> I'm not the author of the benchmark. @Shuo-Jia will handle this.

Yes, It's my mistake, I have updated it, see https://pegasus.apache.org/overview/benchmark/. thanks again! @shenxingwuying ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcyMjkxNTYyMA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/636,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcyMzM3NTk5Ng==,incubator-pegasus,723375996,636,NA,shenxingwuying,6360122,,,NA,2020-11-07T02:21:34Z,2020-11-07T02:21:34Z,Good,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcyMzM3NTk5Ng==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/638,https://api.github.com/repos/apache/incubator-pegasus/issues/638,incubator-pegasus,738139408,638,A document url link not found,shenxingwuying,6360122,,,CLOSED,2020-11-07T02:24:49Z,2021-10-22T10:29:58Z,"document page not found.

http://pegasus.apache.org/community/#contact-us

![image](https://user-images.githubusercontent.com/6360122/98429700-3ee82b80-20e3-11eb-9de5-1673e3e7fe65.png)

![image](https://user-images.githubusercontent.com/6360122/98429723-63440800-20e3-11eb-9951-62df1a779338.png)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/638/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/638,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44mClX,incubator-pegasus,949496151,638,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2021-10-22T10:29:58Z,2021-10-22T10:29:58Z,"Those links have already been added, so this issue should be closed.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44mClX/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/642,https://api.github.com/repos/apache/incubator-pegasus/issues/642,incubator-pegasus,740417322,642,compilation failures of pegasus using gcc9,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2020-11-11T03:02:16Z,2023-05-16T16:33:02Z,"Compilation failure of rdsn in origin/master.

## compiler

gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0

## error outputs

ERROR#1

```
/home/wutao1/git/pegasus/rdsn/include/dsn/utility/time_utils.h: In function ‘void dsn::tools::print_header(FILE*, dsn_log_level_t)’:
/home/wutao1/git/pegasus/rdsn/include/dsn/utility/time_utils.h:47:24: error: ‘%02d’ directive writing between 2 and 11 bytes into a region of size between 0 and 16 [-Werror=format-overflow=]
   47 |             ""%04d-%02d-%02d %02d:%02d:%02d.%03u"",
      |                        ^~~~
/home/wutao1/git/pegasus/rdsn/include/dsn/utility/time_utils.h:47:13: note: directive argument in the range [0, 999]
   47 |             ""%04d-%02d-%02d %02d:%02d:%02d.%03u"",
      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from /usr/include/stdio.h:867,
                 from /usr/include/c++/9/cstdio:42,
                 from /usr/include/c++/9/ext/string_conversions.h:43,
                 from /usr/include/c++/9/bits/basic_string.h:6493,
                 from /usr/include/c++/9/string:55,
                 from /home/wutao1/git/pegasus/rdsn/include/dsn/utility/ports.h:63,
                 from /home/wutao1/git/pegasus/rdsn/include/dsn/utility/synchronize.h:28,
                 from /home/wutao1/git/pegasus/rdsn/include/dsn/utility/singleton_store.h:39,
                 from /home/wutao1/git/pegasus/rdsn/include/dsn/utility/factory_store.h:38,
                 from /home/wutao1/git/pegasus/rdsn/include/dsn/tool_api.h:54,
                 from /home/wutao1/git/pegasus/rdsn/src/utils/simple_logger.h:29,
                 from /home/wutao1/git/pegasus/rdsn/src/utils/simple_logger.cpp:27:
/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:34: note: ‘__builtin___sprintf_chk’ output between 24 and 76 bytes into a destination of size 24
   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,
      |          ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   37 |       __bos (__s), __fmt, __va_arg_pack ());
      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```

ERROR#2

```
/home/wutao1/git/pegasus/rdsn/src/runtime/rpc/rpc_message.cpp: In static member function ‘static dsn::message_ex* dsn::message_ex::copy_message_no_reply(const dsn::message_ex&)’:
/home/wutao1/git/pegasus/rdsn/src/runtime/rpc/rpc_message.cpp:184:50: error: ‘void* memset(void*, int, size_t)’ clearing an object of type ‘dsn::message_header’ {aka ‘struct dsn::message_header’} with no trivial copy-assignment; use assignment or value-initialization instead [-Werror=class-memaccess]
  184 |     memset(msg->header, 0, sizeof(message_header));
      |                                                  ^
In file included from /home/wutao1/git/pegasus/rdsn/src/runtime/rpc/rpc_message.cpp:39:
/home/wutao1/git/pegasus/rdsn/include/dsn/tool-api/rpc_message.h:79:16: note: ‘dsn::message_header’ {aka ‘struct dsn::message_header’} declared here
   79 | typedef struct message_header
      |                ^~~~~~~~~~~~~~
```

ERROR#3

```
/home/wutao1/git/pegasus/rdsn/include/dsn/cpp/json_helper.h:599:9:   required from ‘static bool dsn::json::json_forwarder< <template-parameter-1-1> >::decode(const dsn::blob&, T&) [with T = dsn::dist::block_service::file_metadata]’
/home/wutao1/git/pegasus/rdsn/src/block_service/local/local_service.cpp:247:54:   required from here
/home/wutao1/git/pegasus/rdsn/thirdparty/output/include/rapidjson/document.h:1952:24: error: ‘void* memcpy(void*, const void*, size_t)’ writing to an object of type ‘rapidjson::GenericValue<rapidjson::UTF8<> >::Member’ {aka ‘struct rapidjson::GenericMember<rapidjson::UTF8<>, rapidjson::MemoryPoolAllocator<> >’} with no trivial copy-assignment; use copy-assignment instead [-Werror=class-memaccess]
 1952 |             std::memcpy(m, members, count * sizeof(Member));
      |             ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/wutao1/git/pegasus/rdsn/include/dsn/cpp/json_helper.h:40,
                 from /home/wutao1/git/pegasus/rdsn/src/block_service/local/local_service.cpp:10:
/home/wutao1/git/pegasus/rdsn/thirdparty/output/include/rapidjson/document.h:71:8: note: ‘rapidjson::GenericValue<rapidjson::UTF8<> >::Member’ {aka ‘struct rapidjson::GenericMember<rapidjson::UTF8<>, rapidjson::MemoryPoolAllocator<> >’} declared here
   71 | struct GenericMember {
      |        ^~~~~~~~~~~~~
```

ERROR#4

```
In file included from /home/wutao1/git/pegasus/src/base/test/utils_test.cpp:20:
/home/wutao1/git/pegasus/src/base/test/../pegasus_utils.h: In instantiation of ‘std::__cxx11::list<elem_type> pegasus::utils::top_n<elem_type, compare>::to() [with elem_type = int; compare = std::less<int>]’:
/home/wutao1/git/pegasus/src/base/test/utils_test.cpp:30:56:   required from here
/home/wutao1/git/pegasus/src/base/test/../pegasus_utils.h:65:32: error: moving a local object in a return statement prevents copy elision [-Werror=pessimizing-move]
   65 |         return std::move(result);
      |                                ^
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/642/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/642,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5cYyjM,incubator-pegasus,1550002380,642,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-05-16T16:32:47Z,2023-05-16T16:32:47Z,"The master branch will be built everyday on various of plats including Ubuntu 20.04 with gcc 9.4.0. I'll close this issue, you can reopen it if you find the master branch build it failed, thanks.

ref: https://github.com/apache/incubator-pegasus/actions/runs/4983622046/jobs/8920897200#step:5:50","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5cYyjM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/643,https://api.github.com/repos/apache/incubator-pegasus/issues/643,incubator-pegasus,740691277,643,pegasus shell crashed when it starts,Smityz,22953824,Smilencer,smityz@qq.com,OPEN,2020-11-11T11:25:06Z,2020-11-11T14:23:46Z,"## Bug Report

1. What did you do?
```shell
$ ./run.sh shell -n ***
INFO: parse meta_list from /***
W2020-11-11 19:20:05.527 (1605093605527024651 1b656) unknown.io-thrd.112214: overwrite default thread pool for task RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX from THREAD_POOL_META_SERVER to THREAD_POOL_DEFAULT
W2020-11-11 19:20:05.527 (1605093605527140243 1b656) unknown.io-thrd.112214: overwrite default thread pool for task RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX_ACK from THREAD_POOL_META_SERVER to THREAD_POOL_DEFAULT
./run.sh: line 1577: 112214 Segmentation fault      ./pegasus_shell ${CONFIG} $CLUSTER_NAME
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/643/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/643,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcyNTQ1MTQyNg==,incubator-pegasus,725451426,643,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2020-11-11T14:23:45Z,2020-11-11T14:23:45Z,Have you got the coredump file?,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDcyNTQ1MTQyNg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/645,https://api.github.com/repos/apache/incubator-pegasus/issues/645,incubator-pegasus,748035470,645,./run.sh test should terminate when test files download failed,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2020-11-21T14:59:00Z,2020-11-21T14:59:27Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
`./run.sh test`

2. What did you expect to see?
All unit tests passed.

3. What did you see instead?
Some unit tests failed, because some dependent test files download failed.
There are some output:
```
...
Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.138.19|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 962466 (940K) [application/octet-stream]
Saving to: ‘pegasus-bulk-load-function-test-files.zip’

 2% [===>                                                                                                                                                                                 ] 25,949       284B/s   in 91s

2020-11-21 22:22:07 (284 B/s) - Read error at byte 25949/962466 (Connection reset by peer). Archive:  pegasus-bulk-load-function-test-files.zip
  End-of-central-directory signature not found.  Either this file is not
  a zipfile, or it constitutes one disk of a multi-part archive.  In the
  latter case the central directory and zipfile comment will be found on
  the last disk(s) of this archive.
unzip:  cannot find zipfile directory in one of pegasus-bulk-load-function-test-files.zip or
        pegasus-bulk-load-function-test-files.zip.zip, and cannot find pegasus-bulk-load-function-test-files.zip.ZIP, period.
Prepare files used for bulk load function test succeed
Test start time: Sat Nov 21 22:22:07 CST 2020
...
```

4. What version of Pegasus are you using?
tag 2.1.0-RC5
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/645/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/650,https://api.github.com/repos/apache/incubator-pegasus/issues/650,incubator-pegasus,755015202,650,full_scan in shell client can't terminate by time out,Smityz,22953824,Smilencer,smityz@qq.com,OPEN,2020-12-02T07:15:49Z,2020-12-02T07:15:49Z,"Commands:
```
>>> use stat
OK
>>> full_scan -h prefix -x 2019-10
partition: all
hash_key_filter_type: prefix
hash_key_filter_pattern: ""2019-10""
sort_key_filter_type: no_filter
value_filter_type: no_filter
max_count: -1
timout_ms: 5000
detailed: false
no_value: false
```
When the table is large, it will return values after several minutes, but no hint of timeout","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/650/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/653,https://api.github.com/repos/apache/incubator-pegasus/issues/653,incubator-pegasus,760799216,653,The pegasus log in README.md 404 now,infraio,3055199,Guanghao Zhang,,CLOSED,2020-12-10T01:20:11Z,2021-06-01T06:33:05Z,![pegasus-logo](docs/media-img/pegasus-logo.png),"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/653/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/659,https://api.github.com/repos/apache/incubator-pegasus/issues/659,incubator-pegasus,768432542,659,disk engine buffer size coredump in 2.0.0-write-optim,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2020-12-16T04:11:49Z,2021-08-31T02:29:42Z,"## Bug Report
### Pegasus version
Pegasus 2.0.0-write-optim(984936de68188dc9cccccfa6a17f3ad798fe0364)

### Coredump Stack
```
#0  0x00000036a70328a5 in raise () from /lib64/libc.so.6
#1  0x00000036a7034085 in abort () from /lib64/libc.so.6
#2  0x00007f394626010e in dsn_coredump() () at /home/wutao1/pegasus-release/rdsn/src/core/core/service_api_c.cpp:77
#3  0x00007f39462aad8d in dsn::disk_file::on_write_completed(dsn::aio_task*, void*, dsn::error_code, unsigned long) ()
    at /home/wutao1/pegasus-release/rdsn/src/core/core/disk_engine.cpp:111
#4  0x00007f39462ab5c2 in dsn::disk_engine::complete_io(dsn::aio_task*, dsn::error_code, unsigned int, int) ()
    at /home/wutao1/pegasus-release/rdsn/src/core/core/disk_engine.cpp:338
#5  0x00007f39462fa7f9 in dsn::tools::native_linux_aio_provider::complete_aio(iocb*, int, int) ()
    at /home/wutao1/pegasus-release/rdsn/src/core/tools/common/native_aio_provider.linux.cpp:146
#6  0x00007f39462fa951 in dsn::tools::native_linux_aio_provider::get_event() ()
    at /home/wutao1/pegasus-release/rdsn/src/core/tools/common/native_aio_provider.linux.cpp:124
#7  0x00007f3943376600 in execute_native_thread_routine ()
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#8  0x00000036a7807851 in start_thread () from /lib64/libpthread.so.0
#9  0x00000036a70e811d in clone () from /lib64/libc.so.6
```
### Related log
```
F2020-12-16 11:46:04.969 (1608090364969361715 10ec4) replica.io-thrd.69316: disk_engine.cpp:114:on_write_completed(): assertion expression: size >= this_size
F2020-12-16 11:46:04.969 (1608090364969382372 10ec4) replica.io-thrd.69316: disk_engine.cpp:114:on_write_completed(): written buffer size does not equal to input buffer's size: 3627 vs 3766
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/659/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/659,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgwMDkxOTg1Mw==,incubator-pegasus,800919853,659,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-03-17T09:07:55Z,2021-03-17T09:07:55Z,"This bug is also happen on [2.1.1](https://github.com/apache/incubator-pegasus/tree/v2.1.1)  when load-balance with learning. libaio has been replced by `pwrite` and `pread`, so the bug shouldn't be caused by libaio.  
```
#0  0x00000031b08328a5 in raise () from /lib64/libc.so.6
(gdb) bt
#0  0x00000031b08328a5 in raise () from /lib64/libc.so.6
#1  0x00000031b0834085 in abort () from /lib64/libc.so.6
#2  0x00007f66fe68014e in dsn_coredump () at /home/jiashuo1/work/pegasus/rdsn/src/runtime/service_api_c.cpp:94
#3  0x00007f66fe788458 in dsn::disk_file::on_write_completed (this=<optimized out>, wk=wk@entry=0x705e140, ctx=ctx@entry=0x7f668bbb6b6c, err=..., size=size@entry=2147479552)
    at /home/jiashuo1/work/pegasus/rdsn/src/aio/disk_engine.cpp:127
#4  0x00007f66fe788980 in dsn::disk_engine::complete_io (this=0x7f66feb321f0 <dsn::utils::singleton<dsn::disk_engine>::instance()::_instance>, aio=aio@entry=0x705e140, err=..., bytes=2147479552)
    at /home/jiashuo1/work/pegasus/rdsn/src/aio/disk_engine.cpp:276
#5  0x00007f66fe78fb7e in dsn::aio_provider::complete_io (this=this@entry=0x19081c0, aio=aio@entry=0x705e140, err=..., err@entry=..., bytes=<optimized out>) at /home/jiashuo1/work/pegasus/rdsn/src/aio/aio_provider.cpp:36
#6  0x00007f66fe78d660 in dsn::native_linux_aio_provider::aio_internal (this=0x19081c0, aio_tsk=0x705e140, async=async@entry=true, pbytes=pbytes@entry=0x0)
    at /home/jiashuo1/work/pegasus/rdsn/src/aio/native_linux_aio_provider.cpp:128
#7  0x00007f66fe78d75a in operator() (__closure=<optimized out>) at /home/jiashuo1/work/pegasus/rdsn/src/aio/native_linux_aio_provider.cpp:101
#8  std::_Function_handler<void(), dsn::native_linux_aio_provider::submit_aio_task(dsn::aio_task*)::<lambda()> >::_M_invoke(const std::_Any_data &) (__functor=...)
    at /home/jiashuo1/app/toolchain/gcc540/output/include/c++/5.4.0/functional:1871
#9  0x00007f66fe6d7c61 in dsn::task::exec_internal (this=this@entry=0x2f68cfb30) at /home/jiashuo1/work/pegasus/rdsn/src/runtime/task/task.cpp:176
#10 0x00007f66fe6f1bda in dsn::task_worker::loop (this=0x20ec540) at /home/jiashuo1/work/pegasus/rdsn/src/runtime/task/task_worker.cpp:211
#11 0x00007f66fe6f1ddd in dsn::task_worker::run_internal (this=0x20ec540) at /home/jiashuo1/work/pegasus/rdsn/src/runtime/task/task_worker.cpp:191
#12 0x00007f66facc2fd0 in std::execute_native_thread_routine (__p=<optimized out>) at /home/jiashuo1/app/toolchain/gcc540/objdir/../gcc-5.4.0/libstdc++-v3/src/c++11/thread.cc:84
#13 0x00000031b0c07851 in start_thread () from /lib64/libpthread.so.0
#14 0x00000031b08e811d in clone () from /lib64/libc.so.6

```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgwMDkxOTg1Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/659,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgwNDU0MTgwMQ==,incubator-pegasus,804541801,659,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-03-23T02:47:29Z,2021-03-23T02:47:29Z,"This bug is also recurrent on [2.0.0](https://github.com/apache/incubator-pegasus/tree/v2.0), so we need check 1.12.3~2.0.0 commit change.
Core Stack：
```
#0  0x00007f56c1ae61d7 in raise () from /lib64/libc.so.6
(gdb) bt
#0  0x00007f56c1ae61d7 in raise () from /lib64/libc.so.6
#1  0x00007f56c1ae78c8 in abort () from /lib64/libc.so.6
#2  0x00007f56c5b48b1e in dsn_coredump () at /home/wutao1/pegasus-release/rdsn/src/core/core/service_api_c.cpp:77
#3  0x00007f56c5b9379d in dsn::disk_file::on_write_completed (this=<optimized out>, wk=wk@entry=0xa429360d, ctx=ctx@entry=0x7f56bef5effc, err=..., size=size@entry=2147479552)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/disk_engine.cpp:111
#4  0x00007f56c5b93fd2 in dsn::disk_engine::complete_io (this=0x2139280, aio=0xa429360d, err=..., bytes=bytes@entry=2147479552, delay_milliseconds=delay_milliseconds@entry=0)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/disk_engine.cpp:338
#5  0x00007f56c5beac6e in dsn::aio_provider::complete_io (this=<optimized out>, aio=<optimized out>, err=..., bytes=bytes@entry=2147479552, delay_milliseconds=delay_milliseconds@entry=0)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/aio_provider.cpp:50
#6  0x00007f56c5be3209 in dsn::tools::native_linux_aio_provider::complete_aio (this=this@entry=0x21b5dd0, io=0xedb09c20, bytes=2147479552, err=<optimized out>)
    at /home/wutao1/pegasus-release/rdsn/src/core/tools/common/native_aio_provider.linux.cpp:146
#7  0x00007f56c5be3361 in dsn::tools::native_linux_aio_provider::get_event (this=0x21b5dd0) at /home/wutao1/pegasus-release/rdsn/src/core/tools/common/native_aio_provider.linux.cpp:124
#8  0x00007f56c243e600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>) at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#9  0x00007f56c2f50dc5 in start_thread () from /lib64/libpthread.so.0
#10 0x00007f56c1ba873d in clone () from /lib64/libc.so.6
```

Erro Log：
```
F2021-03-23 10:35:04.156 (1616466904156438374 171b) replica.io-thrd.05915: disk_engine.cpp:114:on_write_completed(): assertion expression: size >= this_size
F2021-03-23 10:35:04.156 (1616466904156486739 171b) replica.io-thrd.05915: disk_engine.cpp:114:on_write_completed(): written buffer size does not equal to input buffer's size: 2147479552 vs -1576887796
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgwNDU0MTgwMQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/659,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgxNzQ2ODEyNQ==,incubator-pegasus,817468125,659,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-04-12T04:24:01Z,2021-04-12T04:24:01Z,"I search all previous core log on cluster, I found it has occured as early as [v1.12.1](https://github.com/apache/incubator-pegasus/tree/v1.12.1) and [1.12.3](https://github.com/apache/incubator-pegasus/tree/v1.12.3) 
```
Reading symbols from /home/work/packages/pegasus/c3srv-browser/a948e89b180b6a5c82d298d0dcc65f7bb770a8be-20200421-120054/pegasus-server-1.12.3-a948e89-glibc2.12-release/bin/pegasus_server...done.
Missing separate debuginfo for /home/work/app/pegasus/c3srv-browser/replica/package/bin/libaio.so.1
Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/7e/f475c5abcc899058c9ab6600a1b65fb1343153.debug
Missing separate debuginfo for /home/work/app/pegasus/c3srv-browser/replica/package/bin/libcrypto.so.10
Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/3a/8d65b9a373c0afaf106f3a979835b16dbeff1a.debug
Missing separate debuginfo for /home/work/app/pegasus/c3srv-browser/replica/package/bin/libsnappy.so.1
Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/e7/f2d4d5e290fe830bfca479729099b0e6adbfd3.debug
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib64/libthread_db.so.1"".
Missing separate debuginfo for /home/work/app/pegasus/c3srv-browser/replica/package/bin/liblz4.so.1
Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/a5/7db374840a9697b5aa9e25a8967b1ac75bfb70.debug
Missing separate debuginfo for /home/work/app/pegasus/c3srv-browser/replica/package/bin/libzstd.so.1
Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/6e/19fb6f4ba75c095047c9b9f564e69d050332c9.debug
Missing separate debuginfo for /home/work/app/pegasus/c3srv-browser/replica/package/bin/libssl.so.10
Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/31/8eab33420b000d542f09b91b716bacab1ad546.debug
Core was generated by `/home/work/app/pegasus/c3srv-browser/replica/package/bin/pegasus_server config.'.
Program terminated with signal 6, Aborted.
#0  0x00007f30859951d7 in raise () from /lib64/libc.so.6
Missing separate debuginfos, use: debuginfo-install glibc-2.17-157.el7_3.1.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.14.1-27.el7_3.x86_64 libcom_err-1.42.9-9.el7.x86_64 libgcc-4.8.5-11.el7.x86_64 libselinux-2.5-6.el7.x86_64 pcre-8.32-15.el7_2.1.x86_64 zlib-1.2.7-17.el7.x86_64
(gdb) #0  0x00007f30859951d7 in raise () from /lib64/libc.so.6
#1  0x00007f30859968c8 in abort () from /lib64/libc.so.6
#2  0x00007f3089941e7e in dsn_coredump ()
    at /home/wutao1/pegasus-release/rdsn/src/core/core/service_api_c.cpp:76
#3  0x00007f308998cafd in dsn::disk_file::on_write_completed (
    this=<optimized out>, wk=wk@entry=0xabdb90310,
    ctx=ctx@entry=0x7f30830aa17c, err=..., size=size@entry=331776)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/disk_engine.cpp:111
#4  0x00007f308998d332 in dsn::disk_engine::complete_io (this=0x20f4900,
    aio=0xabdb90310, err=..., bytes=bytes@entry=331776,
    delay_milliseconds=delay_milliseconds@entry=0)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/disk_engine.cpp:338
#5  0x00007f30899e323e in dsn::aio_provider::complete_io (
    this=<optimized out>, aio=<optimized out>, err=...,
    bytes=bytes@entry=331776, delay_milliseconds=delay_milliseconds@entry=0)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/aio_provider.cpp:50
#6  0x00007f30899db629 in dsn::tools::native_linux_aio_provider::complete_aio
    (this=this@entry=0x21149f0, io=0xe618cd20, bytes=331776,
    err=<optimized out>)
    at /home/wutao1/pegasus-release/rdsn/src/core/tools/common/native_aio_provider.linux.cpp:146
#7  0x00007f30899db781 in dsn::tools::native_linux_aio_provider::get_event (
    this=0x21149f0)
    at /home/wutao1/pegasus-release/rdsn/src/core/tools/common/native_aio_provider.linux.cpp:124
#8  0x00007f30862ed600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#9  0x00007f3086dffdc5 in start_thread () from /lib64/libpthread.so.0
#10 0x00007f3085a5773d in clone () from /lib64/libc.so.6
```

```
Reading symbols from /home/work/packages/pegasus/c3srv-xiaomi/694cbd544436f03d34bfbfcbca0cd9b8f397197e-20191204-103256/pegasus-server-1.12.1-694cbd5-glibc2.12-release/bin/pegasus_server...done.
Missing separate debuginfo for /home/work/app/pegasus/c3srv-xiaomi/replica/package/bin/libaio.so.1
Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/7e/f475c5abcc899058c9ab6600a1b65fb1343153.debug
Missing separate debuginfo for /home/work/app/pegasus/c3srv-xiaomi/replica/package/bin/libcrypto.so.10
Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/3a/8d65b9a373c0afaf106f3a979835b16dbeff1a.debug
Missing separate debuginfo for /home/work/app/pegasus/c3srv-xiaomi/replica/package/bin/libsnappy.so.1
Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/e7/f2d4d5e290fe830bfca479729099b0e6adbfd3.debug
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib64/libthread_db.so.1"".
Missing separate debuginfo for /home/work/app/pegasus/c3srv-xiaomi/replica/package/bin/liblz4.so.1
Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/a5/7db374840a9697b5aa9e25a8967b1ac75bfb70.debug
Missing separate debuginfo for /home/work/app/pegasus/c3srv-xiaomi/replica/package/bin/libzstd.so.1
Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/6e/19fb6f4ba75c095047c9b9f564e69d050332c9.debug
Missing separate debuginfo for /home/work/app/pegasus/c3srv-xiaomi/replica/package/bin/libssl.so.10
Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/31/8eab33420b000d542f09b91b716bacab1ad546.debug
Core was generated by `/home/work/app/pegasus/c3srv-xiaomi/replica/package/bin/pegasus_server config.i'.
Program terminated with signal 6, Aborted.
#0  0x00007ffce20041d7 in raise () from /lib64/libc.so.6
Missing separate debuginfos, use: debuginfo-install bzip2-libs-1.0.6-13.el7.x86_64 cyrus-sasl-lib-2.1.26-20.el7_2.x86_64 elfutils-libelf-0.166-2.el7.x86_64 elfutils-libs-0.166-2.el7.x86_64 glibc-2.17-157.el7_3.1.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.14.1-27.el7_3.x86_64 libattr-2.4.46-12.el7.x86_64 libcap-2.22-8.el7.x86_64 libcom_err-1.42.9-9.el7.x86_64 libcurl-7.29.0-35.el7.centos.x86_64 libgcc-4.8.5-11.el7.x86_64 libidn-1.28-4.el7.x86_64 libselinux-2.5-6.el7.x86_64 libssh2-1.4.3-10.el7_2.1.x86_64 nspr-4.11.0-1.el7_2.x86_64 nss-3.21.0-17.el7.x86_64 nss-softokn-freebl-3.16.2.3-14.4.el7.x86_64 nss-util-3.21.0-2.2.el7_2.x86_64 openldap-2.4.40-13.el7.x86_64 pcre-8.32-15.el7_2.1.x86_64 systemd-libs-219-30.el7.x86_64 xz-libs-5.2.2-1.el7.x86_64 zlib-1.2.7-17.el7.x86_64
(gdb) #0  0x00007ffce20041d7 in raise () from /lib64/libc.so.6
#1  0x00007ffce20058c8 in abort () from /lib64/libc.so.6
#2  0x00007ffce5f947be in dsn_coredump ()
    at /home/wutao1/pegasus-release/rdsn/src/core/core/service_api_c.cpp:76
#3  0x00007ffce5fddebd in dsn::disk_file::on_write_completed (
    this=<optimized out>, wk=wk@entry=0xa4122357c,
    ctx=ctx@entry=0x7ffcdd5ea17c, err=..., size=size@entry=342)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/disk_engine.cpp:111
#4  0x00007ffce5fde6f2 in dsn::disk_engine::complete_io (this=0x2962260,
    aio=0xa4122357c, err=..., bytes=bytes@entry=342,
    delay_milliseconds=delay_milliseconds@entry=0)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/disk_engine.cpp:338
#5  0x00007ffce603944e in dsn::aio_provider::complete_io (
    this=<optimized out>, aio=<optimized out>, err=...,
    bytes=bytes@entry=342, delay_milliseconds=delay_milliseconds@entry=0)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/aio_provider.cpp:50
#6  0x00007ffce6031619 in dsn::tools::native_linux_aio_provider::complete_aio
    (this=this@entry=0x296a720, io=0x103d96a20, bytes=342,
    err=<optimized out>)
    at /home/wutao1/pegasus-release/rdsn/src/core/tools/common/native_aio_provider.linux.cpp:146
#7  0x00007ffce6031771 in dsn::tools::native_linux_aio_provider::get_event (
    this=0x296a720)
    at /home/wutao1/pegasus-release/rdsn/src/core/tools/common/native_aio_provider.linux.cpp:124
#8  0x00007ffce295c600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#9  0x00007ffce34c1dc5 in start_thread () from /lib64/libpthread.so.0
#10 0x00007ffce20c673d in clone () from /lib64/libc.so.6
```
So I think this bug is not caused by the Pegasus program itself, but the `disk interface` actually don't ensure the `write` is 
 reliable.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgxNzQ2ODEyNQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/668,https://api.github.com/repos/apache/incubator-pegasus/issues/668,incubator-pegasus,777425405,668,pegasus文档中不直接支持list数据格式,765308949,43138355,,,CLOSED,2021-01-02T08:21:16Z,2021-01-03T00:55:52Z,您好，看了官方文档发现pegasus不直接支持list数据格式，但是可以参考pika的实现形式，我理解是需要自己在代码里面根据pika的实现方案写一个list的存储方式吗？还是说有现成的api可以调用。,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/668/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/668,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc1MzQ1MzQ1OQ==,incubator-pegasus,753453459,668,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2021-01-02T09:50:25Z,2021-01-02T09:50:25Z,没有现成的api。简单的实现方法是把 key 放到 hashkey，把 list index 作为 sortkey。但这种实现需要注意把 key 打散，避免单个 list 过大，引起热点问题。,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc1MzQ1MzQ1OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/668,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc1MzQ1NDQzMA==,incubator-pegasus,753454430,668,NA,765308949,43138355,,,NA,2021-01-02T10:01:24Z,2021-01-02T10:01:24Z,好的 谢谢了,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc1MzQ1NDQzMA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/687,https://api.github.com/repos/apache/incubator-pegasus/issues/687,incubator-pegasus,795123049,687,coredump on thrift_helper,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,OPEN,2021-01-27T13:56:12Z,2021-01-27T13:56:12Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
If possible, provide a recipe for reproducing the error.

Normal setup

2. What did you expect to see?

No coredump

3. What did you see instead?

```
#1  0x00007f4249b5d8c8 in abort () from /lib64/libc.so.6
#2  0x00007f4249b55146 in __assert_fail_base () from /lib64/libc.so.6
#3  0x00007f4249b551f2 in __assert_fail () from /lib64/libc.so.6
#4  0x00007f424dadcb9f in dsn::binary_reader::read (this=0x7f421f220d40,
    buffer=buffer@entry=0x7f421f220b90 ""\001\001\300MB\177"", sz=sz@entry=2)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/binary_reader.cpp:80
#5  0x00000000005e30e5 in read (len=2,
    buf=0x7f421f220b90 ""\001\001\300MB\177"", this=0x7f421f220c10)
    at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/cpp/serialization_helper/thrift_helper.h:66
#6  apache::thrift::transport::readAll<dsn::binary_reader_transport> (
    trans=..., buf=0x7f421f220b90 ""\001\001\300MB\177"", len=2)
    at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/thrift/transport/TTransport.h:41
#7  0x00000000005d4ae3 in readAll (len=2,
    buf=0x7f421f220b90 ""\001\001\300MB\177"", this=<optimized out>)
    at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/thrift/transport/TTransport.h:121
#8  readI16 (this=<optimized out>, i16=@0x7f421f220bda: 1)
    at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/thrift/protocol/TBinaryProtocol.tcc:360
#9  readFieldBegin (name=..., fieldId=@0x7f421f220bda: 1,
    fieldType=<optimized out>, this=0x7f421f220c30)
    at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/thrift/protocol/TBinaryProtocol.tcc:259
#10 apache::thrift::protocol::TVirtualProtocol<apache::thrift::protocol::TBinaryProtocolT<apache::thrift::transport::TTransport, apache::thrift::protocol::TNetworkBigEndian>, apache::thrift::protocol::TProtocolDefaults>::readFieldBegin_virt (this=0x7f421f220c30, name=...,
    fieldType=@0x7f421f220bdc: apache::thrift::protocol::T_VOID,
    fieldId=@0x7f421f220bda: 1)
    at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/thrift/protocol/TVirtualProtocol.h:415
#11 0x00000000005eb4d0 in readFieldBegin (fieldId=@0x7f421f220bda: 1,
    fieldType=@0x7f421f220bdc: apache::thrift::protocol::T_VOID, name=...,
    this=0x7f421f220c30)
    at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/thrift/protocol/TProtocol.h:451
#12 dsn::unmarshall_thrift_internal<dsn::apps::update_request> (val=...,
    proto=proto@entry=0x7f421f220c30)
    at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/cpp/serialization_helper/thrift_helper.h:669
#13 0x00000000006364b3 in unmarshall_thrift_binary<dsn::apps::update_request>
    (val=..., reader=..., this=<optimized out>)
    at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/cpp/serialization_helper/thrift_helper.h:703
#14 dsn::unmarshall<dsn::apps::update_request> (reader=..., value=...,
    fmt=<optimized out>)
    at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/cpp/serialization.h:73
#15 0x000000000062c8c7 in unmarshall<dsn::apps::update_request> (val=..., msg=
    0x24b8d794)
    at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/cpp/serialization.h:101
#16 internal (req=0x24b8d794, this=0x244d51e78)
    at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/cpp/rpc_holder.h:248
#17 construct<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal, dsn::message_ex*&> (__p=0x244d51e78, this=<optimized out>)
    at /home/wutao1/app/include/c++/4.8.2/ext/new_allocator.h:120
#18 _S_construct<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal, dsn::message_ex*&> (__p=0x244d51e78, __a=...)
    at /home/wutao1/app/include/c++/4.8.2/bits/alloc_traits.h:254
#19 construct<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal, dsn::message_ex*&> (__p=0x244d51e78, __a=...)
    at /home/wutao1/app/include/c++/4.8.2/bits/alloc_traits.h:393
#20 _Sp_counted_ptr_inplace<dsn::message_ex*&> (__a=..., this=0x244d51e60)
    at /home/wutao1/app/include/c++/4.8.2/bits/shared_ptr_base.h:399
#21 construct<std::_Sp_counted_ptr_inplace<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal, std::allocator<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal>, (__gnu_cxx::_Lock_policy)2>, std::allocator<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal> const, dsn::message_ex*&> (
    __p=<optimized out>, this=<synthetic pointer>)
    at /home/wutao1/app/include/c++/4.8.2/ext/new_allocator.h:120
#22 _S_construct<std::_Sp_counted_ptr_inplace<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal, std::allocator<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal>, (__gnu_cxx::_Lock_policy)2>, std::allocator<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal> const, dsn::message_ex*&> (
    __p=<optimized out>, __a=<synthetic pointer>)
    at /home/wutao1/app/include/c++/4.8.2/bits/alloc_traits.h:254
#23 construct<std::_Sp_counted_ptr_inplace<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal, std::allocator<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal>, (__gnu_cxx::_Lock_policy)2>, std::allocator<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal> const, dsn::message_ex*&> (
    __p=<optimized out>, __a=<synthetic pointer>)
    at /home/wutao1/app/include/c++/4.8.2/bits/alloc_traits.h:393
#24 __shared_count<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal, std::allocator<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal>, dsn::message_ex*&> (__a=...,
    this=<optimized out>)
    at /home/wutao1/app/include/c++/4.8.2/bits/shared_ptr_base.h:502
#25 __shared_ptr<std::allocator<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal>, dsn::message_ex*&> (__a=..., __tag=...,
    this=<optimized out>)
    at /home/wutao1/app/include/c++/4.8.2/bits/shared_ptr_base.h:957
#26 shared_ptr<std::allocator<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal>, dsn::message_ex*&> (__a=..., __tag=...,
    this=<optimized out>)
    at /home/wutao1/app/include/c++/4.8.2/bits/shared_ptr.h:316
#27 allocate_shared<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal, std::allocator<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal>, dsn::message_ex*&> (__a=...)
    at /home/wutao1/app/include/c++/4.8.2/bits/shared_ptr.h:598
#28 make_shared<dsn::rpc_holder<dsn::apps::update_request, dsn::apps::update_response>::internal, dsn::message_ex*&> ()
    at /home/wutao1/app/include/c++/4.8.2/bits/shared_ptr.h:614
#29 rpc_holder (req=0x24b8d794, this=0x7f421f220d30)
    at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/cpp/rpc_holder.h:85
#30 auto_reply (req=0x24b8d794)
    at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/cpp/rpc_holder.h:207
#31 pegasus::server::pegasus_server_write::on_batched_writes (
    this=this@entry=0x1350be00, requests=requests@entry=0x7f421f220ec0,
    count=count@entry=1)
    at /home/wutao1/pegasus-release/src/server/pegasus_server_write.cpp:91
#32 0x000000000062d896 in pegasus::server::pegasus_server_write::on_batched_write_requests (this=0x1350be00, requests=0x7f421f220ec0, count=1,
    decree=<optimized out>, timestamp=<optimized out>)
    at /home/wutao1/pegasus-release/src/server/pegasus_server_write.cpp:70
#33 0x00007f424da2d7fa in dsn::replication::replication_app_base::apply_mutation (this=0x4003000, mu=0x240c90dc0)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replication_app_base.cpp:511
#34 0x00007f424d99b689 in dsn::replication::replica::execute_mutation (
    this=0x292d600, mu=...)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica.cpp:268
#35 0x00007f424d997096 in dsn::replication::prepare_list::commit (
    this=0x2df78f0, d=<optimized out>,
    ct=ct@entry=dsn::replication::COMMIT_ALL_READY)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/prepare_list.cpp:182
#36 0x00007f424d99d31f in dsn::replication::replica::do_possible_commit_on_primary (this=this@entry=0x292d600, mu=...)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_2pc.cpp:265
#37 0x00007f424d9a1d9b in dsn::replication::replica::on_append_log_completed (
    this=0x292d600, mu=..., err=..., size=<optimized out>)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_2pc.cpp:448
#38 0x00007f424db1d428 in operator() (__args#1=<optimized out>, __args#0=...,
    this=<optimized out>)
    at /home/wutao1/app/include/c++/4.8.2/functional:2464
#39 dsn::aio_task::exec (this=<optimized out>)
    at /home/wutao1/pegasus-release/rdsn/include/dsn/tool-api/task.h:600
#40 0x00007f424db1afd9 in dsn::task::exec_internal (
    this=this@entry=0x24b8d648)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/task.cpp:180
#41 0x00007f424db2f22d in dsn::task_worker::loop (this=0x2689a20)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:211
#42 0x00007f424db2f3f9 in dsn::task_worker::run_internal (this=0x2689a20)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:191
#43 0x00007f424a4b4600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#44 0x00007f424afc6dc5 in start_thread () from /lib64/libpthread.so.0
#45 0x00007f4249c1e73d in clone () from /lib64/libc.so.6
```


4. What version of Pegasus are you using?

1.12.3","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/687/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/692,https://api.github.com/repos/apache/incubator-pegasus/issues/692,incubator-pegasus,804265984,692,pegasus-kv/pegasus-spark java项目构建失败,huangmiumang,19187765,miumang,,CLOSED,2021-02-09T06:48:29Z,2021-02-20T03:46:55Z,"构建的失败信息如下：
![image](https://user-images.githubusercontent.com/19187765/107326033-bc0dad00-6ae5-11eb-9087-0f91f063e087.png)
请问一哈，这几个jar包是在哪个镜像仓库里呢？
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/692/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/692,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc4MjU1Mjc4Mw==,incubator-pegasus,782552783,692,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-02-20T03:46:52Z,2021-02-20T03:46:52Z,see https://github.com/pegasus-kv/pegasus-spark/issues/53,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc4MjU1Mjc4Mw==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/693,https://api.github.com/repos/apache/incubator-pegasus/issues/693,incubator-pegasus,812506093,693,fix duplicating&learning(1/n): The server crash when balance with duplication,foreverneverer,23136769,Jiashuo,js982986555@live.com,CLOSED,2021-02-20T03:53:23Z,2023-05-16T16:26:07Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

### 1. What did you do?
* dup status is `start`
* set meta lively to balance after adding three node

### 2. What did you see instead?
server crash and the core stack:
```
(gdb) bt
#0  dsn::zlock::lock (this=this@entry=0xa8) at /home/jiashuo1/work/pegasus/rdsn/src/runtime/zlocks.cpp:89
#1  0x00007fd091686daf in zauto_lock (lock=..., this=<synthetic pointer>) at /home/jiashuo1/work/pegasus/rdsn/include/dsn/tool-api/zlocks.h:121
#2  dsn::replication::mutation_log::max_commit_on_disk (this=this@entry=0x0) at /home/jiashuo1/work/pegasus/rdsn/src/replica/mutation_log.cpp:860
#3  0x00007fd0917c68f4 in dsn::replication::load_mutation::run (this=0x21ad2bd90) at /home/jiashuo1/work/pegasus/rdsn/src/replica/duplication/duplication_pipeline.cpp:31
#4  0x00007fd0918f8e9d in dsn::task::exec_internal (this=this@entry=0x27b827a71) at /home/jiashuo1/work/pegasus/rdsn/src/runtime/task/task.cpp:176
#5  0x00007fd0919139fa in dsn::task_worker::loop (this=0x3347bc0) at /home/jiashuo1/work/pegasus/rdsn/src/runtime/task/task_worker.cpp:211
#6  0x00007fd091913bc8 in dsn::task_worker::run_internal (this=0x3347bc0) at /home/jiashuo1/work/pegasus/rdsn/src/runtime/task/task_worker.cpp:191
#7  0x00007fd08de0ffd0 in std::execute_native_thread_routine (__p=<optimized out>) at /home/jiashuo1/app/toolchain/gcc540/objdir/../gcc-5.4.0/libstdc++-v3/src/c++11/thread.cc:84
#8  0x00007fd08e99fdc5 in start_thread () from /lib64/libpthread.so.0
#9  0x00007fd08d57273d in clone () from /lib64/libc.so.6
```

### 3. What version of Pegasus are you using?
[Pegasus 2.1.1](https://github.com/apache/incubator-pegasus/tree/v2.1.1) ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/693/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/693,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc4ODU1MjkxNg==,incubator-pegasus,788552916,693,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-03-02T03:31:28Z,2021-03-02T03:31:28Z,"In today's(2021.3.2) test, the bug recurred","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDc4ODU1MjkxNg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/696,https://api.github.com/repos/apache/incubator-pegasus/issues/696,incubator-pegasus,818427906,696,Release 2.2.0,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,CLOSED,2021-03-01T03:41:51Z,2021-10-22T10:27:17Z,"|   PR (72 TOTAL, 0 PICKED)    |                                          TITLE                                           |
|------------------------------|------------------------------------------------------------------------------------------|
| apache/incubator-pegasus#736       | fix: fix logo missing                                                                          |
| apache/incubator-pegasus#730       | doc: update README.md fix a typo                                                               |
| apache/incubator-pegasus#715       | doc: Grammar fixes                                                                             |
| apache/incubator-pegasus#718       | fix: overload dump_write_request shouldn't auto-reply to client                                |
| apache/incubator-pegasus#708       | fix(script): oprimizations for pegasus_rolling_update.sh                                       |
| apache/incubator-pegasus#711       | fix: build failure on clang9                                                                   |
| apache/incubator-pegasus#716       | fix: shell app_stat removes unexpected columns                                                 |
| apache/incubator-pegasus#710       | fix(hotkey): fix input and output of detect_hotkey in shell                                    |
| apache/incubator-pegasus#707       | fix(restore): make 'old_policy_name' an optional parameter                                     |
| apache/incubator-pegasus#706       | fix: do not get master while raise a new pull request                                          |
| apache/incubator-pegasus#705       | feat: add read rate limiter perfcounter in info collector                                      |
| apache/incubator-pegasus#700       | fix(scripts): skip packaging irrelevant libs                                                   |
| apache/incubator-pegasus#697       | fix: package script on ubuntu1804                                                              |
| apache/incubator-pegasus#676 | fix: pack without libaio                                                                 |
| apache/incubator-pegasus#690 | chore(ci): migrate pegasus ci to github action                                           |
| apache/incubator-pegasus#684 | fix(split): add flush_memtable for copy_checkpoint_to_dir                                |
| apache/incubator-pegasus#686 | feat: remove the dependency of tls memory for redis_parser                               |
| apache/incubator-pegasus#685 | fix(asan): repeated memory release in pegasus_server_write_test.batch_writes             |
| apache/incubator-pegasus#675 | fix: compilation error in pegasus_utils                                                  |
| apache/incubator-pegasus#680 | refactor: reimplement pegasus_server_write::on_batched_write_requests                    |
| apache/incubator-pegasus#679 | feat(hotkey): dynamic change some parameters in hotkey detection                         |
| apache/incubator-pegasus#678 | refactor: use rocksdb_wrapper to reimplement batch operations                            |
| apache/incubator-pegasus#673 | ci: update action-semantic-pull-request to v2.2.0                                        |
| apache/incubator-pegasus#665 | feat(hotkey): add a function test of hotkey detection                                    |
| apache/incubator-pegasus#674 | refactor: remove unused unit tests in pegasus_write_service_impl_test                    |
| apache/incubator-pegasus#671 | refactor: move func ingestion_files to rocksdb_wrapper                                   |
| apache/incubator-pegasus#672 | refactor: use rocksdb_wrapper to reimplement check_and_mutate                            |
| apache/incubator-pegasus#670 | refactor: use rocksdb_wrapper to reimplement empty_put                                   |
| apache/incubator-pegasus#669 | refactor: move write_batch_delete into rocksdb_wrapper                                   |
| apache/incubator-pegasus#666 | refactor: use rocksdb_wrapper::write_batch_put to reimplement check_and_set              |
| apache/incubator-pegasus#657 | docs: add rfc for meta-proxy                                                             |
| apache/incubator-pegasus#667 | refactor: use rocksdb_wrapper::write_batch_put to reimplement incr                       |
| apache/incubator-pegasus#656 | refactor: use rocksdb_wrapper::write_batch_put to reimplement check_and_set              |
| apache/incubator-pegasus#663 | feat: add query app data_version                                                         |
| apache/incubator-pegasus#664 | feat(security): add thirdparty lib when pack server                                      |
| apache/incubator-pegasus#662 | feat(hotkey): add an interface to query hotkey on the specified partition                |
| apache/incubator-pegasus#658 | chore: add config_hdfs.sh                                                                |
| apache/incubator-pegasus#660 | feat(bulk_load): add remote_file_root for start_bulk_load_request                        |
| apache/incubator-pegasus#655 | refactor: use rocksdb_wrapper::get to reimplement check_and_mutate                       |
| apache/incubator-pegasus#654 | refactor: use rocksdb_wrapper::get to reimplement check_and_set                          |
| apache/incubator-pegasus#646 | feat: support costing memtable and index and filter blocks to block c…                   |
| apache/incubator-pegasus#651 | refactor:  use rocksdb_wrapper::get to reimplement incr                                  |
| apache/incubator-pegasus#648 | feat(hotkey): add unit test of hotkey collector                                          |
| apache/incubator-pegasus#652 | fix: fix log bug in multi_get                                                            |
| apache/incubator-pegasus#647 | feat(backup): support using hdfs to backup/restore pegasus tables                        |
| apache/incubator-pegasus#632 | fix(hotkey): fix two hidden dangers in hotkey_collector                                  |
| apache/incubator-pegasus#639 | feat(hotkey): add unit test of internal_collector                                        |
| apache/incubator-pegasus#641 | fix(hotspot): replace partition_resolver to ddl_client                                   |
| apache/incubator-pegasus#644 | refactor(shell): delete the disk info query command                                      |
| apache/incubator-pegasus#640 | feat(hotkey): add thread_safe method to clear collector                                  |
| apache/incubator-pegasus#631 | feat(hotkey): capture data part3 - declare fine collector                                |
| apache/incubator-pegasus#624 | feat(hotkey): capture data part2 - declare coarse collector                              |
| apache/incubator-pegasus#627 | fix: plog aio tasks are assigned replica long thread pool will cause close app block     |
| apache/incubator-pegasus#625 | feat(hotkey): collector can be terminated by timeout                                     |
| apache/incubator-pegasus#614 | docs: propose a rfc of pegasus data version v3                                           |
| apache/incubator-pegasus#621 | feat(hotkey): implement the RPC handler in hotkey_collector                              |
| apache/incubator-pegasus#618 | feat(hotkey): capture data part1 - declare internal collector                            |
| apache/incubator-pegasus#619 | fix: fix unit test of capacity_unit_calculator                                           |
| apache/incubator-pegasus#616 | feat(hotkey): add a state machine of hotkey_collector                                    |
| apache/incubator-pegasus#615 | feat(hotkey): add an interface of hotkey capture                                         |
| apache/incubator-pegasus#603 | feat(hotkey): build a fundamental framework of hotkey detection                          |
| apache/incubator-pegasus#613 | chore: move github pullrequests to commits mail-list                                     |
| apache/incubator-pegasus#611 | feat(shell): add a util to validate the input ip string with list_nodes                  |
| apache/incubator-pegasus#612 | refactor(hotspot): remove hotkey_detect_request in rrdb.thrift                           |
| apache/incubator-pegasus#605 | feat(hotspot): add a function to start hotkey detecting in shell commands                |
| apache/incubator-pegasus#608 | fix: bugfix on incr                                                                      |
| apache/incubator-pegasus#607 | fix(hotspot): using dsn::make_unique instead of std::make_unique                         |
| apache/incubator-pegasus#604 | feat(hotspot): calculator auto detect hotkey in the hot partition                        |
| apache/incubator-pegasus#592 | refactor: split read/write hotspot of hotspot_partition_calculator                       |
| apache/incubator-pegasus#601 | feat(hotspot): add a function to start hotkey detecting in hotspot_partition_calculator  |
| apache/incubator-pegasus#598 | refactor: use db_get to implement incr                                                   |
| apache/incubator-pegasus#597 | refactor(collector): sort out the structure of partition hotspot detection               |

| PR (175 TOTAL, 0 PICKED) |                                                 TITLE                                                  |
|--------------------------|--------------------------------------------------------------------------------------------------------|
| XiaoMi/rdsn#835            | fix: fix a compile error on clang                                                                      |
| XiaoMi/rdsn#837            | fix: update bulk load download thread pool                                                             |
| XiaoMi/rdsn#832            | feat: add remote command to get tcmalloc status                                                        |
| XiaoMi/rdsn#818            | fix: support retry when io write incompletely                                                          |
| XiaoMi/rdsn#781            | fix: update bool app envs                                                                              |
| XiaoMi/rdsn#768            | fix(security): doesn't reject request even if it's rejected by access controller                       |
| XiaoMi/rdsn#829            | feat(bulk_load): meta server adds bulk load ingestion concurrent count restriction                     |
| XiaoMi/rdsn#827            | feat(bulk_load): add bulk load max rollback count                                                      |
| XiaoMi/rdsn#828            | fix(bulk_load): remove possible old directory when start bulk load                                     |
| XiaoMi/rdsn#826            | fix(bulk_load): redownload file if local file damaged                                                  |
| XiaoMi/rdsn#823            | fix(bulk_load): fix remove_local_bulk_load_dir()                                                       |
| XiaoMi/rdsn#824            | fix: add /version http api                                                                             |
| XiaoMi/rdsn#807            | fix(backup_policy): do not try again when got fs errors                                                |
| XiaoMi/rdsn#808            | fix: incorrect help info of meta/replica remote commands                                               |
| XiaoMi/rdsn#806            | fix: compilation errors on clang-9                                                                     |
| XiaoMi/rdsn#805            | feat(http): update query manual compaction interface                                                   |
| XiaoMi/rdsn#803            | build: fix compilation errors on clang-10                                                              |
| XiaoMi/rdsn#801            | feat(http): update query data version                                                                  |
| XiaoMi/rdsn#800            | fix: modify the ouput format for configs http api                                                      |
| XiaoMi/rdsn#799            | fix: remove read size throttling                                                                       |
| XiaoMi/rdsn#780            | fix: fix compilation errors                                                                            |
| XiaoMi/rdsn#368            | refactor: move primary's learning preparation of cache into another function                           |
| XiaoMi/rdsn#770            | fix: enable config runtime-update using flag for disk_cleaner                                          |
| XiaoMi/rdsn#760          | refactor: separate files from replication.thrift                                                       |
| XiaoMi/rdsn#668          | feat(disk_balance): close origin replica and update replica dir                                        |
| XiaoMi/rdsn#754          | refactor: use shared_ptr to replace raw pointer                                                        |
| XiaoMi/rdsn#759          | refactor: remove thrift generated files of fd/simplekv                                                 |
| XiaoMi/rdsn#758          | refactor: remove generated thrift srcs from repo                                                       |
| XiaoMi/rdsn#753          | fix: do not call unregister_rpc_handler when exiting progress                                          |
| XiaoMi/rdsn#752          | fix: make disk_engine destructed after service_engine destructed                                       |
| XiaoMi/rdsn#750          | fix(perf_counter_wrapper): avoid using invalid perf_counters::instance() to remove counter             |
| XiaoMi/rdsn#678          | fix: cluster_name of ddl client from zk root                                                           |
| XiaoMi/rdsn#742          | feat(split): supplement some case after implementing pause cancel split                                |
| XiaoMi/rdsn#757          | fix: memory leak in nfs test                                                                           |
| XiaoMi/rdsn#756          | fix: delete unused flags `rocksdb_target_file_size_base` and `rocksdb_write_buffer_size`               |
| XiaoMi/rdsn#755          | fix: set burst_size of _write_token_bucket in fds_service to double max                                |
| XiaoMi/rdsn#747          | feat(split): replica add validate_partition_hash                                                       |
| XiaoMi/rdsn#749          | refactor: remove the dependency of tls memory for message_ex                                           |
| XiaoMi/rdsn#751          | fix(asan): memory leak in command_manager                                                              |
| XiaoMi/rdsn#748          | fix: fix log print                                                                                     |
| XiaoMi/rdsn#734          | fix(command_manager): avoid using std::remove to erase elements from std::vector                       |
| XiaoMi/rdsn#746          | fix: error reported in build process of pegasus-docker                                                 |
| XiaoMi/rdsn#745          | feat(split): add app_env split_validate_partition_hash                                                 |
| XiaoMi/rdsn#744          | fix: fix lock in notify_stop_split                                                                     |
| XiaoMi/rdsn#726          | feat(split): add query split status interface                                                          |
| XiaoMi/rdsn#729          | fix(split): add flush_memtable for copy_checkpoint_to_dir                                              |
| XiaoMi/rdsn#711          | refactor: reimplement join point with less code                                                        |
| XiaoMi/rdsn#731          | feat(split): add notify_stop_split rpc                                                                 |
| XiaoMi/rdsn#739          | refactor:  remove the dependency of tls memory for some member functions in message_ex                 |
| XiaoMi/rdsn#728          | feat: add throttling for read size and read qps                                                        |
| XiaoMi/rdsn#738          | fix(asan): heap-use-after-free in rpc_read_stream                                                      |
| XiaoMi/rdsn#737          | feat: remove transient_object which is implemented by tls memory                                       |
| XiaoMi/rdsn#736          | fix(hdfs): do not call hdfsDisconnect() after jvm exited                                               |
| XiaoMi/rdsn#735          | fix(perf_counter): remove static map from counter_info                                                 |
| XiaoMi/rdsn#730          | fix: fix the usage of service_engine::get_all_nodes()                                                  |
| XiaoMi/rdsn#733          | fix(asan):  heap-use-after-free caused by replica_test_base::create_test_mutation                      |
| XiaoMi/rdsn#681          | feat(split): replica server handle pause and cancel status                                             |
| XiaoMi/rdsn#725          | fix(split): message header should not be empty                                                         |
| XiaoMi/rdsn#679          | feat(split): add meta control split                                                                    |
| XiaoMi/rdsn#715          | fix: multiple asan errors                                                                              |
| XiaoMi/rdsn#722          | refactor: don't resend message if it is rejected by server                                             |
| XiaoMi/rdsn#723          | refactor: delete find package libaio dependency when build                                             |
| XiaoMi/rdsn#721          | refactor: refactor backup_test                                                                         |
| XiaoMi/rdsn#720          | feat: add read limiter for hdfs                                                                        |
| XiaoMi/rdsn#719          | feat: add http interface to get a specified config                                                     |
| XiaoMi/rdsn#717          | refactor(backup): refactor add_backup_policy                                                           |
| XiaoMi/rdsn#718          | fix(hotkey): add replication_enums of `detect_action::QUERY`                                           |
| XiaoMi/rdsn#704          | feat: add http interface to get all configs                                                            |
| XiaoMi/rdsn#716          | BREAKING CHANGE(backup): remove 'policy_name' from backup files' path                                  |
| XiaoMi/rdsn#712          | fix(security): make mandatory_auth to be a server side config                                          |
| XiaoMi/rdsn#714          | feat: add validation for value updating through http api                                               |
| XiaoMi/rdsn#713          | feat: update query partition data version                                                              |
| XiaoMi/rdsn#710          | feat(hdfs_service): make hdfs_read_batch_size_bytes/hdfs_write_batch_size_bytes mutable                |
| XiaoMi/rdsn#709          | fix(test): add gtest framework to meta tests                                                           |
| XiaoMi/rdsn#696          | feat(bulk_load): add query compaction status http interface                                            |
| XiaoMi/rdsn#690          | feat: support to build a portable rocksdb binary                                                       |
| XiaoMi/rdsn#699          | refactor: fix typo and add comments to security                                                        |
| XiaoMi/rdsn#703          | fix: block service download file return error when file exists                                         |
| XiaoMi/rdsn#707          | feat: add update rocksdb scenario interface                                                            |
| XiaoMi/rdsn#708          | refactor(security): remove mandatory on client side                                                    |
| XiaoMi/rdsn#695          | feat(bulk_load): add update compaction envs http interface                                             |
| XiaoMi/rdsn#706          | feat: add query app data_version interface                                                             |
| XiaoMi/rdsn#702          | feat(security): make enable_acl and mandatory_auth mutable                                             |
| XiaoMi/rdsn#705          | fix(scripts): fix zk version in zk scripts                                                             |
| XiaoMi/rdsn#700          | feat(hotkey): Change RPC to query hotkey                                                               |
| XiaoMi/rdsn#698          | fix: add hash for flag_tag to compatible with gcc < 6.1                                                |
| XiaoMi/rdsn#692          | fix(block_service): download file when local file is different from remote file                        |
| XiaoMi/rdsn#682          | feat: support to modify configs without restart                                                        |
| XiaoMi/rdsn#693          | feat(bulk_load): add start bulk load http interface                                                    |
| XiaoMi/rdsn#694          | feat(security): treat negotiation succeed if server is old version                                     |
| XiaoMi/rdsn#691          | fix(hdfs): fix 'libjvm.so not found' error                                                             |
| XiaoMi/rdsn#689          | fix: zk compile cflag no warn format errors                                                            |
| XiaoMi/rdsn#686          | feat(bulk-load): support user-define remote storage root path                                          |
| XiaoMi/rdsn#688          | fix: add `#pragma once` to defer.h                                                                     |
| XiaoMi/rdsn#685          | fix: compile error of zookeeper 3.4.10 on gcc9                                                         |
| XiaoMi/rdsn#687          | feat(bulk-load): add query bulk load http interface                                                    |
| XiaoMi/rdsn#683          | feat(security): enable access replica when appenv `replica_access_controller.allowed_users` is empty   |
| XiaoMi/rdsn#680          | refactor: remove support_write_vec in aio_task                                                         |
| XiaoMi/rdsn#676          | feat(split): child copy mutation asynchronously                                                        |
| XiaoMi/rdsn#675          | feat(split): secondary start split                                                                     |
| XiaoMi/rdsn#677          | refactor: use THREAD_POOL_BLOCK_SERVICE ro replace THREAD_POOL_FDS_SERVICE/THREAD_POOL_LOCAL_SERVICE   |
| XiaoMi/rdsn#672          | feat(security): implement reset interface for replica access controller                                |
| XiaoMi/rdsn#673          | fix: downgrade hadoop version to 2.8.4                                                                 |
| XiaoMi/rdsn#674          | fix: fmtlib incompatibility problems from 7.1.3 to 5.3.0                                               |
| XiaoMi/rdsn#653          | feat(split): add splitting_replicas while on_config_sync                                               |
| XiaoMi/rdsn#664          | feat(disk_balance): add `do_disk_migrate_replica` to support migrate origin data                       |
| XiaoMi/rdsn#665          | fix: time utils overflow, meta_split_service lock                                                      |
| XiaoMi/rdsn#671          | fix: update thirdparty fmtlib version                                                                  |
| XiaoMi/rdsn#670          | feat(security): implement replica server access controller                                             |
| XiaoMi/rdsn#655          | feat(security): implement meta server access controller                                                |
| XiaoMi/rdsn#647          | feat(backup): add hdfs as an alternative backup provider                                               |
| XiaoMi/rdsn#666          | fix: local_service compile error loading file metadata                                                 |
| XiaoMi/rdsn#646          | refactor: refactor app operation unit tests                                                            |
| XiaoMi/rdsn#660          | feat(disk_balance): support and validate disk migration rpc                                            |
| XiaoMi/rdsn#663          | fix(http): fix parse path from message                                                                 |
| XiaoMi/rdsn#662          | fix(http): fix message buffers size check                                                              |
| XiaoMi/rdsn#661          | fix(backup): use existing block_service_manager when add backup policy                                 |
| XiaoMi/rdsn#651          | refactor: update detail disk info                                                                      |
| XiaoMi/rdsn#658          | chore: migrate ci from travis to github actions                                                        |
| XiaoMi/rdsn#652          | fix(security): fix bug in negotiation_service::on_negotiation_request when rpc_session is closed       |
| XiaoMi/rdsn#654          | feat(split): parent group update partition count                                                       |
| XiaoMi/rdsn#648          | fix(split): fix split register child unit test                                                         |
| XiaoMi/rdsn#645          | feat(split): add update_child_group_partition_count                                                    |
| XiaoMi/rdsn#649          | fix: plog aio tasks are assigned replica long thread pool will cause close app block                   |
| XiaoMi/rdsn#631          | refactor(backup): straighten the execution path of download_file                                       |
| XiaoMi/rdsn#644          | refactor: reduce netprovider unit test executing time                                                  |
| XiaoMi/rdsn#643          | feat(split): update register_child with split_status                                                   |
| XiaoMi/rdsn#633          | perf: optimizing write latency using independent IO queues replace of libaio                           |
| XiaoMi/rdsn#641          | feat(split): add split_status                                                                          |
| XiaoMi/rdsn#642          | feat(split): implement child_handle_split_error                                                        |
| XiaoMi/rdsn#640          | feat(split): add start_partition_split interface                                                       |
| XiaoMi/rdsn#637          | refactor: refactor replica split unit tests                                                            |
| XiaoMi/rdsn#638          | refactor(split): refactor meta start partition split                                                   |
| XiaoMi/rdsn#636          | feat(split): small refactor and fix of replica split                                                   |
| XiaoMi/rdsn#639          | refactor(security): make send/recv message allowed when mandatory_auth is false                        |
| XiaoMi/rdsn#635          | fix: fix fds_service_test                                                                              |
| XiaoMi/rdsn#634          | feat(hotkey): Add an interface of on_detect_hotkey                                                     |
| XiaoMi/rdsn#632          | feat: send hotkey_detect_request in ddl_client                                                         |
| XiaoMi/rdsn#630          | refactor(block_service): remove rebundant functions                                                    |
| XiaoMi/rdsn#629          | refactor(security): add join point to filter the sending message                                       |
| XiaoMi/rdsn#628          | refactor(security): add join point to filter received message                                          |
| XiaoMi/rdsn#626          | refactor(security): add join point to start negotiation                                                |
| XiaoMi/rdsn#624          | refactor(split): move replica split functions into replica_split_manager class                         |
| XiaoMi/rdsn#627          | refactor(fds_service): clean up duplicate code                                                         |
| XiaoMi/rdsn#625          | refactor(backup): move backup rpc handling to replica_backup_server                                    |
| XiaoMi/rdsn#623          | refactor(backup): move common utils out to backup_utils                                                |
| XiaoMi/rdsn#621          | fix: set the tracer type when init                                                                     |
| XiaoMi/rdsn#622          | refactor(security): use blob instead of std::string as the type for msg member of negotiation_request  |
| XiaoMi/rdsn#620          | refactor(security): refactor the send func of client_negotiation                                       |
| XiaoMi/rdsn#619          | refactor(backup): move cold_backup_context out to standalone file                                      |
| XiaoMi/rdsn#615          | refactor(http): add http handler without creating http service                                         |
| XiaoMi/rdsn#618          | feat(security): server_negotiation handle challenge response                                           |
| XiaoMi/rdsn#617          | feat(security): client_negotiation handle challenge                                                    |
| XiaoMi/rdsn#616          | fix(security): fix bug in building cyrus-sasl                                                          |
| XiaoMi/rdsn#613          | feat(security): server_negotiation handle SASL_INITIATE request                                        |
| XiaoMi/rdsn#612          | feat(security): client_negotiation handle mechanism selected response                                  |
| XiaoMi/rdsn#610          | refactor: hide http_server from http api                                                               |
| XiaoMi/rdsn#608          | feat(security): add unit tests for client_negotiation                                                  |
| XiaoMi/rdsn#611          | feat(dup): clear data on meta when duplication removed                                                 |
| XiaoMi/rdsn#604          | feat(security): implement sasl_wrapper which is a wrapper to operate sasl                              |
| XiaoMi/rdsn#607          | feat(security): add unit tests for server_negotiation                                                  |
| XiaoMi/rdsn#609          | feat(security): add unit test for negotiation_service                                                  |
| XiaoMi/rdsn#594          | refactor(http): replace service/method with simple HTTP path                                           |
| XiaoMi/rdsn#606          | refactor(security): close the connection when something is wrong with sever_negotiation                |
| XiaoMi/rdsn#511          | feat: add point for write using latency_tracer tool                                                    |
| XiaoMi/rdsn#599          | feat(security): init sasl                                                                              |
| XiaoMi/rdsn#598          | refactor: use member func instead of macro RPC_CHECK_STATUS                                            |
| XiaoMi/rdsn#600          | feat(security): select mechanism                                                                       |
| XiaoMi/rdsn#595          | feat(security): receive mechanisms which are supported by server_negotiation                           |
| XiaoMi/rdsn#589          | feat(security): make network support security negotiation                                              |
| XiaoMi/rdsn#585          | feat(security): init kerberos                                                                          |
| XiaoMi/rdsn#591          | build: use nodejs/http_parser as a external dep                                                        |
| XiaoMi/rdsn#588          | feat(security): handle SASL_LIST_MECHANISMS by server_negotiation                                      |
| XiaoMi/rdsn#593          | feat(http): support parsing content-type of HTTP request                                               |
| XiaoMi/rdsn#592          | refactor: move http_server.h to dsn/http                                                               |

## Configuration updates

```diff
[pegasus.server]
+ coarse_data_variance_threshold = 3
+ data_capture_hash_bucket_num = 37
+ max_seconds_to_detect_hotkey = 150

+ rocksdb_enable_write_buffer_manager = false
+ rocksdb_total_size_across_write_buffer = 0
+ rocksdb_max_open_files = -1

+ rocksdb_index_type = binary_search | hash_search | two_level_index_search | binary_search_with_first_key
+ rocksdb_partition_filters = false
+ rocksdb_metadata_block_size = 4096
+ rocksdb_cache_index_and_filter_blocks = false
+ rocksdb_pin_top_level_index_and_filter = true
+ rocksdb_cache_index_and_filter_blocks_with_high_priority = true
+ rocksdb_pin_l0_filter_and_index_blocks_in_cache = false

[pegasus.collector]
- hotspot_detect_algorithm
+ enable_hotkey_auto_detect = false
+ hot_partition_threshold = 3
+ occurrence_threshold = 100
+ hotkey_analyse_time_interval_s = 10

[apps.replica]
- pools = THREAD_POOL_DEFAULT,THREAD_POOL_REPLICATION_LONG,THREAD_POOL_REPLICATION,THREAD_POOL_FD,THREAD_POOL_LOCAL_APP,THREAD_POOL_FDS_SERVICE,THREAD_POOL_COMPACT,THREAD_POOL_INGESTION,THREAD_POOL_SLOG
+ pools = THREAD_POOL_DEFAULT,THREAD_POOL_REPLICATION_LONG,THREAD_POOL_REPLICATION,THREAD_POOL_FD,THREAD_POOL_LOCAL_APP,THREAD_POOL_BLOCK_SERVICE,THREAD_POOL_COMPACT,THREAD_POOL_INGESTION,THREAD_POOL_SLOG,THREAD_POOL_PLOG

[apps.meta]
- pools = THREAD_POOL_DEFAULT,THREAD_POOL_META_SERVER,THREAD_POOL_META_STATE,THREAD_POOL_FD,THREAD_POOL_DLOCK,THREAD_POOL_FDS_SERVICE
+ pools = THREAD_POOL_DEFAULT,THREAD_POOL_META_SERVER,THREAD_POOL_META_STATE,THREAD_POOL_FD,THREAD_POOL_DLOCK,THREAD_POOL_BLOCK_SERVICE

+[threadpool.THREAD_POOL_PLOG]
+ name = plog
+ partitioned = true
+ worker_count = 4

-[threadpool.THREAD_POOL_FDS_SERVICE]
- name = fds_service
+[threadpool.THREAD_POOL_BLOCK_SERVICE]
+ name = block_service

[replication]
- bulk_load_provider_root = bulk_load_root
+ hdfs_read_limit_rate_megabytes = 200
+ gc_disk_migration_tmp_replica_interval_seconds = 86400 #1day
+ gc_disk_migration_origin_replica_interval_seconds = 604800 #7day
+ hdfs_read_batch_size_bytes = 67108864 # 64MB
+ hdfs_write_batch_size_bytes = 67108864 # 64MB

+[security]
+ enable_acl = false
+ super_users = 
+ meta_acl_rpc_allow_list = 
+ service_name = 
+ service_fqdn = 
+ sasl_plugin_path = 
+ mandatory_auth = false
+ krb5_keytab =
+ krb5_config =
+ krb5_principal =
+ enable_auth = false
```

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/696/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/712,https://api.github.com/repos/apache/incubator-pegasus/issues/712,incubator-pegasus,847765313,712,Replica server crashed when handle multiget request,zhangyifan27,19500115,Zhang Yifan,chinazhangyifan@163.com,OPEN,2021-04-01T02:43:57Z,2021-04-01T03:18:14Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
If possible, provide a recipe for reproducing the error.

When rolling update a cluster from 2.0-write-optim to 2.1.1 we see one of replica server crashed.
The crashed server is `task 19`, that means we have already rolling update 19 servers successfully, but before rolling this server, it crashed.

2. What did you expect to see?
no crash.

3. What did you see instead?

Coredump stack:
```
(gdb) bt
#0  0x00000000b16a6700 in ?? ()
#1  0x000000000059b44f in rocksdb::DBImpl::MultiGet (this=0x1a641b400, read_options=..., column_family=..., keys=..., values=0x7f77a7a068a0)
    at /home/wutao1/pegasus-release/rocksdb/db/db_impl/db_impl.cc:1742
#2  0x00000000005a79e1 in rocksdb::DB::MultiGet (this=<optimized out>, options=..., keys=..., values=0x7f77a7a068a0) at /home/wutao1/pegasus-release/rocksdb/include/rocksdb/db.h:450
#3  0x0000000000520136 in pegasus::server::pegasus_server_impl::on_multi_get (this=0x5bead00, request=..., reply=...)
    at /home/wutao1/pegasus-release/src/server/pegasus_server_impl.cpp:611
#4  0x000000000050030f in bool dsn::replication::storage_serverlet<dsn::apps::rrdb_service>::register_async_rpc_handler<dsn::apps::multi_get_request, dsn::apps::multi_get_response>(dsn::task_code, char const*, void (*)(dsn::apps::rrdb_service*, dsn::apps::multi_get_request const&, dsn::rpc_replier<dsn::apps::multi_get_response>&))::{lambda(dsn::apps::rrdb_service*, dsn::message_ex*)#1}::operator()(dsn::apps::rrdb_service*, dsn::message_ex*) const () at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/dist/replication/storage_serverlet.h:29
#5  0x0000000000525b0c in operator() (__args#1=0x14cc4ea018, __args#0=0x5bead00, this=<optimized out>) at /home/wutao1/app/include/c++/4.8.2/functional:2464
#6  handle_request (request=0x14cc4ea018, this=0x5bead00) at /home/wutao1/pegasus-release/DSN_ROOT/include/dsn/dist/replication/storage_serverlet.h:80
#7  dsn::apps::rrdb_service::on_request (this=0x5bead00, request=0x14cc4ea018) at /home/wutao1/pegasus-release/src/include/rrdb/rrdb.server.h:17
#8  0x00007f77e24d2f82 in dsn::replication::replica::on_client_read (this=0xb56fce680, request=request@entry=0x14cc4ea018)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica.cpp:186
#9  0x00007f77e2544a9f in dsn::replication::replica_stub::on_client_read (this=0x3554600, id=..., request=0x14cc4ea018)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_stub.cpp:807
#10 0x00007f77e266c219 in dsn::task::exec_internal (this=this@entry=0x14cc4ea1b0) at /home/wutao1/pegasus-release/rdsn/src/core/core/task.cpp:180
#11 0x00007f77e268046d in dsn::task_worker::loop (this=0x3a00c60) at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:211
#12 0x00007f77e2680639 in dsn::task_worker::run_internal (this=0x3a00c60) at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:191
#13 0x00007f77def4e600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#14 0x00007f77dfa60dc5 in start_thread () from /lib64/libpthread.so.0
#15 0x00007f77de6b873d in clone () from /lib64/libc.so.6
```
4. What version of Pegasus are you using?
pegasus-server-2.0-write-optim-984936d-glibc2.12-release","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/712/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/713,https://api.github.com/repos/apache/incubator-pegasus/issues/713,incubator-pegasus,847799916,713,Server crashed after restarting,zhangyifan27,19500115,Zhang Yifan,chinazhangyifan@163.com,OPEN,2021-04-01T03:11:49Z,2021-04-01T03:18:24Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
If possible, provide a recipe for reproducing the error.

Replica server restarted and crashed after a downtime of the cloud host.

2. What did you expect to see?

Restart normally.

3. What did you see instead?

Coredump stack:
```
Program terminated with signal 6, Aborted.
#0  0x00007f25689a51d7 in raise () from /lib64/libc.so.6
Missing separate debuginfos, use: debuginfo-install glibc-2.17-157.el7_3.1.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.14.1-27.el7_3.x86_64 libcom_err-1.42.9-9.el7.x86_64 libgcc-4.8.5-28.el7_5.1.x86_64 libselinux-2.5-6.el7.x86_64 pcre-8.32-15.el7_2.1.x86_64 zlib-1.2.7-17.el7.x86_64
(gdb) #0  0x00007f25689a51d7 in raise () from /lib64/libc.so.6
#1  0x00007f25689a68c8 in abort () from /lib64/libc.so.6
#2  0x00007f256c4e49fe in dsn_coredump ()
    at /home/wutao1/pegasus-release/rdsn/src/core/core/service_api_c.cpp:76
#3  0x00000000006e9284 in pegasus::server::pegasus_server_impl::cancel_background_work (this=0x19b320800, wait=<optimized out>)
    at /home/wutao1/pegasus-release/src/server/pegasus_server_impl.cpp:1615
#4  0x00007f256c3d0a37 in dsn::replication::replica::close (this=0x4420ff80)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica.cpp:394
#5  0x00007f256c42ef6c in dsn::replication::replica_stub::close_replica (
    this=0x3236580, r=...)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_stub.cpp:1872
#6  0x00007f256c42f1c4 in operator() (__closure=<optimized out>)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/lib/replica_stub.cpp:1854
#7  std::_Function_handler<void(), dsn::replication::replica_stub::begin_close_replica(dsn::replication::replica_ptr)::__lambda30>::_M_invoke(const std::_Any_data &) (__functor=...) at /home/wutao1/app/include/c++/4.8.2/functional:2071
#8  0x00007f256c4f6cd9 in dsn::task::exec_internal (this=this@entry=0xc558a2f)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/task.cpp:180
#9  0x00007f256c50aa6d in dsn::task_worker::loop (this=0x2ec9550)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:211
#10 0x00007f256c50ac39 in dsn::task_worker::run_internal (this=0x2ec9550)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/task_worker.cpp:191
#11 0x00007f25692fd600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#12 0x00007f2569f70dc5 in start_thread () from /lib64/libpthread.so.0
#13 0x00007f2568a6773d in clone () from /lib64/libc.so.6
(gdb) quit
```

4. What version of Pegasus are you using?
pegasus-server-1.11.6-9f4e5ae-glibc2.12-release","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/713/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/713,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgxMTYwNzk3MA==,incubator-pegasus,811607970,713,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-04-01T03:18:24Z,2021-04-01T03:18:24Z,"https://github.com/apache/incubator-pegasus/blob/v1.11.6/src/server/pegasus_server_impl.cpp#L1613-L1617

```c++
void pegasus_server_impl::cancel_background_work(bool wait)
{
    dassert(_db != nullptr, """"); // cash here
    rocksdb::CancelAllBackgroundWork(_db, wait);
}
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgxMTYwNzk3MA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/719,https://api.github.com/repos/apache/incubator-pegasus/issues/719,incubator-pegasus,856508161,719,data loss after restarting,ZhongChaoqiang,35595648,Zhong Chaoqiang,,OPEN,2021-04-13T02:28:00Z,2021-04-23T03:00:52Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
after restarting, data in replica my be loss. There are some messages in the log.
`E2020-12-03 21:27:57.929 (1607002077929057014 3b14) replica.replica0.04010000000000d9: replica_init.cpp:313:init_app_and_prepare_list(): 35.24@10.32.82.225:34801: open replica failed, err = ERR_INCOMPLETE_DATA`

`W2020-12-03 21:27:57.951 (1607002077951756963 3b14) replica.replica0.04010000000000d9: replica_init.cpp:188:load(): load_replica: {replica_dir_op} succeed to move directory '/opt/huawei/data1/lemondb_data/replica/reps/35.24.pegasus' to '/opt/huawei/data1/lemondb_data/replica/reps/35.24.pegasus.1607002077951725.err'`

2. What did you expect to see?
restart Successfully 

3. What did you see instead?
data loss after restarting

4. What version of Pegasus are you using?
1.12.0
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/719/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/719,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgxODM4ODY4NQ==,incubator-pegasus,818388685,719,NA,ZhongChaoqiang,35595648,Zhong Chaoqiang,,NA,2021-04-13T02:39:24Z,2021-04-13T02:39:24Z,"下面是分析过程：

1. last_durable_decree为上一次持久化的点，打开replica时从rocksdb的数据目录中读取出来，而init_durable_decree是从.init-info文件中读取出来。.init-info文件一般是在learn的时候生成，我们继续分析日志，发现replica在发生问题前是有发生learn的操作的，并且有如下日志：
`D2020-12-03 21:27:09.499 (1607002029499473085 7754) replica.rep_long3.0404000d000021bd: replica_learn.cpp:1075:on_copy_remote_state_completed(): 35.24@10.32.82.225:34801: on_copy_remote_state_completed[000001a600000002]: learnee = 10.32.82.16:34801, learn_duration = 5 ms, apply checkpoint/log done, err = ERR_OK, last_prepared_decree = (16369 => 16371), last_committed_decree = (16368 => 16371), app_committed_decree = (16368 => 16371), app_durable_decree = (16368 => 16368), remote_committed_decree = 16371, prepare_start_decree = -1, current_learning_status = replication::learner_status::LearningWithoutPrepare`

可以发现经过learn后，last_committed_decree变成了16371，而app_durable_decree还是16368。我们继续分析代码，发现代码中learn后写入.init-info文件中的init_durable_decree的值并不是last_durable_decree的值，而是last_committed_decree的值。

```
err = _app->update_init_info(
      this,
      _stub->_log->on_partition_reset(get_gpid(), _app->last_committed_decree()),
      _private_log->on_partition_reset(get_gpid(), _app->last_committed_decree()),
      _app->last_committed_decree());
```

init_durable_decree应该是记录last_durable_decree的值，而不是last_committed_decree的值；last_committed_decree表示上一次已经提交的修改，但不一定已经持久化了。所以last_committed_decree总是大于或等于last_durable_decree的值的。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgxODM4ODY4NQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/719,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgyMjM4MjkyNQ==,incubator-pegasus,822382925,719,NA,zhangyifan27,19500115,Zhang Yifan,chinazhangyifan@163.com,NA,2021-04-19T11:05:50Z,2021-04-19T11:05:50Z,"@ZhongChaoqiang 
能具体描述一下这个问题要怎么复现吗？我测试了下learn之后重启，并没有复现出`ERR_INCOMPLETE_DATA`相关的log。
另外出现这个错误之后，open replica失败，replica及相关数据会被删除，replica server会重新learn一份数据，应该是不会丢数据的，实际中你们发现丢数据的现象了吗？

看了下replica start这块的逻辑：https://github.com/apache/incubator-pegasus/blob/a948e89b180b6a5c82d298d0dcc65f7bb770a8be/src/server/pegasus_server_impl.cpp#L1706-L1752 `_last_committed_decree`被初始化为rocksdb的last_flushed_decree，随后`_last_durable_decree`被更新成和`_last_committed_decree`一样的值，如果出现`last_durable_decree() < _info.init_durable_decree`，说明replica重启之前可能没有正常flush，导致没有更新last_flushed_decree：https://github.com/apache/incubator-pegasus/blob/a948e89b180b6a5c82d298d0dcc65f7bb770a8be/src/server/pegasus_server_impl.cpp#L1805-L1812
我觉得你可以看下是不是replica server进程退出时没有执行flush导致的这个问题？

`_last_committed_decree`在数据成功写入log和memtable之后会更新，而`_last_durable_decree`好像是打checkpoint时才会更新（这部分不太确定, @neverchanje 可以解答下），不太确定.init_info里面记录_last_durable_decree有没有问题。

另外update_init_info 这块有个注释是说在learn之后需要再一次打checkpoint否则app::open_internal会出错（好像就是你提的这个问题）:
https://github.com/XiaoMi/rdsn/blob/master/src/replica/replica_learn.cpp#L769-L779
所以我觉得可能本身就是这么设计的，init_info里面需要写入_last_committed_decree，至于为什么可能需要其他committer解答下:)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgyMjM4MjkyNQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/719,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgyNTM1MTg0OQ==,incubator-pegasus,825351849,719,NA,ZhongChaoqiang,35595648,Zhong Chaoqiang,,NA,2021-04-23T03:00:52Z,2021-04-23T03:00:52Z,"@zhangyifan27 
谢谢你的恢复！不好意思啊，没有及时看到你的信息！
这个问题比较久远了，我们是在现网碰到的这个问题，具体怎么操作导致的这个问题当时也没有具体说明，但是应该是有多次的重启操作的。
由于我们把err的清理时间（gc_disk_error_replica_interval_seconds）设置过短了，当时的确是丢数据了。（primary的机器下线了，secondary的节点open的时候把数据移到了err目录），所以用户读不到数据了。

还有一个关键日志：在open replica的时候，打印了如下日志。
```
E2020-12-03 21:27:57.929 (1607002077929018092 3b14) replica.replica0.04010000000000d9: replication_app_base.cpp:347:open_internal(): 35.24@10.32.82.225:34801: replica data is not complete coz last_durable_decree(16368) < init_durable_decree(16371)
E2020-12-03 21:27:57.929 (1607002077929048291 3b14) replica.replica0.04010000000000d9: replication_app_base.cpp:353:open_internal(): 35.24@10.32.82.225:34801: open replica app return ERR_INCOMPLETE_DATA
```
这里表明，open replica的时候，我们记录的last_durable_decree是不能少于从.init-info读取出来的init_durable_decree的。因为这样有可能是以为丢失了数据，此时的replica数据应该是不完整的了，所以需要把数据移到err目录下。

怎么样保证last_durable_decree不少于init_durable_decree？由于我们对degree的机制了解的不是很深，目前我们想到的办法是last_committed_decree改成last_durable_decree，应该可以优化这个问题。

怎么重现这个问题我好像没有想到好的方法，所以也是从代码上去分析这个问题的。麻烦你们再帮忙看看是不是会存在这样的问题？谢谢！","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgyNTM1MTg0OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/723,https://api.github.com/repos/apache/incubator-pegasus/issues/723,incubator-pegasus,864663753,723,Proposal: Redesign of Pegasus Scanner ,Smityz,22953824,Smilencer,smityz@qq.com,OPEN,2021-04-22T08:02:57Z,2021-05-17T11:38:24Z,"## Proposal Redesign of Pegasus Scanner

### Background

Pegasus provides three interfaces `on_get_scanner`  `on_scan` and `on_clear_scanner` , for clients to execute scanning tasks.

If we want to full scan the whole table, at first, the client will call `on_get_scanner` on each partition, and then partitions return a `context_id` which is a random number generated by the server to record some parameters such as `hash_key_filter_type`, `batch_size` and the context of this scanning task.

Secondly, the client uses this `context_id` to call `on_scan` and completes scanning in the corresponding partition in turn. Servers will scan the whole data of the table on the disk, and return compliant value to the client in batches.

If the tasking end or any error happened, the client will call `on_clear_scanner` to clear its context_id on the server.

### Problem Statement

In actual use, such a design will cause some problems.

1. **prefix scan is too slow**

If we execute this scanning task:

```shell
full_scan --hash_key_filter_type prefix --hash_key_filter_pattern 2021-04-21
```

Server will scan all the data in the table, then returns the prefix match key of the pattern. But we can speed it up by using prefix seeking futures of RocksDB.

2. **scanning task is easily failed**

Although we have a batch size to limit the scan time, it does not work if the data is sparse. In the case above, we need to scan almost the whole partition but it is possible that there is no row which matches the prefix,then it will be easy to timeout.

### Proposal

**For problem 1**

1. Pegasus store key schema in RocksDB is like `[hashkey_len(2bytes)][hashkey][sortkey]`, so we can't directly use prefix seeking. But we can prefix seek `[01][prefix_pattern]`,`[02][prefix_pattern]`,`[03][prefix_pattern]`...`[65535][prefix_pattern]` in RocksDB.
2. Client can parallelly scan all the partitions instead of one by one.

**For problem 2**

1. We can set a `HeartbeatCheck` during scanning like [Hbase StoreScanner](https://github.com/apache/hbase/blob/048ca4e43fdf8b341c9ade5a9d455f627fc76041/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java#L591), pegasus sever sends heartbeat packets periodically to avoid timeout, which performed like a stream.

2. We can change the way to count batch size: compliant value number -> already scan value number

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/723/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/723,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgyNjI2Nzc0Mg==,incubator-pegasus,826267742,723,NA,shenxingwuying,6360122,,,NA,2021-04-25T06:37:56Z,2021-04-25T06:37:56Z,"Redesign of Pegasus Scanner， to solve the problem scan timeout.
In my opinion，the root cause of the problem is the method of data sort.
Rocksdb's data should use customized Comparator, which will reserve sorted by userkey(hash_key, sort_key), and then 
the prefix filter should very fast. 

Why comparator use the default ByteWiseComparator at the beginning? 
At this time , maybe pegasus can fix to the new comparator(customized Comparator).
To avoid data incompatible, we can support two comparator(add new Comparator), and the new pegasus cluster use new comparator.

1、support postfix，should scan all data，the cost as before,  maybe the filter not important.
2、support prefix，need not scan all data， speed will increase by reduce scans.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgyNjI2Nzc0Mg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/723,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgyNjI2OTE4OQ==,incubator-pegasus,826269189,723,NA,Apache9,4958168,Duo Zhang,palomino219@gmail.com,NA,2021-04-25T06:50:45Z,2021-04-25T06:50:45Z,"Changing comparator will be a pain, as all the old data can not be read any more. Introduce a table level flag to indicate that whether we should use customized comparator? And we also need to test the performance impact of using customized comparator.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgyNjI2OTE4OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/723,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgyNjY0NTkwNg==,incubator-pegasus,826645906,723,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2021-04-26T08:54:37Z,2021-04-26T08:54:37Z,"First of all, we use the default ByteWiseComparator because we design the key schema based on it.
We design the hashkey length ahead of the hashkey bytes in order to prevent key conflict like:

1. hashkey = a, sortkey = xxx

2. hashkey = ax, sortkey = xx

With the default comparator, the two keys are seen as distinct:

```
01axxx
02axxx
```

So we chose this method, but didn't consider that one day we would need prefix filtering of hashkey. So now the problem is:
how can we upgrade our key schema version to support efficient hashkey prefix-filtering, or do other workaround, without modifying the key schema (and also give up support of hashkey sorting), like the above solution that @Smityz came up with.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgyNjY0NTkwNg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/723,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgyNzQzMjM4OA==,incubator-pegasus,827432388,723,NA,Apache9,4958168,Duo Zhang,palomino219@gmail.com,NA,2021-04-27T08:44:53Z,2021-04-27T08:44:53Z,So let's change the comparator and check the performance impact first?,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgyNzQzMjM4OA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/723,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgzNDAzNjEwOA==,incubator-pegasus,834036108,723,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2021-05-07T03:27:28Z,2021-05-07T03:27:28Z,"If there are no compatibility issues, I think changing the comparator is feasible, look forward to your PR @shenxingwuying ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgzNDAzNjEwOA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/723,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDg0MjI1MzI5MQ==,incubator-pegasus,842253291,723,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-05-17T11:38:00Z,2021-05-17T11:38:00Z,"> 1. We can set a `HeartbeatCheck` during scanning like [Hbase StoreScanner](https://github.com/apache/hbase/blob/048ca4e43fdf8b341c9ade5a9d455f627fc76041/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java#L591), pegasus sever sends heartbeat packets periodically to avoid timeout, which performed like a stream

@Apache9 @Smityz  https://github.com/XiaoMi/pegasus-java-client/pull/156 and https://github.com/XiaoMi/pegasus-go-client/pull/86 have fix `next retry failed when timeout`, you can resolve the problem before `refactor scanner `","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDg0MjI1MzI5MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/726,https://api.github.com/repos/apache/incubator-pegasus/issues/726,incubator-pegasus,868572249,726,node crush when node rebalance after adding nodes,foreverneverer,23136769,Jiashuo,js982986555@live.com,OPEN,2021-04-27T07:55:40Z,2021-04-27T10:32:54Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?

- use [v2.2.0-mi-beta3](https://github.com/XiaoMi/rdsn/tree/v2.2.0-mi-beta3)(base [beta2](https://github.com/apache/incubator-pegasus/tree/v2.2.0-mi-beta2) with bulkload-[fix](https://github.com/XiaoMi/rdsn/pull/823)) to test `bulkload`
- add 3 node and rebalance

2. What did you expect to see?
 add nodes sucessfully

3. What did you see instead?
## new node(named 63.bj) crash:
```
#0  0x00000037ee6328a5 in raise () from /lib64/libc.so.6
#1  0x00000037ee634085 in abort () from /lib64/libc.so.6
#2  0x00007fa411ce1395 in __gnu_cxx::__verbose_terminate_handler () at ../../../../libstdc++-v3/libsupc++/vterminate.cc:95
#3  0x00007fa411cdf166 in __cxxabiv1::__terminate (handler=<optimized out>) at ../../../../libstdc++-v3/libsupc++/eh_terminate.cc:47
#4  0x00007fa411cdf1b1 in std::terminate () at ../../../../libstdc++-v3/libsupc++/eh_terminate.cc:57
#5  0x00007fa411cdff0f in __cxxabiv1::__cxa_pure_virtual () at ../../../../libstdc++-v3/libsupc++/pure.cc:50
#6  0x0000000000901fbb in rocksdb::PosixLogger::Flush() ()
#7  0x00000000007b3064 in rocksdb::LogFlush(rocksdb::Logger*) ()
#8  0x00000000007b3a7c in rocksdb::LogFlush(std::shared_ptr<rocksdb::Logger> const&) ()
#9  0x0000000000699e66 in rocksdb::DBImpl::PurgeObsoleteFiles(rocksdb::JobContext&, bool) ()
#10 0x00000000005f4af3 in rocksdb::DBImpl::CloseHelper() ()
#11 0x00000000005f5296 in rocksdb::DBImpl::~DBImpl() ()
#12 0x00000000005f5646 in rocksdb::DBImpl::~DBImpl() ()
#13 0x000000000055de84 in pegasus::server::pegasus_server_impl::release_db (this=this@entry=0x808ae1800) at /home/wutao1/pegasus-release/src/server/pegasus_server_impl.cpp:2777
#14 0x000000000055e8ac in pegasus::server::pegasus_server_impl::stop (this=0x808ae1800, clear_state=<optimized out>) at /home/wutao1/pegasus-release/src/server/pegasus_server_impl.cpp:1562
#15 0x00007fa415593e46 in dsn::replication::replication_app_base::close (this=this@entry=0x808ae1800, clear_state=clear_state@entry=false) at /home/wutao1/pegasus-release/rdsn/src/replica/replication_app_base.cpp:423
#16 0x00007fa4154f92ea in dsn::replication::replica::close (this=0x399b800) at /home/wutao1/pegasus-release/rdsn/src/replica/replica.cpp:436
#17 0x00007fa41555c479 in dsn::replication::replica_stub::close_replica (this=0x1fe4300, r=...) at /home/wutao1/pegasus-release/rdsn/src/replica/replica_stub.cpp:2021
#18 0x00007fa41555c6f4 in operator() (__closure=<optimized out>) at /home/wutao1/pegasus-release/rdsn/src/replica/replica_stub.cpp:2003
#19 std::_Function_handler<void(), dsn::replication::replica_stub::begin_close_replica(dsn::replication::replica_ptr)::<lambda()> >::_M_invoke(const std::_Any_data &) (__functor=...)
    at /home/wutao1/gcc7/include/c++/7.5.0/bits/std_function.h:316
#20 0x00007fa4156ed5f1 in dsn::task::exec_internal (this=this@entry=0x423df7770) at /home/wutao1/pegasus-release/rdsn/src/runtime/task/task.cpp:176
#21 0x00007fa415703b0a in dsn::task_worker::loop (this=0x2091c80) at /home/wutao1/pegasus-release/rdsn/src/runtime/task/task_worker.cpp:211
#22 0x00007fa415703d30 in dsn::task_worker::run_internal (this=0x2091c80) at /home/wutao1/pegasus-release/rdsn/src/runtime/task/task_worker.cpp:191
#23 0x00007fa411d0a1af in std::execute_native_thread_routine (__p=0x27cf8a0) at ../../../../../libstdc++-v3/src/c++11/thread.cc:83
#24 0x00000037eea07851 in start_thread () from /lib64/libpthread.so.0
#25 0x00000037ee6e811d in clone () from /lib64/libc.so.6
```
the related code: [code](https://github.com/apache/incubator-pegasus/blob/v2.2.0-mi-beta2/src/server/pegasus_server_impl.cpp#L2777)
and when restart, it report new core：
```
0x00000037ee6328a5 in raise () from /lib64/libc.so.6
#1  0x00000037ee634085 in abort () from /lib64/libc.so.6
#2  0x00007f17dc4ee395 in __gnu_cxx::__verbose_terminate_handler () at ../../../../libstdc++-v3/libsupc++/vterminate.cc:95
#3  0x00007f17dc4ec166 in __cxxabiv1::__terminate (handler=<optimized out>) at ../../../../libstdc++-v3/libsupc++/eh_terminate.cc:47
#4  0x00007f17dc4eb129 in __cxa_call_terminate (ue_header=ue_header@entry=0x45f171820) at ../../../../libstdc++-v3/libsupc++/eh_call.cc:54
#5  0x00007f17dc4eba98 in __cxxabiv1::__gxx_personality_v0 (version=<optimized out>, actions=6, exception_class=5138137972254386944, ue_header=0x45f171820, context=<optimized out>)
    at ../../../../libstdc++-v3/libsupc++/eh_personality.cc:676
#6  0x00000037f1210323 in ?? () from /lib64/libgcc_s.so.1
#7  0x00000037f12106ad in _Unwind_RaiseException () from /lib64/libgcc_s.so.1
#8  0x00007f17dc4ec3e6 in __cxxabiv1::__cxa_throw (obj=obj@entry=0x45f171840, tinfo=tinfo@entry=0x7f17dc7d3ae0 <typeinfo for std::system_error>, dest=dest@entry=0x7f17dc516e80 <std::system_error::~system_error()>)
    at ../../../../libstdc++-v3/libsupc++/eh_throw.cc:88
#9  0x00007f17dc51714e in std::__throw_system_error (__i=35) at ../../../../../libstdc++-v3/src/c++11/system_error.cc:337
#10 0x00007f17dc51741c in std::thread::join (this=0x55d7a4b08) at ../../../../../libstdc++-v3/src/c++11/thread.cc:139
#11 0x00000000009f23a1 in pegasus::server::pegasus_io_service::~pegasus_io_service (this=0x100a440 <dsn::utils::singleton<pegasus::server::pegasus_io_service>::instance()::_instance>, __in_chrg=<optimized out>)
    at /home/wutao1/pegasus-release/src/reporter/pegasus_io_service.h:52
#12 0x00000037ee635db2 in exit () from /lib64/libc.so.6
#13 0x0000000000a55345 in event_exit ()
#14 0x0000000000a556c7 in event_sock_err ()
#15 0x0000000000a55ce3 in evsig_init_ ()
#16 0x0000000000a56ba8 in poll_init ()
#17 0x0000000000a4f4cd in event_base_new_with_config ()
#18 0x0000000000a4f81b in event_base_new ()
#19 0x00000000009e9835 in pegasus::server::pegasus_counter_reporter::http_post_request (this=<optimized out>, host=..., port=1988, path=..., contentType=..., data=...)
    at /home/wutao1/pegasus-release/src/reporter/pegasus_counter_reporter.cpp:304
#20 0x00000000009e997f in pegasus::server::pegasus_counter_reporter::update_counters_to_falcon (this=this@entry=0xff5940 <dsn::utils::singleton<pegasus::server::pegasus_counter_reporter>::instance()::_instance>, result=..., 
    timestamp=timestamp@entry=1619500564) at /home/wutao1/pegasus-release/src/reporter/pegasus_counter_reporter.cpp:183
#21 0x00000000009e9cd1 in pegasus::server::pegasus_counter_reporter::update (this=this@entry=0xff5940 <dsn::utils::singleton<pegasus::server::pegasus_counter_reporter>::instance()::_instance>)
    at /home/wutao1/pegasus-release/src/reporter/pegasus_counter_reporter.cpp:228
#22 0x00000000009eb8b6 in pegasus::server::pegasus_counter_reporter::on_report_timer (this=0xff5940 <dsn::utils::singleton<pegasus::server::pegasus_counter_reporter>::instance()::_instance>, timer=..., ec=...)
    at /home/wutao1/pegasus-release/src/reporter/pegasus_counter_reporter.cpp:350
#23 0x00000000009f5a4c in __invoke_impl<void, void (pegasus::server::pegasus_counter_reporter::*&)(std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, boost::system::error_code const&), pegasus::server::pegasus_counter_reporter*&, std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime, boost::asio::time_traits<boost::posix_time::ptime> > >&, boost::system::error_code const&> (__t=<optimized out>, __f=
    @0x7f16f351cbb0: (void (pegasus::server::pegasus_counter_reporter::*)(pegasus::server::pegasus_counter_reporter * const, std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime, boost::asio::time_traits<boost::posix_time::ptime> > >, const boost::system::error_code &)) 0x9eb890 <pegasus::server::pegasus_counter_reporter::on_report_timer(std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime, boost::asio::time_traits<boost::posix_time::ptime> > >, boost::system::error_code const&)>) at /home/wutao1/gcc7/include/c++/7.5.0/bits/invoke.h:73
#24 __invoke<void (pegasus::server::pegasus_counter_reporter::*&)(std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, boost::system::error_code const&), pegasus::server::pegasus_counter_reporter*&, std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime, boost::asio::time_traits<boost::posix_time::ptime> > >&, boost::system::error_code const&> (__fn=
    @0x7f16f351cbb0: (void (pegasus::server::pegasus_counter_reporter::*)(pegasus::server::pegasus_counter_reporter * const, std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime, boost::asio::time_traits<boost::posix_time::ptime> > >, const boost::system::error_code &)) 0x9eb890 <pegasus::server::pegasus_counter_reporter::on_report_timer(std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime, boost::asio::time_traits<boost::posix_time::ptime> > >, boost::system::error_code const&)>) at /home/wutao1/gcc7/include/c++/7.5.0/bits/invoke.h:95
#25 __call<void, boost::system::error_code const&, 0, 1, 2> (__args=<optimized out>, this=0x7f16f351cbb0) at /home/wutao1/gcc7/include/c++/7.5.0/functional:467
#26 operator()<const boost::system::error_code&> (this=0x7f16f351cbb0) at /home/wutao1/gcc7/include/c++/7.5.0/functional:551
#27 operator() (this=0x7f16f351cbb0) at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/boost/asio/detail/bind_handler.hpp:65
#28 asio_handler_invoke<boost::asio::detail::binder1<std::_Bind<void (pegasus::server::pegasus_counter_reporter::*(pegasus::server::pegasus_counter_reporter*, std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, std::_Placeholder<1>))(std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, const boost::system::error_code&)>, boost::system::error_code> > (function=...)
    at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/boost/asio/handler_invoke_hook.hpp:69
#29 invoke<boost::asio::detail::binder1<std::_Bind<void (pegasus::server::pegasus_counter_reporter::*(pegasus::server::pegasus_counter_reporter*, std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, std::_Placeholder<1>))(std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, const boost::system::error_code&)>, boost::system::error_code>, std::_Bind<void (pegasus::server::pegasus_counter_reporter::*(pegasus::server::pegasus_counter_reporter*, std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, std::_Placeholder<1>))(std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, const boost::system::error_code&)> > (context=..., function=...) at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/boost/asio/detail/handler_invoke_helpers.hpp:37
#30 complete<boost::asio::detail::binder1<std::_Bind<void (pegasus::server::pegasus_counter_reporter::*(pegasus::server::pegasus_counter_reporter*, std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, std::_Placeholder<1>))(std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, const boost::system::error_code&)>, boost::system::error_code> > (this=<synthetic pointer>, handler=..., function=...)
    at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/boost/asio/detail/handler_work.hpp:82
#31 boost::asio::detail::wait_handler<std::_Bind<void (pegasus::server::pegasus_counter_reporter::*(pegasus::server::pegasus_counter_reporter*, std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime, boost::asio:

#33 do_run_one (ec=..., this_thread=..., lock=..., this=0x428704960) at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/boost/asio/detail/impl/scheduler.ipp:401
#34 run (ec=..., this=0x428704960) at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/boost/asio/detail/impl/scheduler.ipp:154
#35 run (this=<optimized out>) at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/boost/asio/impl/io_context.ipp:62
#36 operator() (__closure=<optimized out>) at /home/wutao1/pegasus-release/src/reporter/pegasus_io_service.h:43
#37 __invoke_impl<void, pegasus::server::pegasus_io_service::pegasus_io_service()::<lambda()> > (__f=<optimized out>) at /home/wutao1/gcc7/include/c++/7.5.0/bits/invoke.h:60
#38 __invoke<pegasus::server::pegasus_io_service::pegasus_io_service()::<lambda()> > (__fn=<optimized out>) at /home/wutao1/gcc7/include/c++/7.5.0/bits/invoke.h:95
#39 _M_invoke<0> (this=<optimized out>) at /home/wutao1/gcc7/include/c++/7.5.0/thread:234
#40 operator() (this=<optimized out>) at /home/wutao1/gcc7/include/c++/7.5.0/thread:243
#41 std::thread::_State_impl<std::thread::_Invoker<std::tuple<pegasus::server::pegasus_io_service::pegasus_io_service()::{lambda()#1}> > >::_M_run() (this=<optimized out>) at /home/wutao1/gcc7/include/c++/7.5.0/thread:186
#42 0x00007f17dc5171af in std::execute_native_thread_routine (__p=0x1341ccb90) at ../../../../../libstdc++-v3/src/c++11/thread.cc:83
#43 0x00000037eea07851 in start_thread () from /lib64/libpthread.so.0
#44 0x00000037ee6e811d in clone () from /lib64/libc.so.6
```
related code: [code](https://github.com/apache/incubator-pegasus/blob/v2.2.0-mi-beta2/src/reporter/pegasus_counter_reporter.cpp#L297)

and restart again, it report new core again:
```
#0  0x00000037ee6328a5 in raise () from /lib64/libc.so.6
#1  0x00000037ee634085 in abort () from /lib64/libc.so.6
#2  0x00007fa60842f50e in dsn_coredump () at /home/wutao1/pegasus-release/rdsn/src/runtime/service_api_c.cpp:94
#3  0x00007fa60826e77a in dsn::replication::mutation_log::mark_new_offset (this=this@entry=0x3dd464c00, size=size@entry=0, create_new_log_if_needed=create_new_log_if_needed@entry=true)
    at /home/wutao1/pegasus-release/rdsn/src/replica/mutation_log.cpp:826
#4  0x00007fa608270ac2 in dsn::replication::mutation_log_private::append(dsn::ref_ptr<dsn::replication::mutation>&, dsn::task_code, dsn::task_tracker*, std::function<void (dsn::error_code, unsigned long)>&&, int, long*) (
    this=0x3dd464c00, mu=..., callback_code=..., tracker=<optimized out>, callback=<optimized out>, hash=0, pending_size=0x0) at /home/wutao1/pegasus-release/rdsn/src/replica/mutation_log.cpp:223
#5  0x00007fa6082c0228 in dsn::replication::replica::replay_mutation (this=0xc1b93000, mu=..., is_private=<optimized out>) at /home/wutao1/pegasus-release/rdsn/src/replica/replica_init.cpp:405
#6  0x00007fa60826d668 in operator() (__args#1=..., __args#0=<optimized out>, this=0x8d2933718) at /home/wutao1/gcc7/include/c++/7.5.0/bits/std_function.h:706
#7  operator() (mu=..., log_length=<optimized out>, __closure=0x8d2933710) at /home/wutao1/pegasus-release/rdsn/src/replica/mutation_log.cpp:649
#8  std::_Function_handler<bool(int, dsn::ref_ptr<dsn::replication::mutation>&), dsn::replication::mutation_log::open(dsn::replication::mutation_log::replay_callback, dsn::replication::mutation_log::io_failure_callback, const std::map<dsn::gpid, long int>&)::<lambda(int, dsn::replication::mutation_ptr&)> >::_M_invoke(const std::_Any_data &, <unknown type in /home/work/app/pegasus/c4tst-bulkload/replica/package/bin/libdsn_replica_server.so, CU 0x4383fd, DIE 0x582d73>, dsn::ref_ptr<dsn::replication::mutation> &) (__functor=..., __args#0=<optimized out>, __args#1=...) at /home/wutao1/gcc7/include/c++/7.5.0/bits/std_function.h:302
#9  0x00007fa608277c02 in operator() (__args#1=..., __args#0=<optimized out>, this=0x7fa5e8c09fb0) at /home/wutao1/gcc7/include/c++/7.5.0/bits/std_function.h:706
#10 dsn::replication::mutation_log::replay_block(dsn::ref_ptr<dsn::replication::log_file>&, std::function<bool (int, dsn::ref_ptr<dsn::replication::mutation>&)>&, unsigned long, long&) (log=..., callback=..., 
    start_offset=<optimized out>, end_offset=@0x7fa5e8c0a068: 1704141) at /home/wutao1/pegasus-release/rdsn/src/replica/mutation_log_replay.cpp:94
#11 0x00007fa60827803c in dsn::replication::mutation_log::replay(dsn::ref_ptr<dsn::replication::log_file>, std::function<bool (int, dsn::ref_ptr<dsn::replication::mutation>&)>, long&) (log=..., callback=..., 
    end_offset=@0x7fa5e8c0a068: 1704141) at /home/wutao1/pegasus-release/rdsn/src/replica/mutation_log_replay.cpp:30
#12 0x00007fa608278356 in dsn::replication::mutation_log::replay(std::map<int, dsn::ref_ptr<dsn::replication::log_file>, std::less<int>, std::allocator<std::pair<int const, dsn::ref_ptr<dsn::replication::log_file> > > >&, std::function<bool (int, dsn::ref_ptr<dsn::replication::mutation>&)>, long&) (logs=..., callback=..., end_offset=@0x7fa5e8c0a068: 1704141) at /home/wutao1/pegasus-release/rdsn/src/replica/mutation_log_replay.cpp:161
#13 0x00007fa60826b6d8 in dsn::replication::mutation_log::open(std::function<bool (int, dsn::ref_ptr<dsn::replication::mutation>&)>, std::function<void (dsn::error_code)>, std::map<dsn::gpid, long, std::less<dsn::gpid>, std::allocator<std::pair<dsn::gpid const, long> > > const&) (this=0x207b680, read_callback=..., write_error_callback=..., replay_condition=...) at /home/wutao1/pegasus-release/rdsn/src/replica/mutation_log.cpp:643
#14 0x00007fa6082eb791 in dsn::replication::replica_stub::initialize (this=0x1faa300, opts=..., clear=clear@entry=false) at /home/wutao1/pegasus-release/rdsn/src/replica/replica_stub.cpp:571
#15 0x00007fa608320c80 in dsn::replication::replication_service_app::start (this=this@entry=0x273b680, args=...) at /home/wutao1/pegasus-release/rdsn/src/replica/replication_service_app.cpp:64
#16 0x0000000000541b89 in pegasus::server::pegasus_replication_service_app::start (this=0x273b680, args=...) at /home/wutao1/pegasus-release/src/server/pegasus_service_app.h:46
#17 0x00007fa608436525 in dsn::service_node::start_app (this=0x20426d0) at /home/wutao1/pegasus-release/rdsn/src/runtime/service_engine.cpp:73
#18 0x00007fa608440dff in dsn::service_control_task::exec (this=<optimized out>) at /home/wutao1/pegasus-release/rdsn/src/runtime/tool_api.cpp:60
#19 0x00007fa6084735f1 in dsn::task::exec_internal (this=this@entry=0x225d0a0) at /home/wutao1/pegasus-release/rdsn/src/runtime/task/task.cpp:176
#20 0x00007fa608489b0a in dsn::task_worker::loop (this=0x2057500) at /home/wutao1/pegasus-release/rdsn/src/runtime/task/task_worker.cpp:211
#21 0x00007fa608489d30 in dsn::task_worker::run_internal (this=0x2057500) at /home/wutao1/pegasus-release/rdsn/src/runtime/task/task_worker.cpp:191
#22 0x00007fa604a901af in std::execute_native_thread_routine (__p=0x2794620) at ../../../../../libstdc++-v3/src/c++11/thread.cc:83
#23 0x00000037eea07851 in start_thread () from /lib64/libpthread.so.0
#24 0x00000037ee6e811d in clone () from /lib64/libc.so.6
```
and log is：
```
E2021-04-27 13:37:25.614 (1619501845614965417 121781) replica.default0.0000db4d00010001: native_linux_aio_provider.cpp:41:open(): create file failed, err = Too many open files
W2021-04-27 13:37:25.615 (1619501845615014676 121781) replica.default0.0000db4d00010001: log_file.cpp:140:create_write(): create log /home/work/ssd12/pegasus/c4tst-bulkload/replica/reps/17.1.pegasus/plog/log.4.223008 failed
E2021-04-27 13:37:25.615 (1619501845615018869 121781) replica.default0.0000db4d00010001: mutation_log.cpp:718:create_new_log_file(): cannot create log file with index 4
F2021-04-27 13:37:25.615 (1619501845615030329 121781) replica.default0.0000db4d00010001: mutation_log.cpp:829:mark_new_offset(): assertion expression: ec == ERR_OK
F2021-04-27 13:37:25.615 (1619501845615055879 121781) replica.default0.0000db4d00010001: mutation_log.cpp:829:mark_new_offset(): 17.1 create new log file failed: ERR_FILE_OPERATION_FAILED
```
## new node(named 61.bj)
the first core:
```
#0  0x00007f3cb17c11d7 in raise () from /lib64/libc.so.6
#1  0x00007f3cb17c28c8 in abort () from /lib64/libc.so.6
#2  0x00007f3cb20f635d in __gnu_cxx::__verbose_terminate_handler () at ../../../../libstdc++-v3/libsupc++/vterminate.cc:50
#3  0x00007f3cb20f4166 in __cxxabiv1::__terminate (handler=<optimized out>) at ../../../../libstdc++-v3/libsupc++/eh_terminate.cc:47
#4  0x00007f3cb20f41b1 in std::terminate () at ../../../../libstdc++-v3/libsupc++/eh_terminate.cc:57
#5  0x00007f3cb20f4f0f in __cxxabiv1::__cxa_pure_virtual () at ../../../../libstdc++-v3/libsupc++/pure.cc:50
#6  0x0000000000614c64 in rocksdb::StopWatch::StopWatch(rocksdb::Env*, rocksdb::Statistics*, unsigned int, unsigned long*, bool, bool) ()
#7  0x000000000065f6ab in rocksdb::DBImpl::WriteImpl(rocksdb::WriteOptions const&, rocksdb::WriteBatch*, rocksdb::WriteCallback*, unsigned long*, unsigned long, bool, unsigned long*, unsigned long, rocksdb::PreReleaseCallback*) ()
#8  0x000000000065ee9a in rocksdb::DBImpl::Write(rocksdb::WriteOptions const&, rocksdb::WriteBatch*) ()
#9  0x00000000005a26b1 in pegasus::server::rocksdb_wrapper::write (this=0x259794090, decree=decree@entry=2194385) at /home/wutao1/pegasus-release/src/server/rocksdb_wrapper.cpp:155
#10 0x00000000005971b3 in batch_commit (decree=2194385, this=0x4fd27cd20) at /home/wutao1/pegasus-release/src/server/pegasus_write_service_impl.h:525
#11 pegasus::server::pegasus_write_service::batch_commit (this=0x2b6010c80, decree=2194385) at /home/wutao1/pegasus-release/src/server/pegasus_write_service.cpp:281
#12 0x000000000058ef50 in pegasus::server::pegasus_server_write::on_batched_writes (this=this@entry=0x2a7ca80, requests=requests@entry=0x7f3c87995390, count=count@entry=1)
    at /home/wutao1/pegasus-release/src/server/pegasus_server_write.cpp:101
#13 0x000000000058f6ce in pegasus::server::pegasus_server_write::on_batched_write_requests (this=0x2a7ca80, requests=0x7f3c87995390, count=1, decree=<optimized out>, timestamp=<optimized out>)
    at /home/wutao1/pegasus-release/src/server/pegasus_server_write.cpp:59
#14 0x00007f3cb61cd84c in dsn::replication::replication_app_base::apply_mutation (this=0x5c0e98000, mu=<optimized out>) at /home/wutao1/pegasus-release/rdsn/src/replica/replication_app_base.cpp:522
#15 0x00007f3cb6133451 in dsn::replication::replica::execute_mutation (this=0x21a580000, mu=...) at /home/wutao1/pegasus-release/rdsn/src/replica/replica.cpp:306
#16 0x00007f3cb612ddcd in operator() (__args#0=..., this=0x1e33866b8) at /home/wutao1/gcc7/include/c++/7.5.0/bits/std_function.h:706
#17 dsn::replication::prepare_list::commit (this=this@entry=0x1e3386630, d=2194387, ct=ct@entry=dsn::replication::COMMIT_TO_DECREE_HARD) at /home/wutao1/pegasus-release/rdsn/src/replica/prepare_list.cpp:160
#18 0x00007f3cb612e2fd in dsn::replication::prepare_list::prepare (this=0x1e3386630, mu=..., status=dsn::replication::partition_status::PS_SECONDARY, pop_all_committed_mutations=pop_all_committed_mutations@entry=false)
    at /home/wutao1/pegasus-release/rdsn/src/replica/prepare_list.cpp:92
#19 0x00007f3cb613ad4e in dsn::replication::replica::on_prepare (this=0x21a580000, request=request@entry=0x8474b00) at /home/wutao1/pegasus-release/rdsn/src/replica/replica_2pc.cpp:458
#20 0x00007f3cb61a1fed in dsn::replication::replica_stub::on_prepare (this=0x2114300, request=0x8474b00) at /home/wutao1/pegasus-release/rdsn/src/replica/replica_stub.cpp:1074
#21 0x00007f3cb63238ee in operator() (__args#0=<optimized out>, this=0x5e0e134d0) at /home/wutao1/gcc7/include/c++/7.5.0/bits/std_function.h:706
#22 dsn::rpc_request_task::exec (this=0x5e0e13400) at /home/wutao1/pegasus-release/rdsn/include/dsn/tool-api/task.h:435
#23 0x00007f3cb63245f1 in dsn::task::exec_internal (this=this@entry=0x5e0e13400) at /home/wutao1/pegasus-release/rdsn/src/runtime/task/task.cpp:176
#24 0x00007f3cb633ab0a in dsn::task_worker::loop (this=0x28c4240) at /home/wutao1/pegasus-release/rdsn/src/runtime/task/task_worker.cpp:211
#25 0x00007f3cb633ad30 in dsn::task_worker::run_internal (this=0x28c4240) at /home/wutao1/pegasus-release/rdsn/src/runtime/task/task_worker.cpp:191
#26 0x00007f3cb211f1af in std::execute_native_thread_routine (__p=0x29007e0) at ../../../../../libstdc++-v3/src/c++11/thread.cc:83
#27 0x00007f3cb33e1dc5 in start_thread () from /lib64/libpthread.so.0
#28 0x00007f3cb188373d in clone () from /lib64/libc.so.6
```
the second core after restart:
```
#0  0x00007f9ee69251d7 in raise () from /lib64/libc.so.6
#1  0x00007f9ee69268c8 in abort () from /lib64/libc.so.6
#2  0x00007f9ee725a395 in __gnu_cxx::__verbose_terminate_handler () at ../../../../libstdc++-v3/libsupc++/vterminate.cc:95
#3  0x00007f9ee7258166 in __cxxabiv1::__terminate (handler=<optimized out>) at ../../../../libstdc++-v3/libsupc++/eh_terminate.cc:47
#4  0x00007f9ee7257129 in __cxa_call_terminate (ue_header=ue_header@entry=0x4c21d6e0) at ../../../../libstdc++-v3/libsupc++/eh_call.cc:54
#5  0x00007f9ee7257a98 in __cxxabiv1::__gxx_personality_v0 (version=<optimized out>, actions=6, exception_class=5138137972254386944, ue_header=0x4c21d6e0, context=<optimized out>)
    at ../../../../libstdc++-v3/libsupc++/eh_personality.cc:676
#6  0x00007f9ee6cc0903 in ?? () from /lib64/libgcc_s.so.1
#7  0x00007f9ee6cc0c9b in _Unwind_RaiseException () from /lib64/libgcc_s.so.1
#8  0x00007f9ee72583e6 in __cxxabiv1::__cxa_throw (obj=obj@entry=0x4c21d700, tinfo=tinfo@entry=0x7f9ee753fae0 <typeinfo for std::system_error>, dest=dest@entry=0x7f9ee7282e80 <std::system_error::~system_error()>)
    at ../../../../libstdc++-v3/libsupc++/eh_throw.cc:88
#9  0x00007f9ee728314e in std::__throw_system_error (__i=35) at ../../../../../libstdc++-v3/src/c++11/system_error.cc:337
#10 0x00007f9ee728341c in std::thread::join (this=0x359a64e90) at ../../../../../libstdc++-v3/src/c++11/thread.cc:139
#11 0x00000000009f23a1 in pegasus::server::pegasus_io_service::~pegasus_io_service (this=0x100a440 <dsn::utils::singleton<pegasus::server::pegasus_io_service>::instance()::_instance>, __in_chrg=<optimized out>)
    at /home/wutao1/pegasus-release/src/reporter/pegasus_io_service.h:52
#12 0x00007f9ee6928a49 in __run_exit_handlers () from /lib64/libc.so.6
#13 0x00007f9ee6928a95 in exit () from /lib64/libc.so.6
#14 0x0000000000a55345 in event_exit ()
#15 0x0000000000a556c7 in event_sock_err ()
#16 0x0000000000a55ce3 in evsig_init_ ()
#17 0x0000000000a56ba8 in poll_init ()
#18 0x0000000000a4f4cd in event_base_new_with_config ()
#19 0x0000000000a4f81b in event_base_new ()
#20 0x00000000009e9835 in pegasus::server::pegasus_counter_reporter::http_post_request (this=<optimized out>, host=..., port=1988, path=..., contentType=..., data=...)
    at /home/wutao1/pegasus-release/src/reporter/pegasus_counter_reporter.cpp:304
#21 0x00000000009e997f in pegasus::server::pegasus_counter_reporter::update_counters_to_falcon (this=this@entry=0xff5940 <dsn::utils::singleton<pegasus::server::pegasus_counter_reporter>::instance()::_instance>, result=..., 
    timestamp=timestamp@entry=1619507640) at /home/wutao1/pegasus-release/src/reporter/pegasus_counter_reporter.cpp:183
#22 0x00000000009e9cd1 in pegasus::server::pegasus_counter_reporter::update (this=this@entry=0xff5940 <dsn::utils::singleton<pegasus::server::pegasus_counter_reporter>::instance()::_instance>)
    at /home/wutao1/pegasus-release/src/reporter/pegasus_counter_reporter.cpp:228
#23 0x00000000009eb8b6 in pegasus::server::pegasus_counter_reporter::on_report_timer (this=0xff5940 <dsn::utils::singleton<pegasus::server::pegasus_counter_reporter>::instance()::_instance>, timer=..., ec=...)
    at /home/wutao1/pegasus-release/src/reporter/pegasus_counter_reporter.cpp:350
#24 0x00000000009f5a4c in __invoke_impl<void, void (pegasus::server::pegasus_counter_reporter::*&)(std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, boost::system::error_code const&), pegasus::server::pegasus_counter_reporter*&, std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime, boost::asio::time_traits<boost::posix_time::ptime> > >&, boost::system::error_code const&> (__t=<optimized out>, __f=
    @0x7f9df796fc50: (void (pegasus::server::pegasus_counter_reporter::*)(pegasus::server::pegasus_counter_reporter * const, std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime, boost::asio::time_traits<boost::posix_time::ptime> > >, const boost::system::error_code &)) 0x9eb890 <pegasus::server::pegasus_counter_reporter::on_report_timer(std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime, boost::asio::time_traits<boost::posix_time::ptime> > >, boost::system::error_code const&)>) at /home/wutao1/gcc7/include/c++/7.5.0/bits/invoke.h:73
#25 __invoke<void (pegasus::server::pegasus_counter_reporter::*&)(std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, boost::system::error_code const&), pegasus::server::pegasus_counter_reporter*&, std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime, boost::asio::time_traits<boost::posix_time::ptime> > >&, boost::system::error_code const&> (__fn=
    @0x7f9df796fc50: (void (pegasus::server::pegasus_counter_reporter::*)(pegasus::server::pegasus_counter_reporter * const, std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime, boost::asio::time_traits<boost::posix_time::ptime> > >, const boost::system::error_code &)) 0x9eb890 <pegasus::server::pegasus_counter_reporter::on_report_timer(std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime, boost::asio::time_traits<boost::posix_time::ptime> > >, boost::system::error_code const&)>) at /home/wutao1/gcc7/include/c++/7.5.0/bits/invoke.h:95
#26 __call<void, boost::system::error_code const&, 0, 1, 2> (__args=<optimized out>, this=0x7f9df796fc50) at /home/wutao1/gcc7/include/c++/7.5.0/functional:467
#27 operator()<const boost::system::error_code&> (this=0x7f9df796fc50) at /home/wutao1/gcc7/include/c++/7.5.0/functional:551
#28 operator() (this=0x7f9df796fc50) at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/boost/asio/detail/bind_handler.hpp:65
#29 asio_handler_invoke<boost::asio::detail::binder1<std::_Bind<void (pegasus::server::pegasus_counter_reporter::*(pegasus::server::pegasus_counter_reporter*, std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, std::_Placeholder<1>))(std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, const boost::system::error_code&)>, boost::system::error_code> > (function=...)
    at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/boost/asio/handler_invoke_hook.hpp:69
#30 invoke<boost::asio::detail::binder1<std::_Bind<void (pegasus::server::pegasus_counter_reporter::*(pegasus::server::pegasus_counter_reporter*, std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, std::_Placeholder<1>))(std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, const boost::system::error_code&)>, boost::system::error_code>, std::_Bind<void (pegasus::server::pegasus_counter_reporter::*(pegasus::server::pegasus_counter_reporter*, std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, std::_Placeholder<1>))(std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, const boost::system::error_code&)> > (context=..., function=...) at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/boost/asio/detail/handler_invoke_helpers.hpp:37
#31 complete<boost::asio::detail::binder1<std::_Bind<void (pegasus::server::pegasus_counter_reporter::*(pegasus::server::pegasus_counter_reporter*, std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, std::_Placeholder<1>))(std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime> >, const boost::system::error_code&)>, boost::system::error_code> > (this=<synthetic pointer>, handler=..., function=...)
    at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/boost/asio/detail/handler_work.hpp:82
#32 boost::asio::detail::wait_handler<std::_Bind<void (pegasus::server::pegasus_counter_reporter::*(pegasus::server::pegasus_counter_reporter*, std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime, boost::asio:---Type <return> to continue, or q <return> to quit---bt
:time_traits<boost::posix_time::ptime> > >, std::_Placeholder<1>))(std::shared_ptr<boost::asio::basic_deadline_timer<boost::posix_time::ptime, boost::asio::time_traits<boost::posix_time::ptime> > >, boost::system::error_code const&)> >::do_complete(void*, boost::asio::detail::scheduler_operation*, boost::system::error_code const&, unsigned long) (owner=0x6aa5cdb30, base=<optimized out>)
    at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/boost/asio/detail/wait_handler.hpp:72
#33 0x00000000009f4dfa in complete (bytes_transferred=<optimized out>, ec=..., owner=0x6aa5cdb30, this=<optimized out>) at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/boost/asio/detail/scheduler_operation.hpp:40
#34 do_run_one (ec=..., this_thread=..., lock=..., this=0x6aa5cdb30) at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/boost/asio/detail/impl/scheduler.ipp:401
#35 run (ec=..., this=0x6aa5cdb30) at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/boost/asio/detail/impl/scheduler.ipp:154
#36 run (this=<optimized out>) at /home/wutao1/pegasus-release/rdsn/thirdparty/output/include/boost/asio/impl/io_context.ipp:62
#37 operator() (__closure=<optimized out>) at /home/wutao1/pegasus-release/src/reporter/pegasus_io_service.h:43
#38 __invoke_impl<void, pegasus::server::pegasus_io_service::pegasus_io_service()::<lambda()> > (__f=<optimized out>) at /home/wutao1/gcc7/include/c++/7.5.0/bits/invoke.h:60
#39 __invoke<pegasus::server::pegasus_io_service::pegasus_io_service()::<lambda()> > (__fn=<optimized out>) at /home/wutao1/gcc7/include/c++/7.5.0/bits/invoke.h:95
#40 _M_invoke<0> (this=<optimized out>) at /home/wutao1/gcc7/include/c++/7.5.0/thread:234
#41 operator() (this=<optimized out>) at /home/wutao1/gcc7/include/c++/7.5.0/thread:243
#42 _ZNSt6thread11_State_implINS_8_InvokerISt5tupleIJZN7pegasus6server18pegasus_io_serviceC4EvEUlvE_EEEEE6_M_runEv (this=<optimized out>) at /home/wutao1/gcc7/include/c++/7.5.0/thread:186
#43 0x00007f9ee72831af in std::execute_native_thread_routine (__p=0x75cba3900) at ../../../../../libstdc++-v3/src/c++11/thread.cc:83
#44 0x00007f9ee8545dc5 in start_thread () from /lib64/libpthread.so.0
#45 0x00007f9ee69e773d in clone () from /lib64/libc.so.6
```

4. What version of Pegasus are you using?
[2.2.0-mi-beta3](https://github.com/XiaoMi/rdsn/tree/v2.2.0-mi-beta3)
GCC7","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/726/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/726,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgyNzQ5MDgwMA==,incubator-pegasus,827490800,726,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-04-27T10:15:18Z,2021-04-27T10:15:18Z,"new core：
```
#0  0x00000037ee6328a5 in raise () from /lib64/libc.so.6
#1  0x00000037ee634085 in abort () from /lib64/libc.so.6
#2  0x00007f5ea0c3950e in dsn_coredump () at /home/wutao1/pegasus-release/rdsn/src/runtime/service_api_c.cpp:94
#3  0x000000000056d226 in pegasus::server::pegasus_server_impl::start (this=0x923e17800, argc=<optimized out>, argv=<optimized out>) at /home/wutao1/pegasus-release/src/server/pegasus_server_impl.cpp:1369
#4  0x00007f5ea0b23d59 in dsn::replication::replication_app_base::open (this=this@entry=0x923e17800) at /home/wutao1/pegasus-release/rdsn/src/replica/replication_app_base.cpp:418
#5  0x00007f5ea0b26448 in dsn::replication::replication_app_base::open_internal (this=this@entry=0x923e17800, r=r@entry=0x1c6902000) at /home/wutao1/pegasus-release/rdsn/src/replica/replication_app_base.cpp:339
#6  0x00007f5ea0acaa73 in dsn::replication::replica::init_app_and_prepare_list (this=0x1c6902000, create_new=<optimized out>) at /home/wutao1/pegasus-release/rdsn/src/replica/replica_init.cpp:222
#7  0x00007f5ea0acc49a in dsn::replication::replica::load (stub=stub@entry=0x2aa8300, dir=dir@entry=0x69e8ef630 ""/home/work/ssd2/pegasus/c4tst-bulkload/replica/reps/46.109.pegasus"")
    at /home/wutao1/pegasus-release/rdsn/src/replica/replica_init.cpp:166
#8  0x00007f5ea0af885a in dsn::replication::replica_stub::open_replica (this=0x2aa8300, app=..., id=..., req=..., req2=...) at /home/wutao1/pegasus-release/rdsn/src/replica/replica_stub.cpp:1885
#9  0x00007f5ea0b01feb in __invoke_impl<void, void (dsn::replication::replica_stub::*&)(dsn::app_info const&, dsn::gpid, std::shared_ptr<dsn::replication::group_check_request>, std::shared_ptr<dsn::replication::configuration_update_request>), dsn::replication::replica_stub*&, dsn::app_info&, dsn::gpid&, std::shared_ptr<dsn::replication::group_check_request>&, std::shared_ptr<dsn::replication::configuration_update_request>&> (__t=<optimized out>, 
    __f=<optimized out>) at /home/wutao1/gcc7/include/c++/7.5.0/bits/invoke.h:73
#10 __invoke<void (dsn::replication::replica_stub::*&)(dsn::app_info const&, dsn::gpid, std::shared_ptr<dsn::replication::group_check_request>, std::shared_ptr<dsn::replication::configuration_update_request>), dsn::replication::replica_stub*&, dsn::app_info&, dsn::gpid&, std::shared_ptr<dsn::replication::group_check_request>&, std::shared_ptr<dsn::replication::configuration_update_request>&> (__fn=<optimized out>)
    at /home/wutao1/gcc7/include/c++/7.5.0/bits/invoke.h:95
#11 __call<void, 0, 1, 2, 3, 4> (__args=<optimized out>, this=<optimized out>) at /home/wutao1/gcc7/include/c++/7.5.0/functional:467
#12 operator()<> (this=<optimized out>) at /home/wutao1/gcc7/include/c++/7.5.0/functional:551
#13 std::_Function_handler<void (), std::_Bind<void (dsn::replication::replica_stub::*(dsn::replication::replica_stub*, dsn::app_info, dsn::gpid, std::shared_ptr<dsn::replication::group_check_request>, std::shared_ptr<dsn::replication::configuration_update_request>))(dsn::app_info const&, dsn::gpid, std::shared_ptr<dsn::replication::group_check_request>, std::shared_ptr<dsn::replication::configuration_update_request>)> >::_M_invoke(std::_Any_data const&) (
    __functor=...) at /home/wutao1/gcc7/include/c++/7.5.0/bits/std_function.h:316
#14 0x00007f5ea0c7d5f1 in dsn::task::exec_internal (this=this@entry=0x53d405950) at /home/wutao1/pegasus-release/rdsn/src/runtime/task/task.cpp:176
#15 0x00007f5ea0c93b0a in dsn::task_worker::loop (this=0x2b55c80) at /home/wutao1/pegasus-release/rdsn/src/runtime/task/task_worker.cpp:211
#16 0x00007f5ea0c93d30 in dsn::task_worker::run_internal (this=0x2b55c80) at /home/wutao1/pegasus-release/rdsn/src/runtime/task/task_worker.cpp:191
#17 0x00007f5e9d29a1af in std::execute_native_thread_routine (__p=0x32938a0) at ../../../../../libstdc++-v3/src/c++11/thread.cc:83
#18 0x00000037eea07851 in start_thread () from /lib64/libpthread.so.0
#19 0x00000037ee6e811d in clone () from /lib64/libc.so.6
```

https://github.com/apache/incubator-pegasus/blob/v2.2.0-mi-beta2/src/server/pegasus_server_impl.cpp#L1369","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgyNzQ5MDgwMA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/727,https://api.github.com/repos/apache/incubator-pegasus/issues/727,incubator-pegasus,868642926,727,Support preserving TTL for copy_data,empiredan,743379,Dan Wang,,CLOSED,2021-04-27T09:13:11Z,2021-10-22T09:52:17Z,"## Support preserving TTL for copy_data

### Background

Sometimes our customs demand a migration to new cluster, which inevitably includes copying data. On the other hand, the customs tend to use TTL to support some business logic. Therefore,  missing TTL in the new cluster will impact greatly on the customs' business, for example, the business reads a data that should have disappeared. 

However, currently copy_data has not yet supported preserving TTL, since it first scans user_data without TTL, leaving ttl_seconds 0 for the later async_set.

### Solution

We can add an option for copy_data, for example, ""-k/--preserve-ttl"". With this option, data will be copied to the target cluster with remaining TTL, if any.

### Implementation

A possible implementation is described as below:
* add ""-k/--preserve-ttl"" option for copy_data in shell commands;
* support scanning user_data with TTL from rocksdb data;
* async_set with the remaining TTL in the target cluster
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/727/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/727,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgzNzk1ODQ2MQ==,incubator-pegasus,837958461,727,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2021-05-11T07:05:18Z,2021-05-11T07:05:18Z,"It sounds like a great idea, would you like to implement it in https://github.com/pegasus-kv/admin-cli ?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgzNzk1ODQ2MQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/727,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgzNzk2MzMxNQ==,incubator-pegasus,837963315,727,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-05-11T07:09:23Z,2021-05-11T07:09:23Z,"> It sounds like a great idea, would you like to implement it in https://github.com/pegasus-kv/admin-cli ?

@empiredan cplus shell will be abandoned, we can consider add the feature in admin-cli. However, for  a large number of data, we actually don't suggest use `copy_data` to migrate data, and we will release `bulkload` feature and offer some tools for migrating data","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgzNzk2MzMxNQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/727,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgzNzk5Njg5Ng==,incubator-pegasus,837996896,727,NA,empiredan,743379,Dan Wang,,NA,2021-05-11T07:36:48Z,2021-05-11T07:36:48Z,"@Smityz @Shuo-Jia OK, no problem, and also look forward to the new tools ! By the way, as with admin-cli, should we add a new `copy_data` command in it ? Since I don't find any other command with the similar function currently in admin-cli.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgzNzk5Njg5Ng==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/727,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgzODAyNTQzMg==,incubator-pegasus,838025432,727,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-05-11T07:58:43Z,2021-05-11T07:58:43Z,"@Smityz is developing  `migrating-tools`,  `admin-cli` hasn't support `copy_data`, if you are interested it, welcome to contribute ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgzODAyNTQzMg==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/727,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgzODA1MjEwMA==,incubator-pegasus,838052100,727,NA,empiredan,743379,Dan Wang,,NA,2021-05-11T08:18:30Z,2021-05-11T08:18:30Z,"> @Smityz is developing `migrating-tools`, `admin-cli` hasn't support `copy_data`, if you are interested it, welcome to contribute

OK, I'll implement `copy_data` in admin-cli. To scan the whole value in rocksdb (both user_data and ttl)，I'll also provide PR for pegasus server-side code.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDgzODA1MjEwMA==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/729,https://api.github.com/repos/apache/incubator-pegasus/issues/729,incubator-pegasus,883345548,729,add a new comparator sorted by keys of user view,shenxingwuying,6360122,,,OPEN,2021-05-10T04:30:40Z,2021-05-11T06:04:23Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**

scan with prefix filter very slow，and the rocksdb‘s order of key（rocksdb’s key） is not the same with the order of user key. 
coz rocksdb‘s key sorted by bytes order and bytes with a length。And because of the order is not sorted by user key, the scan interface's semantics  is not same with hbase and others system. 
Maybe someone will have wrong understanding and use the scan interface with wrong understanding.


**Describe the feature you'd like:**
I will add a new customized Comparator, which will sorted by user key.

**Describe alternatives you've considered:**
scan timeout problem， see the discussion at :  https://github.com/apache/incubator-pegasus/issues/723


**Teachability, Documentation, Adoption, Migration Strategy:**

I will support an new option, which will make sure. new cluster suggest use the new Comparator. but If old cluster has data, the comparator not change.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/729/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/734,https://api.github.com/repos/apache/incubator-pegasus/issues/734,incubator-pegasus,899394616,734,Pegasus process cannot exit gracefully,zhangyifan27,19500115,Zhang Yifan,chinazhangyifan@163.com,CLOSED,2021-05-24T07:48:49Z,2021-07-31T03:52:29Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?

Stop/Restart pegasus progress after performing upload files to or download files from HDFS.

2. What did you expect to see?
Pegasus process exit gracefully.

3. What did you see instead?
Core was generated when pegasus process exit.

4. What version of Pegasus are you using?
v2.2.0-mi-beta0, v2.2.0-mi-beta1, v2.2.0-mi-beta2 

A coredump stack related to `task_engine`:
```
(gdb) bt
#0  0x00000000011af632 in std::vector<dsn::task_worker_pool, std::allocator<dsn::task_worker_pool> >::operator[] (this=0x0, __n=1) at /usr/include/c++/7/bits/stl_vector.h:816
#1  0x00000000011af256 in dsn::task_engine::get_pool (this=0x0, code=1) at /home/zyf/pegasus/rdsn/src/runtime/task/task_engine.h:111
#2  0x00000000011b3599 in dsn::task::enqueue (this=0x59182d0) at /home/zyf/pegasus/rdsn/src/runtime/task/task.cpp:384
#3  0x0000000001388d02 in dsn::tools::simple_timer_service::<lambda(const boost::system::error_code&)>::operator()(const boost::system::error_code &) const (
    __closure=0x7f4510c8ce90, ec=…) at /home/zyf/pegasus/rdsn/src/runtime/task/simple_task_queue.cpp:72
#4  0x000000000138af1b in boost::asio::detail::binder1<dsn::tools::simple_timer_service::add_timer(dsn::task)::<lambda(const boost::system::error_code&)>, boost::system::error_code>::operator()(void) (this=0x7f4510c8ce90) at /home/zyf/pegasus/rdsn/thirdparty/output/include/boost/asio/detail/bind_handler.hpp:65
#5  0x000000000138aed1 in boost::asio::asio_handler_invoke<boost::asio::detail::binder1<dsn::tools::simple_timer_service::add_timer(dsn::task)::<lambda(const boost::system::error_code&)>, boost::system::error_code> >(boost::asio::detail::binder1<dsn::tools::simple_timer_service::add_timer(dsn::task)::<lambda(const boost::system::error_code&)>, boost::system::error_code> &, …) (function=…) at /home/zyf/pegasus/rdsn/thirdparty/output/include/boost/asio/handler_invoke_hook.hpp:69
#6  0x000000000138ae42 in boost_asio_handler_invoke_helpers::invoke<boost::asio::detail::binder1<dsn::tools::simple_timer_service::add_timer(dsn::task)::<lambda(const boost::system::error_code&)>, boost::system::error_code>, dsn::tools::simple_timer_service::add_timer(dsn::task)::<lambda(const boost::system::error_code&)> >(boost::asio::detail::binder1<dsn::tools::simple_timer_service::add_timer(dsn::task)::<lambda(const boost::system::error_code&)>, boost::system::error_code> &, dsn::tools::simple_timer_service::<lambda(const boost::system::error_code&)> &) (function=…, context=…) at /home/zyf/pegasus/rdsn/thirdparty/output/include/boost/asio/detail/handler_invoke_helpers.hpp:37
#7  0x000000000138ad3f in boost::asio::detail::handler_work<dsn::tools::simple_timer_service::add_timer(dsn::task)::<lambda(const boost::system::error_code&)>, boost::asio::system_executor>::complete<boost::asio::detail::binder1<dsn::tools::simple_timer_service::add_timer(dsn::task)::<lambda(const boost::system::error_code&)>, boost::system::error_code> >(boost::asio::detail::binder1<dsn::tools::simple_timer_service::add_timer(dsn::task)::<lambda(const boost::system::error_code&)>, boost::system::error_code> &, dsn::tools::simple_timer_service::<lambda(const boost::system::error_code&)> &) (this=0x7f4510c8ce66, function=…, handler=…)
    at /home/zyf/pegasus/rdsn/thirdparty/output/include/boost/asio/detail/handler_work.hpp:82
#8  0x000000000138a98d in boost::asio::detail::wait_handler<dsn::tools::simple_timer_service::add_timer(dsn::task*)::<lambda(const boost::system::error_code&)> >::do_complete(void *, boost::asio::detail::operation *, const boost::system::error_code &, std::size_t) (owner=0x571a3c0, base=0x590ad20)
    at /home/zyf/pegasus/rdsn/thirdparty/output/include/boost/asio/detail/wait_handler.hpp:72
#9  0x0000000001107d84 in boost::asio::detail::scheduler_operation::complete (this=0x590ad20, owner=0x571a3c0, ec=…, bytes_transferred=0)
    at /home/zyf/pegasus/rdsn/thirdparty/output/include/boost/asio/detail/scheduler_operation.hpp:40
#10 0x000000000110edbb in boost::asio::detail::scheduler::do_run_one (this=0x571a3c0, lock=…, this_thread=…, ec=…)
    at /home/zyf/pegasus/rdsn/thirdparty/output/include/boost/asio/detail/impl/scheduler.ipp:401
#11 0x000000000110e28f in boost::asio::detail::scheduler::run (this=0x571a3c0, ec=…) at /home/zyf/pegasus/rdsn/thirdparty/output/include/boost/asio/detail/impl/scheduler.ipp:154
#12 0x0000000001342ca9 in boost::asio::io_context::run (this=0x5888760, ec=…) at /home/zyf/pegasus/rdsn/thirdparty/output/include/boost/asio/impl/io_context.ipp:70
#13 0x00000000013887ae in dsn::tools::simple_timer_service::<lambda()>::operator()(void) const (__closure=0x563c248)
    at /home/zyf/pegasus/rdsn/src/runtime/task/simple_task_queue.cpp:50
#14 0x0000000001389c0b in std::__invoke_impl<void, dsn::tools::simple_timer_service::start()::<lambda()> >(std::__invoke_other, dsn::tools::simple_timer_service::<lambda()> &&) (
    __f=…) at /usr/include/c++/7/bits/invoke.h:60
#15 0x00000000013896bb in std::__invoke<dsn::tools::simple_timer_service::start()::<lambda()> >(dsn::tools::simple_timer_service::<lambda()> &&) (__fn=…)
    at /usr/include/c++/7/bits/invoke.h:95
#16 0x000000000138d684 in std::thread::_Invoker<std::tuple<dsn::tools::simple_timer_service::start()::<lambda()> > >::_M_invoke<0>(std::_Index_tuple<0>) (this=0x563c248)
    at /usr/include/c++/7/thread:234
#17 0x000000000138d5d0 in std::thread::_Invoker<std::tuple<dsn::tools::simple_timer_service::start()::<lambda()> > >::operator()(void) (this=0x563c248)
    at /usr/include/c++/7/thread:243
#18 0x000000000138d57c in std::thread::_State_impl<std::thread::_Invoker<std::tuple<dsn::tools::simple_timer_service::start()::<lambda()> > > >::_M_run(void) (this=0x563c240)
    at /usr/include/c++/7/thread:186
#19 0x00007f452120ab30 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#20 0x00007f45223d06ba in start_thread (arg=0x7f4510c8f700) at pthread_create.c:333
#21 0x00007f45209564dd in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109

```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/734/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/740,https://api.github.com/repos/apache/incubator-pegasus/issues/740,incubator-pegasus,905170006,740,fix duplicating&learning(2/n): plog may be lost on going duplicating when replica re-open and then learn,foreverneverer,23136769,Jiashuo,js982986555@live.com,CLOSED,2021-05-28T10:53:21Z,2021-10-22T09:53:34Z,"https://github.com/apache/incubator-pegasus/issues/693 has report one problem, but it hasn't be reproduced in later many test,  however, I found this follow problem that can be ususlly reproduced after enough test. so I open the issue and focus fixing this problem

## Bug Report

Please answer these questions before submitting your issue. Thanks!

### 1. What did you do?
* dup status is `start`
* set meta lively to balance after adding three node

### 2. What did you see instead?
server crash and the core stack:
```
#0  0x0000003e8d8328a5 in raise () from /lib64/libc.so.6
#1  0x0000003e8d834085 in abort () from /lib64/libc.so.6
#2  0x00007f24bfcf0d4e in dsn_coredump () at /home/jiashuo1/work/pegasus/rdsn/src/runtime/service_api_c.cpp:78
#3  0x00007f24bfac71a3 in dsn::replication::prepare_list::commit (this=this@entry=0x1546d7d90, d=659887, ct=ct@entry=dsn::replication::COMMIT_TO_DECREE_HARD) at /home/jiashuo1/work/pegasus/rdsn/src/replica/prepare_list.cpp:154
#4  0x00007f24bfac7e6a in dsn::replication::prepare_list::prepare (this=0x1546d7d90, mu=..., status=status@entry=dsn::replication::partition_status::PS_INACTIVE, pop_all_committed_mutations=pop_all_committed_mutations@entry=false)
    at /home/jiashuo1/work/pegasus/rdsn/src/replica/prepare_list.cpp:124
#5  0x00007f24bfbf6d93 in dsn::replication::mutation_batch::add (this=0x25cb33bc0, mu=...) at /home/jiashuo1/work/pegasus/rdsn/src/replica/duplication/mutation_batch.cpp:52
#6  0x00007f24bfbf159b in operator() (mu=..., log_bytes_length=2355, __closure=0x7f246d5caaa0) at /home/jiashuo1/work/pegasus/rdsn/src/replica/duplication/load_from_private_log.cpp:124
#7  std::_Function_handler<bool(int, dsn::ref_ptr<dsn::replication::mutation>&), dsn::replication::load_from_private_log::replay_log_block()::<lambda(int, dsn::replication::mutation_ptr&)> >::_M_invoke(const std::_Any_data &, <unknown type in /home/work/app/pegasus/c4tst-tune/replica/package/bin/libdsn_replica_server.so, CU 0x1ff2be0, DIE 0x20e7113>, dsn::ref_ptr<dsn::replication::mutation> &) (__functor=..., __args#0=<optimized out>, __args#1=...)
    at /home/jiashuo1/app/toolchain/gcc540/output/include/c++/5.4.0/functional:1857
#8  0x00007f24bfabb00e in operator() (__args#1=..., __args#0=2355, this=0x7f246d5caaa0) at /home/jiashuo1/app/toolchain/gcc540/output/include/c++/5.4.0/functional:2267
#9  dsn::replication::mutation_log::replay_block(dsn::ref_ptr<dsn::replication::log_file>&, std::function<bool (int, dsn::ref_ptr<dsn::replication::mutation>&)>&, unsigned long, long&) (log=..., callback=..., 
    start_offset=<optimized out>, end_offset=@0x25cb33bb8: 1004176305) at /home/jiashuo1/work/pegasus/rdsn/src/replica/mutation_log_replay.cpp:94
#10 0x00007f24bfbf0b7c in replay_block (end_offset=<optimized out>, start_offset=<optimized out>, 
    callback=<unknown type in /home/work/app/pegasus/c4tst-tune/replica/package/bin/libdsn_replica_server.so, CU 0x1ff2be0, DIE 0x20b8705>, log=...) at /home/jiashuo1/work/pegasus/rdsn/src/replica/mutation_log.h:149
#11 dsn::replication::load_from_private_log::replay_log_block (this=this@entry=0x25cb33b00) at /home/jiashuo1/work/pegasus/rdsn/src/replica/duplication/load_from_private_log.cpp:131
#12 0x00007f24bfbf12ed in dsn::replication::load_from_private_log::run (this=0x25cb33b00) at /home/jiashuo1/work/pegasus/rdsn/src/replica/duplication/load_from_private_log.cpp:68
#13 0x00007f24bfd1bbbd in dsn::task::exec_internal (this=this@entry=0x114855877) at /home/jiashuo1/work/pegasus/rdsn/src/runtime/task/task.cpp:176
#14 0x00007f24bfd3671a in dsn::task_worker::loop (this=0x3040780) at /home/jiashuo1/work/pegasus/rdsn/src/runtime/task/task_worker.cpp:211
#15 0x00007f24bfd368e8 in dsn::task_worker::run_internal (this=0x3040780) at /home/jiashuo1/work/pegasus/rdsn/src/runtime/task/task_worker.cpp:191
#16 0x00007f24bca55fd0 in std::execute_native_thread_routine (__p=<optimized out>) at /home/jiashuo1/app/toolchain/gcc540/objdir/../gcc-5.4.0/libstdc++-v3/src/c++11/thread.cc:84
#17 0x0000003e8dc07851 in start_thread () from /lib64/libpthread.so.0
#18 0x0000003e8d8e811d in clone () from /lib64/libc.so.6

```

```
F2021-04-20 17:01:01.121 (1618909261121014136 26dc7) replica.rep_long1.04050011002af702: prepare_list.cpp:155:commit(): assertion expression: mu != nullptr && mu->is_logged()
F2021-04-20 17:01:01.121 (1618909261121033079 26dc7) replica.rep_long1.04050011002af702: prepare_list.cpp:155:commit(): [mutation_batch@210.59@10.132.5.5:32801] mutation 659887 is missing in prepare list
```

### 3. What version of Pegasus are you using?
[Pegasus 2.1.1](https://github.com/apache/incubator-pegasus/tree/v2.1.1) 
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/740/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/744,https://api.github.com/repos/apache/incubator-pegasus/issues/744,incubator-pegasus,908110245,744,Feature: table level compaction perf-counter,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2021-06-01T09:45:06Z,2021-09-02T06:39:06Z,"## Feature Request
Rocksdb compaction is disk-consuming, cpu consuming operation, sometimes will affect read or write latency. However, compaction currently only has a machine-level counter, we need to add the table-level counter of compaction.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/744/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/744,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM42Ox5N,incubator-pegasus,909844045,744,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2021-09-01T03:26:57Z,2021-09-01T03:26:57Z,@Shuo-Jia why close this issue? could you add some description?,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM42Ox5N/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/744,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM42UM0T,incubator-pegasus,911265043,744,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-09-02T06:39:06Z,2021-09-02T06:39:06Z,#806 resolved,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM42UM0T/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/745,https://api.github.com/repos/apache/incubator-pegasus/issues/745,incubator-pegasus,909019087,745,Bug: Rocksdb flush callback function onFlushCompleted not triggered ,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2021-06-02T02:47:00Z,2021-12-28T11:34:50Z,"## Bug Report
Since Pegasus 2.0.0, we found that rocksdb flush related per-counter is invalid, the value of them are always be 0. 
I did lots of manual tests to reproduce the bug, finally found out that the rocksdb flush callback function `onFlushCompleted` was not triggered if rocksdb option `atomic_flush` is true. The rocksdb version is 6.6.4, I had read related code roughly and had no idea about the bug, and I didn't find any information about this bug in rocksdb issues.
To fix this bug, Pegasus might should modify rocksdb related code and pay attend to about it in recent rocksdb release version.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/745/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/745,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM41ELBR,incubator-pegasus,890286161,745,NA,zhangyifan27,19500115,Zhang Yifan,chinazhangyifan@163.com,NA,2021-07-31T03:54:14Z,2021-07-31T03:54:14Z,Trace this bug in https://github.com/facebook/rocksdb/issues/8495,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM41ELBR/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/745,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM41SsaO,incubator-pegasus,894092942,745,NA,zhangyifan27,19500115,Zhang Yifan,chinazhangyifan@163.com,NA,2021-08-06T08:22:07Z,2021-08-06T08:22:07Z,"Fixed in https://github.com/facebook/rocksdb/commit/0879c240404b00142ba4718f36cd3f2bd537192d, maybe this commit should be cherry-picked into https://github.com/XiaoMi/pegasus-rocksdb or upgrade pegasus-rocksdb to a newer version including this fix.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM41SsaO/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/745,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47WeSk,incubator-pegasus,995746980,745,NA,empiredan,743379,Dan Wang,,NA,2021-12-16T12:01:33Z,2021-12-16T12:01:33Z,"@hycdong @zhangyifan27 thanks for your work on this bug !

Have we had a plan about the future work ? Since flush-related counters are important for us to analyze the performance problem.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47WeSk/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/745,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47ki-_,incubator-pegasus,999436223,745,NA,empiredan,743379,Dan Wang,,NA,2021-12-22T09:48:12Z,2021-12-22T09:48:12Z,https://github.com/facebook/rocksdb/commit/0879c240404b00142ba4718f36cd3f2bd537192d has been cherry-picked int o pegasus-rocksdb as https://github.com/XiaoMi/pegasus-rocksdb/pull/34,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47ki-_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/746,https://api.github.com/repos/apache/incubator-pegasus/issues/746,incubator-pegasus,909027633,746,Feature: hotpot partition detection enhancement,hycdong,17868458,HeYuchen,377710264@qq.com,OPEN,2021-06-02T02:58:22Z,2021-06-02T02:58:22Z,"## Feature Request
Pegasus has already support hotpot partition detection, even the most hot key in this partition. However, it needs some enhancement to make function much stronger, including:
- Use throughput not qps to find the hot partition
- How to decrease hot partition false positive rate
- How to find hot key atomically
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/746/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/747,https://api.github.com/repos/apache/incubator-pegasus/issues/747,incubator-pegasus,909039281,747,Bug: gc shared_log bug,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2021-06-02T03:14:59Z,2022-03-01T02:23:49Z,"## Bug Report
In Pegasus version `2.0.0`, we noticed some problems about shared_log garbage collection. In Pegasus, gc shared_log is executed in replication_long thread pool and is expected to be executed every 30 seconds. However, we found that almost 30 minutes gc function is not called, and the result of gc shared_log is not meeting expections. After gc, there are almost 800 files which is greatly exceed the limit count.

In conclusion, gc shared_log may have following problems:
- gc timer may be blocked and not triggered for long time
- gc shared_log has bug if there are too many log files should be gc
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/747/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/747,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDg3NzA5MTM2OQ==,incubator-pegasus,877091369,747,NA,zhangyifan27,19500115,Zhang Yifan,chinazhangyifan@163.com,NA,2021-07-09T10:41:06Z,2021-07-09T10:41:06Z,"This bug could be reproduced in heavy write workload.
I added some logs and found that sometimes `plog->flush_once()` took several seconds:
```
D2021-07-09 16:04:12.656 (1625817852656548085 36002) replica.rep_long0.04010000000001fa: replica_stub.cpp:1723:on_gc(): start to flush private log for replica 282.87@10.132.5.5:32801
D2021-07-09 16:04:27.360 (1625817867360696895 36002) replica.rep_long0.04010000000001fa: replica_stub.cpp:1725:on_gc(): flush private log finish for replica 282.87@10.132.5.5:32801
```
As a result, the gc task may take a long time:
```
D2021-07-09 16:04:30.950 (1625817870950904989 36002) replica.rep_long0.04010000000001fa: replica_stub.cpp:1864:on_gc(): finish to garbage collection, time_used_ns = 18294669709
```
In `mutation_log_private::flush_internal()`, we need to acquire `_plock`, maybe there is lock contention between `mutation_log_private::flush_internal` and `mutation_log_private::append`.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/MDEyOklzc3VlQ29tbWVudDg3NzA5MTM2OQ==/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/747,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-4Mbl,incubator-pegasus,1054918373,747,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2022-03-01T02:23:47Z,2022-03-01T02:23:47Z,Shared log was removed before. So close it now.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-4Mbl/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/748,https://api.github.com/repos/apache/incubator-pegasus/issues/748,incubator-pegasus,909050511,748,Feature: decrease bulk load ingestion executing time,hycdong,17868458,HeYuchen,377710264@qq.com,OPEN,2021-06-02T03:33:25Z,2022-07-24T14:53:11Z,"## Feature Request
Pegasus has supported bulk load function, rely on [Rocksdb ingestion](https://github.com/facebook/rocksdb/wiki/Creating-and-Ingesting-SST-files). When ingesting data, Rocksdb will reject write request, so Pegasus will also reject user write requests during bulk load ingesting stage. To decrease this time, Pegasus should modify Rocksdb related code and pay attention to Rocksdb recent releases.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/748/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/754,https://api.github.com/repos/apache/incubator-pegasus/issues/754,incubator-pegasus,913026036,754,Feature: support partition split,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2021-06-07T02:07:08Z,2021-10-22T09:50:30Z,"## Feature Request
As [rdsn issue 69](https://github.com/XiaoMi/rdsn/issues/69) shows, partition split is very important for pegasus.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/754/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/754,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM42LNRg,incubator-pegasus,908907616,754,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2021-08-31T05:18:36Z,2021-08-31T05:18:36Z,"The following are new added before release 2.3.0：
- [x]  https://github.com/XiaoMi/rdsn/pull/286
- [x]  https://github.com/XiaoMi/rdsn/pull/291
- [x]  https://github.com/XiaoMi/rdsn/pull/299
- [x]  https://github.com/XiaoMi/rdsn/pull/309
- [x]  https://github.com/XiaoMi/rdsn/pull/319
- [x]  https://github.com/XiaoMi/rdsn/pull/390
- [x]  https://github.com/XiaoMi/rdsn/pull/394
- [x]  https://github.com/XiaoMi/rdsn/pull/399
- [x]  https://github.com/XiaoMi/rdsn/pull/624
- [x]  https://github.com/XiaoMi/rdsn/pull/636
- [x]  https://github.com/XiaoMi/rdsn/pull/637
- [x]  https://github.com/XiaoMi/rdsn/pull/638
- [x]  https://github.com/XiaoMi/rdsn/pull/640
- [x]  https://github.com/XiaoMi/rdsn/pull/641
- [x]  https://github.com/XiaoMi/rdsn/pull/642
- [x]  https://github.com/XiaoMi/rdsn/pull/643
- [x]  https://github.com/XiaoMi/rdsn/pull/645
- [x]  https://github.com/XiaoMi/rdsn/pull/653
- [x]  https://github.com/XiaoMi/rdsn/pull/654
- [x]  https://github.com/XiaoMi/rdsn/pull/675
- [x]  https://github.com/XiaoMi/rdsn/pull/676
- [x]  https://github.com/XiaoMi/rdsn/pull/679
- [x]  https://github.com/XiaoMi/rdsn/pull/681
- [x]  https://github.com/XiaoMi/rdsn/pull/725
- [x]  https://github.com/XiaoMi/rdsn/pull/726
- [x]  https://github.com/XiaoMi/rdsn/pull/729
- [x]  https://github.com/XiaoMi/rdsn/pull/731
- [x]  https://github.com/XiaoMi/rdsn/pull/742
- [x]  https://github.com/XiaoMi/rdsn/pull/745
- [x]  https://github.com/XiaoMi/rdsn/pull/747
- [x] https://github.com/apache/incubator-pegasus/pull/474
- [x] https://github.com/apache/incubator-pegasus/pull/684

The following are new added in release 2.3.0：
- [x]  https://github.com/XiaoMi/rdsn/pull/727
- [x]  https://github.com/XiaoMi/rdsn/pull/743
- [x]  https://github.com/XiaoMi/rdsn/pull/762
- [x]  https://github.com/XiaoMi/rdsn/pull/763
- [x]  https://github.com/XiaoMi/rdsn/pull/764
- [x]  https://github.com/XiaoMi/rdsn/pull/765
- [x]  https://github.com/XiaoMi/rdsn/pull/766
- [x]  https://github.com/XiaoMi/rdsn/pull/767
- [x]  https://github.com/XiaoMi/rdsn/pull/783
- [x]  https://github.com/XiaoMi/rdsn/pull/784
- [x]  https://github.com/XiaoMi/rdsn/pull/786
- [x]  https://github.com/XiaoMi/rdsn/pull/787
- [x]  https://github.com/XiaoMi/rdsn/pull/788
- [x]  https://github.com/XiaoMi/rdsn/pull/789
- [x]  https://github.com/XiaoMi/rdsn/pull/790
- [x]  https://github.com/XiaoMi/rdsn/pull/791
- [x]  https://github.com/XiaoMi/rdsn/pull/795
- [x]  https://github.com/XiaoMi/rdsn/pull/797
- [x]  https://github.com/XiaoMi/rdsn/pull/798
- [x]  https://github.com/XiaoMi/rdsn/pull/877
- [x]  https://github.com/XiaoMi/rdsn/pull/891
- [x] https://github.com/apache/incubator-pegasus/pull/698
- [x] https://github.com/apache/incubator-pegasus/pull/702
- [x] https://github.com/apache/incubator-pegasus/pull/733
- [x] https://github.com/apache/incubator-pegasus/pull/804","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM42LNRg/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/755,https://api.github.com/repos/apache/incubator-pegasus/issues/755,incubator-pegasus,913027237,755,Feature: one time backup,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2021-06-07T02:10:14Z,2021-10-22T09:50:58Z,"## Feature Request
Pegasus now only support cold backup through policy which is useful for periodically backup, but sometimes users just would like to trigger backup once immediately, so we should also support one time backup.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/755/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/755,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM42LF2t,incubator-pegasus,908877229,755,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2021-08-31T03:57:14Z,2021-08-31T03:57:14Z,"Related PR:
- [x] https://github.com/XiaoMi/rdsn/pull/772
- [x] https://github.com/XiaoMi/rdsn/pull/775
- [x] https://github.com/XiaoMi/rdsn/pull/776
- [x] https://github.com/XiaoMi/rdsn/pull/785
- [x] https://github.com/XiaoMi/rdsn/pull/796
- [x] https://github.com/XiaoMi/rdsn/pull/813
- [x] https://github.com/XiaoMi/rdsn/pull/814 
- [x] https://github.com/XiaoMi/rdsn/pull/817
- [x] https://github.com/apache/incubator-pegasus/pull/725
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM42LF2t/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/756,https://api.github.com/repos/apache/incubator-pegasus/issues/756,incubator-pegasus,913057239,756,Introduce new metric API,zhangyifan27,19500115,Zhang Yifan,chinazhangyifan@163.com,CLOSED,2021-06-07T03:39:24Z,2023-05-16T16:22:50Z,"## Feature Request


RFC: https://github.com/apache/incubator-pegasus/blob/master/rfcs/2020-08-27-metric-api.md

An in progress implement: https://github.com/XiaoMi/rdsn/pull/605","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/756/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/756,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47guTt,incubator-pegasus,998434029,756,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2021-12-21T03:11:12Z,2021-12-21T03:11:12Z,"It is welcomed that @empiredan would like to contribute this feature~ 
However, new metric API may adds compatible problems, including collector upgrade, such monitor and other tools. The commits should not be merged into master immediately but https://github.com/XiaoMi/rdsn/tree/prometheus-dev","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47guTt/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/756,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4_IBAK,incubator-pegasus,1059065866,756,NA,empiredan,743379,Dan Wang,,NA,2022-03-04T11:08:18Z,2022-03-04T11:08:18Z,The issue that tracks the implementation and the schedule of the framework is https://github.com/apache/incubator-pegasus/issues/922.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4_IBAK/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/756,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5cYuxS,incubator-pegasus,1549986898,756,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-05-16T16:22:49Z,2023-05-16T16:22:49Z,"duplicated, will be tracked by https://github.com/apache/incubator-pegasus/issues/922","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5cYuxS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/761,https://api.github.com/repos/apache/incubator-pegasus/issues/761,incubator-pegasus,914639710,761,Feature: support cluster load balance,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2021-06-08T09:05:49Z,2021-10-22T09:51:09Z,"## Feature Request
Current Pegasus load balance strategy will only consider table level balance, if all tables are balanced, Pegasus consider it as balanced. However, all tables balanced is not equal to cluster balanced, especially for a large cluster who has lots of small partition tables. When cluster node count is bigger than the table total partition count, this table's partition won't be moved in current load balance strategy.

Reference for kudu's rebalance strategy [rebalance_algo.cc](https://github.com/apache/kudu/blob/1.14.0/src/kudu/rebalance/rebalance_algo.cc), Pegasus also should provide a cluster load balance strategy.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/761/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/761,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM42LRvx,incubator-pegasus,908925937,761,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2021-08-31T06:02:48Z,2021-08-31T06:02:48Z,"Related PR：
- [x] https://github.com/XiaoMi/rdsn/pull/853
- [x] https://github.com/XiaoMi/rdsn/pull/857
- [x] https://github.com/XiaoMi/rdsn/pull/858
- [x] https://github.com/XiaoMi/rdsn/pull/866
- [x] https://github.com/XiaoMi/rdsn/pull/872
- [x] https://github.com/XiaoMi/rdsn/pull/873
- [x] https://github.com/XiaoMi/rdsn/pull/881
- [x] https://github.com/XiaoMi/rdsn/pull/882
- [x] https://github.com/XiaoMi/rdsn/pull/883
- [x] https://github.com/XiaoMi/rdsn/pull/904
- [x] https://github.com/XiaoMi/rdsn/pull/916 ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM42LRvx/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/764,https://api.github.com/repos/apache/incubator-pegasus/issues/764,incubator-pegasus,922330241,764,fix duplicating&learning(4/n): plog in `/learn` path of learning may be lost at start copy plog process when duplicating,foreverneverer,23136769,Jiashuo,js982986555@live.com,OPEN,2021-06-16T08:48:26Z,2021-06-17T02:11:44Z,"## Bug Report
when I fix `reset_from` bug in testing https://github.com/XiaoMi/rdsn/pull/845, I found the `learn plog` may be incomplete. the follow log show the log learned may be incomplete to start at the source(learnee node).

**Learnee:**
Learnee receve learn request and find should step back(""last_max_decree(9217992) vs learn_start_decree(7255649)"" in log, learn_start_decree step back to be equal with last confirmed decree for duplicating), and plog file selected is index 292(""learned files count 1 (292 => 292)"" in log)
```
D2021-06-11 15:40:06.539 (1623397206539476516 119869) replica.replica17.0400d3f800032c69: mutation_log.cpp:1148:get_learn_state(): gpid(265.18) get_learn_state returns false, private logs count 73 (220 => 292), learned files count 1 (292 => 292): learned_file_start_offset(8504932039) >= valid_start_offset(8504932039) && last_max_decree(9217992) > 0 && last_max_decree(9217992) < learn_start_decree(7255649)


D2021-06-11 15:40:06.539 (1623397206539545453 119869) replica.replica17.0400d3f800032c69: replica_learn.cpp:482:on_learn(): 265.18@10.132.5.5:32801: on_learn[0000006c00000002]: learner = 10.132.5.21:32801, choose to learn private logs, because learn_start_decree steps back for duplication
```
the index 292 log infomation(size=33MB):
```
mi mi 33M 6月  11 17:21 log.292.8504932039
```
context as follow, that is to say, the start decree is 9217993, but not 7255649
```
mutation: gpid=265.18, ballot=106, decree=9217993, timestamp=2021-06-11 15:39:42.084, last_committed_decree=9217992
```

**Learnee:**
Learnee receved the response and start copy remote plog file
```
D2021-06-11 15:40:06.540 (1623397206540251920 51589) replica.replica17.0405001100000283: replica_learn.cpp:601:on_learn_reply(): 265.18@10.132.5.21:32801: on_learn_reply_start=>jiashuo_debug[0000006c00000002]: learnee = 10.132.5.5:32801, learn_duration = 0 ms, response_err = ERR_OK, remote_committed_decree = 9220655, prepare_start_decree = -1, learn_type = replication::learn_type::LT_LOG, learned_buffer_size = 5828, learned_file_count = 1, to_decree_included = 9220655, learn_start_decree = 7255649[9217971], current_learning_status = replication::learner_status::LearningWithoutPrepare

D2021-06-11 15:40:06.565 (1623397206565705781 51624) replica.rep_long2.0405001100000285: replica_learn.cpp:1065:on_copy_remote_state_completed(): 265.18@10.132.5.21:32801: on_copy_remote_state_completed[0000006c00000002]: learnee = 10.132.5.5:32801, learn_duration = 26 ms, copy remote state done, err = ERR_OK, copy_file_count = 1, copy_file_size = 3501690, copy_time_used = 20 ms, local_committed_decree = 9217971, app_committed_decree = 9217971, app_durable_decree = 9217971, prepare_start_decree = -1, current_learning_status = replication::learner_status::LearningWithoutPrepare
```
the file size info is(file size only has 3.4MB(same with result of log,  that is ""copy_file_size = 3501690"")):
```
mi mi 3.4M 6月  11 15:51 log.292.8504932039
```
file context is:
```
mutation: gpid=265.18, ballot=106, decree=9217993, timestamp=2021-06-11 15:39:42.084, last_committed_decree=9217992
```

**That is to say**
* the plog selected whic need learned start decree is not equal or less `true start_decree of duplicating`
* learnee's log send remote, but learner receved file size is not equal with origin file

so when replay the log, it will core dump for mutaion loss:
```
E2021-06-11 15:40:06.588 (1623397206588520392 51624) replica.rep_long2.0405001100000285: replica_learn.cpp:1617:apply_learned_state_from_private_log(): [265.18@10.132.5.21:32801] repaly_before=>jiashuo_debug: step back=true, last_commit_decree=9217971, dup=true
D2021-06-11 15:40:06.588 (1623397206588854941 51624) replica.rep_long2.0405001100000285: mutation_log_replay.cpp:36:replay(): start to replay mutation log /home/work/ssd10/pegasus/c4tst-tune/replica/reps/265.18.pegasus/plog/log.292.8504932039, offset = [8504932039, 8508433729), size = 3501690
F2021-06-11 15:40:06.589 (1623397206589160526 51624) replica.rep_long2.0405001100000285: prepare_list.cpp:155:commit(): assertion expression: mu != nullptr && mu->is_logged()
F2021-06-11 15:40:06.589 (1623397206589180244 51624) replica.rep_long2.0405001100000285: prepare_list.cpp:155:commit(): [265.18@10.132.5.21:32801] mutation 9217972 is missing in prepare list
```

I don't know this bug whether caused only by learn or cause by duplicating, mark it and test later

### Vresion
https://github.com/apache/incubator-pegasus/tree/v2.2.0-RC0","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/764/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/765,https://api.github.com/repos/apache/incubator-pegasus/issues/765,incubator-pegasus,922338316,765,fix duplicating&learning(3/n): plog will be lost if decree is `step back` case when duplicating,foreverneverer,23136769,Jiashuo,js982986555@live.com,CLOSED,2021-06-16T08:56:13Z,2021-10-22T09:53:25Z,"## Bug Report
for duplicating, if some log hasn't been confirmed, it will be reserve and send the potential secondary, the case which is named step back. for this case, we need `reset`log files in `/learn` as current plog  file in `/plog`, the origin code https://github.com/XiaoMi/rdsn/blob/v2.2/src/replica/replica_learn.cpp#L1526 point this. However, the reset_from is not been implemented correctly, XiaoMi/rdsn#845 is resolve the case. 

## Version
https://github.com/apache/incubator-pegasus/tree/v2.2.0-RC0","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/765/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/766,https://api.github.com/repos/apache/incubator-pegasus/issues/766,incubator-pegasus,922341256,766,fix duplicating(&learning) problem,foreverneverer,23136769,Jiashuo,js982986555@live.com,OPEN,2021-06-16T08:59:03Z,2023-10-16T15:52:46Z,"duplicating exist some bug, this issue mark the problem:
- [x] #693 

- [x] #740 
XiaoMi/rdsn#838

- [x] #765 
XiaoMi/rdsn#845

- [ ] #764

- [ ] #767 

- [ ] #770

- [x] #783
XiaoMi/rdsn#860

- [x] #784
XiaoMi/rdsn#861

- [x] XiaoMi/rdsn#862
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/766/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/766,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5pMIPK,incubator-pegasus,1764787146,766,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-10-16T15:52:46Z,2023-10-16T15:52:46Z,- [ ] https://github.com/apache/incubator-pegasus/pull/1639,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5pMIPK/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/767,https://api.github.com/repos/apache/incubator-pegasus/issues/767,incubator-pegasus,922366242,767,fix duplicating&learning(5/n): the plog will be gc before comfirmed,foreverneverer,23136769,Jiashuo,js982986555@live.com,OPEN,2021-06-16T09:23:24Z,2021-06-16T09:34:51Z,"## Bug Report
XiaoMi/rdsn#845 is fixing the bug #765, and test show the plog will not be lost when replay in learning, but in latest two test, when start duplicating to be ready replay and send, the log report the `max_gced_decree > start_decree`:
```
F2021-06-16 14:47:25.525 (1623826045525670453 19943) replica.rep_long9.0405001600000040: replica_duplicator.cpp:176:verify_start_decree(): assertion expression: max_gced_decree < start_decree
D2021-06-16 14:47:25.525 (1623826045525692473 19854) replica.default5.0401000400000021: pegasus_mutation_duplicator.cpp:95:pegasus_mutation_duplicator(): [268.20@10.132.5.1:32801] initialize mutation duplicator for local cluster [id:2], remote cluster [id:1, addr:c4tst-dup1]
F2021-06-16 14:47:25.525 (1623826045525701911 19943) replica.rep_long9.0405001600000040: replica_duplicator.cpp:176:verify_start_decree(): the logs haven't yet duplicated were accidentally truncated [max_gced_decree: 2809371, start_decree: 1, confirmed_decree: 0, last_decree: 0]
```

I haven't found the cause
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/767/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/770,https://api.github.com/repos/apache/incubator-pegasus/issues/770,incubator-pegasus,927812158,770,fix duplicating&learning(6/n): unknown core dump when loop restart,foreverneverer,23136769,Jiashuo,js982986555@live.com,OPEN,2021-06-23T03:05:29Z,2021-06-23T03:05:29Z,"## Bug Report
```
#0  remove_all_duplications (this=<optimized out>) at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/src/replica/duplication/replica_duplicator_manager.h:79
#1  update_duplication_map (new_dup_map=..., this=<optimized out>) at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/src/replica/duplication/replica_duplicator_manager.h:32
#2  dsn::replication::duplication_sync_timer::update_duplication_map (this=this@entry=0x1225e5200, dup_map=...) at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/src/replica/duplication/duplication_sync_timer.cpp:86
#3  0x00007fbba15a02db in dsn::replication::duplication_sync_timer::on_duplication_sync_reply (this=0x1225e5200, err=..., resp=...) at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/src/replica/duplication/duplication_sync_timer.cpp:72
#4  0x00007fbba15a03b2 in operator() (err=..., __closure=0x2c4ca8ba0) at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/src/replica/duplication/duplication_sync_timer.cpp:59
#5  operator() (req=<optimized out>, resp=<optimized out>, err=..., __closure=0x2c4ca8ba0) at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/include/dsn/cpp/rpc_holder.h:164
#6  std::_Function_handler<void(dsn::error_code, dsn::message_ex*, dsn::message_ex*), dsn::rpc_holder<TRequest, TResponse>::call(const dsn::rpc_address&, dsn::task_tracker*, TCallback&&, int) [with TCallback = dsn::replication::duplication_sync_timer::run()::<lambda(dsn::error_code)>; TRequest = dsn::replication::duplication_sync_request; TResponse = dsn::replication::duplication_sync_response; dsn::task_ptr = dsn::ref_ptr<dsn::task>]::<lambda(dsn::error_code, dsn::message_ex*, dsn::message_ex*)> >::_M_invoke(const std::_Any_data &, <unknown type in /home/work/app/pegasus/c4tst-tune/replica/package/bin/libdsn_replica_server.so, CU 0x23624f8, DIE 0x24c32d9>, <unknown type in /home/work/app/pegasus/c4tst-tune/replica/package/bin/libdsn_replica_server.so, CU 0x23624f8, DIE 0x24c32de>, <unknown type in /home/work/app/pegasus/c4tst-tune/replica/package/bin/libdsn_replica_server.so, CU 0x23624f8, DIE 0x24c32e3>) (__functor=..., __args#0=<optimized out>, __args#1=<optimized out>, __args#2=<optimized out>)
    at /home/jiashuo1/app/toolchain/gcc540/output/include/c++/5.4.0/functional:1871
#7  0x00007fbba175a417 in operator() (__args#2=0x379ff0f20, __args#1=0x82eca580, __args#0=..., this=<optimized out>) at /home/jiashuo1/app/toolchain/gcc540/output/include/c++/5.4.0/functional:2267
#8  dsn::rpc_response_task::exec (this=<optimized out>) at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/include/dsn/tool-api/task.h:477
#9  0x00007fbba175db61 in dsn::task::exec_internal (this=this@entry=0x217605f00) at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/src/runtime/task/task.cpp:176
#10 0x00007fbba177811a in dsn::task_worker::loop (this=0x1e4b2c0) at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/src/runtime/task/task_worker.cpp:211
#11 0x00007fbba177831d in dsn::task_worker::run_internal (this=0x1e4b2c0) at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/src/runtime/task/task_worker.cpp:191
#12 0x00007fbb9dd30fd0 in std::execute_native_thread_routine (__p=<optimized out>) at /home/jiashuo1/app/toolchain/gcc540/objdir/../gcc-5.4.0/libstdc++-v3/src/c++11/thread.cc:84
#13 0x0000003dc0807851 in start_thread () from /lib64/libpthread.so.0
#14 0x0000003dc04e811d in clone () from /lib64/libc.so.6
```
code url: https://github.com/Shuo-Jia/rdsn/blob/v2.2.1-mi-beta1/src/replica/duplication/replica_duplicator_manager.h#L79
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/770/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/771,https://api.github.com/repos/apache/incubator-pegasus/issues/771,incubator-pegasus,928042070,771,group check failed during ingestion,levy5307,22141103,赵立伟,zlw5307@163.com,OPEN,2021-06-23T09:17:08Z,2021-06-23T09:17:08Z,"## Feature Request
In current implementation, write operations are rejected during ingestion. This will cause the commit failure in group check.
So we should optimize the behavior of group check during ingestion

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/771/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/772,https://api.github.com/repos/apache/incubator-pegasus/issues/772,incubator-pegasus,928115037,772,implement data version 3,levy5307,22141103,赵立伟,zlw5307@163.com,CLOSED,2021-06-23T10:38:45Z,2022-03-01T02:22:14Z,RFC: https://github.com/apache/incubator-pegasus/blob/master/rfcs/2020-10-09-data-version-v3.md,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/772/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/772,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-4MMr,incubator-pegasus,1054917419,772,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2022-03-01T02:22:13Z,2022-03-01T02:22:13Z,https://github.com/apache/incubator-pegasus/tree/data-version-2,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-4MMr/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/773,https://api.github.com/repos/apache/incubator-pegasus/issues/773,incubator-pegasus,928115467,773,implement user specified compaction,levy5307,22141103,赵立伟,zlw5307@163.com,CLOSED,2021-06-23T10:39:17Z,2021-10-22T09:51:20Z,RFC: https://github.com/apache/incubator-pegasus/blob/master/rfcs/2021-05-27-user-specified-compaction.md,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/773/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/773,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM42Lf1Z,incubator-pegasus,908983641,773,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2021-08-31T07:44:06Z,2021-08-31T07:44:06Z,"Related PR：
- [x] https://github.com/XiaoMi/rdsn/pull/849
- [x] https://github.com/apache/incubator-pegasus/pull/731
- [x] https://github.com/apache/incubator-pegasus/pull/738
- [x] https://github.com/apache/incubator-pegasus/pull/741
- [x] https://github.com/apache/incubator-pegasus/pull/749
- [x] https://github.com/apache/incubator-pegasus/pull/750
- [x] https://github.com/apache/incubator-pegasus/pull/751
- [x] https://github.com/apache/incubator-pegasus/pull/760
- [x] https://github.com/apache/incubator-pegasus/pull/768
- [x] https://github.com/apache/incubator-pegasus/pull/769
- [x] https://github.com/apache/incubator-pegasus/pull/776
- [x] https://github.com/apache/incubator-pegasus/pull/780
- [x] https://github.com/apache/incubator-pegasus/pull/814","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM42Lf1Z/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/778,https://api.github.com/repos/apache/incubator-pegasus/issues/778,incubator-pegasus,939629240,778,Add rate limiter for backup request,levy5307,22141103,赵立伟,zlw5307@163.com,CLOSED,2021-07-08T08:52:23Z,2021-10-22T09:51:38Z,"## Feature Request

Previously, we implemented a feature which name is backup request. And we don't add a rate limiter for it. So it will cause jitter if it's qps or throughput is too large.

In the implementation of backup request, it will only accept the first response it receives. Which means if we reject a backup request immediately after server receives it, the response corresponding to the request which sending to primary replica will be ignored. So we can't simply reject the backup request when it is received. 

One way to deal with this problem is adding a rate limiter on client side. But there is a problem that we can't easily change the limiting values according to the actual situation.

In my opinion, we should add rate limiter on server side. And we may don't send a response to the backup request that has been limited. In this way, client will not receive a response corresponding to the backup request, and it can wait for the response from primary replica. ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/778/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/783,https://api.github.com/repos/apache/incubator-pegasus/issues/783,incubator-pegasus,947505133,783,fix duplicating&learning(7/n): plog won't be delete after remove duplication,foreverneverer,23136769,Jiashuo,js982986555@live.com,CLOSED,2021-07-19T10:23:40Z,2021-10-22T09:53:16Z,"## Bug Report

We found if `table` remove duplication, the plog reserved of secondary won't be delete:
```
D2021-07-15 16:57:02.263 (1626339422263108828 58813) replica.replica19.0400e581a5c343ed: replica_check.cpp:161:on_group_check(): [2.45@...] process group check, primary = ..., ballot = 511, status = replication::partition_status::PS_SECONDARY, last_committed_decree = 1312472570, confirmed_decree = -1

D2021-07-15 17:05:27.201 (1626339927201001491 58813) replica.replica19.04050003000001e1: replica_chkpt.cpp:88:on_checkpoint_timer(): [2.45@....] gc_private replication::partition_status::PS_SECONDARY: delay gc for duplication: min_confirmed_decree(1311988516) last_durable_decree(1312482381)
```

As above,  we remove the duplication and then the primary  `confirmed_decree = -1`, but the result don't async to secondaryso that it still keep `min_confirmed_decree(1311988516)`

Check the code https://github.com/XiaoMi/rdsn/blob/v2.2.0/src/replica/replica_check.cpp#L176
```c++
if (request.__isset.confirmed_decree) {
        _duplication_mgr->update_confirmed_decree_if_secondary(request.confirmed_decree);
}
```
and found if the `confirmed_decree < 0`, it won't execute to update for secondary
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/783/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/783,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM40pFdi,incubator-pegasus,883185506,783,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-07-20T08:01:23Z,2021-07-20T08:01:23Z,"I test it and re-produce the bug case:
```
D2021-07-20 15:36:41.127 (1626766601127458749 182139) replica.replica23.0400c73b0011587c: replica_check.cpp:161:on_group_check(): [1.0@....] process group check, primary = ...., ballot = 3, status = replication::partition_status::PS_SECONDARY, last_committed_decree = 714319, confirmed_decree = -1


D2021-07-20 15:46:31.112 (1626767191112107396 182116) replica.replica0.0407000800000004: replica_chkpt.cpp:88:on_checkpoint_timer(): [1.1@....] gc_private replication::partition_status::PS_SECONDARY: delay gc for duplication: min_confirmed_decree(46142) last_durable_decree(670771)
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM40pFdi/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/784,https://api.github.com/repos/apache/incubator-pegasus/issues/784,incubator-pegasus,947519012,784,fix duplicating&learning(8/n): last_commit_decreee can't be updated to within the scope of replay log(start~end),foreverneverer,23136769,Jiashuo,js982986555@live.com,CLOSED,2021-07-19T10:42:17Z,2021-10-22T09:52:59Z,"## Bug Report

We found server will crash for the [code](https://github.com/XiaoMi/rdsn/blob/v2.2.0/src/replica/prepare_list.cpp#L154):
```c++
dassert_replica(
                mu != nullptr && mu->is_logged(), ""mutation {} is missing in prepare list"", d0);
            dcheck_ge_replica(mu->data.header.ballot, last_bt);
```

In the past, it usually happen under learning, see :
 #740 https://github.com/XiaoMi/rdsn/pull/838
 #765 XiaoMi/rdsn#845 
But it also may be the `last_commit_decreee`of duplication is inited to be not right value,  which casue it is out of the `_start_decree`and `_end_decree` when re-build a duplication after remove:
```
D2021-07-17 09:19:09.977 (1626484749977461388 58769) replica.default7.0401000600ce0ff2: replica_duplicator.cpp:47:start_dup(): [2.8@....] starting duplication {""dupid"":1626484748,""status"":""DS_START"",""remote"":""..."",""confirmed"":-1,""app"":""...""} [last_decree: 1233232877, confirmed_decree: -1]

F2021-07-17 09:19:12.897 (1626484752897595383 58847) replica.rep_long2.040500065f0a50eb: prepare_list.cpp:155:commit(): [mutation_batch@2.8@...] mutation 1233229648 is missing in prepare list
```
As above, when start duplication, the `_start_decree` will be inited to  `last_decree = 1233232877`, which is equal with `last_progress`:
```c++
void load_mutation::run()
{
    decree last_decree = _duplicator->progress().last_decree;
    _start_decree = last_decree + 1;
    if (_replica->private_log()->max_commit_on_disk() < _start_decree) {
        // wait 100ms for next try if no mutation was added.
        repeat(100_ms);
        return;
    }

    _log_on_disk->set_start_decree(_start_decree);
    _log_on_disk->async();
}
```
However, the `last_commit_decree` is init to be `confirmed_decree = -1`:
```c++
mutation_batch::mutation_batch(replica_duplicator *r) : replica_base(r)
{
    // Prepend a special tag identifying this is a mutation_batch,
    // so `dxxx_replica` logging in prepare_list will print along with its real caller.
    // This helps for debugging.
    replica_base base(
        r->get_gpid(), std::string(""mutation_batch@"") + r->replica_name(), r->app_name());
    _mutation_buffer =
        make_unique<prepare_list>(&base, 0, PREPARE_LIST_NUM_ENTRIES, [this](mutation_ptr &mu) {
            // committer
            add_mutation_if_valid(mu, _loaded_mutations, _start_decree);
        });

    // start duplication from confirmed_decree
    _mutation_buffer->reset(r->progress().confirmed_decree);
}
``` 
 but the fatal log show the `last_commit_decree=1233229648` which is out of `_stat_decree=last_decree = 1233232877`, so I think it happen error when update `last_commit_decree`in the later logic.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/784/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/784,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM40u6KB,incubator-pegasus,884712065,784,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-07-22T07:33:34Z,2021-07-22T07:33:34Z,"# Bug resolve

### root casue:
* `add_dup {table} {cluster}` without `-f`, means the duplication will start immediately, but not wait duplication-sync with meta-server
* the mutation_buffer will reset to initial value(value=-1) of`confirmed_decree`: 
   **[last_commit_decree=-1, start=-1, end=-1]**
* when prepare, it will reset to equal with current mutation replayed in https://github.com/XiaoMi/rdsn/blob/v2.2.0/src/replica/prepare_list.cpp#L121  
https://github.com/XiaoMi/rdsn/blob/v2.2.0/src/replica/prepare_list.cpp#L130
we assume the current mutation is [last_commit_decree=56, current_decree=100], and the mutation_buffer:**[last_commit_decree=50, start=100, end=100]**
* in the nexp loop, it will commit decree from `last_commit_decree+1=56`, but it out of range of [start~end], and return` null` in code https://github.com/XiaoMi/rdsn/blob/v2.2.0/src/replica/prepare_list.cpp#L152 and dassert false
* if current mutation is [last_commit_decree=99, current_decree=100], it also say the **current_decree - last_commit_decree = 1, it won't casue the error**, since the [code](https://github.com/XiaoMi/rdsn/blob/v2.2.0/src/replica/test/replica_test_base.h#L64)  create a `good plog` so that  the [test code ](https://github.com/XiaoMi/rdsn/blob/v2.2.0/src/replica/duplication/test/load_from_private_log_test.cpp#L91)
don't find the bug
### resolution
force start wait the duplication-sync, and then the confirmed will be set valid value and no longer error","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM40u6KB/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/785,https://api.github.com/repos/apache/incubator-pegasus/issues/785,incubator-pegasus,948528782,785,reduce the number of times of marshall for response,levy5307,22141103,赵立伟,zlw5307@163.com,OPEN,2021-07-20T10:43:19Z,2022-03-01T02:14:37Z,"## Bug Report

Previously, server will reply a response to client if it receives a request, which will marshall the corresponding response.  For a write request from client, all of primary and secondaries will  response for it, which means the response's marshall operation is excuted by three times.  We can do some optimizations for this situation, and don't marshall response for secondaries. 

Given the marshall is heavy cost in cpu and mem, it will improve the throughput of our cluster if we implement these optimizations.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/785/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/786,https://api.github.com/repos/apache/incubator-pegasus/issues/786,incubator-pegasus,949238488,786,Feature enhancement: drop some client requests if its in-queue time exceed client timeout,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2021-07-21T01:32:45Z,2021-10-22T09:53:46Z,"## Feature Request

When the client requests sent into Pegasus, it will firstly be in queue, then execution. When server load added, sometimes in-queue time may shortly increase. If the request's in-queue time exceed the client timeout(recorded in request header), those requests can be dropped, it will be timeout finnally, no need to be executed. 

We plan to add this config for following RPC code:
```
RPC_RRDB_RRDB_PUT
RPC_RRDB_RRDB_MULTI_PUT
RPC_RRDB_RRDB_REMOVE
RPC_RRDB_RRDB_MULTI_REMOVE
RPC_RRDB_RRDB_DUPLICATE
RPC_RRDB_RRDB_GET
RPC_RRDB_RRDB_MULTI_GET
RPC_RRDB_RRDB_TTL
```
Those RPC codes are not add it:
```
// not_idempotent write task codes won't added
RPC_RRDB_RRDB_INCR
RPC_RRDB_RRDB_CHECK_AND_SET
RPC_RRDB_RRDB_CHECK_AND_MUTATE
// scan related rpc codes won't added
RPC_RRDB_RRDB_SORTKEY_COUNT
RPC_RRDB_RRDB_GET_SCANNER
RPC_RRDB_RRDB_SCAN
RPC_RRDB_RRDB_CLEAR_SCANNER
```

The following perf-counters will be added:
```
zion*profiler*RPC_RRDB_RRDB_PUT.rpc.dropped
zion*profiler*RPC_RRDB_RRDB_MULTI_PUT.rpc.dropped
zion*profiler*RPC_RRDB_RRDB_REMOVE.rpc.dropped
zion*profiler*RPC_RRDB_RRDB_MULTI_REMOVE.rpc.dropped
zion*profiler*RPC_RRDB_RRDB_DUPLICATE.rpc.dropped
zion*profiler*RPC_RRDB_RRDB_GET.rpc.dropped
zion*profiler*RPC_RRDB_RRDB_MULTI_GET.rpc.dropped
zion*profiler*RPC_RRDB_RRDB_TTL.rpc.dropped
```



","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/786/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/787,https://api.github.com/repos/apache/incubator-pegasus/issues/787,incubator-pegasus,951181732,787,Feature: reject user write requests when disk space is insufficient,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2021-07-23T01:47:27Z,2021-10-22T09:51:51Z,"## Feature Request

When disk space is full, any write will lead to coredump. So when the disk space is insufficient, we should reject user write requests for safety. 
If one disk available space is below the threshold, all replica on this disk will reject user write request.
This feature is implemented by [rdsn#833](https://github.com/XiaoMi/rdsn/pull/833), [rdsn#851](https://github.com/XiaoMi/rdsn/pull/851).

```diff
[replication]
+reject_write_when_disk_insufficient=true
+disk_min_available_space_ratio=10
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/787/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/788,https://api.github.com/repos/apache/incubator-pegasus/issues/788,incubator-pegasus,951188007,788,Feature: support broken disk check while starting and adding disk dynamically,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2021-07-23T02:03:24Z,2021-10-22T09:52:02Z,"## Feature Request

In current implementation, if a disk is broken, replica server will core, and server can't be started automically. It can be started after adding the broken disk path into black_list manually. If the disk repaired, we should stop replica server instance firstly, then remove the disk path from black_list, then restart the instance. This is not convenient when broken disk happened.

[rdsn#834](https://github.com/XiaoMi/rdsn/pull/834) supports disk check while initialization, if a disk is broken, its path will be excluded and replica instance can start succeed.
[rdsn#839](https://github.com/XiaoMi/rdsn/pull/839) supports a rpc for adding a new disk, no need to restart the replica instance. 
NOTICE: if you add a totally new disk, not in the config files, you should add this disk path and tag in data_dir configuration.

```diff
[replication]
+ignore_broken_disk=true
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/788/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/789,https://api.github.com/repos/apache/incubator-pegasus/issues/789,incubator-pegasus,951193285,789,Feature enhancement: support release all tcmalloc reserved not-used memory,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2021-07-23T02:15:55Z,2021-10-22T09:53:40Z,"## Feature Request

Pegasus now support trying to release tcmalloc reserved not-used memory periodically. We define a max-reserved percentage, if the reserved not-used memory exceed the threshold, server will release the exceeded memory back to operating system. However, sometimes memory increase repaidly before the check triggered. So we need to add a command to release all reserved not-used memory immediately when system memory is seriously short.
NOTICE: Strongly recommend using this command when seriously lack of memory, releasing all reserved memory may affect performance.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/789/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/794,https://api.github.com/repos/apache/incubator-pegasus/issues/794,incubator-pegasus,952614829,794,Suspected memory leak when client reconnect to servers,Smityz,22953824,Smilencer,smityz@qq.com,CLOSED,2021-07-26T07:56:12Z,2021-11-08T05:07:55Z,"![Screenshot from 2021-07-26 14-13-03](https://user-images.githubusercontent.com/22953824/126952537-e9247cb0-a77f-4d69-9f61-7db5744c5011.png)
Memory has increased dramatically when clients reconnect to servers
```
D2021-07-25 12:24:34.347 (1627187074347482202 27a12) replica.io-thrd.162322: network.cpp:647:on_server_session_disconnected(): client ip  has still 1 of sessions to this server
D2021-07-25 12:24:34.349 (1627187074349986469 27a12) replica.io-thrd.162322: asio_rpc_session.cpp:95:operator()(): asio read fromfailed: End of file
D2021-07-25 12:24:34.350 (1627187074350015886 27a12) replica.io-thrd.162322: network.cpp:638:on_server_session_disconnected(): session  disconnected, the total client sessions count remains 4571
D2021-07-25 12:24:34.350 (1627187074350024918 27a12) replica.io-thrd.162322: network.cpp:647:on_server_session_disconnected(): client ip  has still 1 of sessions to this server
D2021-07-25 12:24:34.350 (1627187074350850176 27a14) replica.io-thrd.162324: asio_rpc_session.cpp:95:operator()(): asio read from  failed: End of file
D2021-07-25 12:24:34.350 (1627187074350881639 27a14) replica.io-thrd.162324: network.cpp:638:on_server_session_disconnected(): session  disconnected, the total client sessions count remains 4570
```
but QPS and CU are not increasing
![Screenshot from 2021-07-26 15-55-26](https://user-images.githubusercontent.com/22953824/126953506-9a4e0f86-ef38-4b2e-b244-065c8cbbd01a.png)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/794/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/795,https://api.github.com/repos/apache/incubator-pegasus/issues/795,incubator-pegasus,955387035,795,Feature: add an env to enable or disable block cache of an app,ZhongChaoqiang,35595648,Zhong Chaoqiang,,CLOSED,2021-07-29T02:07:59Z,2021-07-31T02:19:31Z,"Sometimes, there are some apps of different business in a cluster.Some apps want to enable block cache, others my be not.
So we need add an env to enable or disable block cache of an app.

We control it by fill_cache option of rocksdb::ReadOptions.
If fill_cache is true, when reading data from rocksdb, the data block will be filled into cache.
If fill_cache change to false,the data block will not be filled into cache.But we do not need to clear the block of cached.When block cache is full, the oldest block will be remove by LRU.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/795/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/796,https://api.github.com/repos/apache/incubator-pegasus/issues/796,incubator-pegasus,955597113,796,change secondary commit to async execute in background thread,levy5307,22141103,赵立伟,zlw5307@163.com,CLOSED,2021-07-29T08:32:45Z,2022-03-01T02:13:00Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
In previous implementation, secondary will commit in next prepare request, which will increase latency of prepare request. I think we should add a rpc runs in background to do commit work, to decrease the latency of write request.

For more details: https://levy5307.github.io/blog/2pc/#pacifica
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/796/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/796,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM41i4l7,incubator-pegasus,898337147,796,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-08-13T09:50:39Z,2021-08-13T09:50:39Z,"I agree, however,  its implementation may be more simple but no background task: we just need put [prepare](https://github.com/XiaoMi/rdsn/blob/master/src/replica/replica_2pc.cpp#L488) after the `ack_prepare_message`, for [example](https://github.com/XiaoMi/rdsn/blob/7e3eab786fb5e0675d84eac22d5508b03538d5f4/src/replica/replica_2pc.cpp#L592):
```diff
    if (err == ERR_OK && status() != partition_status::PS_ERROR) {
        _private_log->append(mu, LPC_WRITE_REPLICATION_LOG_COMMON, &_tracker, nullptr);
    }
+ prepre.....
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM41i4l7/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/796,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM41i-Lm,incubator-pegasus,898360038,796,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2021-08-13T10:29:38Z,2021-08-13T10:29:38Z,"> I agree, however, its implementation may be more simple but no background task: we just need put [prepare](https://github.com/XiaoMi/rdsn/blob/master/src/replica/replica_2pc.cpp#L488) after the `ack_prepare_message`, for [example](https://github.com/XiaoMi/rdsn/blob/7e3eab786fb5e0675d84eac22d5508b03538d5f4/src/replica/replica_2pc.cpp#L592):
> 
> ```diff
>     if (err == ERR_OK && status() != partition_status::PS_ERROR) {
>         _private_log->append(mu, LPC_WRITE_REPLICATION_LOG_COMMON, &_tracker, nullptr);
>     }
> + prepre.....
> ```

Sounds reasonable :）","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM41i-Lm/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/797,https://api.github.com/repos/apache/incubator-pegasus/issues/797,incubator-pegasus,956564515,797,Feature enhancement: throttling by the count of kvs,ZhongChaoqiang,35595648,Zhong Chaoqiang,,OPEN,2021-07-30T09:16:08Z,2021-08-04T08:57:42Z,"Now, write request can be throttled by qps or size, read request can be throttled by qps.
But there are some weaknesses of qps or size:

- If throttling by qps,one request may contain different size of data, it's hard to control the pressure.
- If throttling by size, it's difficult for users to calculate the required quota, and it can't be used for read request.

Therefore,we added a new throttling based on the count of kvs.In some scenarios, it may be better than the current way.
Advantage as below:

- More accurate control
- More intuitive to users
- Both read and write requests can be used

**Implementation principle:**
In message_ header#msg_context, add a new variable kv_count, which is used to record how many kvs are included in the request.The server will perform throttling according to the value of kv_count.

But for some requests, such as multiget by range, scan, etc, may not be supported very good.Because it is difficult to calculate the real count of kvs on the client side.In this case, I think we can perform throttling in pegasus instead of rdsn.
For scan, maybe we can set kv_count by batch_size of scan.(not implemented yet)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/797/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/797,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM41CO8V,incubator-pegasus,889777941,797,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-07-30T09:50:30Z,2021-07-30T09:50:30Z,"> * If throttling by qps,one request may contain different size of data, it's hard to control the pressure.
> * If throttling by size, it's difficult for users to calculate the required quota, and it can't be used for read request.

- `throttling by qps` and  `throttling by size` can be set at the same time
-  the suggestion seem don't resolve the `read size throttling`



> The server will perform throttling according to the value of kv_count

The qps throttling is for `RPC`, but not `record count`. if the total size is permissible(for example,  size is small, but the count is large), we should allow it, it's also say we don't care the `kv count` in this case.  or if you want limit `large kv_count`, we support the feature in java client, which limit the count <= 1000

So I think you can set ` throttling by qps` and `throttling by size` at same time to resolve your problem","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM41CO8V/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/797,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM41GTIa,incubator-pegasus,890843674,797,NA,ZhongChaoqiang,35595648,Zhong Chaoqiang,,NA,2021-08-02T08:42:17Z,2021-08-02T08:42:17Z,"> > * If throttling by qps,one request may contain different size of data, it's hard to control the pressure.
> > * If throttling by size, it's difficult for users to calculate the required quota, and it can't be used for read request.
> 
> * `throttling by qps` and  `throttling by size` can be set at the same time
> * the suggestion seem don't resolve the `read size throttling`
> 
> > The server will perform throttling according to the value of kv_count
> 
> The qps throttling is for `RPC`, but not `record count`. if the total size is permissible(for example, size is small, but the count is large), we should allow it, it's also say we don't care the `kv count` in this case. or if you want limit `large kv_count`, we support the feature in java client, which limit the count <= 1000
> 
> So I think you can set ` throttling by qps` and `throttling by size` at same time to resolve your problem

Yes, that's how we wanted to use it before. But throttling by size can't be used for read request。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM41GTIa/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/797,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM41MkgL,incubator-pegasus,892487691,797,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-08-04T08:56:49Z,2021-08-04T08:56:49Z,"`read throttling by size` still need some time. if has no other question, you can choose close the issue","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM41MkgL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/801,https://api.github.com/repos/apache/incubator-pegasus/issues/801,incubator-pegasus,970273579,801,Performance: optimize read&&write latency performance,foreverneverer,23136769,Jiashuo,js982986555@live.com,OPEN,2021-08-13T10:27:43Z,2021-11-19T02:36:37Z,"# Description

Read/Write performance bottlenecks often occur in some case:
1. [add](https://github.com/apache/incubator-pegasus/blob/master/scripts/pegasus_add_node_list.sh) or [offline](https://github.com/apache/incubator-pegasus/blob/master/scripts/pegasus_offline_node_list.sh) node
2. long-tail latency in daily case

Here record some sub-issue and pull-request:

- [x] https://github.com/apache/incubator-pegasus/pull/528
- [x] https://github.com/apache/incubator-pegasus/pull/543
- [x] https://github.com/XiaoMi/rdsn/pull/633
- [x] https://github.com/XiaoMi/rdsn/pull/568
- [x] https://github.com/XiaoMi/rdsn/pull/461
- [x] https://github.com/apache/incubator-pegasus/issues/796 
        https://github.com/XiaoMi/rdsn/pull/909
- [ ] https://github.com/apache/incubator-pegasus/issues/802
- [x] https://github.com/XiaoMi/rdsn/pull/944","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/801/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/802,https://api.github.com/repos/apache/incubator-pegasus/issues/802,incubator-pegasus,970309878,802,perf: seperate copy checkpoint from `learn` flow ,foreverneverer,23136769,Jiashuo,js982986555@live.com,OPEN,2021-08-13T11:21:24Z,2021-08-15T08:44:11Z,"Copy large checkpoint when learn has huge impact, In the past, we only think it because of the increase of `io write load`. However, I did two comparative tests and find the design of `nfs copy checkpoint` may be  unreasonable.

### Test One: `restore` vs `learn` with same rate
In theroy,  if io write rate is same, `restore` should has same impact with `learn`, but the test show different result:
| Rate(MB/s)   | case     | io util    |  P99   |
| --------   | ----- | -----:   | :----: |
| 400 | restore      | 20%      |   50+ms    |
| 400  | learn      | 80%      |   1000+ms    |

From above, I found same io rate has different IO util and cause different latency. But if you think  only IO utill casue the result, please see the next test

### Test Two:  `learn` vs `no learn` but with same write rate and same io utils:
In this test, I keep the `io utils` and `write rate` same, the only difference is another test has no `learn` flow, we get the result:
 | Rate(MB/s)   | case     | io util    |  P99   |
| --------   | ----- | -----:   | :----: |
| 400 | no learn      | 80%      |   200+ms    |
| 400  | learn      | 80%      |   1000+ms    |

From above, I found that the latency of `io util load of 80%`caused  only account for a small part, the large impact is casused by `learn` logic

## Conclusion
The reason of write latency has two part:

- io load increase: 200ms
- learn thread task queue: 800ms

So if we re-implement `learn checkpoint`, surpose current latency is 1000ms, and then:

1. if seprate copy checkpoint from `learn` flow in `write thread pool` so as to it won't competing thread resources, latency can decrease 1000ms to 200ms( see test2)
2. if improve copy checkpoint has same io utils impact with `restore`, latecny can decrease from 200ms to 50ms(see test1)

That is to say, actually, `copy checkpoint` should be a independent task like `restore`download file from
remote filesystem and only cause small io utils and little latency impact.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/802/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/803,https://api.github.com/repos/apache/incubator-pegasus/issues/803,incubator-pegasus,981007487,803,bug: core dump when learn,foreverneverer,23136769,Jiashuo,js982986555@live.com,OPEN,2021-08-27T08:30:23Z,2021-08-27T08:30:23Z,"## Bug Report
### Pegasus Version [2.2.2](https://github.com/apache/incubator-pegasus/tree/v2.2.2) 
### Core Dump
```
#1  0x00007f33542c68c8 in abort () from /lib64/libc.so.6
#2  0x00007f3358e1d4ee in dsn_coredump () at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/src/runtime/service_api_c.cpp:94
#3  0x00007f3358bedf36 in dsn::replication::replica::add_potential_secondary (this=0xe0981000, proposal=...) at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/src/replica/replica_config.cpp:165
#4  0x00007f3358bf1057 in dsn::replication::replica::on_config_proposal (this=<optimized out>, proposal=...) at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/src/replica/replica_config.cpp:90
#5  0x00007f3358c61f62 in dsn::replication::replica_stub::on_config_proposal (this=0x2f71800, proposal=...) at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/src/replica/replica_stub.cpp:925
#6  0x00007f3358c85dc5 in bool dsn::serverlet<dsn::replication::replica_stub>::register_rpc_handler<dsn::replication::configuration_update_request>(dsn::task_code, char const*, void (dsn::replication::replica_stub::*)(dsn::replication::configuration_update_request const&))::{lambda(dsn::message_ex*)#1}::operator()(dsn::message_ex*) const (__closure=0x97d8bdc0, request=<optimized out>) at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/include/dsn/cpp/serverlet.h:169
#7  0x00007f3358c85dc5 in bool dsn::serverlet<dsn::replication::replica_stub>::register_rpc_handler<dsn::replication::configuration_update_request>(dsn::task_code, char const*, void (dsn::replication::replica_stub::*)(dsn::replication::configuration_update_request const&))::{lambda(dsn::message_ex*)#1}::operator()(dsn::message_ex*) const () from /home/work/app/pegasus/tjwqtst-staging/replica/package/bin/libdsn_replica_server.so
#8  0x00007f3358e71cce in operator() (__args#0=0x91a16e70, this=0x7ac681d0) at /home/jiashuo1/app/toolchain/gcc540/output/include/c++/5.4.0/functional:2267
#9  dsn::rpc_request_task::exec (this=0x7ac68100) at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/include/dsn/tool-api/task.h:435
#10 0x00007f3358e753d1 in dsn::task::exec_internal (this=this@entry=0x7ac68100) at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/src/runtime/task/task.cpp:176
#11 0x00007f3358e8f98a in dsn::task_worker::loop (this=0x35fca80) at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/src/runtime/task/task_worker.cpp:211
#12 0x00007f3358e8fb8d in dsn::task_worker::run_internal (this=0x35fca80) at /home/jiashuo1/work/pegasus-2.2/pegasus/rdsn/src/runtime/task/task_worker.cpp:191
#13 0x00007f3354c24fd0 in std::execute_native_thread_routine (__p=<optimized out>) at /home/jiashuo1/app/toolchain/gcc540/objdir/../gcc-5.4.0/libstdc++-v3/src/c++11/thread.cc:84
```

```
F2021-08-27 15:59:39.88 (1630051179088789086 185958) replica.replica11.0400d631045f08e1: replica_config.cpp:167:add_potential_secondary(): assertion expression: !_primary_states.check_exist(proposal.node, partition_status::PS_PRIMARY)
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/803/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/807,https://api.github.com/repos/apache/incubator-pegasus/issues/807,incubator-pegasus,984824182,807,test,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2021-09-01T07:46:50Z,2021-09-01T07:47:38Z,"## General Question

Before asking a question, make sure you have:

- Searched open and closed [GitHub issues](https://github.com/apache/incubator-pegasus/issues)
- Read the documentation:
  - [Pegasus Doc](https://pegasus.apache.org)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/807/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/808,https://api.github.com/repos/apache/incubator-pegasus/issues/808,incubator-pegasus,986228550,808,Multiget and scan iteration count bug,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2021-09-02T06:36:36Z,2021-10-22T09:52:35Z,"## Feature enhancemant and bug report
As https://github.com/apache/incubator-pegasus/issues/486 shows, `Pegasus version 1.12.3` adds iteration count and iteration time check while calling rocksdb range read. 

For example, client use multiget interface and set `max_kv_count` option to control the expected value count this round would like to get. If this round don't get completed data, server will return false and the last read key-value pair, the next round can specify that key to start next round. 

### Case1. When total_count < max_kv_count <= max_iteration_count
> Assuming: `total_count = 50`, `max_kv_count=100`, server `max_iteration_count=3000`
1. [1, 50] value are expired -> can't get any value, return true
2. [1, 20] value are expired, [21, 50] is valid -> get value [21, 50], count = 30, return true
3. [1, 50] value are all valid -> get value [1, 50], count = 50, return true

### Case2. When total_count > max_kv_count >= max_iteration
> Assuming: `total_count = 4000`, `max_kv_count=3500`, server `max_iteration_count=3000`
1. [1, 3000] value are all expired, [3001, 4000] are valid -> can't get any value, return false
2. [1, 500] value are expired, [501, 4000] are valid -> get value [501,3000], count = 2500, return false
3. [1, 4000] value are all valid -> get value [1, 3000], count = 3000, return false

### Case3. When the total_count > max_iteration_count > max_kv_count
> Assuming: `total_count = 4000`, `max_kv_count=100`, server `max_iteration_count=3000`
1. [1, 3000] value are all expired, [3001, 4000] are valid -> can't get any value, return false
2. **[1, 2950] value are expired, [2951, 4000] are valid -> can't get any value, return false**
3. **[1, 100] value are all expired, [101, 4000] are valid -> can't get any value, return false**
4. **[1, 20] value are expired, [21, 4000] is valid -> get value [21,100], count = 80, return false**
5. [1, 4000] value are all valid -> get value [1, 100], count = 100, return false

However, the **Bold** cases above don't meet the client expectation, the expected result are:
- [1, 2950] value are expired, [2951, 4000] are valid -> value[2951, 3000], count = 50, return false
- [1, 100] value are all expired, [101, 4000] are valid -> value [101, 200], count = 100, return false
- [1, 20] value are expired, [21, 4000] is valid -> value[21, 120], count = 100, return false

In particular, for case2-1 and case3-1, if too many value expired or filtered (exceed the max_iteration_count), server **SHOULD NOT** iterate more to get a valid value.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/808/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/810,https://api.github.com/repos/apache/incubator-pegasus/issues/810,incubator-pegasus,988834414,810,Need a `read_throttling_by_size` feature,Smityz,22953824,Smilencer,smityz@qq.com,CLOSED,2021-09-06T06:57:20Z,2021-10-22T10:24:57Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
No, but I think it's necessary for a database to implement this function.

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->
Limit the read throttling like write throttling

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/810/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/810,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44mBsO,incubator-pegasus,949492494,810,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2021-10-22T10:24:57Z,2021-10-22T10:24:57Z,"This issue is same as #830, this duplicated issue is closed.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44mBsO/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/812,https://api.github.com/repos/apache/incubator-pegasus/issues/812,incubator-pegasus,989568595,812,Need a dynamic parameter to limit the task queue length,Smityz,22953824,Smilencer,smityz@qq.com,CLOSED,2021-09-07T03:59:32Z,2023-05-16T16:21:41Z,"I think it's necessary for Pegasus to limit its task queue length.
In some high network flow scenarios, the task queue will get congestion. Like this
![Screenshot from 2021-09-07 11-40-44](https://user-images.githubusercontent.com/22953824/132280831-53d4aafb-c6cc-4b83-8329-99fb08225827.png)
It will cause to OOM problem.
So I think we should control the maximum length of the task queue to avoid this problem.

In the formal codes, we have already implemented this function.
https://github.com/XiaoMi/rdsn/blob/master/src/runtime/task/task_queue.cpp#L83
But it is not easy to use. At first, it is a static config, we should rolling all the replicas to let it take effect. Secondly, I think it is good to control the max length on `task_queue` level rather than `task` level.

So I want to pull requests to make the following changes:
1. use `dsn/utility/flags.h` or remote_command to dynamically change the value.
2. use change the enqueue limiter from rpc level to task_queue level.

Relative PR(not finished now)
https://github.com/XiaoMi/rdsn/pull/902


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/812/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/813,https://api.github.com/repos/apache/incubator-pegasus/issues/813,incubator-pegasus,991084974,813,Support compiling pegasus in Macos,xunliu,3677382,Xun,,CLOSED,2021-09-08T12:29:29Z,2022-06-21T07:24:33Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
NONE.

**Describe the feature you'd like:**
Using clion to develop c++ projects in macos is a very popular way. 
If Pegasus supports it, more developers will be added to the Pegasus community. 
So, I think this is very important.

**Describe alternatives you've considered:**
NONE.

**Teachability, Documentation, Adoption, Migration Strategy:**
NONE.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/813/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/813,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48w4Jc,incubator-pegasus,1019445852,813,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-01-23T09:27:12Z,2022-01-23T09:27:12Z,"Now I've improved the project to successfully build on MacOS 12.1 (with Intel processor), you can review the PR https://github.com/XiaoMi/rdsn/pull/1034 and https://github.com/apache/incubator-pegasus/pull/891","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48w4Jc/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/813,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM49ABi4,incubator-pegasus,1023416504,813,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-01-27T16:34:48Z,2022-01-27T16:34:48Z,"@xunliu We have tried to support building Pegasus & rdsn on macOS, successfully on my envrioment (macOS 12.1, Intel processor), you can have a try.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM49ABi4/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/815,https://api.github.com/repos/apache/incubator-pegasus/issues/815,incubator-pegasus,999825243,815,potential replica may not be closed when dropped app,ZhongChaoqiang,35595648,Zhong Chaoqiang,,OPEN,2021-09-18T01:52:20Z,2021-09-18T02:08:50Z,"## Bug Report
删除表的时候 如果该表有部分分片处于PS_POTENTIAL_SECONDARY状态,有概率出现replica-server遗留了potential状态的replica无法关闭

版本为2.0.0

下面是删除app(appid=3)后，通过remote_command查询到的replica信息
```
D2021-09-17 06:40:44.606 (1631832044606185642 4017) replica.rep_long5.04010000000000b5: replica_stub.cpp:1707:on_gc(): start to garbage collection, replica_count = 4
D2021-09-17 06:40:44.606 (1631832044606199774 4017) replica.rep_long5.04010000000000b5: replica_stub.cpp:1746:on_gc(): gc_shared: gc condition for 3.97@xxxxxxxxx:54801, status = replication::partition_status::PS_POTENTIAL_SECONDARY, garbage_max_decree = 37803, last_durable_decree= 37804, plog_max_commit_on_disk = 37803
D2021-09-17 06:40:44.606 (1631832044606206186 4017) replica.rep_long5.04010000000000b5: replica_stub.cpp:1746:on_gc(): gc_shared: gc condition for 3.13@xxxxxxxxx:54801, status = replication::partition_status::PS_POTENTIAL_SECONDARY, garbage_max_decree = 37894, last_durable_decree= 37895, plog_max_commit_on_disk = 37894
D2021-09-17 06:40:44.606 (1631832044606229313 4017) replica.rep_long5.04010000000000b5: replica_stub.cpp:1746:on_gc(): gc_shared: gc condition for 3.49@xxxxxxxxx:54801, status = replication::partition_status::PS_POTENTIAL_SECONDARY, garbage_max_decree = 37838, last_durable_decree= 37839, plog_max_commit_on_disk = 37838
D2021-09-17 06:40:44.606 (1631832044606232808 4017) replica.rep_long5.04010000000000b5: replica_stub.cpp:1746:on_gc(): gc_shared: gc condition for 3.61@xxxxxxxxx:54801, status = replication::partition_status::PS_POTENTIAL_SECONDARY, garbage_max_decree = 37882, last_durable_decree= 37883, plog_max_commit_on_disk = 37882
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/815/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/815,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM429wxj,incubator-pegasus,922160227,815,NA,ZhongChaoqiang,35595648,Zhong Chaoqiang,,NA,2021-09-18T02:00:09Z,2021-09-18T02:00:09Z,"初步分析：
potential状态的replica在learning结束,但状态未切换到secondary前,如果drop该表,会触发该问题：
`D2021-09-16 20:21:42.890 (1631794902890873823 3fba) replica.replica10.0404000a00000ca5: replica_learn.cpp:1430:on_learn_completion_notification_reply(): 3.13@xxxxxxxxx:54801: on_learn_completion_notification_reply[0000000c00000002]: learnee = xxxxxxxxx:54801, learn_duration = 2358 ms, response_err = ERR_OK`

删除app后，replicaserver在同步meta的信息的时候，由于on_node_query_reply_scatter2并不会删除potiential状态的replica，所以造成了这些replica一直存在。这有可能会导致slog一直不能执行gc。
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM429wxj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/818,https://api.github.com/repos/apache/incubator-pegasus/issues/818,incubator-pegasus,1004210851,818,Release 2.3.0,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2021-09-22T12:01:29Z,2022-08-09T08:38:20Z,"Since Pegasus 2.2.0 (released on June 2021), there are 170 commits, including several useful features and significant bug fix. We are ready to release Apache Pegasus 2.3.0. 
# New features
## Partition split
Supporting scalability for table. One partition will be divided into two partitions. If the original partition count is 4, after partition split, the new partition count will be 8. More details can be found: [partition-split-design-documents](https://pegasus.apache.org/2020/02/06/partition-split-design.html). 
Related pull request in this release:
- https://github.com/XiaoMi/rdsn/pull/727
- https://github.com/XiaoMi/rdsn/pull/743
- https://github.com/XiaoMi/rdsn/pull/762
- https://github.com/XiaoMi/rdsn/pull/763
- https://github.com/XiaoMi/rdsn/pull/764
- https://github.com/XiaoMi/rdsn/pull/765
- https://github.com/XiaoMi/rdsn/pull/766
- https://github.com/XiaoMi/rdsn/pull/767
- https://github.com/XiaoMi/rdsn/pull/783
- https://github.com/XiaoMi/rdsn/pull/784
- https://github.com/XiaoMi/rdsn/pull/786
- https://github.com/XiaoMi/rdsn/pull/787
- https://github.com/XiaoMi/rdsn/pull/788
- https://github.com/XiaoMi/rdsn/pull/789
- https://github.com/XiaoMi/rdsn/pull/790
- https://github.com/XiaoMi/rdsn/pull/791
- https://github.com/XiaoMi/rdsn/pull/795
- https://github.com/XiaoMi/rdsn/pull/797
- https://github.com/XiaoMi/rdsn/pull/798
- https://github.com/XiaoMi/rdsn/pull/877
- https://github.com/XiaoMi/rdsn/pull/891
- https://github.com/apache/incubator-pegasus/pull/698
- https://github.com/apache/incubator-pegasus/pull/702
- https://github.com/apache/incubator-pegasus/pull/733
- https://github.com/apache/incubator-pegasus/pull/804

More detailed pull requests can be found: [#754]

## User-defined compaction strategy
Supporting user specified compaction policy, more details can be found: [user-specified-compaction-RFC](https://github.com/apache/incubator-pegasus/blob/master/rfcs/2021-05-27-user-specified-compaction.md). Related pull requests:
- https://github.com/XiaoMi/rdsn/pull/849
- https://github.com/apache/incubator-pegasus/pull/731
- https://github.com/apache/incubator-pegasus/pull/738
- https://github.com/apache/incubator-pegasus/pull/741
- https://github.com/apache/incubator-pegasus/pull/749
- https://github.com/apache/incubator-pegasus/pull/750
- https://github.com/apache/incubator-pegasus/pull/751
- https://github.com/apache/incubator-pegasus/pull/760
- https://github.com/apache/incubator-pegasus/pull/768
- https://github.com/apache/incubator-pegasus/pull/769
- https://github.com/apache/incubator-pegasus/pull/776
- https://github.com/apache/incubator-pegasus/pull/780
- https://github.com/apache/incubator-pegasus/pull/814

## Cluster load balance
Supporting whole cluster load balance, more details can be found: [#761], related pull requests:
- https://github.com/XiaoMi/rdsn/pull/853
- https://github.com/XiaoMi/rdsn/pull/857
- https://github.com/XiaoMi/rdsn/pull/858
- https://github.com/XiaoMi/rdsn/pull/866
- https://github.com/XiaoMi/rdsn/pull/872
- https://github.com/XiaoMi/rdsn/pull/873
- https://github.com/XiaoMi/rdsn/pull/881
- https://github.com/XiaoMi/rdsn/pull/882
- https://github.com/XiaoMi/rdsn/pull/883
- https://github.com/XiaoMi/rdsn/pull/904
- https://github.com/XiaoMi/rdsn/pull/916 

## One time backup
Supporting trigger backup once immediately, more details can be found: [#755], related pull requests:
- https://github.com/XiaoMi/rdsn/pull/772
- https://github.com/XiaoMi/rdsn/pull/775
- https://github.com/XiaoMi/rdsn/pull/776
- https://github.com/XiaoMi/rdsn/pull/785
- https://github.com/XiaoMi/rdsn/pull/796
- https://github.com/XiaoMi/rdsn/pull/813
- https://github.com/XiaoMi/rdsn/pull/814 
- https://github.com/XiaoMi/rdsn/pull/817
- https://github.com/apache/incubator-pegasus/pull/725

# Enhancement
- Support uint8 in data input and output
    - https://github.com/XiaoMi/rdsn/pull/792
- Support user specified restore path 
    - https://github.com/XiaoMi/rdsn/pull/816
    - https://github.com/XiaoMi/rdsn/pull/822
- Add rate limiter for backup request
    - https://github.com/XiaoMi/rdsn/pull/855
    - https://github.com/apache/incubator-pegasus/pull/779
- Support reject client write requests while disk space insufficient
    - https://github.com/XiaoMi/rdsn/pull/833
    - https://github.com/XiaoMi/rdsn/pull/851
- Support broken disk check while initialization and add disk dynamically
    - https://github.com/XiaoMi/rdsn/pull/834
    - https://github.com/XiaoMi/rdsn/pull/839
    - https://github.com/XiaoMi/rdsn/pull/842   
- Support release all tcmalloc reserved but not used memory
    -  https://github.com/XiaoMi/rdsn/pull/864
- Support disable block cache of an app
    -  https://github.com/XiaoMi/rdsn/pull/865
    -  https://github.com/apache/incubator-pegasus/pull/792
- Support token bucket in fds configurable
    - https://github.com/XiaoMi/rdsn/pull/874
- Add a new thread pool to process range read
    - https://github.com/XiaoMi/rdsn/pull/856
    - https://github.com/apache/incubator-pegasus/pull/782
- Support preserving TTL while executing copy_data
    -  https://github.com/apache/incubator-pegasus/pull/752
- Support nfs server rate limit
    -   https://github.com/XiaoMi/rdsn/pull/901
- Range read count enhancement
    -  https://github.com/apache/incubator-pegasus/pull/811

# New perf-counters
- Partition split related counters
    - https://github.com/XiaoMi/rdsn/pull/789
    - https://github.com/XiaoMi/rdsn/pull/891
    - https://github.com/apache/incubator-pegasus/pull/804
- Backup request limiter counter
    -  https://github.com/XiaoMi/rdsn/pull/855
    - https://github.com/apache/incubator-pegasus/pull/779
- Dropped timeout rpc count counter
    - https://github.com/XiaoMi/rdsn/pull/859
    - https://github.com/XiaoMi/rdsn/pull/867
    - https://github.com/apache/incubator-pegasus/pull/793
- Server session count counter
    -  https://github.com/XiaoMi/rdsn/pull/867
- Table-level reject user write request count while bulk load
    - https://github.com/XiaoMi/rdsn/pull/895
    - https://github.com/apache/incubator-pegasus/pull/805
- Table-level hotpot partition count counter
    - https://github.com/apache/incubator-pegasus/pull/732
- Backup request size counter
    - https://github.com/apache/incubator-pegasus/pull/742 
- RocksDB read/wrire amplification and hit rate
    - https://github.com/apache/incubator-pegasus/pull/774
    - https://github.com/apache/incubator-pegasus/pull/800
- Unmarshall failed request count counter
    -  https://github.com/apache/incubator-pegasus/pull/790
- Table-level compaction counters
    -  https://github.com/apache/incubator-pegasus/pull/806

# Bug Fix
## Duplication related fix
- https://github.com/XiaoMi/rdsn/pull/838
- https://github.com/XiaoMi/rdsn/pull/845
- https://github.com/XiaoMi/rdsn/pull/860
- https://github.com/XiaoMi/rdsn/pull/861
- https://github.com/XiaoMi/rdsn/pull/862
## Graceful exit
- https://github.com/XiaoMi/rdsn/pull/841
- https://github.com/XiaoMi/rdsn/pull/843
## Thrift unmarshall fix
- https://github.com/XiaoMi/rdsn/pull/863
- https://github.com/apache/incubator-pegasus/pull/790
## Asan fix
- https://github.com/XiaoMi/rdsn/pull/773
- https://github.com/XiaoMi/rdsn/pull/868
- https://github.com/XiaoMi/rdsn/pull/869
- https://github.com/XiaoMi/rdsn/pull/870
- https://github.com/XiaoMi/rdsn/pull/871
- https://github.com/XiaoMi/rdsn/pull/875
## Others
- Fix shell can not find factory type provider: https://github.com/XiaoMi/rdsn/pull/774
- Fix unit tests can not execute destructor:  https://github.com/XiaoMi/rdsn/pull/793
- Fix init_table_level_latency_counter metric name error: https://github.com/XiaoMi/rdsn/pull/830
- Fix cold backup execution while policy is disabled: https://github.com/XiaoMi/rdsn/pull/840
- Fix C++ client won't retry while receiving splitting or disk_insufficient: https://github.com/XiaoMi/rdsn/pull/852,  https://github.com/apache/incubator-pegasus/pull/777
- Fix tracer mutation name: https://github.com/XiaoMi/rdsn/pull/876
- Support single replica for load balance check: https://github.com/XiaoMi/rdsn/pull/932
- Fix mutation_log_test failed when compile with debug mode: https://github.com/XiaoMi/rdsn/pull/933
- Add incr and duplicate qps into total_wrire_qps: https://github.com/apache/incubator-pegasus/pull/721
- Fix dependency while pack tools: https://github.com/apache/incubator-pegasus/pull/758
- Fix rolling_update script hint message: https://github.com/apache/incubator-pegasus/pull/759
- Fix move hdfs script into script folder: https://github.com/apache/incubator-pegasus/pull/775
- Use reference to catch exceptions: https://github.com/apache/incubator-pegasus/pull/798
- Fix pack script: https://github.com/apache/incubator-pegasus/pull/821
- Fix full scan bug: https://github.com/apache/incubator-pegasus/pull/825
- Fix prometheus counter name bug: https://github.com/apache/incubator-pegasus/pull/828

# Refactor
## Refactor load balance
- https://github.com/XiaoMi/rdsn/pull/874
- https://github.com/XiaoMi/rdsn/pull/854
- https://github.com/XiaoMi/rdsn/pull/884
- https://github.com/XiaoMi/rdsn/pull/885
- https://github.com/XiaoMi/rdsn/pull/886
- https://github.com/XiaoMi/rdsn/pull/887
- https://github.com/XiaoMi/rdsn/pull/890
- https://github.com/XiaoMi/rdsn/pull/892
- https://github.com/XiaoMi/rdsn/pull/893
- https://github.com/XiaoMi/rdsn/pull/896
- https://github.com/XiaoMi/rdsn/pull/897
- https://github.com/XiaoMi/rdsn/pull/899
- https://github.com/XiaoMi/rdsn/pull/900
## Refactor pegasus_value_schema
-  https://github.com/apache/incubator-pegasus/pull/709
- https://github.com/apache/incubator-pegasus/pull/722
- https://github.com/apache/incubator-pegasus/pull/735
- https://github.com/apache/incubator-pegasus/pull/737
- https://github.com/apache/incubator-pegasus/pull/739
## Others
- Remove useless structure admission controller: https://github.com/XiaoMi/rdsn/pull/684
- Refactor nfs client and nfs server: https://github.com/XiaoMi/rdsn/pull/771
- Remove useless parameters of aio_internal: https://github.com/XiaoMi/rdsn/pull/777, https://github.com/XiaoMi/rdsn/pull/782
- Separate aio from runtime: https://github.com/XiaoMi/rdsn/pull/778, https://github.com/XiaoMi/rdsn/pull/794, https://github.com/apache/incubator-pegasus/pull/699, https://github.com/apache/incubator-pegasus/pull/703
- Remove useless profile commands from remote command: https://github.com/XiaoMi/rdsn/pull/810
- Remove explorer: https://github.com/XiaoMi/rdsn/pull/811
- Refactor replication_options: https://github.com/XiaoMi/rdsn/pull/831
- Remove table_stats structure: https://github.com/apache/incubator-pegasus/pull/720

# No code update
- Disk Migration RFC: https://github.com/apache/incubator-pegasus/pull/695
- Support compile user specific RocksDB:  https://github.com/apache/incubator-pegasus/pull/753
- License update:
    - https://github.com/XiaoMi/rdsn/pull/819
    - https://github.com/XiaoMi/rdsn/pull/821
    - https://github.com/XiaoMi/rdsn/pull/825
- Workflow update:
    - https://github.com/XiaoMi/rdsn/pull/820
    - https://github.com/XiaoMi/rdsn/pull/836
    - https://github.com/XiaoMi/rdsn/pull/844
    - https://github.com/XiaoMi/rdsn/pull/880
    - https://github.com/apache/incubator-pegasus/pull/763
- Scripts:
    - https://github.com/apache/incubator-pegasus/pull/714
    - https://github.com/apache/incubator-pegasus/pull/762
    - https://github.com/apache/incubator-pegasus/pull/781
    - https://github.com/apache/incubator-pegasus/pull/799

# Configurations
```diff
[apps.replica]
- pools = THREAD_POOL_DEFAULT,THREAD_POOL_REPLICATION_LONG,THREAD_POOL_REPLICATION,THREAD_POOL_FD,THREAD_POOL_LOCAL_APP,THREAD_POOL_BLOCK_SERVICE,THREAD_POOL_COMPACT,THREAD_POOL_INGESTION,THREAD_POOL_SLOG,THREAD_POOL_PLOG
+ pools = THREAD_POOL_DEFAULT,THREAD_POOL_REPLICATION_LONG,THREAD_POOL_REPLICATION,THREAD_POOL_FD,THREAD_POOL_LOCAL_APP,THREAD_POOL_BLOCK_SERVICE,THREAD_POOL_COMPACT,THREAD_POOL_INGESTION,THREAD_POOL_SLOG,THREAD_POOL_PLOG,THREAD_POOL_SCAN

+[threadpool.THREAD_POOL_SCAN]
+  name = scan_query
+  partitioned = false
+  worker_priority = THREAD_xPRIORITY_NORMAL
+  worker_count = 24

[meta_server]
+ balance_cluster=false
+ balance_op_count_per_round=10

[nfs]
+ max_send_rate_megabytes=500

[replication]
+ reject_write_when_disk_insufficient=true
+ disk_min_available_space_ratio=10
+ ignore_broken_disk=true

[pegasus.server]
+ read_amp_bytes_per_bit = 0 # 0 means disble read amp counter
- update_rdb_stat_interval = 600
+ update_rdb_stat_interval = 60

// Add drop timeout request config for the specific task code
[task.RPC_RRDB_RRDB_PUT]
+  rpc_request_dropped_before_execution_when_timeout = true

[task.RPC_RRDB_RRDB_GET]
+  rpc_request_dropped_before_execution_when_timeout = true
```

# Perf-Counters
```diff
// partition split related
+ replica*eon.replica_stub*replicas.splitting.count
+ replica*eon.replica_stub*replicas.splitting.max.duration.time(ms)
+ replica*eon.replica_stub*replicas.splitting.max.async.learn.time(ms)
+ replica*eon.replica_stub*replicas.splitting.max.copy.file.size
+ replica*eon.replica_stub*replicas.splitting.recent.start.count
+ replica*eon.replica_stub*replicas.splitting.recent.copy.file.count
+ replica*eon.replica_stub*replicas.splitting.recent.copy.file.size
+ replica*eon.replica_stub*replicas.splitting.recent.copy.mutation.count
+ replica*eon.replica_stub*replicas.splitting.succ.count
+ replica*eon.replica_stub*replicas.splitting.fail.count
+ replica*eon.replica*recent.write.splitting.reject.count@[gpid]
+ replica*eon.replica*recent.read.splitting.reject.count@[gpid]
+ collector*app.pegasus*app.stat.recent_write_splitting_reject_count#[table_name]
+ collector*app.pegasus*app.stat.recent_read_splitting_reject_count#[table_name]

// backup request throttling
+ replica*eon.replica*recent.backup.request.throttling.delay.count@[table_name]
+ replica*eon.replica*recent.backup.request.throttling.reject.count@[table_name]
+ collector*app.pegasus*app.stat.recent_backup_request_throttling_delay_count#[table_name]
+ collector*app.pegasus*app.stat.recent_backup_request_throttling_reject_count#[table_name]

// table-level hotpot partition count
+ collector*app.pegasus*app.stat.hotspots.temp.read.total#[table_name]
+ collector*app.pegasus*app.stat.hotspots.temp.write.total#[table_name]

// backup request size
+ replica*app.pegasus*backup_request_bytes@[gpid]
+ collector*app.pegasus*backup_request_bytes@[table_name]

// rocksdb read write amplification, hit count
+ replica*app.pegasus*rdb.read_amplification@[gpid]
+ replica*app.pegasus*rdb.write_amplification@[gpid]
+ replica*app.pegasus.rdb.read_memtable_total_count@[gpid]
+ replica*app.pegasus.rdb.read_memtable_hit_count@[gpid]
+ replica*app.pegasus*rdb.read_l0_hit_count@[gpid]
+ replica*app.pegasus*rdb.read_l1_hit_count@[gpid]
+ replica*app.pegasus*rdb.read_l2andup_hit_count@[gpid]
+ collector*app.pegasus*app.stat.rdb_read_amplification#[table_name]
+ collector*app.pegasus*app.stat.rdb.write_amplification#[table_name]
+ collector*app.pegasus*app.stat.rdb.read_memtable_hit_rate#[table_name]
+ collector*app.pegasus*app.stat.rdb.read_l0_hit_rate#[table_name]
+ collector*app.pegasus*app.stat.rdb.read_l1_hit_rate#[table_name]
+ collector*app.pegasus*app.stat.rdb.read_l2andup_hit_rate#[table_name]

// session count 
+ server*network*client_session_count

// bulk load reject write request
- replica_stub.bulk.load.ingestion.reject.write.count
+ replica*eon.replica*recent.write.bulk.load.ingestion.reject.count@[gpid]
+ collector*app.pegasus*app.stat.recent_write_bulk_load_ingestion_reject_count#[table_name]

// RocksDB compaction
+ collector*app.pegasus*app.stat.recent_rdb_compaction_input_bytes#[table_name]
+ collector*app.pegasus*app.stat.recent_rdb_compaction_output_bytes#[table_name]

// unmarshall failed count
+ replica*app.pegasus*recent_corrupt_write_count@[gpid]

// If drop timeout request for task, the counter will be added, for example(RPC_RRDB_RRDB_PUT):
+ zion*profiler*RPC_RRDB_RRDB_PUT.rpc.dropped
```

# Performance
The following result is tested by YCSB, and the latency unit is us.
Case | client and thread | R:W | R-QPS | R-Avg | R-P99 | W-QPS | W-Avg | W-P99
-- | -- | -- | -- | -- | -- | -- | -- | --
Write Only | 3 clients * 15 threads | 0:1 | - | - | - | 42386 | 1060 | 6628
Read Only | 3 clients * 50 threads | 1:0 | 331623 | 585 | 2611 | - | - | -
Read Write |  3 clients * 30 threads | 1:1 | 38766 | 1067 | 15521 | 38774 | 1246 | 7791
Read Write |  3 clients * 15 threads | 1:3 | 13140 | 819 | 11460 | 39428 | 863 | 4884
Read Write |  3 clients * 15 threads | 1:30 | 1552 | 937 | 9524 | 46570 | 930 | 5315
Read Write |  3 clients * 30 threads | 3:1 | 93746 | 623 | 6389 | 31246 | 996 | 5543
Read Write |  3 clients * 50 threads | 30:1 | 254534 | 560 | 2627 | 8481 | 901 | 3269

# Contributors
[acelyc111](https://github.com/acelyc111)
[cauchy1988](https://github.com/cauchy1988)
[empiredan](https://github.com/empiredan)
[hycdong](https://github.com/hycdong)
[levy5307](https://github.com/levy5307)
[lidingshengHHU](https://github.com/lidingshengHHU)
[neverchanje](https://github.com/neverchanje)
[padmejin](https://github.com/padmejin)
[Shuo-Jia](https://github.com/Shuo-Jia)
[Smityz](https://github.com/Smityz)
[zhangyifan27](https://github.com/zhangyifan27)
[ZhongChaoqiang](https://github.com/ZhongChaoqiang)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/818/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/819,https://api.github.com/repos/apache/incubator-pegasus/issues/819,incubator-pegasus,1007972212,819,support jemalloc,levy5307,22141103,赵立伟,zlw5307@163.com,CLOSED,2021-09-27T10:23:52Z,2022-07-19T03:48:36Z,"## Feature Request

make pegasus to support jemalloc

### Related pull requests

https://github.com/XiaoMi/rdsn/pull/910","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/819/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/819,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM43e78h,incubator-pegasus,930856737,819,NA,empiredan,743379,Dan Wang,,NA,2021-09-30T06:40:02Z,2021-09-30T06:40:02Z,"## Support to set/get dirty_decay_ms and muzzy_decay_ms dynamically

Since we've supported building rDSN with jemalloc in https://github.com/XiaoMi/rdsn/pull/910, it's necessary for us to  supporting setting/getting dirty_decay_ms and muzzy_decay_ms dynamically. 

For details please see https://github.com/XiaoMi/rdsn/pull/928","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM43e78h/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/820,https://api.github.com/repos/apache/incubator-pegasus/issues/820,incubator-pegasus,1010600310,820,bug in greedy_load_balancer,levy5307,22141103,赵立伟,zlw5307@163.com,CLOSED,2021-09-29T07:34:23Z,2021-10-12T04:17:03Z,"## Bug Report

If alive replica server count is less than 2, It will produce coredump in all these 2 replica servers if balance is going to run. This will cause the cluster to become completely unavailable

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/820/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/820,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44ERCf,incubator-pegasus,940642463,820,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2021-10-12T04:17:00Z,2021-10-12T04:17:00Z,fixed in https://github.com/XiaoMi/rdsn/pull/932,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44ERCf/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/823,https://api.github.com/repos/apache/incubator-pegasus/issues/823,incubator-pegasus,1023385507,823,Chore: License problems,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2021-10-12T05:37:26Z,2023-05-16T16:20:23Z,"## Introduction
As Pegasus has already donated to ASF, all files are expected to have license header, either ASF license, or thirdparty copyright. 

Pegasus is a license-complex project, includes rDSN, a distributed system framework maintained by Microsoft, whose license is MIT. Xiaomi engineers fork rDSN repo and developed pegasus based on it, so some files' copyrights belongs to Xiaomi, whose license is Apache 2.0. Based on history problems, there are still some files lack of any license header or copyright. 

This issue is to record all of them, and raise the discussion about the Pegasus license problems.

## Files lack of license header
There are sereval types files without license header:
- Source files (`.h` or `.cpp`)
- CMakeList files (`.txt` etc)
- Config files (`.ini`)
- Scripts (`.sh`, `.py`)
- Others (`.md`, `.thrift` etc)

Detailed file list can be found in this issue below.

## TODO
- [ ] Add license headers for files without it
- [ ] Update LICENSE file with all 3rd party license

## Appendix (file lists)
### Source files
- [ ] rdsn/include/dsn/cpp/serialization_helper/dsn.layer2_types.h (thrift-auto-generated)
- [ ] rdsn/include/dsn/cpp/serialization_helper/dsn_types.h
- [ ] rdsn/include/dsn/dist/replication/replication_enums.h
- [ ] rdsn/include/dsn/dist/replication/storage_serverlet.h
- [ ] rdsn/include/dsn/tool-api/threadpool_code.h
- [ ] rdsn/include/dsn/utility/binary_reader.h
- [ ] rdsn/include/dsn/utility/binary_writer.h
- [ ] rdsn/include/dsn/utility/crc.h
- [ ] rdsn/include/dsn/utility/error_code.h
- [ ] rdsn/include/dsn/utility/strings.h
- [ ] rdsn/src/failure_detector/test/failure_detector.cpp
- [ ] rdsn/src/meta/test/server_state_test.cpp
- [ ] rdsn/src/meta/test/meta_state/meta_state_service.cpp
- [ ] rdsn/src/meta/test/meta_partition_guardian_test.cpp
- [ ] rdsn/src/meta/test/state_sync_test.cpp
- [ ] rdsn/src/meta/test/meta_data.cpp
- [ ] rdsn/src/meta/test/dump_file.cpp
- [ ] rdsn/src/meta/test/json_compacity.cpp
- [ ] rdsn/src/meta/test/balancer_validator.cpp
- [ ] rdsn/src/meta/test/misc/misc.cpp
- [ ] rdsn/src/meta/test/balancer_simulator/balancer_simulator.cpp
- [ ] rdsn/src/meta/server_load_balancer.cpp
- [ ] rdsn/src/nfs/nfs_node.cpp
- [ ] rdsn/src/nfs/test/main.cpp
- [ ] rdsn/src/perf_counter/perf_counter.cpp
- [ ] rdsn/src/replica/storage/simple_kv/simple_kv.client.h
- [ ] rdsn/src/replica/test/replication_service_test_app.h
- [ ] rdsn/src/runtime/build_config.h
- [ ] rdsn/src/runtime/dsn.layer2_types.cpp (thrift-auto-generated)
- [ ] rdsn/src/utils/test/autoref_ptr_test.cpp
- [ ] rdsn/src/utils/binary_writer.cpp
- [ ] rdsn/src/utils/safe_strerror_posix.cpp
- [ ] rdsn/src/utils/binary_reader.cpp
- [ ] rdsn/src/utils/crc.cpp
- [ ] src/base/rrdb_types.cpp (thrift-auto-generated)
- [ ] src/include/rrdb/rrdb_types.h (thrift-auto-generated)
- [ ] src/include/pegasus/git_commit.h (build-generated gitignored)

### Compile files
 - [ ] rdsn/bin/dsn.cmake
 - [ ] rdsn/src/aio/CMakeLists.txt
 - [ ] rdsn/src/aio/test/CMakeLists.txt
 - [ ] rdsn/src/block_service/fds/CMakeLists.txt
 - [ ] rdsn/src/block_service/hdfs/CMakeLists.txt
 - [ ] rdsn/src/block_service/local/CMakeLists.txt
 - [ ] rdsn/src/client/CMakeLists.txt
 - [ ] rdsn/src/common/CMakeLists.txt
 - [ ] rdsn/src/common/test/CMakeLists.txt
 - [ ] rdsn/src/failure_detector/CMakeLists.txt
 - [ ] rdsn/src/failure_detector/test/CMakeLists.txt
 - [ ] rdsn/src/http/test/CMakeLists.txt
 - [ ] rdsn/src/meta/CMakeLists.txt
 - [ ] rdsn/src/meta/test/balancer_simulator/CMakeLists.txt
 - [ ] rdsn/src/meta/test/meta_state/CMakeLists.txt
 - [ ] rdsn/src/meta/test/CMakeLists.txt
 - [ ] rdsn/src/nfs/CMakeLists.txt
 - [ ] rdsn/src/nfs/test/CMakeLists.txt
 - [ ] rdsn/src/perf_counter/CMakeLists.txt
 - [ ] rdsn/src/perf_counter/test/CMakeLists.txt
 - [ ] rdsn/src/remote_cmd/CMakeLists.txt
 - [ ] rdsn/src/replica/split/test/CMakeLists.txt
 - [ ] rdsn/src/replica/CMakeLists.txt
 - [ ] rdsn/src/replica/duplication/test/CMakeLists.txt
 - [ ] rdsn/src/replica/test/CMakeLists.txt
 - [ ] rdsn/src/replica/backup/test/CMakeLists.txt
 - [ ] rdsn/src/replica/bulk_load/test/CMakeLists.txt
 - [ ] rdsn/src/runtime/CMakeLists.txt
 - [ ] rdsn/src/runtime/test/CMakeLists.txt
 - [ ] rdsn/src/runtime/task/CMakeLists.txt
 - [ ] rdsn/src/runtime/security/CMakeLists.txt
 - [ ] rdsn/src/runtime/rpc/CMakeLists.txt
 - [ ] rdsn/src/tools/CMakeLists.txt
 - [ ] rdsn/src/utils/CMakeLists.txt
 - [ ] rdsn/src/utils/test/CMakeLists.txt
 - [ ] rdsn/src/zookeeper/CMakeLists.txt
 - [ ] rdsn/src/zookeeper/test/CMakeLists.txt
 - [ ] rdsn/src/replica/storage/CMakeLists.txt
 - [ ] rdsn/src/replica/storage/simple_kv/CMakeLists.txt
 - [ ] rdsn/src/replica/storage/simple_kv/test/CMakeLists.txt
 - [ ] rdsn/src/CMakeLists.txt

### Scripts
 - [ ] rdsn/scripts/linux/learn_stat.py
 - [ ] rdsn/scripts/linux/stop_zk.sh
 - [ ] rdsn/scripts/linux/run-clang-format.sh
 - [ ] rdsn/scripts/linux/install.sh
 - [ ] rdsn/scripts/linux/start_zk.sh
 - [ ] rdsn/scripts/linux/clear_zk.sh
 - [ ] rdsn/scripts/linux/build.sh
 - [ ] rdsn/scripts/linux/run-clang-format.py
 - [ ] rdsn/src/aio/test/clear.sh
 - [ ] rdsn/src/aio/test/run.sh
 - [ ] rdsn/src/common/test/run.sh
 - [ ] rdsn/src/failure_detector/test/clear.sh
 - [ ] rdsn/src/failure_detector/test/run.sh
 - [ ] rdsn/src/http/test/run.sh
 - [ ] rdsn/src/meta/test/meta_state/clear.sh
 - [ ] rdsn/src/meta/test/meta_state/run.sh
 - [ ] rdsn/src/meta/test/clear.sh
 - [ ] rdsn/src/nfs/test/clear.sh
 - [ ] rdsn/src/perf_counter/test/clear.sh
 - [ ] rdsn/src/perf_counter/test/run.sh
 - [ ] rdsn/src/replica/split/test/run.sh
 - [ ] rdsn/src/replica/duplication/test/run.sh
 - [ ] rdsn/src/replica/test/run.sh
 - [ ] rdsn/src/replica/backup/test/run.sh
 - [ ] rdsn/src/replica/bulk_load/test/run.sh
 - [ ] rdsn/src/runtime/test/run.sh
 - [ ] rdsn/src/runtime/test/clear.sh
 - [ ] rdsn/src/runtime/test/run.sh
 - [ ] rdsn/src/runtime/test/clear.sh
 - [ ] rdsn/src/utils/test/clear.sh
 - [ ] rdsn/src/utils/test/run.sh
 - [ ] rdsn/src/zookeeper/test/clear.sh
 - [ ] rdsn/src/zookeeper/test/run.sh
 - [ ] rdsn/src/replica/storage/simple_kv/clear.sh
 - [ ] rdsn/src/replica/storage/simple_kv/test/addcase.sh
 - [ ] rdsn/src/replica/storage/simple_kv/run.sh
 - [ ] rdsn/src/replica/storage/simple_kv/test/clear.sh
 - [ ] rdsn/src/replica/storage/simple_kv/test/run.sh
 - [ ] rdsn/src/replica/storage/simple_kv/clear.sh

### Config files(.ini files)
- [ ] rdsn/src/aio/test/config.ini
- [ ] rdsn/src/block_service/test/config-test.ini
- [ ] rdsn/src/common/test/config-test.ini
- [ ] rdsn/src/failure_detector/test/config-test.ini
- [ ] rdsn/src/failure_detector/test/config-whitelist-test.ini
- [ ] rdsn/src/failure_detector/test/config-whitelist-test-failed.ini
- [ ] rdsn/src/nfs/test/config.ini
- [ ] rdsn/src/replica/split/test/config-test.ini
- [ ] rdsn/src/replica/duplication/test/config-test.ini
- [ ] rdsn/src/replica/test/config-test.ini
- [ ] rdsn/src/replica/backup/test/config-test.ini
- [ ] rdsn/src/replica/bulk_load/test/config-test.ini
- [ ] all `ini` files below rdsn/src/replica/storage/simple_kv
- [ ] rdsn/src/runtime/test/config-test.ini
- [ ] rdsn/src/runtime/test/config-test-sim.ini
- [ ] rdsn/src/runtime/test/config-test-corrupt-message.ini
- [ ] rdsn/src/utils/test/config-sample.ini
- [ ] rdsn/src/utils/test/config-dup-key.ini
- [ ] rdsn/src/utils/test/config-unmatch-section.ini
- [ ] rdsn/src/utils/test/config-no-section.ini
- [ ] rdsn/src/utils/test/config-bad-section.ini
- [ ] rdsn/src/utils/test/config-empty.ini
- [ ] rdsn/src/utils/test/config-no-key.ini
- [ ] rdsn/src/utils/test/config-null-section.ini
- [ ] rdsn/src/utils/test/config-dup-section.ini
- [ ] rdsn/src/zookeeper/test/config-test.ini
- [ ] src/base/test/config.ini
- [ ] src/geo/test/config.ini
- [ ] src/geo/bench/config.ini
- [ ] src/redis_protocol/proxy_ut/config.ini
- [ ] src/redis_protocol/proxy/config.ini
- [ ] src/sample/config.ini
- [ ] src/server/test/config.ini
- [ ] src/server/config.ini
- [ ] src/server/config.min.ini
- [ ] src/shell/config.ini
- [ ] src/test/pressure_test/config-pressure.ini
- [ ] src/test/upgrade_test/config.ini
- [ ] src/test/function_test/config.ini
- [ ] src/test/bench_test/config.ini
- [ ] src/test/kill_test/config.ini

### Others
- [ ] rdsn/src/aio/test/copy_source.txt
- [ ] rdsn/src/failure_detector/fd.thrift
- [ ] rdsn/src/failure_detector/test/gtest.filter
- [ ] rdsn/src/meta/test/meta_state/clear.cmd
- [ ] rdsn/src/meta/test/meta_state/gtest.filter
- [ ] rdsn/src/meta/test/suite1
- [ ] rdsn/src/meta/test/suite2
- [ ] rdsn/src/nfs/nfs.thrift
- [ ] rdsn/src/nfs/test/nfs_test_file1
- [ ] rdsn/src/nfs/test/nfs_test_file2
- [ ] rdsn/src/remote_cmd/command.thrift
- [ ] rdsn/src/replica/duplication/test/log.1.0.all_loaded_are_write_empties
- [ ] rdsn/src/replica/duplication/test/log.1.0.handle_real_private_log
- [ ] rdsn/src/replica/duplication/test/log.1.0.handle_real_private_log2
- [ ] all `act` files below rdsn/src/replica/storage/simple_kv
- [ ] rdsn/src/runtime/test/gtest.filter
- [ ] rdsn/src/runtime/test/command.txt
- [ ] rdsn/src/dsn.thrift
- [ ] rdsn/src/dsn.layer2.thrift
- [ ] rfcs/2021-05-27-user-specified-compaction.md
- [ ] rfcs/2021-02-22-disk-migrator.md
- [ ] rfcs/2020-12-15-meta-proxy.md
- [ ] rfcs/2020-08-27-metric-api.md
- [ ] rfcs/2020-10-09-data-version-v3.md
- [ ] src/sample/README
- [ ] README.md","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/823/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/823,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-14hv,incubator-pegasus,1054312559,823,NA,nguyenm100,11294872,,,NA,2022-02-28T14:27:38Z,2022-02-28T14:27:38Z,"Hi, I noticed that license problem is one of the issues highlight by apache (https://whimsy.apache.org/board/minutes/Pegasus.html) to prevent upgrade out of incubation.  Is this expected to be resolved relatively soon? (next 2-3mo?).  tx","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-14hv/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/823,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5D7fhe,incubator-pegasus,1139669086,823,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-05-27T14:24:16Z,2022-05-27T14:24:16Z,"> Hi, I noticed that license problem is one of the issues highlight by apache (https://whimsy.apache.org/board/minutes/Pegasus.html) to prevent upgrade out of incubation. Is this expected to be resolved relatively soon? (next 2-3mo?). tx

Now we are going to resolve these problems.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5D7fhe/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/824,https://api.github.com/repos/apache/incubator-pegasus/issues/824,incubator-pegasus,1024743715,824,'full scan' can't scan data completely when one replicaserver restart which causing some primary partition transferring,cauchy1988,7292411,,,CLOSED,2021-10-13T03:53:13Z,2021-11-08T10:39:19Z,"After stopping inputting data to the storage, when using get_unordered_scanners to scan the database data, if a certain replicaserver is restarted, there will be a phenomenon that the data of some shards is not scanned completely and correspond scanner exits early;

The reasons for the problem are analyzed by viewing the code as follows:
In the scanning process, the client requests the server to be divided into two parts, on_get_scanner and on_scan. on_get_scanner: mainly establishes a context with the server;  while on_scan: the client continuously scans data from the server

When restarting a replicaserver, some primary shards will be transferred to the new replicaserver, so the context determined by the client and server through on_get_scanner is lost; at this time, the client will have its own retry mechanism in on_get_scanner and the new replicaserver to determine the new Context,  but at this time, the client will pass the last scanned key as start_key to on_get_scanner;  the on_get_scanner function will mistakenly think that the intention of this scan is a fixed hashkey scan instead of a full scan, because start_key is a non-empty string
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/824/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/824,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44JCrq,incubator-pegasus,941894378,824,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-10-13T03:56:46Z,2021-10-13T03:56:46Z,"The latest version of java && go has fixed the bug: 
https://github.com/XiaoMi/pegasus-java-client/pull/156
https://github.com/XiaoMi/pegasus-go-client/pull/86","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44JCrq/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/824,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44JEwk,incubator-pegasus,941902884,824,NA,cauchy1988,7292411,,,NA,2021-10-13T04:12:58Z,2021-10-13T04:12:58Z,"i think the best way to fix this is in the server side :   in this way, client  scanner's process  can  resume from break-point","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44JEwk/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/824,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44JFdK,incubator-pegasus,941905738,824,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2021-10-13T04:17:56Z,2021-10-13T04:17:56Z,"> i think the best way to fix this is in the server side : in this way, client scanner's process can resume from break-point

yes, but it's difficult to achieve. You can put an issue of your design.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44JFdK/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/824,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44JH-C,incubator-pegasus,941916034,824,NA,cauchy1988,7292411,,,NA,2021-10-13T04:43:25Z,2021-10-13T04:43:25Z,"@Smityz @Shuo-Jia it is not the problem fixed in XiaoMi/pegasus-java-client#156  and XiaoMi/pegasus-go-client#86 ;
 it's a new problem; 
int this problem 
![image](https://user-images.githubusercontent.com/7292411/137068265-a7b7a030-0bea-4f87-b968-c025c16bd7d6.png)
in this problem, server side lost its context with the client and java client's logic will meet the  block showing in the above picture;
int this case,  java client will  call 'on_get_scanners' again and restart the scan process,  but at this time, the ""start_key"" field int the request struct will be filled with hashkey then cause the bug  i have said above ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44JH-C/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/824,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44JgV6,incubator-pegasus,942015866,824,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2021-10-13T07:39:35Z,2021-10-13T07:39:35Z,okay，I see,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44JgV6/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/826,https://api.github.com/repos/apache/incubator-pegasus/issues/826,incubator-pegasus,1025073792,826,Can't build C++ client by tutorial ,Smityz,22953824,Smilencer,smityz@qq.com,CLOSED,2021-10-13T10:31:06Z,2023-05-16T16:19:51Z,"```
../lib/libpegasus_client_static.a(sasl_server_wrapper.cpp.o): In function `dsn::security::sasl_server_wrapper::start(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, dsn::blob const&, dsn::blob&)':
/home/tangyanzhao/Code/pegasus/rdsn/src/runtime/security/sasl_server_wrapper.cpp:43: undefined reference to `dsn::fail::_S_FAIL_POINT_ENABLED'
/home/tangyanzhao/Code/pegasus/rdsn/src/runtime/security/sasl_server_wrapper.cpp:51: undefined reference to `sasl_server_start'
/home/tangyanzhao/Code/pegasus/rdsn/src/runtime/security/sasl_server_wrapper.cpp:43: undefined reference to `dsn::fail::eval[abi:cxx11](dsn::string_view)'
/home/tangyanzhao/Code/pegasus/rdsn/src/runtime/security/sasl_server_wrapper.cpp:43: undefined reference to `dsn::error_code::try_get(char const*, dsn::error_code)'
../lib/libpegasus_client_static.a(sasl_server_wrapper.cpp.o): In function `dsn::security::sasl_server_wrapper::init()':
/home/tangyanzhao/Code/pegasus/rdsn/src/runtime/security/sasl_server_wrapper.cpp:31: undefined reference to `dsn::fail::_S_FAIL_POINT_ENABLED'
/home/tangyanzhao/Code/pegasus/rdsn/src/runtime/security/sasl_server_wrapper.cpp:37: undefined reference to `sasl_server_new'
/home/tangyanzhao/Code/pegasus/rdsn/src/runtime/security/sasl_server_wrapper.cpp:31: undefined reference to `dsn::fail::eval[abi:cxx11](dsn::string_view)'
/home/tangyanzhao/Code/pegasus/rdsn/src/runtime/security/sasl_server_wrapper.cpp:31: undefined reference to `dsn::error_code::try_get(char const*, dsn::error_code)'
../lib/libpegasus_client_static.a(sasl_server_wrapper.cpp.o): In function `dsn::security::sasl_server_wrapper::step(dsn::blob const&, dsn::blob&)':
/home/tangyanzhao/Code/pegasus/rdsn/src/runtime/security/sasl_server_wrapper.cpp:59: undefined reference to `dsn::fail::_S_FAIL_POINT_ENABLED'
/home/tangyanzhao/Code/pegasus/rdsn/src/runtime/security/sasl_server_wrapper.cpp:66: undefined reference to `sasl_server_step'
/home/tangyanzhao/Code/pegasus/rdsn/src/runtime/security/sasl_server_wrapper.cpp:59: undefined reference to `dsn::fail::eval[abi:cxx11](dsn::string_view)'
/home/tangyanzhao/Code/pegasus/rdsn/src/runtime/security/sasl_server_wrapper.cpp:59: undefined reference to `dsn::error_code::try_get(char const*, dsn::error_code)'
../lib/libpegasus_client_static.a(sasl_server_wrapper.cpp.o): In function `_GLOBAL__sub_I_sasl_server_wrapper.cpp':
/home/tangyanzhao/Code/pegasus/rdsn/include/dsn/utility/error_code.h:132: undefined reference to `dsn::error_code::error_code(char const*)'
/home/tangyanzhao/Code/pegasus/rdsn/include/dsn/utility/error_code.h:131: undefined reference to `dsn::error_code::error_code(char const*)'
/home/tangyanzhao/Code/pegasus/rdsn/include/dsn/utility/error_code.h:130: undefined reference to `dsn::error_code::error_code(char const*)'
/home/tangyanzhao/Code/pegasus/rdsn/include/dsn/utility/error_code.h:129: undefined reference to `dsn::error_code::error_code(char const*)'
/home/tangyanzhao/Code/pegasus/rdsn/include/dsn/utility/error_code.h:128: undefined reference to `dsn::error_code::error_code(char const*)'
../lib/libpegasus_client_static.a(sasl_server_wrapper.cpp.o):/home/tangyanzhao/Code/pegasus/rdsn/include/dsn/utility/error_code.h:127: more undefined references to `dsn::error_code::error_code(char const*)' follow
../lib/libpegasus_client_static.a(builtin_counters.cpp.o): In function `dsn::builtin_counters::update_counters()':
/home/tangyanzhao/Code/pegasus/rdsn/src/perf_counter/builtin_counters.cpp:44: undefined reference to `dsn::utils::process_mem_usage(double&, double&)'
/home/tangyanzhao/Code/pegasus/rdsn/src/perf_counter/builtin_counters.cpp:49: undefined reference to `dsn_log_start_level'
/home/tangyanzhao/Code/pegasus/rdsn/src/perf_counter/builtin_counters.cpp:49: undefined reference to `dsn_logf(char const*, char const*, int, dsn_log_level_t, char const*, ...)'
collect2: error: ld returned 1 exit status
make: *** [pegasus_cpp_sample] Error 1
```
Can't build C++ client by tutorial in the https://pegasus.apache.org/clients/cpp-client
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/826/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/826,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5cYtnp,incubator-pegasus,1549982185,826,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-05-16T16:19:51Z,2023-05-16T16:19:51Z,"The master branch has added `sample` as a sub-directory which contains the usage of C++ client, and will be built everyday on various of plats. I'll close this issue, you can reopen it if you find the master branch build it failed, thanks.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5cYtnp/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/827,https://api.github.com/repos/apache/incubator-pegasus/issues/827,incubator-pegasus,1027165443,827,ut：meta server core，when recovery debug ut execute,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2021-10-15T07:36:57Z,2021-12-30T03:23:49Z,"after `./run.sh test` on type debug system program，I failed at recovery test.
![wecom-temp-bb6a04fa3098e75073cdb475d55cbeab](https://user-images.githubusercontent.com/48315319/137444617-73b28598-67b1-48f9-adeb-ab726b4f5feb.png)
meta server core msg
![image](https://user-images.githubusercontent.com/48315319/137449947-bdd1c7bb-c4a4-4332-ad1f-c7581ac893a2.png)


And then I add some message logs，I find field ""duplicating"" on struct app_info is null when I create a table.I am not sure what happened between create table and recovery, But I think the initial value should be false.
After set ""duplicating""=false for app_info on file rdsn/src/dsn.layer2.thrift, ut passed.
diff struct app_info init code:
![wecom-temp-aaf4b0eea7d821ebccac06b00b40bfda](https://user-images.githubusercontent.com/48315319/137450021-70cd01da-cafd-4c30-acb3-268b32ff917e.png)

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/827/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/827,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44l8Zd,incubator-pegasus,949470813,827,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2021-10-22T09:56:22Z,2021-10-22T09:56:22Z,Thanks for your nice work~ How about raise a pull request to fix this problem?,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44l8Zd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/827,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47Ir4d,incubator-pegasus,992132637,827,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2021-12-13T05:54:45Z,2021-12-13T05:54:45Z,Fixed at https://github.com/XiaoMi/rdsn/pull/976,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47Ir4d/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/830,https://api.github.com/repos/apache/incubator-pegasus/issues/830,incubator-pegasus,1030191573,830,Support read throttling by size,Smityz,22953824,Smilencer,smityz@qq.com,CLOSED,2021-10-19T11:31:12Z,2021-11-03T09:01:41Z,"### Design
I use `folly::BasicDynamicTokenBucket` to implement a throttling controller, which can limit read throughput accurately.
the throttling controller is divided into two parts to count data:
`token_bucket_throttling_controller::only_count` is used to count the read throughput.
`token_bucket_throttling_controller::control` is used to return the error code for the new coming request if the read throughput exceeds our limit.
This design guarantees that the request which is processing now won't be interrupted.

### Perf_counter
I reuse the `_counter_recent_read_throttling_reject_count` perf_counter to record the reject request.

### Additional parameters
I add a parameter named `read_throttling_by_size_request_count` to control the default value for the token bucket in `token_bucket_throttling_controller::control` because we don't know how much throughput it will make at the beginning. In my test, the default value 1 is available.

### How to use
like other read/write controllers, I save this value in the table-env. Due to not supporting `delay` method, you can use it like
`table-env set replica.read_throttling_by_size 30M` and it also supports the historical style like `20000*delay*100,20000*reject*100`, but `delay` won't be used.

_admin-cli example_
```
Pegasus-AdminCli-1.1.0 » ls
+-----+-----------+------------+-----------+----------------+---------------+------------+---------------+-----------------+
| ID  |   Name    | Partitions | Unhealthy | WriteUnhealthy | ReadUnhealthy | CreateTime | WReqRateLimit | WBytesRateLimit |
+-----+-----------+------------+-----------+----------------+---------------+------------+---------------+-----------------+
| 309 | test      | 4          | 0         | 0              | 0             | 2021-11-01 |               |                 |
| 312 | tyz_test1 | 64         | 0         | 0              | 0             | 2021-11-01 |               |                 |
+-----+-----------+------------+-----------+----------------+---------------+------------+---------------+-----------------+
Pegasus-AdminCli-1.1.0 » use tyz_test1
ok
Pegasus-AdminCli-1.1.0 » table-env set replica.read_throttling_by_size 100M
Pegasus-AdminCli-1.1.0 » table-env ls
{
  ""replica.read_throttling_by_size"": ""100M""
}

```

### Function test
![image](https://user-images.githubusercontent.com/22953824/139642396-3a50c0ab-1653-4b9b-b4be-c945b174f5cf.png)


### Related PR
https://github.com/apache/incubator-pegasus/pull/829
https://github.com/XiaoMi/rdsn/pull/938

### Split PR
https://github.com/XiaoMi/rdsn/pull/939 add an interface for rpc_holder to set error
https://github.com/XiaoMi/rdsn/pull/940 add an interface to get read_throttling_reject_count perf counter
https://github.com/XiaoMi/rdsn/pull/941 add a token_bucket_throttling_controller ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/830/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/832,https://api.github.com/repos/apache/incubator-pegasus/issues/832,incubator-pegasus,1032016635,832,partitions are unbalanced after the table is created,ZhongChaoqiang,35595648,Zhong Chaoqiang,,CLOSED,2021-10-21T03:21:13Z,2021-12-30T03:23:18Z,"1. What did you do?
There are 5 nodes in our onebox cluster. 
We create a app of 100 partitions.But partitions are unbalanced after the table is created.
```
>>> create test_100 -p 100
create app test_100 succeed, waiting for app ready
test_100 not ready yet, still waiting... (0/100)
test_100 not ready yet, still waiting... (0/100)
test_100 not ready yet, still waiting... (0/100)
test_100 not ready yet, still waiting... (0/100)
test_100 not ready yet, still waiting... (81/100)
test_100 not ready yet, still waiting... (81/100)
test_100 not ready yet, still waiting... (81/100)
test_100 not ready yet, still waiting... (81/100)
test_100 not ready yet, still waiting... (81/100)
test_100 not ready yet, still waiting... (99/100)
test_100 not ready yet, still waiting... (99/100)
test_100 not ready yet, still waiting... (99/100)
test_100 not ready yet, still waiting... (99/100)
test_100 not ready yet, still waiting... (99/100)
test_100 is ready now: (100/100)
test_100 is ready now!
create app ""test_100"" succeed
>>> ls -d
[general_info]
app_id  status     app_name  app_type  partition_count  replica_count  is_stateful  create_time          drop_time  drop_expire  envs_count  is_bulkload
3       AVAILABLE  test_100  pegasus   100              3              true         2021-10-21_11:16:44  -          -            0           false

[healthy_info]
app_id  app_name  partition_count  fully_healthy  unhealthy  write_unhealthy  read_unhealthy
3       test_100  100              100            0          0                0

[summary]
total_app_count            : 1
fully_healthy_app_count    : 1
unhealthy_app_count        : 0
write_unhealthy_app_count  : 0
read_unhealthy_app_count   : 0

>>> nodes -d
[details]
address             status    replica_count  primary_count  secondary_count
172.XXX.XXX.XXX:24361  ALIVE                58             20               38
172.XXX.XXX.XXX:24362  ALIVE                67             20               47
172.XXX.XXX.XXX:24363  ALIVE                60             20               40
172.XXX.XXX.XXX:24364  ALIVE                57             20               37
172.XXX.XXX.XXX:24365  ALIVE                58             20               38

[summary]
total_node_count    : 5
alive_node_count    : 5
unalive_node_count  : 0
```

4. What version of Pegasus are you using?
2.0.0
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/832/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/832,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44h3yz,incubator-pegasus,948403379,832,NA,ZhongChaoqiang,35595648,Zhong Chaoqiang,,NA,2021-10-21T08:58:28Z,2021-10-21T08:58:28Z,"建表时，meta是通过cure流程来分replica的。在分配secondary的replica时，是通过寻找最少replica的节点的方式，来觉得当前replica应该分配到哪个节点上。
判断一个节点的replica数据是如下函数：
```
    int32_t newly_partitions::partition_count(int32_t app_id)
    {
        return owner->partition_count(app_id) + partitions[app_id];
    }
```
这其中包含了两个变量，第一个是节点已经上线的replica的数量，第二个是正在分配中的replica的数量。理论上当一个分配中的replica上线后，第一个变量要加1，第二个变量要减1。但是实际上两个操作并不是同时完成的，所以当某个分片上线后，就可能导致这个函数计算出来数量比实际数量要大。从而导致分配不均匀了。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM44h3yz/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/838,https://api.github.com/repos/apache/incubator-pegasus/issues/838,incubator-pegasus,1053342402,838,how to handle meta server update failure?,MoonShining,8097849,Moon,,CLOSED,2021-11-15T08:16:45Z,2021-11-18T06:35:51Z,"hi i have several questions about how pegasus handle some corner case

![image](https://user-images.githubusercontent.com/8097849/141746001-0ea75ac5-d106-427f-8de6-5a9424f745cc.png)

how pegasus handle meta server update failure in step4?， just retry forever？

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/838/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/838,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM45zZoE,incubator-pegasus,969775620,838,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2021-11-16T03:17:16Z,2021-11-16T03:17:16Z,"This step4 is for meta server to update new PartitionConfiguration into zk. If meta server failed to store metadata, its operation will depend on the error. If the error is timeout, it will retry, otherwise, meta server will kill itself and restart. In current design, zk is considered as a stable and reliable component, stores cluster metadata.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM45zZoE/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/838,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM454uLt,incubator-pegasus,971170541,838,NA,MoonShining,8097849,Moon,,NA,2021-11-17T04:25:29Z,2021-11-17T04:25:29Z,"@hycdong thanks for your explanation

another question is,  in step3 primary will send new config to metaserver, what to do if send fail? maybe kill primay itself?  ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM454uLt/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/838,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM455GIv,incubator-pegasus,971268655,838,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2021-11-17T07:01:02Z,2021-11-17T07:01:02Z,"In pegasus, the leader vote is handled by meta server, meaning which replica is primary and which is secondary is determined by meta server, not the replica group. 
In step2, primary will try to downgrade itself, and it will send a downgrade request to meta server, after that its status is not primary, but a status called inactive. Then, the meta server will choose a suitable secondary and a new primary. And then, meta server will send proposal to vote primary and add secondary to let replica group healthy(1 primary and 2 secondaries). 
The word in the picture simplifies the process, the request sent to meta is actually NOT the new PartitionConfiguration without the failed secondary, but the request downgrade primary itself. And if the primary failed while sending request, it doesn't matter, and meta server will know this replica group unhealthy in other ways. 
Hoping my answer helps you.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM455GIv/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/838,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM45-GK1,incubator-pegasus,972579509,838,NA,MoonShining,8097849,Moon,,NA,2021-11-18T06:35:51Z,2021-11-18T06:35:51Z,thx,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM45-GK1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/840,incubator-pegasus,1056850642,840,add 'feature flags' rcp interface  for version compatability between server and client,cauchy1988,7292411,,,OPEN,2021-11-18T02:20:50Z,2022-01-27T02:29:19Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

We have optimized an interface implementation of the pegasus java  client. The specific way is to define a new 'task_code' 、bind it to a new read interface function implemented by the server side and then reimplement the interface of the java client by calling the new read rpc interface represented by the new 'task_code'; but this will cause the problem of version matching between the client side and the server side: the new version client can not visit old version server because old version server cannot recognize the new  task-code in the request-struct;
 
Since my company is a ""to b"" company, we can not upgrade all the deployed pegasus-server cluster in a comparatively short time;
 We don't want to influence the usage of the old client interface either,  so we implement a new java client interface instead with the same functionality and make it compatible with the new and old version server at the same time.
 
How to implement the client interface in compatible with the new and old version server at the same time ? 
 
We have once used an unconsidered method:
we added a 'version'  field  which represents server code version to  the 'configuration_query_by_index_response'  structure returned  from the metaserver in the rpc-call represented by the task-code 'RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX' and then the java client interface implementation can do the corresponding  logic based by this fetched 'version' field： if ‘version' is higher than some pre-specified number, the remain logic will execute new optimized code and call the new rpc read interface ; otherwise, it will be the same with the old logic
 
This  'unconsidered' method is called 'unconsidered' because it  has the following problems:
(1) 'version' field can only represents metaserver's version, not all the servers' version in the same pegasus cluster, especially when upgrading is in progress
(2) it is unreasonble to use an  unrelated rpc-call and semantically unrelated thrift structure 'configuration_query_by_index_response' to fetch 'version' field
(3) It is very inflexible to judge the features supported by the server simply by comparing  the 'code version'
 
**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->

So later we realized that the version matching problem is a general problem, we thought of a more general method which we call it 'feature flags method', and the idea is inspired from 'kudu', the method is as following：
1、we can define new features as enums coded in the client side and server side at the same time
```
enum rpc_feature_flag {
       UNKNOWN = 0, 
        FEATURE_1 = 1,
}
```
2、we can also define a new 'task_code', for example 'RPC_CM_FEATURE_NEGOTIATE' and corresponding rpc related definitions：
```
struct negotiate_request {
 }

struct negotiate_response {
    1:list<rpc_feature_flag>  supported_features;
 }
service rrdb {
    negotiate_response negotiate(1:negotiate_request request);
}
```
3、we can then define a new class inherited from 'serverlet' ， implement the real ""negotiate"" rpc handler function and register it to 'rpc_engine'  in this class when related 'service_app' start
 
4、when a java or other language's  client  establishing connection with each instance of the server side, it also do the 'negotiate' rpc call to each of them, then it can 'know'  the features each server supported, thus can make next decisions
 
 
The above 'feature flags  method'  is not particularly detailed, for example it have not  described what the client would do when one of a  'negotiate' rpc  fails
I put it forward here, because I want to hear the opinions of the great gods here!


**Describe alternatives you've considered:**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->

**Teachability, Documentation, Adoption, Migration Strategy:**
<!-- If you can, explain some scenarios how users might use this, situations it would be helpful in. Any API designs, mockups, or diagrams are also helpful. -->
I think the server  side code change will  not be so hard,  as the following  pr
https://github.com/cauchy1988/incubator-pegasus/pull/1/files

Hower the client side  may be a little  difficult,  and  even for me it’s a bit vague to what extent the function needs to be implemented, the following pdf is the java client side design I have thought of
[feature negotiation client side simple design.pdf](https://github.com/apache/incubator-pegasus/files/7656247/feature.negotiation.client.side.simple.design.pdf)
 
In the content of above pdf, 'batchGet3' is the client interface we newly implemented which is a read-optimized client interface. The main logic is: a new BATCH_GET rpc interface is added in the server side; when the client calls batchGet3, if the client finds that the server supports BATCH_GET rpc Then send a BATCH_GET rpc request to the server, if it is found that the server does not support, then transfer to batchGet2 logic","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/840/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM46HmPs,incubator-pegasus,975070188,840,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2021-11-22T04:08:26Z,2021-11-22T04:08:26Z,"I think this is a common use case, not only in your company's case.
Server side version and client side version may not upgrade synchronized in real world, server may be newer than client, and client may also be newer than server.
And, some features may be introduced since some version, and it's possible to remove some features since some version.
Do such a feature based negotiation is reasonable.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM46HmPs/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM46Ro5R,incubator-pegasus,977702481,840,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-11-24T09:39:03Z,2021-11-24T09:39:03Z,"> I think this is a common use case, not only in your company's case. Server side version and client side version may not upgrade synchronized in real world, server may be newer than client, and client may also be newer than server. And, some features may be introduced since some version, and it's possible to remove some features since some version. Do such a feature based negotiation is reasonable.

+1, the future is necessary","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM46Ro5R/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM46zFRH,incubator-pegasus,986469447,840,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2021-12-06T06:10:12Z,2021-12-06T06:10:12Z,"It's a good design, but why not use `version` to determine which implementation to use","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM46zFRH/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM46zIEE,incubator-pegasus,986480900,840,NA,cauchy1988,7292411,,,NA,2021-12-06T06:35:00Z,2021-12-06T06:35:00Z,"> It's a good design, but why not use `version` to determine which implementation to use

This is also a solution, but you need to remember the mapping between each version and it's corresponding featurelist in both client and server side","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM46zIEE/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47gtBQ,incubator-pegasus,998428752,840,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2021-12-21T02:59:10Z,2021-12-21T02:59:10Z,"That's a good idea to use `feature_map` rather than a version to do client negotiation~

In current implmenetaion, client will firstly create connection with meta server, and get partition address map, then send read/write requests to replica server directly. I notice that the `negotiate_feature_flag` is added in rrdb.thrift according to https://github.com/cauchy1988/incubator-pegasus/pull/1/files, which means client should firstly connect to meta server, then send negotiate request to replica server, if the negotitate failed, client should do lots of thems such as downgrade batchget3 into batchget2 and other consistent-related error handling. 

It seems that the negotiation is designed especially for the batchget3 interface or user request. It can't handle any server request negotiation. In my view, a general negotitation plan should handle user request and server ddl request. For example, client sends negotiation request to meta server, if the operation is not supported, it will reject the connection of this client. Then client could choose to downgrade or do operation. I wonder if you have any plan about to implement a more general negotitation plan?

Besides, I have the following compatible questions:
1. old client -> new server, will have any compatible problem?
2. new client -> old server, how old server handle `BATCH_GET` code? will have any compatible problem?
3. old server upgrade to new server, will it have any compatible problem?

At the end, I DO think your issue is a good idea to solve user request version problems, and expecting your anwser and discussion~","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47gtBQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47hx3Y,incubator-pegasus,998710744,840,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-12-21T11:42:24Z,2021-12-21T11:42:24Z,"
> Hower the client side may be a little difficult, and even for me it’s a bit vague to what extent the function needs to be implemented, the following pdf is the java client side design I have thought of
> [feature negotiation client side simple design.pdf](https://github.com/apache/incubator-pegasus/files/7656247/feature.negotiation.client.side.simple.design.pdf)

I notice that the `negotiation` seem to be apply to `meta` and `replica`,   I consider  whether just `negotiation` with `meta` to fetch the feature supporting list. we don't have to handle the different version among `replica`, `meta` can represents the current cluster version. if client request is not supported by replica server, replica server will return error, and triger update `meta` config update. Of course, the meta server need upgrade at last , I think it can be acceptable.

Just consider `meta` maybe let your implement more simple.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47hx3Y/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47hyOP,incubator-pegasus,998712207,840,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-12-21T11:45:07Z,2021-12-21T11:45:07Z,"If you still want to handle the replica version, you can let meta to sync the replica server in `config_sync` and return the client.

In short, I think it will be clearer if you only deal with the interaction with meta","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47hyOP/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47-xqW,incubator-pegasus,1006312086,840,NA,cauchy1988,7292411,,,NA,2022-01-06T06:13:11Z,2022-01-06T06:13:11Z,"> It seems that the negotiation is designed especially for the batchget3 interface or

My idea is: the new server itself should be compatible with the old client, this is a common practice in the industry;
in this issue I focus on solving the problem of compatibility with the old server when the new client develops new functions;
Then answer your three questions above:
1. old client-->new server: The new server itself needs to be compatible with the old client as mentioned above, so this is not a problem
2. new client --> old server: as the method of this issue, downgraded to the old way of use
3. Old server upgrade to new server: firstly, there is no problem with the old client continuing to use. secondly , the new client will follow the client's implementation plan in this issue: it will ensure that all downstream services support the new feature before sending the rpc of the new protocol  Request, otherwise use the old","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47-xqW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47-yvB,incubator-pegasus,1006316481,840,NA,cauchy1988,7292411,,,NA,2022-01-06T06:24:36Z,2022-01-06T06:24:36Z,"> If you still want to handle the replica version, you can let meta to sync the replica server in `config_sync` and return the client.
> 
> In short, I think it will be clearer if you only deal with the interaction with meta

it's really a simpler way; let me think for a while","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47-yvB/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47-4of,incubator-pegasus,1006340639,840,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2022-01-06T07:24:16Z,2022-01-06T07:24:16Z,"1. Periodic update server version in the client 
2. The client automatically matches the corresponding implementation of the server. (like if server_version > 2.4 then use batchget3 else use batchget2)

In this way, we don't need to change the codes in the server. Clients can handle everything about Negotiation.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47-4of/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47_Fyd,incubator-pegasus,1006394525,840,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-01-06T09:06:22Z,2022-01-06T09:06:22Z,"> It's a good design, but why not use `version` to determine which implementation to use

Some features may be introduced since some version, and it's possible to remove some features since some version, it's diffcult and complex to maintainace such a mapping.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47_Fyd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47_IB3,incubator-pegasus,1006403703,840,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-01-06T09:20:40Z,2022-01-06T09:20:40Z,"


> > Hower the client side may be a little difficult, and even for me it’s a bit vague to what extent the function needs to be implemented, the following pdf is the java client side design I have thought of
> > [feature negotiation client side simple design.pdf](https://github.com/apache/incubator-pegasus/files/7656247/feature.negotiation.client.side.simple.design.pdf)
> 
> I notice that the `negotiation` seem to be apply to `meta` and `replica`, I consider whether just `negotiation` with `meta` to fetch the feature supporting list. we don't have to handle the different version among `replica`, `meta` can represents the current cluster version. if client request is not supported by replica server, replica server will return error, and triger update `meta` config update. Of course, the meta server need upgrade at last , I think it can be acceptable.
> 
> Just consider `meta` maybe let your implement more simple.

Agree, client fetch ""feature map"" from meta server would be more simple, and also reduce rpcs between client and servers. We can extend 'config_sync' betwen MS and RS to include replica server's 'feature map'. And it's reasonable for MS to know RS 'feature map'.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47_IB3/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47_ZjM,incubator-pegasus,1006475468,840,NA,cauchy1988,7292411,,,NA,2022-01-06T10:56:23Z,2022-01-06T10:56:23Z,"> If you still want to handle the replica version, you can let meta to sync the replica server in `config_sync` and return the client.
> 
> In short, I think it will be clearer if you only deal with the interaction with meta

implement this issue in this way ? @acelyc111 @hycdong @Smityz @Shuo-Jia  Any of you have any comments？","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47_ZjM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48PchM,incubator-pegasus,1010681932,840,NA,cauchy1988,7292411,,,NA,2022-01-12T06:26:59Z,2022-01-12T06:26:59Z,"> If you still want to handle the replica version, you can let meta to sync the replica server in `config_sync` and return the client.
> 
> In short, I think it will be clearer if you only deal with the interaction with meta

I thought about how to implement it， and draw a simple picture as below
![image](https://user-images.githubusercontent.com/7292411/149073904-23d00661-59b3-477a-92e0-b5a1390d8964.png)

The main process of implementation is divided into two parts:
1、Interaction between leader meta server and replica server
2、interaction between java client and leader meta server
The two parts will be described as follows
 
1、Interaction between leader meta server and replica server
Leader meta server stores the list of feature lists supported by each replica server in its own local memory; at the same time, this list should be modified according to three possible situations: (1) The feature list for some replica server stored in leader meta server is different  to  feature list received from the config sync request sent by the replica server  (2) A replica server was originally not in the local memory records (it was considered to be down before) but is judged alive by the leader meta server, and its corresponding feature list record needs to be added later by corresponding config sync rpc (3) A replica server has been delayed Sending a heartbeat to the leader meta server; so it was judged dead by the leader meta server, and its entire feature list record needs to be deleted from the local memory records
 
Situation(2) and(3) will be triggered by pegasus already realized 'failure_detector' mechanism
and thus easy to reuse
 
2、The interaction between java client and leader meta server
The java client may pull outdated feature list data, so we need a timer repeated scheduled function process used to continuously update the local feature list periodically to solve this problem

However This implementation still has the following problems:
(1)The server implementation is more complicated and needs to be well tested
(2)Since the client only pulls the  features  list from the lead meta server, the client may pull the list of old version, so the client can only start a periodically scheduled task and continuously pull the feature list from the leader meta server to update its local one
(3)The codes which meta server interact with replica server and supported feature definition are all in the rdsn, but  the business code of BATCH_GET rpc is in the pegaus.   although in the future the two code repository will combine into one  

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48PchM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48sxSC,incubator-pegasus,1018369154,840,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2022-01-21T10:16:33Z,2022-01-21T10:16:33Z,"## 1
> The java client may pull outdated feature list data, so we need a timer repeated scheduled function process used to continuously update the local feature list periodically...

Refer to implementation `config update`,  you just update after the response of replica is `no support`. rather than boot one scheduled task

## 2
> The server implementation is more complicated and needs to be well tested

You still need implement same or more code on server side when you use the old design which client negotiation replica server, and the new design just apply the logic to meta and you can re-use `config-sync`, so I think it shouldn't be `more complicated`. 

You list the update case in 
> according to three possible situations

Actually, you may not consider these different case,  the `config-sync` just `run` and store the latest value. 

Just like @acelyc111 say: 

> We can extend 'config_sync' betwen MS and RS to include replica server's 'feature map'


## 3

> so the client can only start a periodically scheduled task

As above, refer to implementation of `meta config` , you don't need periodically scheduled task.

## 4
> but the business code of BATCH_GET rpc is in the pegaus

They are two feature, and locate different git-rep is acceptable, it will be improved after they are combined
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48sxSC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48s1IE,incubator-pegasus,1018384900,840,NA,cauchy1988,7292411,,,NA,2022-01-21T10:36:26Z,2022-01-21T10:36:26Z,"> ## 1
> > The java client may pull outdated feature list data, so we need a timer repeated scheduled function process used to continuously update the local feature list periodically...
> 
> Refer to implementation `config update`, you just update after the response of replica is `no support`. rather than boot one scheduled task
> 
> ## 2
> > The server implementation is more complicated and needs to be well tested
> 
> You still need implement same or more code on server side when you use the old design which client negotiation replica server, and the new design just apply the logic to meta and you can re-use `config-sync`, so I think it shouldn't be `more complicated`.
> 
> You list the update case in
> 
> > according to three possible situations
> 
> Actually, you may not consider these different case, the `config-sync` just `run` and store the latest value.
> 
> Just like @acelyc111 say:
> 
> > We can extend 'config_sync' betwen MS and RS to include replica server's 'feature map'
> 
> ## 3
> > so the client can only start a periodically scheduled task
> 
> As above, refer to implementation of `meta config` , you don't need periodically scheduled task.
> 
> ## 4
> > but the business code of BATCH_GET rpc is in the pegaus
> 
> They are two feature, and locate different git-rep is acceptable, it will be improved after they are combined



**1**
I think it is still inevitable to add a scheduled timer update-process, think about this scenerio: java client fetched an already outdated feature list from leader metaserver, and this feature list doesn't contains 'BATCH_GET' feature, then java client will run in old logic; however, the whole server cluster has been upgraded to new version which support the 'BATCH_GET' feature; the dilemma is that : java client will not get the no support response from the replica because replica also support old-logic-interface, and thus the client will go on running in old logic forever........

**2**
meta server should remember the feature list of all the replica server in a 'map like' datastructure, and judge one feature is supported only when all featurelist of every replica server contains this feature;    so we should dynamically update this ""map-like' datastructure  in three possible conditions i decribed ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48s1IE/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48s4oY,incubator-pegasus,1018399256,840,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2022-01-21T10:55:56Z,2022-01-21T10:55:56Z,"> meta server should remember the feature list of all the replica server

Well, I see, you can select to resolve it based different cases. Another resolution is  the response just fill the `alive node`when query `feature_list`:
```c++
node_state _state; // global, `node_state` is regarded as latest `nodes-map config` in rdsn

feature_map _feature; // global, it may include expired node or config;

feature_map resp;// the actually need resp
for(n: node_state) {
resp.put(n, _feature[n]);
}
```

client can triger update if the config is still older and receive error. It's also say, the config update is lazy, which no need be precisely
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48s4oY/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48s5Q_,incubator-pegasus,1018401855,840,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2022-01-21T10:59:35Z,2022-01-21T10:59:35Z,"> java client will not get the no support response

Yeah, I see, you are right","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48s5Q_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48wS5d,incubator-pegasus,1019293277,840,NA,cauchy1988,7292411,,,NA,2022-01-22T15:34:39Z,2022-01-22T15:34:39Z,"> > java client will not get the no support response
> 
> Yeah, I see, you are right

so sadly, I should implement a periodical running task to update the newest feature list from the leader meta server,  can have any better way?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48wS5d/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/840,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM489n12,incubator-pegasus,1022786934,840,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2022-01-27T02:29:19Z,2022-01-27T02:29:19Z,"> > > java client will not get the no support response
> > 
> > 
> > Yeah, I see, you are right
> 
> so sadly, I should implement a periodical running task to update the newest feature list from the leader meta server, can have any better way?

Ok, it's feasible","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM489n12/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/841,https://api.github.com/repos/apache/incubator-pegasus/issues/841,incubator-pegasus,1061014828,841,build sample of c++ client 2.1.0 failed,yanz0920,29600746,,,CLOSED,2021-11-23T09:32:23Z,2023-05-16T16:16:02Z,"## Bug Report

Following the instructions in https://pegasus.apache.org/docs/build/compile-from-source/, I have build the pegasus successfully. BUT, I failed in building the sample in  pegasus-client-2.1.0-non-git-glibc2.27-release/sample, here is the outputs:

../lib/libpegasus_client_static.a(pegasus_client_impl.cpp.o): In function `pegasus::client::pegasus_client_impl::get_error_string(int) const':
/software/apache-pegasus-2.1.0-incubating-src/src/client_lib/pegasus_client_impl.cpp:1272: undefined reference to `dsn_log_start_level'
/software/apache-pegasus-2.1.0-incubating-src/src/client_lib/pegasus_client_impl.cpp:1272: undefined reference to `dsn_logf(char const*, char const*, int, dsn_log_level_t, char const*, ...)'
/software/apache-pegasus-2.1.0-incubating-src/src/client_lib/pegasus_client_impl.cpp:1272: undefined reference to `dsn_logf(char const*, char const*, int, dsn_log_level_t, char const*, ...)'
../lib/libpegasus_client_static.a(pegasus_client_impl.cpp.o): In function `pegasus::client::pegasus_client_impl::get_client_error(int)':
/software/apache-pegasus-2.1.0-incubating-src/src/client_lib/pegasus_client_impl.cpp:1310: undefined reference to `dsn_log_start_level'
/software/apache-pegasus-2.1.0-incubating-src/src/client_lib/pegasus_client_impl.cpp:1310: undefined reference to `dsn::error_code::to_string() const'
/software/apache-pegasus-2.1.0-incubating-src/src/client_lib/pegasus_client_impl.cpp:1310: undefined reference to `dsn_logf(char const*, char const*, int, dsn_log_level_t, char const*, ...)'
../lib/libpegasus_client_static.a(pegasus_client_impl.cpp.o): In function `pegasus::client::pegasus_client_impl::get_scanner(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, pegasus::pegasus_client::scan_options const&, pegasus::pegasus_client::pegasus_scanner*&)':
/software/apache-pegasus-2.1.0-incubating-src/src/client_lib/pegasus_client_impl.cpp:1136: undefined reference to `dsn_log_start_level'
/software/apache-pegasus-2.1.0-incubating-src/src/client_lib/pegasus_client_impl.cpp:1141: undefined reference to `dsn_log_start_level'
/software/apache-pegasus-2.1.0-incubating-src/src/client_lib/pegasus_client_impl.cpp:1141: undefined reference to `dsn_logf(char const*, char const*, int, dsn_log_level_t, char const*, ...)'
/software/apache-pegasus-2.1.0-incubating-src/src/client_lib/pegasus_client_impl.cpp:1136: undefined reference to `dsn_logf(char const*, char const*, int, dsn_log_level_t, char const*, ...)'
../lib/libpegasus_client_static.a(pegasus_client_impl.cpp.o): In function `pegasus::client::pegasus_client_impl::async_duplicate(dsn::rpc_holder<dsn::apps::duplicate_request, dsn::apps::duplicate_response>, std::function<void (dsn::error_code)>&&, dsn::task_tracker*)':
/software/apache-pegasus-2.1.0-incubating-src/DSN_ROOT/include/dsn/cpp/rpc_holder.h:128: undefined reference to `dsn_log_start_level'
/software/apache-pegasus-2.1.0-incubating-src/DSN_ROOT/include/dsn/cpp/rpc_holder.h:128: undefined reference to `dsn_logf(char const*, char const*, int, dsn_log_level_t, char const*, ...)'
/software/apache-pegasus-2.1.0-incubating-src/DSN_ROOT/include/dsn/cpp/rpc_holder.h:128: undefined reference to `dsn_logf(char const*, char const*, int, dsn_log_level_t, char const*, ...)'
../lib/libpegasus_client_static.a(pegasus_client_impl.cpp.o): In function `pegasus::client::pegasus_client_impl::pegasus_client_impl(char const*, char const*)':
/software/apache-pegasus-2.1.0-incubating-src/src/client_lib/pegasus_client_impl.cpp:51: undefined reference to `dsn::rpc_address::assign_group(char const*)'
../lib/libpegasus_client_static.a(pegasus_client_impl.cpp.o): In function `pegasus::client::pegasus_client_impl::pegasus_client_impl(char const*, char const*)':
/software/apache-pegasus-2.1.0-incubating-src/DSN_ROOT/include/dsn/tool-api/group_address.h:57: undefined reference to `dsn::rpc_address::~rpc_address()'
/software/apache-pegasus-2.1.0-incubating-src/DSN_ROOT/include/dsn/tool-api/group_address.h:57: undefined reference to `dsn::rpc_address::rpc_address(dsn::rpc_address const&)'
../lib/libpegasus_client_static.a(pegasus_client_impl.cpp.o): In function `pegasus::client::pegasus_client_impl::pegasus_client_impl(char const*, char const*)':
/usr/include/c++/7/ext/new_allocator.h:136: undefined reference to `dsn::rpc_address::rpc_address(dsn::rpc_address const&)'
...","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/841/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/841,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5cYsLz,incubator-pegasus,1549976307,841,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-05-16T16:15:59Z,2023-05-16T16:15:59Z,"The master branch has added `sample` as a subdirectory, and will be built everyday on various of plats. I'll close this issue, you can reopen it if you find the master branch build it failed, thanks.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5cYsLz/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/844,https://api.github.com/repos/apache/incubator-pegasus/issues/844,incubator-pegasus,1066962737,844,coredump in redis parser,levy5307,22141103,赵立伟,zlw5307@163.com,OPEN,2021-11-30T08:56:39Z,2022-01-28T09:33:01Z,"## Bug Report
### Pegasus Version 2.0.0
### Core Dump
```
(gdb) #0  operator++ (this=<synthetic pointer>)
    at /home/wutao1/app/include/c++/4.8.2/bits/stl_list.h:235
#1  __distance<std::_List_const_iterator<pegasus::geo::SearchResult> > (
    __last=..., __first=...)
    at /home/wutao1/app/include/c++/4.8.2/bits/stl_iterator_base_funcs.h:82
#2  distance<std::_List_const_iterator<pegasus::geo::SearchResult> > (
    __last=..., __first=...)
    at /home/wutao1/app/include/c++/4.8.2/bits/stl_iterator_base_funcs.h:118
#3  size (this=0x7f5fe9a40bc0)
    at /home/wutao1/app/include/c++/4.8.2/bits/stl_list.h:874
#4  pegasus::proxy::redis_parser::process_geo_radius_result(pegasus::proxy::redis_parser::message_entry&, std::string const&, bool, bool, bool, int, std::list<pegasus::geo::SearchResult, std::allocator<pegasus::geo::SearchResult> >&&) (
    this=0x26b15998, entry=..., unit=..., WITHCOORD=false, WITHDIST=false,
    WITHHASH=true, ec=0,
    results=<unknown type in /home/work/packages/pegasus/c3srv-xiaoai-user_portrait_earthquake-proxy/5d969e89cb42934800e326a2771844bffb2d3f40-20200616-114911/pegasus-tools-2.0.0-5d969e8-glibc2.12-release/DSN_ROOT/bin/pegasus_rproxy/pegasus_rproxy, CU 0x14f9e7, DIE 0x286507>)
    at /home/wutao1/pegasus-release/src/redis_protocol/proxy_lib/redis_parser.cpp:1095
#5  0x0000000000476858 in operator() (results=<optimized out>,
    ec=<optimized out>, __closure=<optimized out>)
    at /home/wutao1/pegasus-release/src/redis_protocol/proxy_lib/redis_parser.cpp:853
#6  std::_Function_handler<void(int, std::list<pegasus::geo::SearchResult, std::allocator<pegasus::geo::SearchResult> >&&), pegasus::proxy::redis_parser::geo_radius(pegasus::proxy::redis_parser::message_entry&)::__lambda37>::_M_invoke(const std::_Any_data &, int, <unknown type in /home/work/packages/pegasus/c3srv-xiaoai-user_portrait_earthquake-proxy/5d969e89cb42934800e326a2771844bffb2d3f40-20200616-114911/pegasus-tools-2.0.0-5d969e8-glibc2.12-release/DSN_ROOT/bin/pegasus_rproxy/pegasus_rproxy, CU 0x14f9e7, DIE 0x286679>) (__functor=...,
    __args#0=<optimized out>, __args#1=<optimized out>)
    at /home/wutao1/app/include/c++/4.8.2/functional:2071
#7  0x000000000049263f in operator() (results_=<optimized out>,
    __closure=0xe57a5a0)
    at /home/wutao1/pegasus-release/src/geo/lib/geo_client.cpp:527
#8  std::_Function_handler<void(std::list<std::list<pegasus::geo::SearchResult, std::allocator<pegasus::geo::SearchResult> >, std::allocator<std::list<pegasus::geo::SearchResult, std::allocator<pegasus::geo::SearchResult> > > >&&), pegasus::geo::geo_client::async_search_radial(const S2LatLng&, double, int, pegasus::geo::geo_client::SortType, int, pegasus::geo::geo_search_callback_t&&)::__lambda42>::_M_invoke(const std::_Any_data &, <unknown type in /home/work/packages/pegasus/c3srv-xiaoai-user_portrait_earthquake-proxy/5d969e89cb42934800e326a2771844bffb2d3f40-20200616-114911/pegasus-tools-2.0.0-5d969e8-glibc2.12-release/DSN_ROOT/bin/pegasus_rproxy/pegasus_rproxy, CU 0x2a7df2, DIE 0x380d88>) (
    __functor=..., __args#0=<optimized out>)
    at /home/wutao1/app/include/c++/4.8.2/functional:2071
#9  0x0000000000495041 in operator() (geo_hash_key=<optimized out>,
    info=<optimized out>, value=<optimized out>,
    geo_sort_key=<optimized out>, ret=<optimized out>, __closure=0x812bc20,
    this=<optimized out>)
    at /home/wutao1/pegasus-release/src/geo/lib/geo_client.cpp:886
#10 std::_Function_handler<void(int, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&&, pegasus::pegasus_client::internal_info&&), pegasus::geo::geo_client::do_scan(pegasus::pegasus_client::pegasus_scanner_wrapper, std::shared_ptr<S2Cap>, int, pegasus::geo::geo_client::scan_one_area_callback_t&&, std::list<pegasus::geo::SearchResult>&)::__lambda49>::_M_invoke(const std::_Any_data &, int, <unknown type in /home/work/packages/pegasus/c3srv-xiaoai-user_portrait_earthquake-proxy/5d969e89cb42934800e326a2771844bffb2d3f40-20200616-114911/pegasus-tools-2.0.0-5d969e8-glibc2.12-release/DSN_ROOT/bin/pegasus_rproxy/pegasus_rproxy, CU 0x2a7df2, DIE 0x392d84>, <unknown type in /home/work/packages/pegasus/c3srv-xiaoai-user_portrait_earthquake-proxy/5d969e89cb42934800e326a2771844bffb2d3f40-20200616-114911/pegasus-tools-2.0.0-5d969e8-glibc2.12-release/DSN_ROOT/bin/pegasus_rproxy/pegasus_rproxy, CU 0x2a7df2, DIE 0x392d89>, <unknown type in /home/work/packages/pegasus/c3srv-xiaoai-user_portrait_earthquake-proxy/5d969e89cb42934800e326a2771844bffb2d3f40-20200616-114911/pegasus-tools-2.0.0-5d969e8-glibc2.12-release/DSN_ROOT/bin/pegasus_rproxy/pegasus_rproxy, CU 0x2a7df2, DIE 0x392d8e>, <unknown type in /home/work/packages/pegasus/c3srv-xiaoai-user_portrait_earthquake-proxy/5d969e89cb42934800e326a2771844bffb2d3f40-20200616-114911/pegasus-tools-2.0.0-5d969e8-glibc2.12-release/DSN_ROOT/bin/pegasus_rproxy/pegasus_rproxy, CU 0x2a7df2, DIE 0x392d93>) (__functor=...,
    __args#0=<optimized out>, __args#1=<optimized out>,
    __args#2=<optimized out>, __args#3=<optimized out>,
    __args#4=<optimized out>)
    at /home/wutao1/app/include/c++/4.8.2/functional:2071
#11 0x000000000054eb11 in pegasus::client::pegasus_client_impl::pegasus_scanner_impl::_async_next_internal (this=this@entry=0x3784d7a0)
    at /home/wutao1/pegasus-release/src/client_lib/pegasus_scanner_impl.cpp:115
#12 0x000000000054f499 in pegasus::client::pegasus_client_impl::pegasus_scanner_impl::_on_scan_response (this=0x3784d7a0, err=..., req=<optimized out>,
    resp=<optimized out>)
    at /home/wutao1/pegasus-release/src/client_lib/pegasus_scanner_impl.cpp:230
#13 0x000000000054f95f in operator() (resp=<optimized out>,
    req=<optimized out>, err=..., __closure=<optimized out>)
    at /home/wutao1/pegasus-release/src/client_lib/pegasus_scanner_impl.cpp:205
#14 std::_Function_handler<void(dsn::error_code, dsn::message_ex*, dsn::message_ex*), pegasus::client::pegasus_client_impl::pegasus_scanner_impl::_start_scan()::__lambda16>::_M_invoke(const std::_Any_data &, dsn::error_code, dsn::message_ex *, dsn::message_ex *) (__functor=..., __args#0=...,
    __args#1=<optimized out>, __args#2=<optimized out>)
    at /home/wutao1/app/include/c++/4.8.2/functional:2071
#15 0x0000000000528317 in operator() (__args#2=0x55453cb8,
    __args#1=0x56d91436, __args#0=..., this=0x21ee37b0)
    at /home/wutao1/app/include/c++/4.8.2/functional:2464
#16 operator() (resp=0x55453cb8, req=0x56d91436, err=...,
    __closure=0x21ee37a0)
    at /home/wutao1/pegasus-release/rdsn/src/dist/replication/client/partition_resolver.cpp:94
#17 std::_Function_handler<void(dsn::error_code, dsn::message_ex*, dsn::message_ex*), dsn::replication::partition_resolver::call_task(const rpc_response_task_ptr&)::__lambda7>::_M_invoke(const std::_Any_data &, dsn::error_code, dsn::message_ex *, dsn::message_ex *) (__functor=..., __args#0=...,
    __args#1=0x56d91436, __args#2=0x55453cb8)
    at /home/wutao1/app/include/c++/4.8.2/functional:2071
#18 0x00000000005f8d1c in operator() (__args#2=<optimized out>,
    __args#1=<optimized out>, __args#0=..., this=<optimized out>)
    at /home/wutao1/app/include/c++/4.8.2/functional:2464
#19 dsn::rpc_response_task::exec (this=<optimized out>)
    at /home/wutao1/pegasus-release/rdsn/include/dsn/tool-api/task.h:480
#20 0x00000000005f68f9 in dsn::task::exec_internal (this=0x56d9161d)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/task.cpp:180
#21 0x00000000005f7241 in dsn::rpc_response_task::enqueue (this=0x56d9161d,
    err=..., err@entry=..., reply=reply@entry=0x55453cb8)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/task.cpp:573
#22 0x0000000000646f47 in dsn::rpc_client_matcher::on_recv_reply (
    this=0x2d90858, net=<optimized out>, key=<optimized out>,
    reply=reply@entry=0x55453cb8, delay_ms=delay_ms@entry=0)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/rpc_engine.cpp:186
#23 0x000000000066a6ff in dsn::rpc_session::on_recv_message (
    this=this@entry=0x2eb7040, msg=0x55453cb8, delay_ms=delay_ms@entry=0)
    at /home/wutao1/pegasus-release/rdsn/src/core/core/network.cpp:411
#24 0x000000000065fd79 in on_message_read (msg=<optimized out>,
    this=0x2eb7040)
    at /home/wutao1/pegasus-release/rdsn/src/core/tools/common/asio_rpc_session.h:63
#25 operator() (length=<optimized out>, __closure=0x7f5fe9a412d0, ec=...)
    at /home/wutao1/pegasus-release/rdsn/src/core/tools/common/asio_rpc_session.cpp:115
#26 operator() (this=0x7f5fe9a412d0)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/bind_handler.hpp:127
#27 asio_handler_invoke<boost::asio::detail::binder2<dsn::tools::asio_rpc_session::do_read(int)::__lambda2, boost::system::error_code, long unsigned int> > (
    function=...)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/handler_invoke_hook.hpp:69
#28 invoke<boost::asio::detail::binder2<dsn::tools::asio_rpc_session::do_read(int)::__lambda2, boost::system::error_code, long unsigned int>, dsn::tools::asio_rpc_session::do_read(int)::__lambda2> (context=..., function=...)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/handler_invoke_helpers.hpp:37
#29 boost::asio::detail::reactive_socket_recv_op<boost::asio::mutable_buffers_1, dsn::tools::asio_rpc_session::do_read(int)::__lambda2>::do_complete(boost::asio::detail::io_service_impl *, boost::asio::detail::operation *, const boost::system::error_code &, std::size_t) (owner=<optimized out>,
    base=<optimized out>)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/reactive_socket_recv_op.hpp:110
#30 0x00000000004a1379 in complete (bytes_transferred=<optimized out>,
    ec=..., owner=..., this=<optimized out>)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/task_io_service_operation.hpp:38
#31 do_run_one (ec=..., this_thread=..., lock=..., this=0x2da20e0)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/impl/task_io_service.ipp:372
#32 boost::asio::detail::task_io_service::run (this=0x2da20e0, ec=...)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/detail/impl/task_io_service.ipp:149
#33 0x0000000000657f86 in run (this=<optimized out>, ec=...)
    at /home/wutao1/boost_1_58_0/output/include/boost/asio/impl/io_service.ipp:66
#34 operator() (__closure=0x2eadf30)
    at /home/wutao1/pegasus-release/rdsn/src/core/tools/common/asio_net_provider.cpp:79
#35 _M_invoke<> (this=0x2eadf30)
    at /home/wutao1/app/include/c++/4.8.2/functional:1732
#36 operator() (this=0x2eadf30)
    at /home/wutao1/app/include/c++/4.8.2/functional:1720
#37 std::thread::_Impl<std::_Bind_simple<dsn::tools::asio_network_provider::start(dsn::rpc_channel, int, bool)::__lambda2()> >::_M_run(void) (this=0x2eadf18)
    at /home/wutao1/app/include/c++/4.8.2/thread:115
#38 0x00007f5feb3d3600 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>)
    at /home/qinzuoyan/git.xiaomi/pegasus/toolchain/objdir/../gcc-4.8.2/libstdc++-v3/src/c++11/thread.cc:84
#39 0x00007f5febee5dc5 in start_thread () from /lib64/libpthread.so.0
#40 0x00007f5feab3d73d in clone () from /lib64/libc.so.6
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/844/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/845,https://api.github.com/repos/apache/incubator-pegasus/issues/845,incubator-pegasus,1070440703,845,Feature: serialize time-consuming operations,GiantKing,10461183,,giantkingww@163.com,OPEN,2021-12-03T10:26:32Z,2021-12-21T07:19:56Z,"## Feature Request
There are some time-consuming operations in Pegasus:
recall, balance, backup, bulk load, restore, manual compact, partition split

It will have an impact on online service while it proceeding. Especially they are running in concurrent. So we have to serialize these operations.

We can add an operation status lock for meta. Before a operation begins, acquire this lock first. And release this lock as it finish.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/845/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/846,https://api.github.com/repos/apache/incubator-pegasus/issues/846,incubator-pegasus,1070448306,846,folly-rate-limiter in nfs may be block,foreverneverer,23136769,Jiashuo,js982986555@live.com,OPEN,2021-12-03T10:35:14Z,2021-12-21T07:20:17Z,"## Bug Report

We found the [folly-rate-limiter](https://github.com/XiaoMi/rdsn/blob/v2.2.3/src/nfs/nfs_client_impl.cpp#L276) may be ineffective and block forever though it is inited as large value. however, it will recovery after reseting it via [remote-command](https://github.com/XiaoMi/rdsn/blob/v2.2.3/src/nfs/nfs_client_impl.cpp#L564) 

We haven't found the reason and we are ready to add `switch` to control whether open the limiter. ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/846/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/847,https://api.github.com/repos/apache/incubator-pegasus/issues/847,incubator-pegasus,1071686840,847,support pegasus connect zookeeper which one use kerberos,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2021-12-06T03:28:07Z,2022-04-14T06:29:48Z,"## Feature Request
**Is your feature request related to a problem? Please describe:**
In our business, pegasus cluster need to connect zookeeper which use kerberos protocol. I want support it. 

**Describe the feature you'd like:**
Before version 2.3, we support it by customized zookeeper c client which convert java client use JNI. From version 3.7.0, zookeeper support c client use kerberos. I want to introduce it for pegasus.
Through my test, two parameter is required for 'zookeeper_init_sasl' different from 'zookeeper_init' which func get handle that represents a connection to the ZooKeeper service.

- Current implementation
```
_handle = zookeeper_init(zookeeper_session_mgr::instance().zoo_hosts(),
                         global_watcher,
                         zookeeper_session_mgr::instance().timeout(),
                         nullptr,
                         this,
                         0);
```

- Support Kerberos implementation
```
zoo_sasl_params_t sasl_params = { 0 };
sasl_params.service =""zookeeper""
sasl_params.mechlist = ""GSSAPI"";
_handle = zookeeper_init_sasl(zookeeper_session_mgr::instance().zoo_hosts(),
                              global_watcher,
                              zookeeper_session_mgr::instance().timeout(),
                              nullptr,
                              this,  
                              0,
                              NULL,
                              &sasl_params);
```

When we have completed the above initialization and own valid kerberos ticket, we finised this job. How to get valid kerberos ticket already implemented on our code (kinit_context.cpp: https://github.com/XiaoMi/rdsn/blob/fc41809ce1622a47a535a2316df91d4d626f35ed/src/runtime/security/kinit_context.cpp).

Finally, we need to add some configuration items:

1. FLAGS_enable_zookeeper_kerberos
2. FLAGS_zookeeper_kerberos_service_name: It's always zookeeper, but it can change more

by the way，this issue is same as https://github.com/XiaoMi/rdsn/issues/85","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/847/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/847,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM46zHGI,incubator-pegasus,986476936,847,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2021-12-06T06:25:24Z,2021-12-06T06:25:24Z,It's a good idea for pegasus to adapt to various environments. can you pull a request for the function?,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM46zHGI/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/848,https://api.github.com/repos/apache/incubator-pegasus/issues/848,incubator-pegasus,1073065578,848,Feature: support grouped flag validator,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2021-12-07T08:31:44Z,2021-12-30T03:22:50Z,https://github.com/XiaoMi/rdsn/pull/978,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/848/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/848,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47IvAD,incubator-pegasus,992145411,848,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2021-12-13T06:21:18Z,2021-12-13T06:21:18Z,Merged in https://github.com/XiaoMi/rdsn/pull/978,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47IvAD/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/849,https://api.github.com/repos/apache/incubator-pegasus/issues/849,incubator-pegasus,1073069049,849,Feature: restrict the replica_count while creating app,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2021-12-07T08:35:50Z,2021-12-28T11:35:54Z,https://github.com/XiaoMi/rdsn/pull/963,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/849/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/849,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47ufT9,incubator-pegasus,1002042621,849,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2021-12-28T11:35:44Z,2021-12-28T11:35:44Z,XiaoMi/rdsn#963 merged,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47ufT9/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/850,https://api.github.com/repos/apache/incubator-pegasus/issues/850,incubator-pegasus,1073096242,850,Feature: meta server support start and query manual compaction,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2021-12-07T09:06:54Z,2021-12-30T03:26:36Z,"In current implementation, meta server doesn't know any information about manual compaction. But manual compaction is a disk-consuming operation, we are planning to control such operations through meta server, including following:
1. Replica servers report manual compaction status to meta server
2. Meta server support starting manual compaction
3. Meta server support querying manual compaction status","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/850/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/851,https://api.github.com/repos/apache/incubator-pegasus/issues/851,incubator-pegasus,1073113248,851,Feature: support table online migration through backup and bulk load,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2021-12-07T09:22:31Z,2022-08-01T09:18:51Z,"In current implementation, table online migration is supported by duplication, this issue raise a new plan to implement it through cold backup and bulk load. In RocksDB, bulk load support a parameter called `ingest_behind`. If `ingest_behind=false`, the bulk loaded data is newest data, otherwise, the data is oldest. 
As a result, we raise backup and bulk load to implement table online migration, the migration step is:
1. client write old table and new table together
2. old table create a backup
3. convert backup into sst files
4. bulk load sst files with  `ingest_behind=true`
5. client stop write old table

The new online migration plan includes following code:
1. Add a new app_envs to control rocksdb ingest_behind option (immutable-option)
2. Add allow_ingest_behind for start bulk load request
3. Add a script to update rocksdb ingest_behind option (by close and reopen rocksdb)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/851/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/855,https://api.github.com/repos/apache/incubator-pegasus/issues/855,incubator-pegasus,1076514485,855,split the meta_function_level functions are replica clearing and replica balancing,happydongyaoyao,39403741,,,OPEN,2021-12-10T07:11:44Z,2023-01-13T06:27:59Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
When the disk capacity of the backend node of the cluster is insufficient, we first drop useless tables, but at this time, we do not dare to turn on the cluster load balancing function to automatically clean gar replicas, because we are worry about that the disk space will be filled up by the subsequent load balancing operation.

**Describe the feature you'd like:**
We want to be able to separate replica gar clearing and cluster replica load balancing from meta_lively function.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/855/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/855,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47DH0r,incubator-pegasus,990674219,855,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2021-12-10T07:14:15Z,2021-12-10T07:14:15Z,"Good idea, can you give a PR about this function?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47DH0r/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/855,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47DJN8,incubator-pegasus,990679932,855,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2021-12-10T07:24:48Z,2021-12-10T07:24:48Z,LGTM,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47DJN8/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/855,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SVb0a,incubator-pegasus,1381350682,855,NA,VirendraYadav1234,95998389,Virendra Yadav,,NA,2023-01-13T06:00:06Z,2023-01-13T06:00:06Z,I want to work on this issue please assign me this issue,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SVb0a/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/856,https://api.github.com/repos/apache/incubator-pegasus/issues/856,incubator-pegasus,1076903499,856,Feature: bulk load manager,GiantKing,10461183,,giantkingww@163.com,CLOSED,2021-12-10T14:25:49Z,2022-01-10T06:38:16Z,"## Feature Request
In the current implementation, bulkload result saved in external component. This increases the complexity of system deployment. I think it is better to record this result in the meta server. And then we can carry out bulk load directly.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/856/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/856,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47ZhDz,incubator-pegasus,996544755,856,NA,GiantKing,10461183,,giantkingww@163.com,NA,2021-12-17T08:55:38Z,2021-12-17T08:55:38Z,"https://github.com/apache/incubator-pegasus/pull/858
https://github.com/XiaoMi/rdsn/pull/986","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47ZhDz/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/859,https://api.github.com/repos/apache/incubator-pegasus/issues/859,incubator-pegasus,1085623775,859,Remove shared_log in dural WAL,levy5307,22141103,赵立伟,zlw5307@163.com,CLOSED,2021-12-21T09:33:43Z,2022-02-09T03:19:10Z,"## Feature Request

Remove shared_log, to improve cluster throughput by taking full advantages of  SSD's write concurrency

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/859/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/861,https://api.github.com/repos/apache/incubator-pegasus/issues/861,incubator-pegasus,1090854318,861,enhancemant(bulk_load): avoid unnecessary repeated ingestion,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2021-12-30T03:35:51Z,2022-01-25T06:05:29Z,"If one partition ingestion failed, all partitions will rollback to download stage, which will lead to repeated ingestion. However, if the primary and secondaries of a partition are not changed, and this partition has been ingested succeed, it should not ingest files again, because ingestion is heavy-disk-load operation and will reject user write requests. As a result, bulk load should avoid repeated ingestion.
If one partition has ingested succeed, its group addresses will stored in zk, after this partition turn to ingestion status again, it will check if this partition has ever ingestion succeed and the partitions' addresses not changed, it won't send ingestion again.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/861/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/863,https://api.github.com/repos/apache/incubator-pegasus/issues/863,incubator-pegasus,1091533878,863,Refactor socket lock in asio_rpc_session to asio::strand,Smityz,22953824,Smilencer,smityz@qq.com,CLOSED,2021-12-31T11:22:04Z,2022-01-24T09:28:43Z,"In this PR https://github.com/XiaoMi/rdsn/pull/263, we want to fix the core dump caused by  race condition, but the problem still happens in v2.2.2, So I want to use asio::strand to fix this bug.

Ref: https://www.boost.org/doc/libs/1_75_0/doc/html/boost_asio/overview/core/strands.html","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/863/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/865,https://api.github.com/repos/apache/incubator-pegasus/issues/865,incubator-pegasus,1094291574,865,Feature: support to get/set the replication factor of each table,empiredan,743379,Dan Wang,,CLOSED,2022-01-05T12:16:33Z,2022-06-21T07:23:24Z,"# 1. Motivation

Replication factor (RF), is the number of nodes (i.e. replica servers) where each partition of a table is replicated. In Apache Pegasus, we also call it *max replica count* or *replica count*.

Initially, the minimum RF was 3; Then, in order to reduce the cost for our users, in https://github.com/XiaoMi/rdsn/pull/932 we've supported RF=1 and RF=2, though it's not reliable enough for most distributed systems while RF is 1 or 2. 

In other words, the minimum RF has become 1. Imagine a scenario that we had 2 replica servers in a cluster where a table can only be created with RF=1 or RF=2. As the volume of data continues to grow, the capacity of  this cluster is not enough; Also, it's not scalable and reliable for the 2 replica servers.

Now we would add another 2 nodes into this cluster and increase the RF of each table to 3 to improve the reliability and scalability. Therefore, besides that RF is specified while creating table, it should be changeable after the table is created. To make RF settable dynamically, appropriate commands should be added into the management tool such as Pegasus Shell to query or update the RF.

# 2. Limitation

Previously we've restricted the RF while creating table in https://github.com/XiaoMi/rdsn/pull/963. These restriction rules will also be applied as the prerequisites before the RF could be updated successfully.

# 3. Usage

As is described above, the RF could be queried or updated by Pegasus Shell. Here is the patterns of the commands.

It should be noted that to follow the naming convention in Pegasus, `replica count` is used in the command to represent the RF.

## 3.1 Query the RF

```shell
get_replica_count <table_name>
```

After this command is executed, the RF of the specified table will be printed if it's successful; Otherwise the error will be printed.

## 3.2 Update the RF

```shell
set_replica_count <table_name> <replica_count>
```

Since this command will update the RF of a table, in case of incorrect operations a prompt will be printed immediately after the command is executed to ask if the administrator chooses to continue to update the RF. Then, it will tell the command is finished successfully, or, similarly with `get_replica_count`,  the error will be printed.

# 4. Implementation

Here we only illustrate the process of `set_replica_count`, since `get_replica_count` is quite simple.

## 4.1 Basic process

The whole process contains the following steps:
* Administrator inputs and confirms `set_replica_count` command by Pegasus Shell;
* The `set_replica_count` request is sent to Primary Meta Server;
* Meta Server checks whether the requested RF is valid, whether the table exists, etc.;
* Meta Server sets an environment variable for the table in case a duplicate request is processed;
* Meta Server submits to ZooKeeper to change the meta data;
* Meta Server repairs the number of replicas of the specified table to reach the target RF.

As the saying goes, ""A picture is worth a thousand words"". The process described above can be illustrated as the following sequence diagram:
![image](https://user-images.githubusercontent.com/743379/148215462-2ac493a3-df6c-4eb4-a273-67c2b7c90295.png)

Obviously it's showed in the sequence diagram that:
* Both partition-level and table-level meta data should be updated, with partition-levels first and table-level successively;
* Each partition is set one by one, and all the meta data are updated asynchronously.

As for the 2 levels of meta data, see the following section for details.

## 4.2 Set an environment variable to prevent the duplicate requests

After the request that updates RF is issued, however, another duplicate request may also be issued by mistake. Since the whole process that updates RF is not atomic, this may lead to undefined result.

In case the process is disrupted, we can first set an environment variable indicating that RF of the table is being updated. Once then another duplicate request is received, the env is checked first and this request will be rejected.

After the whole process is finished, which means RF has been updated safely, the env will be removed. Then, a new request that update RF can be accepted.

## 4.3 Update the meta data

An example is given here to describe both partition-level and table-level meta data.

Suppose that there is a table named `test2` with id `5`.

First we can list all the tables in the ZooKeeper Shell:
```shell
[zk: localhost:2181(CONNECTED) 5] ls /pegasus_cluster/apps/5
[0, 1, 2, 3, 4, 5, 6, 7]
```

Then, table-level meta data of the target table (id `5`) can be found as below, where `max_replica_count` is the RF to be updated:
```shell
[zk: localhost:2181(CONNECTED) 4] get /pegasus_cluster/apps/5
{""status"":""app_status::AS_AVAILABLE"",""app_type"":""pegasus"",""app_name"":""test2"",""app_id"":5,""partition_count"":8,""envs"":{},""is_stateful"":1,""max_replica_count"":2,""expire_second"":0,""create_second"":1611726087,""drop_second"":0,""duplicating"":0,""init_partition_count"":-1}
```

Under table-level is the partition-level meta data showed as below where there is also a `max_replica_count`:
```shell
[zk: localhost:2181(CONNECTED) 3] get /pegasus_cluster/apps/5/0
{""pid"":""5.0"",""ballot"":11,""max_replica_count"":2,""primary"":""10.20.156.227:34801"",""secondaries"":[""10.20.156.228:34801""],""last_drops"":[],""last_committed_decree"":3799,""partition_flags"":0}
```

## 4.4 Repair the number of replicas to reach RF

In Meta Server there is some thread in charge of detecting periodically (each `lb_interval_ms`), trying to repair the number of replicas of the table to reach the demanded RF. 

Thus we can draw a conclusion that it will take about `lb_interval_ms` (typically 10s) from when the meta data is updated until the number of replicas is repaired.

# 5. Schedule

This feature has been implemented and tested, then released as the internal version. In the internal release, https://github.com/XiaoMi/rdsn/pull/963 and this feature  are in the same PR. I split the monolithic PR into 2 independent ones:
* The first PR whose purpose is to provide some rules used to check whether an RF is valid, has been committed and merged.
* And another PR will be this feature. 

However there are still some tasks that haven't be done. Previously all of our scenarios are to increase the RF. We haven't tested decreasing the RF. 

Also, after the RF is decreased, there are 2 situations:
* No node is removed, and then the cluster is balanced;
* One or more nodes are removed, then the cluster is balanced.

For both situations, some operations should be tested together as a whole set of steps, such as `scripts/pegasus_offline_node_list.sh`.

Therefore after the unfinished jobs are done, this feature will be committed.

- [x] https://github.com/XiaoMi/rdsn/pull/1061
- [x] https://github.com/XiaoMi/rdsn/pull/1072
- [x] https://github.com/XiaoMi/rdsn/pull/1077
- [x] https://github.com/XiaoMi/rdsn/pull/1087
- [x] https://github.com/XiaoMi/rdsn/pull/1107
- [x] https://github.com/XiaoMi/rdsn/pull/1108
- [x] https://github.com/XiaoMi/rdsn/pull/1109
- [x] https://github.com/XiaoMi/rdsn/pull/1110
- [x] https://github.com/apache/incubator-pegasus/issues/995
- [x] https://github.com/apache/incubator-pegasus/pull/914
- [x] https://github.com/apache/incubator-pegasus/pull/999","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/865/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/865,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM478iH6,incubator-pegasus,1005724154,865,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2022-01-05T14:18:35Z,2022-01-05T14:18:35Z,Can load balancer help to recover the cluster after RF changes?,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM478iH6/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/865,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47-eKi,incubator-pegasus,1006232226,865,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2022-01-06T02:22:04Z,2022-01-06T02:22:04Z,"Good enhancement~

I have some questions about updating table replica count:
1. There're also cluster-level config called `max_replica_count`, will it be confict if cluster `max_replica_count` is not equal to table `max_replica_count`? Will important features work such as user read/write, learn, load balance, rolling_update?
2. Table `app_info` file will store in each replica disk (_dir/.app_info), if `max_replica_count` is updated, this file should also be updated, how to update it?
3. In your description, `cure` (executed each 10s) will delete redunant replica or add lacking replica. Could you please add some manual test results?

Expecting your reply~","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47-eKi/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/865,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47-has,incubator-pegasus,1006245548,865,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2022-01-06T03:00:24Z,2022-01-06T03:00:24Z,"> Can load balancer help to recover the cluster after RF changes?

It has no business with load balance when RF  changed","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47-has/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/865,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47-h4S,incubator-pegasus,1006247442,865,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2022-01-06T03:05:02Z,2022-01-06T03:05:02Z,"> Each partition is set one by one, and all the meta data are updated asynchronously.

Does it has problem when each partition and meta data updated asynchronously?  Assuming that we are changing RF from 1 to 3. Because they are updated asynchronously, there will be a moment that partition 0 has 3 replicas and partition 1 has only one replica. Will it work well?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47-h4S/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/865,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47_0Ga,incubator-pegasus,1006584218,865,NA,empiredan,743379,Dan Wang,,NA,2022-01-06T13:17:07Z,2022-01-06T13:17:07Z,"> Good enhancement~
> 
> I have some questions about updating table replica count:
> 
> 1. There're also cluster-level config called `max_replica_count`, will it be confict if cluster `max_replica_count` is not equal to table `max_replica_count`? Will important features work such as user read/write, learn, load balance, rolling_update?
> 2. Table `app_info` file will store in each replica disk (_dir/.app_info), if `max_replica_count` is updated, this file should also be updated, how to update it?
> 3. In your description, `cure` (executed each 10s) will delete redunant replica or add lacking replica. Could you please add some manual test results?
> 
> Expecting your reply~

Ok, let's discuss one by one:
1.  Does cluster-level config mean `max_allowed_replica_count` in https://github.com/XiaoMi/rdsn/pull/963 ? Actually `max_replica_count` is the RF, and `max_allowed_replica_count` is a the upper bound to which an RF can be set. Or we should rename `max_replica_count` as replication_factor to avoid misunderstanding ?
2. Good question ! This implementation has't written into `.app_info`, this is a bug ! To guarantee the atomicity, I think first the request `set_replica_count` can be replicated to each replica; Received the request, each replica writes it into `slog`, and then writes the new `max_replica_count` into `.app_info`. Even if a replica server fails during the process, the request will not be lost.
3. As is described in this issue, increasing `max_replica_count` has been tested sufficiently, however decreasing `max_replica_count` has not been tested. I'll test decreasing later to see if redundant replica can be processed correctly.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47_0Ga/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/865,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47_2bH,incubator-pegasus,1006593735,865,NA,empiredan,743379,Dan Wang,,NA,2022-01-06T13:29:49Z,2022-01-06T13:29:49Z,"> > Can load balancer help to recover the cluster after RF changes?
> 
> It has no business with load balance when RF changed

Yeah, as @levy5307 has said, I can explain this.

Actually `set_replica_count` just sets the meta data held by ZooKeeper. The number of replicas is repaired by `cure`, which is also mentioned by @hycdong.

For example, if we want to increase `max_replica_count`, say, from 2 to 3. A secondary should be added to reach RF 3. If  there's no dropped node, it will be added by `emergency`; Otherwise, a dropped node will be selected to deploy the replica. For details please see `partition_guardian::on_missing_secondary`.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47_2bH/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/865,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47_4lw,incubator-pegasus,1006602608,865,NA,empiredan,743379,Dan Wang,,NA,2022-01-06T13:44:06Z,2022-01-06T13:44:06Z,"> > Each partition is set one by one, and all the meta data are updated asynchronously.
> 
> Does it has problem when each partition and meta data updated asynchronously? Assuming that we are changing RF from 1 to 3. Because they are updated asynchronously, there will be a moment that partition 0 has 3 replicas and partition 1 has only one replica. Will it work well?

This is a problem of atomicity as has been discussed with @hycdong . Actually all the table-level and partition-level meta data, together with `.app_info` should be updated in a ""transaction"". Unfortunately, the whole process is not atomic except that we can introduce a mechanism to guarantee all the operations can be managed in a way of transaction.

However, in practice the update process of all meta data is very fast, usually finished within a second.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47_4lw/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/865,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48CMCb,incubator-pegasus,1007206555,865,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2022-01-07T07:50:51Z,2022-01-07T07:50:51Z,"> > Good enhancement~
> > I have some questions about updating table replica count:
> > 
> > 1. There're also cluster-level config called `max_replica_count`, will it be confict if cluster `max_replica_count` is not equal to table `max_replica_count`? Will important features work such as user read/write, learn, load balance, rolling_update?
> > 2. Table `app_info` file will store in each replica disk (_dir/.app_info), if `max_replica_count` is updated, this file should also be updated, how to update it?
> > 3. In your description, `cure` (executed each 10s) will delete redunant replica or add lacking replica. Could you please add some manual test results?
> > 
> > Expecting your reply~
> 
> Ok, let's discuss one by one:
> 
> 1. Does cluster-level config mean `max_allowed_replica_count` in [feat: restrict the replication factor while creating app XiaoMi/rdsn#963](https://github.com/XiaoMi/rdsn/pull/963) ? Actually `max_replica_count` is the RF, and `max_allowed_replica_count` is a the upper bound to which an RF can be set. Or we should rename `max_replica_count` as replication_factor to avoid misunderstanding ?
> 2. Good question ! This implementation has't written into `.app_info`, this is a bug ! To guarantee the atomicity, I think first the request `set_replica_count` can be replicated to each replica; Received the request, each replica writes it into `slog`, and then writes the new `max_replica_count` into `.app_info`. Even if a replica server fails during the process, the request will not be lost.
> 3. As is described in this issue, increasing `max_replica_count` has been tested sufficiently, however decreasing `max_replica_count` has not been tested. I'll test decreasing later to see if redundant replica can be processed correctly.

Thanks for your reply~ Continue to discuss:
1.  I mean meta option called `max_replicas_in_group` , you can reference it here:
https://github.com/XiaoMi/rdsn/blob/289eb4609ae781e3bcc6bbc6bfeae64dfd8fa785/src/meta/meta_options.cpp#L160-L161
2.  I suggest that you can update `max_replica_count` through `on_config_sync` into `.app_info`
3.  I think decreasing replica count may have some problems, expecting your test result~","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48CMCb/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/865,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48CkEl,incubator-pegasus,1007304997,865,NA,empiredan,743379,Dan Wang,,NA,2022-01-07T10:34:33Z,2022-01-07T10:34:33Z,"> > > Good enhancement~
> > > I have some questions about updating table replica count:
> > > 
> > > 1. There're also cluster-level config called `max_replica_count`, will it be confict if cluster `max_replica_count` is not equal to table `max_replica_count`? Will important features work such as user read/write, learn, load balance, rolling_update?
> > > 2. Table `app_info` file will store in each replica disk (_dir/.app_info), if `max_replica_count` is updated, this file should also be updated, how to update it?
> > > 3. In your description, `cure` (executed each 10s) will delete redunant replica or add lacking replica. Could you please add some manual test results?
> > > 
> > > Expecting your reply~
> > 
> > 
> > Ok, let's discuss one by one:
> > 
> > 1. Does cluster-level config mean `max_allowed_replica_count` in [feat: restrict the replication factor while creating app XiaoMi/rdsn#963](https://github.com/XiaoMi/rdsn/pull/963) ? Actually `max_replica_count` is the RF, and `max_allowed_replica_count` is a the upper bound to which an RF can be set. Or we should rename `max_replica_count` as replication_factor to avoid misunderstanding ?
> > 2. Good question ! This implementation has't written into `.app_info`, this is a bug ! To guarantee the atomicity, I think first the request `set_replica_count` can be replicated to each replica; Received the request, each replica writes it into `slog`, and then writes the new `max_replica_count` into `.app_info`. Even if a replica server fails during the process, the request will not be lost.
> > 3. As is described in this issue, increasing `max_replica_count` has been tested sufficiently, however decreasing `max_replica_count` has not been tested. I'll test decreasing later to see if redundant replica can be processed correctly.
> 
> Thanks for your reply~ Continue to discuss:
> 
> 1. I mean meta option called `max_replicas_in_group` , you can reference it here:
>    https://github.com/XiaoMi/rdsn/blob/289eb4609ae781e3bcc6bbc6bfeae64dfd8fa785/src/meta/meta_options.cpp#L160-L161
> 2. I suggest that you can update `max_replica_count` through `on_config_sync` into `.app_info`
> 3. I think decreasing replica count may have some problems, expecting your test result~

Thanks for suggestion !

1. I think we can eliminate `max_replicas_in_group` and turn to partition-level `max_replica_count`. I'll explain this as below:

Now `max_replicas_in_group` is only used to update `config_context::MAX_REPLICA_COUNT_IN_GRROUP`; And `config_context::MAX_REPLICA_COUNT_IN_GRROUP` is only used in:
```c++
void config_context::check_size()
{
    // when add learner, it is possible that replica_count > max_replica_count, so we
    // need to remove things from dropped only when it's not empty.
    while (replica_count(*config_owner) + dropped.size() > MAX_REPLICA_COUNT_IN_GRROUP &&
           !dropped.empty()) {
        dropped.erase(dropped.begin());
        prefered_dropped = (int)dropped.size() - 1;
    }
}
```

However, `MAX_REPLICA_COUNT_IN_GRROUP` can be replaced with partition-level `max_replica_count` as below:

```c++
void config_context::check_size()
{
    // when add learner, it is possible that replica_count > max_replica_count, so we
    // need to remove things from dropped only when it's not empty.
    while (replica_count(*config_owner) + dropped.size() > config_owner->max_replica_count &&
           !dropped.empty()) {
        dropped.erase(dropped.begin());
        prefered_dropped = (int)dropped.size() - 1;
    }
}
```

The `partition_configuration` pointed by `config_owner` is updated in `server_state::on_update_configuration_on_remote_reply`, which will be called by `set_replica_count`.
 
2. Good idea ! Each `config_sync_interval_ms` (typically 30 ms) the replica server requests `query_configuration_by_node` to the meta server; Received the response, if the ballot of the response is newer than the local, the replica server will update every table-level and partition-level `max_replica_count` to the one of the response. Is it right ?

3. Actually decreasing replica count has been forbidden till now by us. Decreasing is more error-prone than increasing. If we decide to support decreasing replica count, I'll test sufficiently.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48CkEl/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/865,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48Ximp,incubator-pegasus,1012804009,865,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2022-01-14T06:13:08Z,2022-01-14T06:13:08Z,"> > > > Good enhancement~
> > > > I have some questions about updating table replica count:
> > > > 
> > > > 1. There're also cluster-level config called `max_replica_count`, will it be confict if cluster `max_replica_count` is not equal to table `max_replica_count`? Will important features work such as user read/write, learn, load balance, rolling_update?
> > > > 2. Table `app_info` file will store in each replica disk (_dir/.app_info), if `max_replica_count` is updated, this file should also be updated, how to update it?
> > > > 3. In your description, `cure` (executed each 10s) will delete redunant replica or add lacking replica. Could you please add some manual test results?
> > > > 
> > > > Expecting your reply~
> > > 
> > > 
> > > Ok, let's discuss one by one:
> > > 
> > > 1. Does cluster-level config mean `max_allowed_replica_count` in [feat: restrict the replication factor while creating app XiaoMi/rdsn#963](https://github.com/XiaoMi/rdsn/pull/963) ? Actually `max_replica_count` is the RF, and `max_allowed_replica_count` is a the upper bound to which an RF can be set. Or we should rename `max_replica_count` as replication_factor to avoid misunderstanding ?
> > > 2. Good question ! This implementation has't written into `.app_info`, this is a bug ! To guarantee the atomicity, I think first the request `set_replica_count` can be replicated to each replica; Received the request, each replica writes it into `slog`, and then writes the new `max_replica_count` into `.app_info`. Even if a replica server fails during the process, the request will not be lost.
> > > 3. As is described in this issue, increasing `max_replica_count` has been tested sufficiently, however decreasing `max_replica_count` has not been tested. I'll test decreasing later to see if redundant replica can be processed correctly.
> > 
> > 
> > Thanks for your reply~ Continue to discuss:
> > 
> > 1. I mean meta option called `max_replicas_in_group` , you can reference it here:
> >    https://github.com/XiaoMi/rdsn/blob/289eb4609ae781e3bcc6bbc6bfeae64dfd8fa785/src/meta/meta_options.cpp#L160-L161
> > 2. I suggest that you can update `max_replica_count` through `on_config_sync` into `.app_info`
> > 3. I think decreasing replica count may have some problems, expecting your test result~
> 
> Thanks for suggestion !
> 
> 1. I think we can eliminate `max_replicas_in_group` and turn to partition-level `max_replica_count`. I'll explain this as below:
> 
> Now `max_replicas_in_group` is only used to update `config_context::MAX_REPLICA_COUNT_IN_GRROUP`; And `config_context::MAX_REPLICA_COUNT_IN_GRROUP` is only used in:
> 
> ```c++
> void config_context::check_size()
> {
>     // when add learner, it is possible that replica_count > max_replica_count, so we
>     // need to remove things from dropped only when it's not empty.
>     while (replica_count(*config_owner) + dropped.size() > MAX_REPLICA_COUNT_IN_GRROUP &&
>            !dropped.empty()) {
>         dropped.erase(dropped.begin());
>         prefered_dropped = (int)dropped.size() - 1;
>     }
> }
> ```
> 
> However, `MAX_REPLICA_COUNT_IN_GRROUP` can be replaced with partition-level `max_replica_count` as below:
> 
> ```c++
> void config_context::check_size()
> {
>     // when add learner, it is possible that replica_count > max_replica_count, so we
>     // need to remove things from dropped only when it's not empty.
>     while (replica_count(*config_owner) + dropped.size() > config_owner->max_replica_count &&
>            !dropped.empty()) {
>         dropped.erase(dropped.begin());
>         prefered_dropped = (int)dropped.size() - 1;
>     }
> }
> ```
> 
> The `partition_configuration` pointed by `config_owner` is updated in `server_state::on_update_configuration_on_remote_reply`, which will be called by `set_replica_count`.
> 
> 2. Good idea ! Each `config_sync_interval_ms` (typically 30 ms) the replica server requests `query_configuration_by_node` to the meta server; Received the response, if the ballot of the response is newer than the local, the replica server will update every table-level and partition-level `max_replica_count` to the one of the response. Is it right ?
> 3. Actually decreasing replica count has been forbidden till now by us. Decreasing is more error-prone than increasing. If we decide to support decreasing replica count, I'll test sufficiently.

Thanks for your reply~
1. If you have already checked that table level replica_count can replace cluster `max_replicas_in_group`, just go ahead~
2. Yes, I recommend update `.app_info` file through `on_config_sync` rpc
3. Okay. How about update this issue title from `update the replication factor` into `increase the replication factor`? As the decreasing is not supported.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48Ximp/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/865,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48XmSu,incubator-pegasus,1012819118,865,NA,empiredan,743379,Dan Wang,,NA,2022-01-14T06:51:16Z,2022-01-14T06:51:16Z,"> > > > > Good enhancement~
> > > > > I have some questions about updating table replica count:
> > > > > 
> > > > > 1. There're also cluster-level config called `max_replica_count`, will it be confict if cluster `max_replica_count` is not equal to table `max_replica_count`? Will important features work such as user read/write, learn, load balance, rolling_update?
> > > > > 2. Table `app_info` file will store in each replica disk (_dir/.app_info), if `max_replica_count` is updated, this file should also be updated, how to update it?
> > > > > 3. In your description, `cure` (executed each 10s) will delete redunant replica or add lacking replica. Could you please add some manual test results?
> > > > > 
> > > > > Expecting your reply~
> > > > 
> > > > 
> > > > Ok, let's discuss one by one:
> > > > 
> > > > 1. Does cluster-level config mean `max_allowed_replica_count` in [feat: restrict the replication factor while creating app XiaoMi/rdsn#963](https://github.com/XiaoMi/rdsn/pull/963) ? Actually `max_replica_count` is the RF, and `max_allowed_replica_count` is a the upper bound to which an RF can be set. Or we should rename `max_replica_count` as replication_factor to avoid misunderstanding ?
> > > > 2. Good question ! This implementation has't written into `.app_info`, this is a bug ! To guarantee the atomicity, I think first the request `set_replica_count` can be replicated to each replica; Received the request, each replica writes it into `slog`, and then writes the new `max_replica_count` into `.app_info`. Even if a replica server fails during the process, the request will not be lost.
> > > > 3. As is described in this issue, increasing `max_replica_count` has been tested sufficiently, however decreasing `max_replica_count` has not been tested. I'll test decreasing later to see if redundant replica can be processed correctly.
> > > 
> > > 
> > > Thanks for your reply~ Continue to discuss:
> > > 
> > > 1. I mean meta option called `max_replicas_in_group` , you can reference it here:
> > >    https://github.com/XiaoMi/rdsn/blob/289eb4609ae781e3bcc6bbc6bfeae64dfd8fa785/src/meta/meta_options.cpp#L160-L161
> > > 2. I suggest that you can update `max_replica_count` through `on_config_sync` into `.app_info`
> > > 3. I think decreasing replica count may have some problems, expecting your test result~
> > 
> > 
> > Thanks for suggestion !
> > 
> > 1. I think we can eliminate `max_replicas_in_group` and turn to partition-level `max_replica_count`. I'll explain this as below:
> > 
> > Now `max_replicas_in_group` is only used to update `config_context::MAX_REPLICA_COUNT_IN_GRROUP`; And `config_context::MAX_REPLICA_COUNT_IN_GRROUP` is only used in:
> > ```c++
> > void config_context::check_size()
> > {
> >     // when add learner, it is possible that replica_count > max_replica_count, so we
> >     // need to remove things from dropped only when it's not empty.
> >     while (replica_count(*config_owner) + dropped.size() > MAX_REPLICA_COUNT_IN_GRROUP &&
> >            !dropped.empty()) {
> >         dropped.erase(dropped.begin());
> >         prefered_dropped = (int)dropped.size() - 1;
> >     }
> > }
> > ```
> > 
> > 
> >     
> >       
> >     
> > 
> >       
> >     
> > 
> >     
> >   
> > However, `MAX_REPLICA_COUNT_IN_GRROUP` can be replaced with partition-level `max_replica_count` as below:
> > ```c++
> > void config_context::check_size()
> > {
> >     // when add learner, it is possible that replica_count > max_replica_count, so we
> >     // need to remove things from dropped only when it's not empty.
> >     while (replica_count(*config_owner) + dropped.size() > config_owner->max_replica_count &&
> >            !dropped.empty()) {
> >         dropped.erase(dropped.begin());
> >         prefered_dropped = (int)dropped.size() - 1;
> >     }
> > }
> > ```
> > 
> > 
> >     
> >       
> >     
> > 
> >       
> >     
> > 
> >     
> >   
> > The `partition_configuration` pointed by `config_owner` is updated in `server_state::on_update_configuration_on_remote_reply`, which will be called by `set_replica_count`.
> > 
> > 2. Good idea ! Each `config_sync_interval_ms` (typically 30 ms) the replica server requests `query_configuration_by_node` to the meta server; Received the response, if the ballot of the response is newer than the local, the replica server will update every table-level and partition-level `max_replica_count` to the one of the response. Is it right ?
> > 3. Actually decreasing replica count has been forbidden till now by us. Decreasing is more error-prone than increasing. If we decide to support decreasing replica count, I'll test sufficiently.
> 
> Thanks for your reply~
> 
> 1. If you have already checked that table level replica_count can replace cluster `max_replicas_in_group`, just go ahead~
> 2. Yes, I recommend update `.app_info` file through `on_config_sync` rpc
> 3. Okay. How about update this issue title from `update the replication factor` into `increase the replication factor`? As the decreasing is not supported.

Ok, we can first support increasing the replication factor. Decreasing can be supported later if needed.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48XmSu/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/865,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4829RN,incubator-pegasus,1021039693,865,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2022-01-25T10:31:43Z,2022-01-25T10:31:43Z,"> > > > > > Good enhancement~
> > > > > > I have some questions about updating table replica count:
> > > > > > 
> > > > > > 1. There're also cluster-level config called `max_replica_count`, will it be confict if cluster `max_replica_count` is not equal to table `max_replica_count`? Will important features work such as user read/write, learn, load balance, rolling_update?
> > > > > > 2. Table `app_info` file will store in each replica disk (_dir/.app_info), if `max_replica_count` is updated, this file should also be updated, how to update it?
> > > > > > 3. In your description, `cure` (executed each 10s) will delete redunant replica or add lacking replica. Could you please add some manual test results?
> > > > > > 
> > > > > > Expecting your reply~
> > > > > 
> > > > > 
> > > > > Ok, let's discuss one by one:
> > > > > 
> > > > > 1. Does cluster-level config mean `max_allowed_replica_count` in [feat: restrict the replication factor while creating app XiaoMi/rdsn#963](https://github.com/XiaoMi/rdsn/pull/963) ? Actually `max_replica_count` is the RF, and `max_allowed_replica_count` is a the upper bound to which an RF can be set. Or we should rename `max_replica_count` as replication_factor to avoid misunderstanding ?
> > > > > 2. Good question ! This implementation has't written into `.app_info`, this is a bug ! To guarantee the atomicity, I think first the request `set_replica_count` can be replicated to each replica; Received the request, each replica writes it into `slog`, and then writes the new `max_replica_count` into `.app_info`. Even if a replica server fails during the process, the request will not be lost.
> > > > > 3. As is described in this issue, increasing `max_replica_count` has been tested sufficiently, however decreasing `max_replica_count` has not been tested. I'll test decreasing later to see if redundant replica can be processed correctly.
> > > > 
> > > > 
> > > > Thanks for your reply~ Continue to discuss:
> > > > 
> > > > 1. I mean meta option called `max_replicas_in_group` , you can reference it here:
> > > >    https://github.com/XiaoMi/rdsn/blob/289eb4609ae781e3bcc6bbc6bfeae64dfd8fa785/src/meta/meta_options.cpp#L160-L161
> > > > 2. I suggest that you can update `max_replica_count` through `on_config_sync` into `.app_info`
> > > > 3. I think decreasing replica count may have some problems, expecting your test result~
> > > 
> > > 
> > > Thanks for suggestion !
> > > 
> > > 1. I think we can eliminate `max_replicas_in_group` and turn to partition-level `max_replica_count`. I'll explain this as below:
> > > 
> > > Now `max_replicas_in_group` is only used to update `config_context::MAX_REPLICA_COUNT_IN_GRROUP`; And `config_context::MAX_REPLICA_COUNT_IN_GRROUP` is only used in:
> > > ```c++
> > > void config_context::check_size()
> > > {
> > >     // when add learner, it is possible that replica_count > max_replica_count, so we
> > >     // need to remove things from dropped only when it's not empty.
> > >     while (replica_count(*config_owner) + dropped.size() > MAX_REPLICA_COUNT_IN_GRROUP &&
> > >            !dropped.empty()) {
> > >         dropped.erase(dropped.begin());
> > >         prefered_dropped = (int)dropped.size() - 1;
> > >     }
> > > }
> > > ```
> > > 
> > > 
> > >     
> > >       
> > >     
> > > 
> > >       
> > >     
> > > 
> > >     
> > >   
> > > However, `MAX_REPLICA_COUNT_IN_GRROUP` can be replaced with partition-level `max_replica_count` as below:
> > > ```c++
> > > void config_context::check_size()
> > > {
> > >     // when add learner, it is possible that replica_count > max_replica_count, so we
> > >     // need to remove things from dropped only when it's not empty.
> > >     while (replica_count(*config_owner) + dropped.size() > config_owner->max_replica_count &&
> > >            !dropped.empty()) {
> > >         dropped.erase(dropped.begin());
> > >         prefered_dropped = (int)dropped.size() - 1;
> > >     }
> > > }
> > > ```
> > > 
> > > 
> > >     
> > >       
> > >     
> > > 
> > >       
> > >     
> > > 
> > >     
> > >   
> > > The `partition_configuration` pointed by `config_owner` is updated in `server_state::on_update_configuration_on_remote_reply`, which will be called by `set_replica_count`.
> > > 
> > > 2. Good idea ! Each `config_sync_interval_ms` (typically 30 ms) the replica server requests `query_configuration_by_node` to the meta server; Received the response, if the ballot of the response is newer than the local, the replica server will update every table-level and partition-level `max_replica_count` to the one of the response. Is it right ?
> > > 3. Actually decreasing replica count has been forbidden till now by us. Decreasing is more error-prone than increasing. If we decide to support decreasing replica count, I'll test sufficiently.
> > 
> > 
> > Thanks for your reply~
> > 
> > 1. If you have already checked that table level replica_count can replace cluster `max_replicas_in_group`, just go ahead~
> > 2. Yes, I recommend update `.app_info` file through `on_config_sync` rpc
> > 3. Okay. How about update this issue title from `update the replication factor` into `increase the replication factor`? As the decreasing is not supported.
> 
> Ok, we can first support increasing the replication factor. Decreasing can be supported later if needed.

I aggree with you about renaming `max_replica_count` to `replication_factor` : )","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4829RN/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/865,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM486zqd,incubator-pegasus,1022048925,865,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2022-01-26T10:07:20Z,2022-01-26T10:07:20Z,"Besides, I agree with you about removing `max_replicas_in_group`, but we should keep `config_context::MAX_REPLICA_COUNT_IN_GRROUP` as partition-level `max_replica_count` + 1","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM486zqd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/865,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5EPeDQ,incubator-pegasus,1144905936,865,NA,empiredan,743379,Dan Wang,,NA,2022-06-02T14:03:44Z,2022-06-02T14:03:44Z,"> > > > > > Good enhancement~
> > > > > > I have some questions about updating table replica count:
> > > > > > 
> > > > > > 1. There're also cluster-level config called `max_replica_count`, will it be confict if cluster `max_replica_count` is not equal to table `max_replica_count`? Will important features work such as user read/write, learn, load balance, rolling_update?
> > > > > > 2. Table `app_info` file will store in each replica disk (_dir/.app_info), if `max_replica_count` is updated, this file should also be updated, how to update it?
> > > > > > 3. In your description, `cure` (executed each 10s) will delete redunant replica or add lacking replica. Could you please add some manual test results?
> > > > > > 
> > > > > > Expecting your reply~
> > > > > 
> > > > > 
> > > > > Ok, let's discuss one by one:
> > > > > 
> > > > > 1. Does cluster-level config mean `max_allowed_replica_count` in [feat: restrict the replication factor while creating app XiaoMi/rdsn#963](https://github.com/XiaoMi/rdsn/pull/963) ? Actually `max_replica_count` is the RF, and `max_allowed_replica_count` is a the upper bound to which an RF can be set. Or we should rename `max_replica_count` as replication_factor to avoid misunderstanding ?
> > > > > 2. Good question ! This implementation has't written into `.app_info`, this is a bug ! To guarantee the atomicity, I think first the request `set_replica_count` can be replicated to each replica; Received the request, each replica writes it into `slog`, and then writes the new `max_replica_count` into `.app_info`. Even if a replica server fails during the process, the request will not be lost.
> > > > > 3. As is described in this issue, increasing `max_replica_count` has been tested sufficiently, however decreasing `max_replica_count` has not been tested. I'll test decreasing later to see if redundant replica can be processed correctly.
> > > > 
> > > > 
> > > > Thanks for your reply~ Continue to discuss:
> > > > 
> > > > 1. I mean meta option called `max_replicas_in_group` , you can reference it here:
> > > >    https://github.com/XiaoMi/rdsn/blob/289eb4609ae781e3bcc6bbc6bfeae64dfd8fa785/src/meta/meta_options.cpp#L160-L161
> > > > 2. I suggest that you can update `max_replica_count` through `on_config_sync` into `.app_info`
> > > > 3. I think decreasing replica count may have some problems, expecting your test result~
> > > 
> > > 
> > > Thanks for suggestion !
> > > 
> > > 1. I think we can eliminate `max_replicas_in_group` and turn to partition-level `max_replica_count`. I'll explain this as below:
> > > 
> > > Now `max_replicas_in_group` is only used to update `config_context::MAX_REPLICA_COUNT_IN_GRROUP`; And `config_context::MAX_REPLICA_COUNT_IN_GRROUP` is only used in:
> > > ```c++
> > > void config_context::check_size()
> > > {
> > >     // when add learner, it is possible that replica_count > max_replica_count, so we
> > >     // need to remove things from dropped only when it's not empty.
> > >     while (replica_count(*config_owner) + dropped.size() > MAX_REPLICA_COUNT_IN_GRROUP &&
> > >            !dropped.empty()) {
> > >         dropped.erase(dropped.begin());
> > >         prefered_dropped = (int)dropped.size() - 1;
> > >     }
> > > }
> > > ```
> > > 
> > > 
> > >     
> > >       
> > >     
> > > 
> > >       
> > >     
> > > 
> > >     
> > >   
> > > However, `MAX_REPLICA_COUNT_IN_GRROUP` can be replaced with partition-level `max_replica_count` as below:
> > > ```c++
> > > void config_context::check_size()
> > > {
> > >     // when add learner, it is possible that replica_count > max_replica_count, so we
> > >     // need to remove things from dropped only when it's not empty.
> > >     while (replica_count(*config_owner) + dropped.size() > config_owner->max_replica_count &&
> > >            !dropped.empty()) {
> > >         dropped.erase(dropped.begin());
> > >         prefered_dropped = (int)dropped.size() - 1;
> > >     }
> > > }
> > > ```
> > > 
> > > 
> > >     
> > >       
> > >     
> > > 
> > >       
> > >     
> > > 
> > >     
> > >   
> > > The `partition_configuration` pointed by `config_owner` is updated in `server_state::on_update_configuration_on_remote_reply`, which will be called by `set_replica_count`.
> > > 
> > > 2. Good idea ! Each `config_sync_interval_ms` (typically 30 ms) the replica server requests `query_configuration_by_node` to the meta server; Received the response, if the ballot of the response is newer than the local, the replica server will update every table-level and partition-level `max_replica_count` to the one of the response. Is it right ?
> > > 3. Actually decreasing replica count has been forbidden till now by us. Decreasing is more error-prone than increasing. If we decide to support decreasing replica count, I'll test sufficiently.
> > 
> > 
> > Thanks for your reply~
> > 
> > 1. If you have already checked that table level replica_count can replace cluster `max_replicas_in_group`, just go ahead~
> > 2. Yes, I recommend update `.app_info` file through `on_config_sync` rpc
> > 3. Okay. How about update this issue title from `update the replication factor` into `increase the replication factor`? As the decreasing is not supported.
> 
> Ok, we can first support increasing the replication factor. Decreasing can be supported later if needed.

Now our implementation has supported to decrease the replication factor. 

It is noticeable that the redundant replica data will be garbage-collected only when meta level is lively, which means `set_meta_level lively` should be executed.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5EPeDQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/866,https://api.github.com/repos/apache/incubator-pegasus/issues/866,incubator-pegasus,1095045828,866,Server 2.0+ using much more network and disk bandwidths,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2022-01-06T07:30:11Z,2022-01-10T06:21:13Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
If possible, provide a recipe for reproducing the error.

2. What did you expect to see?

3. What did you see instead?

4. What version of Pegasus are you using?
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/866/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/866,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47_HRB,incubator-pegasus,1006400577,866,NA,padmejin,89557328,,,NA,2022-01-06T09:15:34Z,2022-01-06T09:15:34Z,"1. What did you do?
Recently we were trying to upgrade pegasus from 1.12.3 to 2.0, then upgrade to latest version 2.3. We did some performance tests by making request to clusters with different backend version, and then compared all the metrics between two clusters. We found that pegasus 2.0 are using much more network/disk bandwidths than pegasus 1.12.3.

Eventually we narrowed down the problem to a typical case: when we were sending all write requests to a cluster with only one table that only contained one partition and 3 replications, it was obvious that on the host where the primary replica was stored, the upstream bandwidth is much greater than the downstream bandwidth.

2. What did you expect to see?
On the host where the primary replica was stored, the upstream bandwidths should be at most 2 times larger than the downstream bandwidth.

3. What did you see instead?
 the upstream bandwidths is about 5 to 10 times larger than the downstream bandwidth.

4. What version of Pegasus are you using?
pegasus 2.0","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM47_HRB/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/869,https://api.github.com/repos/apache/incubator-pegasus/issues/869,incubator-pegasus,1095123565,869,Feature: require add issue reference in description using github action when pull request,foreverneverer,23136769,Jiashuo,js982986555@live.com,CLOSED,2022-01-06T09:22:42Z,2022-01-27T16:30:22Z,"add issue for every pr can help us tracking the problem, we can use github action to check if contain issue ref ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/869/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/871,https://api.github.com/repos/apache/incubator-pegasus/issues/871,incubator-pegasus,1097503513,871,enhancement(bulk load): create connection asynchronously,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2022-01-10T06:25:50Z,2022-01-10T06:30:35Z,"When bulk load started, replica server will firstly connect to the remote file provider and download metadata file synchronously. However, creating connection may cost seconds in production environment, will sometimes affect performance. As a result, connection should be created asynchronously
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/871/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/871,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48HaB_,incubator-pegasus,1008574591,871,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2022-01-10T06:26:12Z,2022-01-10T06:26:12Z,https://github.com/XiaoMi/rdsn/pull/952,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48HaB_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/872,https://api.github.com/repos/apache/incubator-pegasus/issues/872,incubator-pegasus,1097504879,872,Bulk load enhancement,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2022-01-10T06:28:25Z,2022-08-01T09:18:16Z,"- [x] #871
- [x] #873 
- [x] #874
- [x] #839  
- [x] #856  
- [x] #877 
- [x] #861 
- [x] #885
- [x] #886
- [x] [fix: fix md5 lack during block service download](https://github.com/XiaoMi/rdsn/pull/1102)
- [x] [fix(bulk_load): fix bug that poping all committed mutations ineffective](https://github.com/XiaoMi/rdsn/pull/1102)
- [x] [feat(bulk_load): support clear last bulk load state rpc](https://github.com/XiaoMi/rdsn/pull/1103)
- [x] #968 
- [x] [feat(bulk_load): update downloading to avoid blocking default thread pool](https://github.com/XiaoMi/rdsn/pull/1104)
- [x] [feat(bulk_load): support different tables can execute bulk load concurrently](https://github.com/XiaoMi/rdsn/pull/1105)
- [x] #975","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/872/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/873,https://api.github.com/repos/apache/incubator-pegasus/issues/873,incubator-pegasus,1097505896,873,enhancement(bulk_load): not rollback to downloading if bulk load meet error in succeed stage,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2022-01-10T06:30:28Z,2022-01-10T06:36:33Z,"When bulk load meet errors in normal stage, it will rollback to downloading stage, including succeed stage. However, there are only some cleanup works in this stage such as delete files and context update. It is unnecessary to rollback to downloading, which will ingestion repeatedly, especially for replica will remove garbage files and reset context while starting a new bulk load process.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/873/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/873,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48Habe,incubator-pegasus,1008576222,873,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2022-01-10T06:31:09Z,2022-01-10T06:31:09Z,"https://github.com/XiaoMi/rdsn/pull/958
https://github.com/XiaoMi/rdsn/pull/1004","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48Habe/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/874,https://api.github.com/repos/apache/incubator-pegasus/issues/874,incubator-pegasus,1097508663,874,enhancement(bulk_load): decrease repeated bulk_load_request,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2022-01-10T06:35:24Z,2022-01-10T06:36:41Z,"- remove bulk load request short interval time
- add unhealthy partition check","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/874/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/874,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48Ha2d,incubator-pegasus,1008577949,874,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2022-01-10T06:35:40Z,2022-01-10T06:35:40Z,"https://github.com/XiaoMi/rdsn/pull/959
https://github.com/XiaoMi/rdsn/pull/960
https://github.com/XiaoMi/rdsn/pull/962
https://github.com/XiaoMi/rdsn/pull/964
https://github.com/XiaoMi/rdsn/pull/967","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48Ha2d/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/875,https://api.github.com/repos/apache/incubator-pegasus/issues/875,incubator-pegasus,1097546581,875,Refactor and simplify code by useful macros,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-01-10T07:33:59Z,2022-01-12T05:54:05Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
There are some duplicate code in rdsn and pegasus project, we can simplify them by some useful macros.

**Describe the feature you'd like:**
We can add some macros like:
```
// Return the given status if it is not ERR_OK.
#define ERR_LOG_AND_RETURN_NOT_OK(s, ...)                                                          \
    do {                                                                                           \
        error_code _err = (s);                                                                     \
        if (dsn_unlikely(_err != ERR_OK)) {                                                        \
            derror_f(""{}: {}"", _err, fmt::format(__VA_ARGS__));                                    \
            return _err;                                                                           \
        }                                                                                          \
    } while (0)

// Return the given status if condition is not true.
#define ERR_LOG_AND_RETURN_NOT_TRUE(s, err, ...)                                                   \
    do {                                                                                           \
        if (dsn_unlikely(!(s))) {                                                                  \
            derror_f(""{}: {}"", err, fmt::format(__VA_ARGS__));                                     \
            return err;                                                                            \
        }                                                                                          \
    } while (0)
```

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/875/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/876,https://api.github.com/repos/apache/incubator-pegasus/issues/876,incubator-pegasus,1097608092,876,Remove deprecated replica .info file,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-01-10T08:43:48Z,2022-01-10T14:17:13Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
Replica's `.info` file has been deprecated long time ago before Pegasus 2.0, we don't have to keep related code, because higher version Pegasus must be upgraded from 2.0, on which version this file must has been removed.

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->
Remove these related code.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/876/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/877,https://api.github.com/repos/apache/incubator-pegasus/issues/877,incubator-pegasus,1097640271,877,enhancement(bulk_load): ingestion option change from copy into move,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2022-01-10T09:19:53Z,2022-01-20T03:26:33Z," IngestExternalFileOptions is the option for ingest operation.

As RocksDB code(https://github.com/facebook/rocksdb/blob/v6.6.4/include/rocksdb/options.h#L1443) show:

> struct IngestExternalFileOptions {
> // Can be set to true to move the files instead of copying them.
> bool move_files = false;
> ...
>}

If IngestExternalFileOptions.move_files = true, ingest will move files, and its default value is false.

Setting move_files as true can speed up ingestion and decrease disk write throughput during ingestion.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/877/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/877,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48HzZT,incubator-pegasus,1008678483,877,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2022-01-10T09:20:10Z,2022-01-10T09:20:10Z,#864 ,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48HzZT/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/879,https://api.github.com/repos/apache/incubator-pegasus/issues/879,incubator-pegasus,1097683093,879,Bug: `.init-info` file lost on server with XFS file system after power failure and reboot,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-01-10T10:04:42Z,2022-01-25T03:14:09Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
- create a table in a cluster, and write some data, check these data is readable
- power off suddenly
- reboot the servers
- check these data again, some data will be lost

2. What did you expect to see?
no data lost

3. What did you see instead?
some data lost
```
XFS has a dedicated sysctl variable for setting the writeback interval with a default value of 3000.
Warning: While larger values may increase performance, they also increase the severity of data loss caused by a power outage.
```
https://wiki.archlinux.org/title/XFS#Sync_interval

4. What version of Pegasus are you using?
1.12.3 (new versions have the same problem)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/879/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/879,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48LM1N,incubator-pegasus,1009569101,879,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2022-01-11T03:35:02Z,2022-01-11T03:35:02Z,What are the symptoms of a cluster losing data? Partitions are healthy but data loss?,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48LM1N/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/882,https://api.github.com/repos/apache/incubator-pegasus/issues/882,incubator-pegasus,1107767463,882,avoid to call add_point in latency_tracer,levy5307,22141103,赵立伟,zlw5307@163.com,CLOSED,2022-01-19T07:50:03Z,2022-01-19T08:45:58Z,"## Feature Request

In previous implementation,  the function `latency_tracer::add_point` will always called even if `enable_tracer` is false. And before the calling of this function, `fmt::format(""{}:{}:{}"", __FILENAME__, __LINE__, __FUNCTION__)` will be called to generate the parameter need by it, which will cause waste of cpu cycle and memory usage.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/882/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/883,https://api.github.com/repos/apache/incubator-pegasus/issues/883,incubator-pegasus,1107806553,883,Feature: support latency tracer,foreverneverer,23136769,Jiashuo,js982986555@live.com,CLOSED,2022-01-19T08:35:34Z,2022-07-19T07:20:42Z,"## Feature Request

use latency tracer to debug bottleneck
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/883/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/884,https://api.github.com/repos/apache/incubator-pegasus/issues/884,incubator-pegasus,1108046152,884,Feature: add ‘BATCH_GET’ rpc request for read optimization,cauchy1988,7292411,,,CLOSED,2022-01-19T12:32:05Z,2022-04-14T06:15:09Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

We would like to implement a reading optimization interface for the pegasus client

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->
the way we wanna to realize is as follows:
(1) first, on the server side,  we add a read interface with the binding task code 'RPC_RRDB_RRDB_BATCH_GET'.  This new interface of each server handles the reading of multiple items in the same partition.  The items being requested can have different hashkey which is the key difference to the 'RPC_RRDB_RRDB_MULTI_GET' interface
(2) second, we realize a interface in the client side, taking java client as an example, the declaration is as following:
```
public int batchGet3(List<Pair<byte[], byte[]>> keys, List<Pair<PException, byte[]>> results, int timeout) throws PException
```
(3)the main design is as follows:
a、RPC_RRDB_RRDB_BATCH_GET request rpc protocol
```
struct batch_get_request {
    1:list<full_key> keys;
}
 
struct full_key {
    1:base.blob hash_key;
    2:base.blob sorted_key;
}
 
struct batch_get_response {
    1:i32               error;
    2:list<full_data>   data;
    3:i32               app_id;
    4:i32               partition_index;
    6:string            server;
}
 
struct full_data {
    1:base.blob hash_key;
    2:base.blob sorted_key;
    3:base.blob value;
    4:bool exists //true 表述数据存在， false 表示数据不存在
}
 
service rrdb
{
  batch_get_response batch_get(1:batch_get_request request);
}
```
b、The main process of the client side
![image](https://user-images.githubusercontent.com/7292411/150133807-0a5b81f8-3199-4d4e-99aa-db67eeb8c3fa.png)

Looking forward to your opinions！

**Describe alternatives you've considered:**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->

**Teachability, Documentation, Adoption, Migration Strategy:**
<!-- If you can, explain some scenarios how users might use this, situations it would be helpful in. Any API designs, mockups, or diagrams are also helpful. -->

<div yne-bulb-block=""paragraph"" style=""white-space: pre-wrap; text-align: left; line-height: 1.75; font-size: 14px;"">We have done a performance test with JMeter on a three node machine:</div><div yne-bulb-block=""paragraph"" style=""white-space: pre-wrap; line-height: 1.75; font-size: 14px;"">The comparison of time consumption and throughput is as follows:</div><div yne-bulb-block=""table"" style=""overflow: auto;"">

  | latenry-average(ms) | latency-median(ms) | 90%(ms) | 95%(ms) | 99%(ms) | throughput
-- | -- | -- | -- | -- | -- | --
batchGet2 | 1 | 1 | 2 | 2 | 4 | 622.0/sec
batchGet3 | 1 | 1 | 2 | 2 | 3 | 849.1/sec

</div><div yne-bulb-block=""paragraph"" style=""white-space: pre-wrap; text-align: left; line-height: 1.75; font-size: 14px;""><span style=""color: rgb(32, 33, 36); background-color: rgb(248, 249, 250);"">We generate 1000 pieces of data per request</span></div><div yne-bulb-block=""paragraph"" style=""white-space: pre-wrap; text-align: left; line-height: 1.75; font-size: 14px;""><span style=""color: rgb(32, 33, 36); background-color: rgb(248, 249, 250);"">As can be seen in the above figure: The performance of reads of using the java client interface batchGet3 by using BATCH_GET RPC is more preferable than the client interface batchGet2 by  using GET RPC</span></div><div yne-bulb-block=""paragraph"" style=""white-space: pre-wrap; text-align: left; line-height: 1.75; font-size: 14px;""><br></div><!--5f39ae17-8c62-4a45-bc43-b32064c9388a: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>","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/884/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/884,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48sTYj,incubator-pegasus,1018246691,884,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2022-01-21T07:19:58Z,2022-01-21T07:19:58Z,"Consider whether there is a more general resolution to support all operations, including get, set, del and so on.

One example you can  refer to [checkAndMutate](https://github.com/XiaoMi/pegasus-java-client/blob/master/idl/rrdb.thrift#L202), which can batch send different `write` operation, and only support same hashKey.

Refer to `checkAndMutate`,  You may need define  rdb like this:
```thrift
struct batch_request
{
...
    1:list<request>   batch_list;
...
}

struct request {
    1:operation_type operation; // it is very important, you need judge the request  type
    2: base.blob       hash_key // this is diff from checkAndmutate, which don't include it
    3:base.blob        sort_key;
    4:base.blob        value;
    5:i32              set_expire_ts_seconds;
}
```

On server side, you just need one method to judge `operation_type` and forward corresponding existed method, of course, which is also like `checkAndMutate` [handler of server side](https://github.com/apache/incubator-pegasus/blob/master/src/server/pegasus_write_service_impl.h#L358).


This just is one suggestion, I haven't make sure whether it is feasible, However, I think the interface should be as general as possible, rather than one requirement need one interface.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48sTYj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/884,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48sUQY,incubator-pegasus,1018250264,884,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2022-01-21T07:26:21Z,2022-01-21T07:26:21Z,"> You may need define rdb like this:

You may need provide two  rdb type for `on_client_write` and `on_client_read`","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48sUQY/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/884,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48s4Go,incubator-pegasus,1018397096,884,NA,cauchy1988,7292411,,,NA,2022-01-21T10:52:44Z,2022-01-21T10:52:44Z,"> Consider whether there is a more general resolution to support all operations, including get, set, del and so on.
> 
> One example you can refer to [checkAndMutate](https://github.com/XiaoMi/pegasus-java-client/blob/master/idl/rrdb.thrift#L202), which can batch send different `write` operation, and only support same hashKey.
> 
> Refer to `checkAndMutate`, You may need define rdb like this:
> 
> ```thrift
> struct batch_request
> {
> ...
>     1:list<request>   batch_list;
> ...
> }
> 
> struct request {
>     1:operation_type operation; // it is very important, you need judge the request  type
>     2: base.blob       hash_key // this is diff from checkAndmutate, which don't include it
>     3:base.blob        sort_key;
>     4:base.blob        value;
>     5:i32              set_expire_ts_seconds;
> }
> ```
> 
> On server side, you just need one method to judge `operation_type` and forward corresponding existed method, of course, which is also like `checkAndMutate` [handler of server side](https://github.com/apache/incubator-pegasus/blob/master/src/server/pegasus_write_service_impl.h#L358).
> 
> This just is one suggestion, I haven't make sure whether it is feasible, However, I think the interface should be as general as possible, rather than one requirement need one interface.

Thank you, it is very helpful to me; I will think more roughly and come up with a more considerable solution, when this is done, i'll consult you for later 'niubility' advise ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48s4Go/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/884,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48w94A,incubator-pegasus,1019469312,884,NA,cauchy1988,7292411,,,NA,2022-01-23T11:57:28Z,2022-01-23T11:57:28Z,"> > You may need define rdb like this:
> 
> You may need provide two rdb type for `on_client_write` and `on_client_read`

I feel that adding a batch interface that mixes read, write, and delete operations is not very appropriate, for the following reasons:
1. The example of check_and_mutate you quoted is an example of cas-like operation on a hashkey, which is different from the usual read, write, and delete.
2. The rpc interface with a single responsibility is simple and clear. As far as the kv database is concerned, there are actually not many rpc interfaces for real reading and writing.
3. 'BATCH_GET' rpc interface style is consistent with the previous interface styles of GET, SET, MULTI_GET, etc.
4. The subsequent functions of ""batch set"" and ""batch del"" are still under discussion. I think pegasus may not support these two functions, because the success of ""partial modification"" will bring additional design and consideration, while ""batch get"" is only for reading, Even if the read fails, it will not affect the data
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48w94A/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/884,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48xzg7,incubator-pegasus,1019689019,884,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-01-24T03:49:50Z,2022-01-24T03:49:50Z,"checkAndMutate is a SET type interface in all cases, it is different from batchGet/Set/Del interfaces.
1. The responses are different, for SET/DEL type interfaces, there are no values responed, but for GET type interfaces, you should design a thrfit struct to contain values, and the related hashkeys and sortkeys along.
2. Mix all type of requests in one RPC will make code coupled. Imagine the pains we suffered from `multi_get_request`.

So I support to distinguish these RPCs, and try to avoid overdesign.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48xzg7/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/884,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4825Cd,incubator-pegasus,1021022365,884,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2022-01-25T10:13:10Z,2022-01-25T10:13:10Z,"## 1
> 1. there are no values responed, but for GET type interfaces,

Some explanations added before as follow:

> You may need provide two rdb type for on_client_write and on_client_read.

So actually we need define `wrtite_request` and `read_request` rpc

## 2 
> 2. Mix all type of requests in one RPC will make code coupled

On the contrary, I  think that using a unified interface will make code maintenance easier. In addition, we can refer https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchWriteItem.html to consider it again

## 3
> 4. The subsequent functions of ""batch set"" and ""batch del"" are still under discussion. I think pegasus may not support these two functions, because the success of ""partial modification"" will bring additional design and consideration
 
Haha, actually, I hope support `batch set` to improve online irrigation data rate.




I think may need others join to decide it. @neverchanje @hycdong @levy5307 @Smityz @GiantKing 
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4825Cd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/884,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM482-IQ,incubator-pegasus,1021043216,884,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-01-25T10:35:16Z,2022-01-25T10:35:16Z,Doesn't dynamodb's BatchWriteItem API only operate on puts and deletes ? BatchGetItem is for get operations.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM482-IQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/884,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM482-3F,incubator-pegasus,1021046213,884,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2022-01-25T10:38:53Z,2022-01-25T10:38:53Z,"Yeah, so I say that define two type, `read_request` and `write_request`,  read support `get` and so on, write support `set`, `del` and so on.

dynamodb don't have `multi` operation, We can consider that if support `batch_multi_get`.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM482-3F/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/884,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM483IRf,incubator-pegasus,1021084767,884,NA,cauchy1988,7292411,,,NA,2022-01-25T11:19:46Z,2022-01-25T11:19:46Z,"> Yeah, so I say that define two type, `read_request` and `write_request`, read support `get` and so on, write support `set`, `del` and so on.
> 
> dynamodb don't have `multi` operation, We can consider that if support `batch_multi_get`.

read operation only have 'one type' that is 'get'; so  if i have not misunderstood your ideas, i can implement the ""batch read"" rpc interface based on following thrift definitions
```
struct read_request {
    1:dsn.blob hash_key;
    2:dsn.blob sort_key;
}

struct batch_read_request {
    1:list<read_request> requests;
}

struct read_response {
    1:dsn.blob hash_key;
    2:dsn.blob sort_key;
    3:dsn.blob value;
    4:bool exists // true mean data exists, false means not
}

struct batch_read_response {
    1:i32               error;
    2:list<read_response>   data;
    3:i32               app_id;
    4:i32               partition_index;
    6:string            server;
}
``` 
it's almostly the same with what i have initally comed up;

'batch set' and ' batch del' rpc interface use common thrift definition and  add a 'operation_type' field to the definition to differentiate them ?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM483IRf/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/884,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM483If5,incubator-pegasus,1021085689,884,NA,cauchy1988,7292411,,,NA,2022-01-25T11:20:45Z,2022-01-25T11:20:45Z,"> Yeah, so I say that define two type, `read_request` and `write_request`, read support `get` and so on, write support `set`, `del` and so on.
> 
> dynamodb don't have `multi` operation, We can consider that if support `batch_multi_get`.

if i misunderstood you ideas, then i want chat  with you in weixin; heihei","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM483If5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/885,https://api.github.com/repos/apache/incubator-pegasus/issues/885,incubator-pegasus,1108815575,885,enhancement(bulk_load): add option to contorl if verify before ingestion ,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2022-01-20T03:36:58Z,2022-01-21T10:44:39Z,"In current implmentation, before rocksdb execute ingestion, replica will compare file size and md5 with the content of metadata.
However, this verification will cost lots of disk resource. Removing verification will decrease 98% disk.io.read_bytes(15GB -> 300M) in cluster tests. As a result, we should make it as an option, and set it as false for those cluster willing to have better performance during ingestion.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/885/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/886,https://api.github.com/repos/apache/incubator-pegasus/issues/886,incubator-pegasus,1108919461,886,enhancement(bulk_load): support disk-level concurrent ingesting count,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2022-01-20T06:38:34Z,2022-01-29T07:24:41Z,"In current implementation, meta server control max concurrent ingestion partition count, for example, if max_concurrent_count is 4, meta server will send 4 ingestion request to replica servers, and won't send the fifth request until previous ingestion succeed or failed. Current restriction is precise enough, which can not control concurrent ingesting count of one node, we need to provide another percise plan, a disk-level concurrent ingesting count.

Meta server adds `ingestion_context` to store ingesting partitions, including its node address and disk info. The new plan provides the node max ingesting count and disk max ingesting count. Before sending ingestion request, meta server will check if node and disk max count will be exceed after sending the request, if will not exceed, the request will be sent. Besides, when ingestion succeed or failed, the content of ingestion_context will be reset.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/886/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/887,https://api.github.com/repos/apache/incubator-pegasus/issues/887,incubator-pegasus,1108924968,887,Refactor: refactor some code,levy5307,22141103,赵立伟,zlw5307@163.com,OPEN,2022-01-20T06:47:36Z,2023-05-30T01:45:59Z,"## Feature Request
 
Code refactor to make code clean
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/887/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/889,https://api.github.com/repos/apache/incubator-pegasus/issues/889,incubator-pegasus,1108989848,889,Feature: implement long adder to optimize the counter of new metrics system,empiredan,743379,Dan Wang,,CLOSED,2022-01-20T08:12:56Z,2022-01-28T09:30:27Z,"# 1. Motivation

In the old metrics system, to be precise, the counter is not defined independently as a metric type, though it could be implemented by `dsn::perf_counter_number_atomic` or `dsn::perf_counter_volatile_number_atomic`.

Using thread ID as the hash code,  `dsn::perf_counter_number_atomic` has tried to distribute the atomic increment evenly over the `DIVIDE_CONTAINER`. Each element in `DIVIDE_CONTAINER` is a 8-byte `std::atomic<int64_t>`. The final value of the counter can be got by summing all the elements of `DIVIDE_CONTAINER`.

However, threads run on different cpus are likely to update the data within the same cache line  (typically 64 bytes for x86). This will lead to [false sharing](https://en.wikipedia.org/wiki/False_sharing) problem which will degrade performance. In comparison with a single `std::atomic<int64_t>` as the counter, it consumes more memory while sometimes getting even worse performance.

Therefore, in [new metrics system](https://github.com/apache/incubator-pegasus/issues/756), we need new implementations that eliminate false sharing, to improve the performance and consume less memory, if possible.

# 2. Implementation

## 2.1 concurrent_long_adder

To eliminate false sharing, first we can introduce a new struct that aligns a `std::atomic<int64_t>` with a cache line. Then, allocate an array of the new structs in an aligned region of memory. The size of this array is equal to the number of cpus.

Since threads run on different cpus are more likely to be hashed to the different cache line of this array, the probability of false sharing can be significantly reduced. The implementation based on this idea is called `concurrent_long_adder`.

`concurrent_long_adder` achieves high performance at the cost of more memory. The more cpu cores the machines has, the more memory it will consumed. The relationship between the cpu cores and memory usage is as below: 
|The number of cpus|Memory usage|
| :----: | :----: |
|2|128 bytes|
|4|256 bytes|
|8|512 bytes|
|16|1024 bytes|
|32|2048 bytes|

Since `dsn::perf_counter_number_atomic` will consume 856 bytes, if the machine has less than 16 cores, `concurrent_long_adder` will consume less memory than `dsn::perf_counter_number_atomic`.

Nevertheless, due to the memory consumption, `concurrent_long_adder` is recommended to be used under the scenario that a counter is updated very frequently.

## 2.2 striped_long_adder

Inspired by https://github.com/apache/kudu/commit/01233222e594c9e460e541a29875998e1e3b873c, `striped_long_adder` is implemented to trade off between the performance and memory usage.

The idea of this long adder is that it has a *base* 8-byte `std::atomic<int64_t>`. If the long adder is not updated frequently, it will not consume extra memory; Otherwise, once collision is detected, it will allocate the same memory bytes that `concurrent_long_adder` consumes in case the performance is degraded.

Since more instructions are used to detect the collision, `striped_long_adder` is slower than `concurrent_long_adder`. However, it can balance between the performance and memory usage. Therefore, `striped_long_adder` is used as the default long adder and is appropriate for the scenario that a counter is not updated frequently.

# 3. Usage

For the reason that virtual function will consume extra memory and slow down the execution, template is used to wrap the long adder to provide the unified API. For details please see `long_adder_wrapper` in `include/dsn/utility/long_adder.h`.

# 4. Performance

The performance test is quite simple: a number of threads is started, and each thread execute a number of atomic increment executions. Besides `striped_long_adder` and `concurrent_long_adder`, another 2 kinds of long adders are introduced to compare the execution performance.

`simple_long_adder` is just a wrapper of a single `std::atomic<int64_t>`. `divided_long_adder` is nearly the same with `dsn::perf_counter_number_atomic` except that virtual functions are removed.

## 4.1 Hardware configurations

```
cpu: Intel Xeon Processor (Cascadelake)
cores: 4
memory: 32GB
```

## 4.2 Test results

## 4.2.1 2 threads with each 1,000,000,000 operations

|The type of long adder|Duration(seconds)|
| :----: | :----: |
|simple_long_adder|41.8089|
|divided_long_adder|45.3746|
|striped_long_adder|34.4589|
|concurrent_long_adder|11.9191|

## 4.2.2 4 threads with each 1,000,000,000 operations

|The type of long adder|Duration(seconds)|
| :----: | :----: |
|simple_long_adder|94.9675|
|divided_long_adder|101.167|
|striped_long_adder|36.1651|
|concurrent_long_adder|12.3893|

## 4.2.3 8 threads with each 1,000,000,000 operations
|The type of long adder|Duration(seconds)|
| :----: | :----: |
|simple_long_adder|183.897|
|divided_long_adder|185.945|
|striped_long_adder|70.5208|
|concurrent_long_adder|47.1702|

## 4.2.4 16 threads with each 1,000,000,000 operations
|The type of long adder|Duration(seconds)|
| :----: | :----: |
|simple_long_adder|334.987|
|divided_long_adder|326.552|
|striped_long_adder|141.043|
|concurrent_long_adder|92.6725|","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/889/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/889,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48xx70,incubator-pegasus,1019682548,889,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2022-01-24T03:29:01Z,2022-01-24T03:29:01Z,"Through `perf_counter` may not cost lots of time in our flame graph(capture in the read Scenes).
![cp](https://user-images.githubusercontent.com/22953824/150717253-cf425278-6d1d-4d32-975f-f45b93c2cbc6.svg)

I think it's still worth optimizing it.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48xx70/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/889,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48x1zj,incubator-pegasus,1019698403,889,NA,empiredan,743379,Dan Wang,,NA,2022-01-24T04:10:56Z,2022-01-24T04:10:56Z,"> Through `perf_counter` may not cost lots of time in our flame graph(capture in the read Scenes). ![cp](https://user-images.githubusercontent.com/22953824/150717253-cf425278-6d1d-4d32-975f-f45b93c2cbc6.svg)
> 
> I think it's still worth optimizing it.

Thanks for providing such a cool flame graph !

Yeah I think currently most of our tasks that are sampled by the counter will be executed during several milliseconds or more: it may consume lots of memory, and involve network transmission or disk operations. If L1 cache is not large enough, it's likely that the counter has been evicted from L1 cache and must be loaded from memory again. This will not lead to false sharing.

On the other hand, since we will adopt striped_long_adder based on striped64 as the default implementation for the counter, it will reduce the memory usage: a long duration will greatly reduce the possibility of collision, thus a single 8-byte `atomic<int64_t>` is usually enough; by contrast, `dsn::perf_counter_number_atomic` will consume 856 bytes.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48x1zj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/892,https://api.github.com/repos/apache/incubator-pegasus/issues/892,incubator-pegasus,1112324012,892,Feature: Enhance the ease of use and efficiency of duplication,foreverneverer,23136769,Jiashuo,js982986555@live.com,CLOSED,2022-01-24T08:31:11Z,2022-04-14T06:14:34Z,"## Feature Request

### Duplication has some shortcoming：
![选区_999(788)](https://user-images.githubusercontent.com/23136769/150751109-919b867c-3906-4d86-8def-5788ac0f002f.png)

- **It depends remote filesystem to sync the checkpoint:** 
        It leads to complexity in use, which make you have to support HDFS or FDS, and must start duplicate in three steps: 1. upload checkpoint in master 2. restore checkpoint in follower 3. start sync plog data in master.
- **The synchronization of plog data only sends a single mutation at each RPC:**
        The rate of sync is slow and cause the plog pile up in sometimes.

### The issue request will hope enhance it:
![选区_999(789)](https://user-images.githubusercontent.com/23136769/150752425-c7fd1db1-baea-4a79-8989-f35d86ae794f.png)

- Use `learn` to replace `backup-restore` to sync checkpoint
- Use `batch-send` to replace `single-send` to sync plog
- Connect the above two processes to automate them, and the establishment of duplication can be completed with one command ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/892/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/892,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48yspC,incubator-pegasus,1019923010,892,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2022-01-24T10:04:34Z,2022-01-24T10:04:34Z,"### The flow chart is as follow: 
![选区_999(791)](https://user-images.githubusercontent.com/23136769/150759571-0323797c-5110-4432-ba51-8a5bdba37b70.png)

#### NOTE: The implementation refactor previous `DS_START` to three stage: `DS_PREPARE`, `DS_APP`, `DS_LOG`, and will be explained in follow descreption

1. **master meta**: receive create dup table request and mark current `max_plog_commit_decree` as start plog point
2. **master meta**: init the dup status as `DS_PREPARE`,  which means master replica need prepare latest_checkpoint  for duplicating
3. **master replica**: generate latest checkpoint , which require `laster_durable_decee >= max_plog_commit_decree`
4. **master replica**: update all checkpoint has been prepared and response master meta
5. **master meta**: change to `DS_APP`, which means all checkpoints have prepared and follower can duplicate it
6. **master meta**: send create table request to follower meta, and require the table init with `master replica checkpoint`(like restore progress which init table with `backup replica checkpoint`)
7. **follower meta**: create dup table with dup info
8. **follower replica**: start open or init replica 
9. **follower replica**: duplicate checkpoint from master replica before init replica, and load replica using duplicated data
10. **master meta**: check the follower table if create completed periodically
11. **master meta**: if completed, change to `DS_LOG`, which means master replica can send plog data to follower table
12. **master replica**: start pipeline to batch send plog data.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48yspC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/892,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM482Bpx,incubator-pegasus,1020795505,892,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2022-01-25T04:18:06Z,2022-01-25T04:18:06Z,"The motivation is fair, lgtm.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM482Bpx/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/892,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48313L,incubator-pegasus,1021271499,892,NA,cauchy1988,7292411,,,NA,2022-01-25T14:58:46Z,2022-01-25T14:58:46Z,"although, I'm  not so familiar with the  whole pegasus system, but according to your descriptions, I think  the original design is easier to realize but hard or inconvenient to use, on the other hand, the new design i think is a little  complex to implement because you may handle more possible corner case; hower the new design will make ’duplication-usage‘ more friendly  and easy to use， looking forward to your awesome implementation~ ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM48313L/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/893,https://api.github.com/repos/apache/incubator-pegasus/issues/893,incubator-pegasus,1112529903,893,"If backup request is timeout, meta server will core dump",Smityz,22953824,Smilencer,smityz@qq.com,CLOSED,2022-01-24T11:35:22Z,2022-01-26T16:01:19Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/893/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/894,https://api.github.com/repos/apache/incubator-pegasus/issues/894,incubator-pegasus,1113475819,894,Some higher version cmake produce filename as 'compiler_depend.ts',acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-01-25T06:17:56Z,2022-01-27T16:27:58Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
Using cmake 3.22.1 to build Pegasus, file name in log is incorrectly produced as 'compiler_depend.ts'
```
D2022-01-25 13:57:53.521 (1643090273521305878 2cff) unknown.io-thrd.11519: compiler_depend.ts:466:run(): process(11519) start: 1643090273510, date: 2022-01-25 13:57:53.510
D2022-01-25 13:57:53.527 (1643090273527174447 2cff) unknown.io-thrd.11519: compiler_depend.ts:122:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.17.0.18), input interface("""")
D2022-01-25 13:57:53.527 (1643090273527188517 2cff) unknown.io-thrd.11519: compiler_depend.ts:481:start(): [replica] network client started at port 3, channel = RPC_CHANNEL_TCP, fmt = NET_HDR_DSN ...
D2022-01-25 13:57:53.527 (1643090273527382792 2cff) unknown.io-thrd.11519: compiler_depend.ts:122:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.17.0.18), input interface("""")
D2022-01-25 13:57:53.527 (1643090273527507816 2cff) unknown.io-thrd.11519: compiler_depend.ts:481:start(): [replica] network client started at port 3, channel = RPC_CHANNEL_UDP, fmt = NET_HDR_DSN ...
D2022-01-25 13:57:53.527 (1643090273527571287 2cff) unknown.io-thrd.11519: compiler_depend.ts:122:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.17.0.18), input interface("""")
D2022-01-25 13:57:53.527 (1643090273527574518 2cff) unknown.io-thrd.11519: compiler_depend.ts:481:start(): [replica] network client started at port 3, channel = RPC_CHANNEL_TCP, fmt = NET_HDR_HTTP ...
D2022-01-25 13:57:53.527 (1643090273527636049 2cff) unknown.io-thrd.11519: compiler_depend.ts:122:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.17.0.18), input interface("""")
D2022-01-25 13:57:53.527 (1643090273527718361 2cff) unknown.io-thrd.11519: compiler_depend.ts:481:start(): [replica] network client started at port 3, channel = RPC_CHANNEL_UDP, fmt = NET_HDR_HTTP ...
D2022-01-25 13:57:53.527 (1643090273527833455 2cff) unknown.io-thrd.11519: compiler_depend.ts:122:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.17.0.18), input interface("""")
```

2. What did you expect to see?
Should use the real file names.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/894/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/895,https://api.github.com/repos/apache/incubator-pegasus/issues/895,incubator-pegasus,1114750775,895,"the single partition data dir is deleted, restart the service and it will be newly created",WHBANG,38547944,,,CLOSED,2022-01-26T08:26:52Z,2022-06-21T07:22:51Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?

- create a 1 replica table, write some data, check these data is readable
- stop replica server
- delete data directory( replica/, replica/reps/, replica/reps/m.n.pegasus：delete any level of directory can reproduce)
- start replica server
- the deleted partitions are rebuild and the data is lost

2. What did you expect to see?
print error log, partition status is unhealthy

3. What did you see instead?
no error log and the deleted partitions are rebuild

4. What version of Pegasus are you using?
2.0.0","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/895/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/895,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM486jJY,incubator-pegasus,1021981272,895,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2022-01-26T08:41:30Z,2022-01-26T08:41:30Z,"Thanks for your report~
I have some problems with your cases:
- Table have one partition, and how many replicas of it? 1 replica or 3 replicas (by default) ?
- How many replica server nodes in your case?

Expecting your answer.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM486jJY/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/895,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM486noD,incubator-pegasus,1021999619,895,NA,WHBANG,38547944,,,NA,2022-01-26T09:06:37Z,2022-01-26T09:06:37Z,"> Thanks for your report~ I have some problems with your cases:
> 
> * Table have one partition, and how many replicas of it? 1 replica or 3 replicas (by default) ?
> * How many replica server nodes in your case?
> 
> Expecting your answer.

- i am sorry, the description is wrong, I have modified it, it is 1 replica table, partition num is random, i have tested on 1 and 3 replica server nodes;
- only one replica table have this problem;","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM486noD/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/895,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4866QD,incubator-pegasus,1022075907,895,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2022-01-26T10:41:15Z,2022-01-26T10:41:15Z,"> > Thanks for your report~ I have some problems with your cases:
> > 
> > * Table have one partition, and how many replicas of it? 1 replica or 3 replicas (by default) ?
> > * How many replica server nodes in your case?
> > 
> > Expecting your answer.
> 
> * i am sorry, the description is wrong, I have modified it, it is 1 replica table, partition num is random, i have tested on 1 and 3 replica server nodes;
> * only one replica table have this problem;

Thanks for your answer, I got your case.

I suppose your table has 4 partitions with only 1 replica, the stopped replica server called `serverA`, the data directory you delete is `replica/reps/1.0.pegasus`.
- After you stop serverA, and delete serverA directory 'replica/reps/1.0.pegasus', meta server noticed partition 1.0 can not be found in serverA, it will wait for serverA alive.
- Restart serverA, meta server noticed it, then tried to recover partition 1.0 on serverA, then create directory `replica/reps/1.0.pegasus`.

As a result, the directory will be created, and this table only have one replica, as you directly deleted the data(in `replica/reps/1.0.pegasus`), its data won't exist. In multi-replica cases, it will learn data from other replicas.
I think it is not bug, in your case, data is lost and metadata are still stored in meta server and zk, meta server will recreate directory but can not recover the data.
Hoping my answer is helpful, expecting your reply~","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4866QD/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/895,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM487Cff,incubator-pegasus,1022109663,895,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-01-26T11:26:09Z,2022-01-26T11:26:09Z,"IMO, the cluster would better report this issue, not running normally as nothing happened, trigger a coredump or something like that, of course, it's an optional feature, we can disable it by config.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM487Cff/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/895,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM487DfK,incubator-pegasus,1022113738,895,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-01-26T11:31:33Z,2022-01-26T11:31:33Z,"Even for the 3 replica factor table, we can check the consistency of decree/ballot on meta server/zk and the primary replica.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM487DfK/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/895,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM489m_K,incubator-pegasus,1022783434,895,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2022-01-27T02:21:36Z,2022-01-27T02:21:36Z,"Thanks for @acelyc111 reply, I got the point. The problem is that for 1 replica table, meta server wouldn't know if the data lost. I think it is a enhancement for DDD case.

> Even for the 3 replica factor table, we can check the consistency of decree/ballot on meta server/zk and the primary replica.

Firstly, in any replica factor, ballot is only controlled by meta server, it is unnecessary to check its consistency.

Secondly, in current implementation, meta server doesn't do any decree consistency check, the decree even not persistent on zk only in meta memory. In most our production environment, table has 3 replica, meta server as the cluster controller, only cares which replica has newer data, and meta server will compare decree only in DDD situation. If 3 replica factor only lost primary. meta server will not  decree compare, just upgrade one of it secondary.

Back to your case, when your case happened, replica server would be willing to core but not meta server. Meta server collect decree information from replica servers, it can not compare the exact decree with the reported one. You can add a decree check when replica server receive the assign_primary request.

Hoping my answer is helpful, expecting your reply~","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM489m_K/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/895,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM49dG-F,incubator-pegasus,1031040901,895,NA,WHBANG,38547944,,,NA,2022-02-07T03:38:04Z,2022-02-07T03:38:04Z,"@hycdong @acelyc111 hi，I am very happy to participate in the community building of Pegasus. I have made a small modifications to this issue. 
Happy new year～I am looking forward to your suggestions～
https://github.com/XiaoMi/rdsn/pull/1044","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM49dG-F/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/896,https://api.github.com/repos/apache/incubator-pegasus/issues/896,incubator-pegasus,1115716205,896,Fix: remove min_live_node_count_for_unfreeze from meta_options to ensure it can be correctly updated dynamically,empiredan,743379,Dan Wang,,CLOSED,2022-01-27T03:02:19Z,2022-01-27T16:27:03Z,"Since https://github.com/XiaoMi/rdsn/pull/932 `min_live_node_count_for_unfreeze` has been defined as a mutable FLAG. However, `min_live_node_count_for_unfreeze` is still set as a property of `meta_options`. Once `FLAGS_min_live_node_count_for_unfreeze` is updated dynamically, `min_live_node_count_for_unfreeze` in `meta_options` will still be kept as the old value. Therefore `min_live_node_count_for_unfreeze` should be removed from `meta_options` in order to correctly update `FLAGS_min_live_node_count_for_unfreeze` dynamically.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/896/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/898,https://api.github.com/repos/apache/incubator-pegasus/issues/898,incubator-pegasus,1118157989,898,should update gcc and cmake checker,Smityz,22953824,Smilencer,smityz@qq.com,CLOSED,2022-01-29T09:54:26Z,2022-07-19T15:52:22Z,Ref: https://github.com/XiaoMi/rdsn/pull/257,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/898/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/898,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM49Fm8e,incubator-pegasus,1024880414,898,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-01-29T10:01:04Z,2022-01-29T10:01:04Z,Better to describe why @Smityz,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM49Fm8e/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/900,https://api.github.com/repos/apache/incubator-pegasus/issues/900,incubator-pegasus,1118167779,900,Pegasus compile failed on Ubuntu after supporting build on MacOS,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2022-01-29T10:25:36Z,2022-02-07T15:02:44Z,"## Bug Report
Gcc 5.4.0, Cmake 3.13.2
Use Pegasus master(59dc6bd46805e18f405c3a03ebba5a8a591ce029) and rdsn master(2dd67e0885e36f964633c17afc7d91086517dfaf), complie failed, the error like following.

## Complie failed errors
```
Building...
Scanning dependencies of target pegasus_base
[  0%] Building CXX object base/CMakeFiles/pegasus_base.dir/pegasus_const.cpp.o
[  1%] Building CXX object base/CMakeFiles/pegasus_base.dir/pegasus_utils.cpp.o
[  2%] Building CXX object base/CMakeFiles/pegasus_base.dir/rrdb_types.cpp.o
[  2%] Building CXX object base/CMakeFiles/pegasus_base.dir/value_schema_manager.cpp.o
[  3%] Building CXX object base/CMakeFiles/pegasus_base.dir/value_schema_v0.cpp.o
[  4%] Building CXX object base/CMakeFiles/pegasus_base.dir/value_schema_v1.cpp.o
[  4%] Building CXX object base/CMakeFiles/pegasus_base.dir/value_schema_v2.cpp.o
Scanning dependencies of target pegasus_client_impl_objects
[  5%] Building CXX object client_lib/CMakeFiles/pegasus_client_impl_objects.dir/client_factory.cpp.o
[  5%] Building CXX object client_lib/CMakeFiles/pegasus_client_impl_objects.dir/mutation.cpp.o
[  6%] Building CXX object client_lib/CMakeFiles/pegasus_client_impl_objects.dir/pegasus_client_factory_impl.cpp.o
[  7%] Building CXX object client_lib/CMakeFiles/pegasus_client_impl_objects.dir/pegasus_client_impl.cpp.o
[  7%] Building CXX object client_lib/CMakeFiles/pegasus_client_impl_objects.dir/pegasus_scanner_impl.cpp.o
In file included from /home/heyuchen/work/pegasus_github/pegasus/DSN_ROOT/include/dsn/c/api_layer1.h:33:0,
                 from /home/heyuchen/work/pegasus_github/pegasus/DSN_ROOT/include/dsn/service_api_c.h:85,
                 from /home/heyuchen/work/pegasus_github/pegasus/DSN_ROOT/include/dsn/service_api_cpp.h:38,
                 from /home/heyuchen/work/pegasus_github/pegasus/src/include/rrdb/rrdb_types.h:18,
                 from /home/heyuchen/work/pegasus_github/pegasus/src/include/rrdb/rrdb.client.h:22,
                 from /home/heyuchen/work/pegasus_github/pegasus/src/client_lib/pegasus_client_impl.h:24,
                 from /home/heyuchen/work/pegasus_github/pegasus/src/client_lib/pegasus_client_factory_impl.h:24,
                 from /home/heyuchen/work/pegasus_github/pegasus/src/client_lib/client_factory.cpp:21:
/home/heyuchen/work/pegasus_github/pegasus/DSN_ROOT/include/dsn/tool-api/task_tracker.h: In member function ‘void dsn::trackable_task::set_tracker(dsn::task_tracker*, dsn::task*)’:
/home/heyuchen/work/pegasus_github/pegasus/DSN_ROOT/include/dsn/tool-api/task_tracker.h:179:121: error: ‘__FILENAME__’ was not declared in this scope
In file included from /home/heyuchen/work/pegasus_github/pegasus/DSN_ROOT/include/dsn/c/api_layer1.h:33:0,
                 from /home/heyuchen/work/pegasus_github/pegasus/DSN_ROOT/include/dsn/service_api_c.h:85,
                 from /home/heyuchen/work/pegasus_github/pegasus/DSN_ROOT/include/dsn/service_api_cpp.h:38,
                 from /home/heyuchen/work/pegasus_github/pegasus/src/include/rrdb/rrdb_types.h:18,
                 from /home/heyuchen/work/pegasus_github/pegasus/src/include/rrdb/rrdb.client.h:22,
                 from /home/heyuchen/work/pegasus_github/pegasus/src/client_lib/pegasus_client_impl.h:24,
                 from /home/heyuchen/work/pegasus_github/pegasus/src/client_lib/pegasus_client_factory_impl.h:24,
                 from /home/heyuchen/work/pegasus_github/pegasus/src/client_lib/client_factory.cpp:21:
......
Makefile:129: recipe for target 'all' failed
make: *** [all] Error 2
ERROR: build pegasus failed
ERROR: build pegasus failed
```

## How to fix it
I reset commits supporting build on MacOS, and can build succeed in my envrioment.
https://github.com/XiaoMi/rdsn/pull/1034 and #891
I suggest that we should revert those pull requests, and create a new branch called supporting-mac, and merge those pull requests into supporting-mac branch not master branch. 
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/900/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/900,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM49FzwD,incubator-pegasus,1024932867,900,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-01-29T15:33:05Z,2022-01-29T15:33:05Z,"I think we'd better try to keep building successfully on both Linux and macOS on master branch, the portability of an open source project is a basic requirement.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM49FzwD/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/900,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM49c-pD,incubator-pegasus,1031006787,900,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2022-02-07T02:16:08Z,2022-02-07T02:16:08Z,"> I think we'd better try to keep building successfully on both Linux and macOS on master branch, the portability of an open source project is a basic requirement.

Sure~ I agree that the best way is keep building successfully on both Linux and MacOS, but the precondition is that we can quickly fix the current Linux building failed problem. 
Could you please estimate that how long the bug can be fixed? If it can be fixed quickly, we can remain current code waiting the new pull request. If it will take a few time, I suggest revert firstly, it doesn't mean that I disagree supporting building on MaxOS, it just a temporary transition plan, I also expect that master branch can build succeed on MacOS and Linux.
For the newest pull request, it will compile failed in ci process, can reference: https://github.com/apache/incubator-pegasus/runs/4990539990?check_suite_focus=true","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM49c-pD/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/902,https://api.github.com/repos/apache/incubator-pegasus/issues/902,incubator-pegasus,1126755787,902,Bug: can not get last_flushed_decree from meta cf occasionally when rolling_update from 2.0.x,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2022-02-08T03:42:17Z,2022-05-07T01:39:39Z,"## Bug Report
When rolling_update Pegasus server from 2.0.0 to 2.2.0, replica server generate a coredump that could not find the last_flushed_decree occasionally. 
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/902/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/902,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM49q-Je,incubator-pegasus,1034674782,902,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2022-02-10T09:14:52Z,2022-02-10T09:14:52Z,"# Basic background
In Pegasus 1.x version, each write request will write request-decree into RocksDB Manifest file, which is hard for us to upgrade RocksDB official new release. To remove this logic, Pegasus firstly release 2.0.0, which will write request-decree both into Manifest file and RocksDB column family called `meta`, and read it from Manifest file. In the following release 2.1.0, 2.2.0, decree will only be read and wrote through meta column family.

# When and how will it happend
When the cluster upgrade Pegasus from 2.0.0 to higher version(for example 2.2.0), the bug will be happened satifying all following conditions:
1. Cluster has a table doesn't trigger flush memtable after upgrading to 2.0.0
2. One partition of this table is not close gracefully when upgrading to 2.2.0 (When close function called, partition will flush memtable, inluding meta column famlity, decree would be found while reopen RocksDB instance).

# How to avoid it
To avoid this bug, we should gurantee partitions would be closed gracefully in upgrade process.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM49q-Je/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/903,https://api.github.com/repos/apache/incubator-pegasus/issues/903,incubator-pegasus,1127119600,903,Bug: Fix common problem,foreverneverer,23136769,Jiashuo,js982986555@live.com,OPEN,2022-02-08T11:17:13Z,2022-04-26T07:15:42Z,"## Bug Report
This issue is used for tracking some little bug
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/903/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/904,https://api.github.com/repos/apache/incubator-pegasus/issues/904,incubator-pegasus,1127913259,904,redis-proxy invalid header type,pyfdtic,37363212,,,OPEN,2022-02-09T00:47:11Z,2022-02-09T08:17:17Z,"按照这里的文档配置 redis-proxy, https://pegasus.apache.org/api/redis , redis-proxy 可以正常运行, 但是使用 redis-cli 链接时, 无法读取/写入数据, 并报错:
```
E2022-02-08 23:58:53.494 (1644364733494737681 15)  proxy.io-thrd.00015: network.cpp:240:prepare_parser(): invalid header type, remote_client = 127.0.0.1:40514, header_type = '*5\0D\0A'
E2022-02-08 23:58:53.494 (1644364733494766106 15)  proxy.io-thrd.00015: asio_rpc_session.cpp:121:operator()(): asio read from 127.0.0.1:40514 failed


E2022-02-08 23:57:25.30 (1644364645030020869 15)  proxy.io-thrd.00015: network.cpp:240:prepare_parser(): invalid header type, remote_client = 127.0.0.1:40510, header_type = '*3\0D\0A'
E2022-02-08 23:57:25.30 (1644364645030040541 15)  proxy.io-thrd.00015: asio_rpc_session.cpp:121:operator()(): asio read from 127.0.0.1:40510 failed

```
`header_type = '*3\0D\0A'` 是 redis protocol 解析 redis command 的内容, 使用 redis-cli 2.8/5.0.4/6.2.6 测试均有问题
pegasus 版本 2.3.0 

请问, 有可能是什么原因呢?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/904/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/904,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM49mYz_,incubator-pegasus,1033473279,904,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-02-09T08:16:28Z,2022-02-09T08:16:28Z,redis-cli发送什么指令的时候报的错？,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM49mYz_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/905,https://api.github.com/repos/apache/incubator-pegasus/issues/905,incubator-pegasus,1128223443,905,bug: batchMultiGet java client interface may not fetch all data required by the input parameter,cauchy1988,7292411,,,CLOSED,2022-02-09T08:46:26Z,2022-02-16T05:07:24Z,"## Bug Report

It's just  a java client bug, which i have mentioned in  : https://github.com/XiaoMi/pegasus-java-client/issues/176
 
the fix pr i have tried is in : https://github.com/XiaoMi/pegasus-java-client/pull/177","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/905/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/905,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-DiEj,incubator-pegasus,1041113379,905,NA,cauchy1988,7292411,,,NA,2022-02-16T05:07:23Z,2022-02-16T05:07:23Z,done,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-DiEj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/906,https://api.github.com/repos/apache/incubator-pegasus/issues/906,incubator-pegasus,1129699354,906,Bug: flush_all_family_columns failed because of Shutdown in progress,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2022-02-10T09:25:13Z,2022-03-14T02:55:14Z,"## Bug Report
In Pegasus server 2.x version, flush will always be failed while closing replica server. The error log is like:
```
E2022-02-10 16:24:51.130 (1644481491130138280 26fd0) replica.rep_long4.040400070000107c: 
pegasus_server_impl.cpp:2618:flush_all_family_columns(): 
[<app_id>.<partition_index>@<ip>:<port>] flush failed, error = Shutdown in progress:
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/906/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/906,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM49vH44,incubator-pegasus,1035763256,906,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2022-02-11T02:17:18Z,2022-02-11T02:17:18Z,"# Root case
In current Pegasus implementation, when close a replica, we will call following RocksDB functions in turn:
```
// in replica::close() function
rocksdb::CancelAllBackgroundWork(_db, true);
// in pegasus_server::stop() function
_db->Flush(...);
```
Pegasus now use RocksDB 6.6.4, I have read the source code of RocksDB 6.6.4, and found out that Flush function will always be failed if it called after CancelAllBackgroundWork. 
According to https://github.com/facebook/rocksdb/blob/v6.6.4/db/db_impl/db_impl.cc#L429, CancelAllBackgourndWork will `shutting_down_ = true`.
Reference https://github.com/facebook/rocksdb/blob/v6.6.4/db/db_impl/db_impl_compaction_flush.cc#L1766-L1769, Flush will return `Status::ShutdownInProgress()` error if `shutting_down_ = true`.

However, it doesn't mean flush is not triggered in current close process. Function `CancelAllBackgroundWork` will also call flush function if it notice that there are memtables need to be flushed.

# How to fix it
Adjust the function order in close process, call flush function before CancelAllBackgroundWork.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM49vH44/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/908,https://api.github.com/repos/apache/incubator-pegasus/issues/908,incubator-pegasus,1129867053,908,Fix: drop redundant config_status::pending_proposal,empiredan,743379,Dan Wang,,CLOSED,2022-02-10T11:14:44Z,2022-02-21T12:26:23Z,"Since `config_status::pending_proposal` is no longer used, we can drop it from rDSN.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/908/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/910,https://api.github.com/repos/apache/incubator-pegasus/issues/910,incubator-pegasus,1136683593,910,Fix(script): change the library versions of poco in pack scripts,empiredan,743379,Dan Wang,,CLOSED,2022-02-14T02:58:05Z,2022-02-14T09:08:14Z,"Since poco has been bumped from 1.7.8 to 1.11.1 in https://github.com/XiaoMi/rdsn/pull/1034, its lib version has also been changed from then on. Run `./run.sh pack_server` will lead to the following error:
```
Packaging pegasus server 2.1.SNAPSHOT (d43693b2bb22c0ef6182ec6e1e45c5e7117cd22b) glibc-2.17 debug ...
‘./DSN_ROOT/bin/pegasus_server/pegasus_server’ -> ‘pegasus-server-2.1.SNAPSHOT-d43693b-glibc2.17-debug/bin/pegasus_server’
‘./DSN_ROOT/lib/libdsn_meta_server.so’ -> ‘pegasus-server-2.1.SNAPSHOT-d43693b-glibc2.17-debug/bin/libdsn_meta_server.so’
‘./DSN_ROOT/lib/libdsn_replica_server.so’ -> ‘pegasus-server-2.1.SNAPSHOT-d43693b-glibc2.17-debug/bin/libdsn_replica_server.so’
‘./DSN_ROOT/lib/libdsn_utils.so’ -> ‘pegasus-server-2.1.SNAPSHOT-d43693b-glibc2.17-debug/bin/libdsn_utils.so’
cp: cannot stat ‘./rdsn/thirdparty/output/lib/libPoco*.so.48’: No such file or directory
ERROR: copy file failed: cp ./rdsn/thirdparty/output/lib/libPoco*.so.48 pegasus-server-2.1.SNAPSHOT-d43693b-glibc2.17-debug/bin
```

Running `./run.sh pack_tools` will also has the similar problem. Therefore, we should fix the library versions of poco in pack scripts.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/910/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/913,https://api.github.com/repos/apache/incubator-pegasus/issues/913,incubator-pegasus,1138295287,913,support jemalloc,levy5307,22141103,赵立伟,zlw5307@163.com,CLOSED,2022-02-15T07:31:22Z,2022-02-15T07:57:05Z,"## Feature Request

It's well known that [jemalloc](http://jemalloc.net/) is an excellent memory allocator, especially scalable for multi-threaded applications. It's adopted or supported by many famous projects, such as redis, clickhouse, rocksdb, etc.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/913/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/915,https://api.github.com/repos/apache/incubator-pegasus/issues/915,incubator-pegasus,1148998545,915,有没有编译好的发布包提供啊，自己编译实在太痛苦了，求一个可用的release包,mabohao1991,36003122,,,CLOSED,2022-02-24T08:28:16Z,2022-03-29T03:14:25Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->

**Describe alternatives you've considered:**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->

**Teachability, Documentation, Adoption, Migration Strategy:**
<!-- If you can, explain some scenarios how users might use this, situations it would be helpful in. Any API designs, mockups, or diagrams are also helpful. -->
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/915/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/915,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-j8-H,incubator-pegasus,1049612167,915,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2022-02-24T08:34:04Z,2022-02-24T08:34:04Z,You can take a look at this [page](https://github.com/apache/incubator-pegasus/releases),"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-j8-H/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/915,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-kJxG,incubator-pegasus,1049664582,915,NA,mabohao1991,36003122,,,NA,2022-02-24T09:38:56Z,2022-02-24T09:38:56Z,"> 你可以看看这个[页面](https://github.com/apache/incubator-pegasus/releases)

您好，这个页面我在手册里面看到了，但是没有找到编译好的包下载链接，能帮忙再给下提示吗？多谢了","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-kJxG/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/915,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-kLOS,incubator-pegasus,1049670546,915,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2022-02-24T09:45:52Z,2022-02-24T09:45:52Z,"> > 你可以看看这个[页面](https://github.com/apache/incubator-pegasus/releases)
> 
> 您好，这个页面我在手册里面看到了，但是没有找到编译好的包下载链接，能帮忙再给下提示吗？多谢了

你往下翻就行，在2.0.0里面有编译好的包，后面的版本确实没有。不过编译好的包，在你们的平台上不一定能跑的通啊，可能会有兼容性问题的 : (","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-kLOS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/915,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-kLUm,incubator-pegasus,1049670950,915,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2022-02-24T09:46:21Z,2022-02-24T09:46:21Z,"> > 你可以看看这个[页面](https://github.com/apache/incubator-pegasus/releases)
> 
> 您好，这个页面我在手册里面看到了，但是没有找到编译好的包下载链接，能帮忙再给下提示吗？多谢了

你们这边是公司要用Pegasus，还是个人感兴趣玩一下？","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-kLUm/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/915,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-kq0O,incubator-pegasus,1049799950,915,NA,mabohao1991,36003122,,,NA,2022-02-24T12:15:33Z,2022-02-24T12:15:33Z,"> 

我们公司也在用Pegasus做的二次开发，我自己想弄个原生的玩一下，顺便看看代码，但是我2.1.0在docker里面没有编译通过，所以就想弄一个编好的版本，跑一下子","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-kq0O/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/915,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-kq_n,incubator-pegasus,1049800679,915,NA,mabohao1991,36003122,,,NA,2022-02-24T12:16:27Z,2022-02-24T12:16:27Z,"> > > 你可以看看这个[页面](https://github.com/apache/incubator-pegasus/releases)
> > 
> > 
> > 您好，这个页面我在手册里面看到了，但是没有找到编译好的包下载链接，能帮忙再给下提示吗？多谢了
> 
> 你往下翻就行，在2.0.0里面有编译好的包，后面的版本确实没有。不过编译好的包，在你们的平台上不一定能跑的通啊，可能会有兼容性问题的 : (

我想问一下，这个包适用哪些平台啊，或者有docker镜像吗？","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-kq_n/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/915,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-nDEk,incubator-pegasus,1050423588,915,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2022-02-25T01:21:50Z,2022-02-25T01:21:50Z,"> > > > 你可以看看这个[页面](https://github.com/apache/incubator-pegasus/releases)
> > > 
> > > 
> > > 您好，这个页面我在手册里面看到了，但是没有找到编译好的包下载链接，能帮忙再给下提示吗？多谢了
> > 
> > 
> > 你往下翻就行，在2.0.0里面有编译好的包，后面的版本确实没有。不过编译好的包，在你们的平台上不一定能跑的通啊，可能会有兼容性问题的 : (
> 
> 我想问一下，这个包适用哪些平台啊，或者有docker镜像吗？

@Shuo-Jia @hycdong 你们跟一下这个问题吧","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-nDEk/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/915,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-nHfl,incubator-pegasus,1050441701,915,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2022-02-25T02:01:57Z,2022-02-25T02:01:57Z,"https://pegasus.apache.org/docs/build/compile-by-docker/
https://github.com/pegasus-kv/pegasus-docker/tree/2.1.0/pegasus-build-env
可以参照这两个试试在docker上编译2.1.0","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-nHfl/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/915,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-n41B,incubator-pegasus,1050643777,915,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2022-02-25T08:34:00Z,2022-02-25T08:34:00Z,"> > 
> 
> 我们公司也在用Pegasus做的二次开发，我自己想弄个原生的玩一下，顺便看看代码，但是我2.1.0在docker里面没有编译通过，所以就想弄一个编好的版本，跑一下子

请问你们这边是哪个公司？目前已经上线Pegasus了吗？","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-n41B/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/915,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-n5cZ,incubator-pegasus,1050646297,915,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2022-02-25T08:36:57Z,2022-02-25T08:36:57Z,我微信号18500688786，方便的话可以加一下我 : ),"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4-n5cZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/918,https://api.github.com/repos/apache/incubator-pegasus/issues/918,incubator-pegasus,1156668733,918,error while loading shared libraries: libzstd.so.1,aubdiy,9246111,AUB,,CLOSED,2022-03-02T07:35:53Z,2022-06-01T03:08:10Z,"## Bug Report
版本: 2.3.0
问题:  使用 docker 编译成功, 但执行 start_onebox 后, pegasus  并没有启动, 提示缺少 ` libzstd.so.1`

步骤:
1. 使用 docker 编译
```
docker run -v /data/test/apache-pegasus-2.3.0-incubating-src:/root/pegasus \
           apachepegasus/build-env:centos7 \
           /bin/bash -c ""./run.sh build -c""
docker run -v /data/test/apache-pegasus-2.3.0-incubating-src:/root/pegasus \
           apachepegasus/build-env:centos7 \
           /bin/bash -c ""./run.sh pack_server""
docker run -v /data/test/apache-pegasus-2.3.0-incubating-src:/root/pegasus \
           apachepegasus/build-env:centos7 \
           /bin/bash -c ""./run.sh pack_tools""
```

2. 设置 library,  使用 'pegasus-server-2.3.0-non-git-glibc2.17-release/bin/config_hdfs.sh' 脚本中的
```
JAVA_JVM_LIBRARY_DIR=$(dirname $(find ""${JAVA_HOME}/"" -name libjvm.so  | head -1))
export LD_LIBRARY_PATH=${JAVA_JVM_LIBRARY_DIR}:$LD_LIBRARY_PATH
```

3. 启动 onebox
```
./run.sh start_onebox

...
...
...

Decompressing zookeeper...
JMX enabled by default
Using config: /data/ljx/apache-pegasus-2.3.0-incubating-src/.zk_install/zookeeper-3.4.6/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
Zookeeper started at port 22181
starting server
cd /data/ljx/apache-pegasus-2.3.0-incubating-src/onebox/meta1 && /data/ljx/apache-pegasus-2.3.0-incubating-src/onebox/meta1/pegasus_server config.ini -app_list meta &>result &
cd /data/ljx/apache-pegasus-2.3.0-incubating-src/onebox/meta2 && /data/ljx/apache-pegasus-2.3.0-incubating-src/onebox/meta2/pegasus_server config.ini -app_list meta &>result &
cd /data/ljx/apache-pegasus-2.3.0-incubating-src/onebox/meta3 && /data/ljx/apache-pegasus-2.3.0-incubating-src/onebox/meta3/pegasus_server config.ini -app_list meta &>result &
cd /data/ljx/apache-pegasus-2.3.0-incubating-src/onebox/replica1 && /data/ljx/apache-pegasus-2.3.0-incubating-src/onebox/replica1/pegasus_server config.ini -app_list replica &>result &
cd /data/ljx/apache-pegasus-2.3.0-incubating-src/onebox/replica2 && /data/ljx/apache-pegasus-2.3.0-incubating-src/onebox/replica2/pegasus_server config.ini -app_list replica &>result &
cd /data/ljx/apache-pegasus-2.3.0-incubating-src/onebox/replica3 && /data/ljx/apache-pegasus-2.3.0-incubating-src/onebox/replica3/pegasus_server config.ini -app_list replica &>result &
```

4. 查看状态 `./run.sh list_onebox`, 无任何进程,   查看日志错误,提示缺少 `libzstd.so.1`
```
 cat onebox/replica1/result

/data/test/apache-pegasus-2.3.0-incubating-src/onebox/replica1/pegasus_server: error while loading shared libraries: libzstd.so.1: cannot open shared object file: No such file or directory
```

---
不用 docker,  直接在 centos7 上进行编译,   同时使用 `export LD_LIBRARY_PATH=${JAVA_JVM_LIBRARY_DIR}:$LD_LIBRARY_PATH`  设置好 library(如果不手动设置 library, 依然无法启动)
一切正常

---
1.  猜测 doker 编译的包, 无法启动 onebox,   可能是由于镜像中library 不全导致的, 
2. 直接编译的包,  需要手动设置 library 才能正常运行,  但是 onebox 文档中没有提及,    建议文档中增加说明, 或者优化启动脚本, 使其自动设置 library","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/918/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/918,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5CIJkj,incubator-pegasus,1109432611,918,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2022-04-26T07:14:03Z,2022-04-26T07:14:03Z,"镜像库环境应该都是全的，建议重新拉取最新的编译环境：
```shell
docker pull apache/pegasus:build-env-centos7
```

然后进入编译环境，在编译环境中，克隆代码并编译，然后测试onebox","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5CIJkj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/922,https://api.github.com/repos/apache/incubator-pegasus/issues/922,incubator-pegasus,1159510579,922,Feature: implement the framework of the new metrics system,empiredan,743379,Dan Wang,,OPEN,2022-03-04T10:51:38Z,2023-01-28T06:56:50Z,"# 1. Motivation

The root cause of introducing a new metrics system is the unreasonable naming of the perf-counter, as it's been described in [2020-08-27-metric-api.md](https://github.com/apache/incubator-pegasus/blob/master/rfcs/2020-08-27-metric-api.md). The naming is verbose: besides that there are meaningless words in it, some terms can be moved as the labels/tags which tend to have common properties.

There're also other reasons to refactor the perf-counter. 

Firstly, in perf-counter, the types of metrics have not been strictly separated, such as gauge and counter. For example, *set* is not an efficient operation for counter.

Secondly, as the base class, perf-counter has all the abstract interfaces for each type of metric. Every sub class has to implement all the abstract interfaces. For example, it is unreasonable that a gauge has *get_percentile* method, though it can simply be implemented as `dassert(false, ...);`.

Thirdly, there're performance problems for `perf_counter_number_atomic`. It should be optimized and a more efficient counter should be provided.

# 2. The Framework

Even if we need to implement a new framework, an alternative solution is that we can turn to mature and active metrics lib for c++. However, there's few of this kind of libs. https://github.com/jupp0r/prometheus-cpp is a possible candidate. It implements [Prometheus Data Model](https://prometheus.io/docs/concepts/data_model/) which is naturally supported by Prometheus. It can also be exported and translated into the other data model of metrics. Nevertheless, its implementation is not efficient enough for high-concurrency system; even sampling for [Summary](https://prometheus.io/docs/concepts/metric_types/#summary) (a metric type of Prometheus, also known as percentile) will block the task thread.

Therefore, just like what we've done before, we should introduce an abstraction level for the new metrics system with well-defined metric types; then, collect the metric data and translate them into the metric models supported by each specific monitoring system, such as Prometheus or Open Falcon; finally, the metric data are gathered by the monitoring system and can be observed very conveniently. 

There are some requirements for the new abstraction:
* It should be able to applied to all of the existing metrics;
* It should support to manage both of common and exclusive properties of the metrics, which later may be used as labels/tags;
* It should provide separate and well-defined metric types;
* The provided metric sampling should be fast enough without excessive cpu and memory consumption, and never block the task thread.

The new abstraction level is inspired by [Kudu Metrics](https://github.com/apache/kudu/blame/master/src/kudu/util/metrics.h). Next I'll describe how it is implemented.

## 2.1 Metric Registry

All of the metrics are maintained in the metric registry. The metric registry is a singleton. While a metric entity (see 2.2) is instantiated, it will be registered into the registry.

The main use of registry is to collect all of metric data and export to the specific data sink (see 2.5), since it knows every metric.

## 2.2 Metric Entity Prototype

Each metric has a corresponding *level*, which is called an *entity*, such as the memory of a **server**, the put latency of a **table**, the get latency of a **replica**. All of **server**, **table** and **replica** are entities.

`METRIC_DEFINE_entity(...)` macro can be used to define an entity prototype. An entity prototype is an abstract class of an entity. To become a concrete entity, a prototype should be instantiated with some attributions. As is mentioned in 2.1, during instantiation the entity will also be registered into the registry with its unique ID.

For example, a **server** can be instantiated with its IP or hostname; a **table** can be instantiated with its app id or name; a **replica** can be instantiated with its app id and partition id.

## 2.3 Metric Prototype

Similar with the entity prototype, a metric should also be instantiated from a metric prototype. A metric prototype defines the basic meta information of a metric, including the entity it is attached to, its name, its unit (such as bytes, milliseconds, operations per second, etc.), and its description. To define a metric prototype, just use `METRIC_DEFINE_*` macro.

Once a metric prototype is defined, it can be used to instantiate a metric, which will also be attached to the specific entity thus maintained by a registry.

## 2.4 The Types of Metrics

According to the current metrics we've used in Pegasus, there are 5 types for the new framework: gauge, counter, volatile counter, meter and percentile. For simplicity，a brief introduction is given for each type; the implementation details will be described in the each issue of the types. 

### 2.4.1 The Gauge

A gauge is a point-in-time measurement. The value got from a gauge is exactly the same as what it has been recently set. 

There are 2 primitive types supported for the gauge: `int64_t` for a value of integer and `double` for a value of floating-point.

An example of a gauge is `meta*eon.meta_service*unalive_nodes`, which shows the current number of dead replica servers. 

### 2.4.2 The Counter

Typically a counter is a metric that only monotonically increases ([Counter](https://prometheus.io/docs/concepts/metric_types/#counter) in Prometheus). Similarly, in pegasus, there are metrics of this kind of counter, such as `zion*profiler*RPC_RRDB_RRDB_PUT.cancelled`.

However, in Pegasus there are also another kind of counter that may sometimes decrease, such as `replica*app.pegasus*manual.compact.running.count`, which will increase while a new manual compact is started, and decrease while it is finished.

Therefore, the counter implemented for Pegasus should support both increase and decrease, just like what has been done in [Counters](https://metrics.dropwizard.io/4.2.0/manual/core.html#man-core-counters) of [Dropwizard](https://www.dropwizard.io/).

### 2.4.3 The Volatile Counter

Fetching value from a general counter is trivial: just return the current value of the counter is ok.

However, many metrics are ""recent"" in Pegasus, which means that the historically-accumulated count is ignored. If a counter is ""recent"", it will be reset to 0 immediately after its value is fetched. Thus the ""recent"" is the duration between 2 successive accesses to the counter. For example `replica*eon.failure_detector*recent_beacon_fail_count` means how many failed beacons there are recently.

According to this scenario, a special counter (called a volatile counter) is implemented. The only difference between it and a general counter is that once the value is read, a volatile counter will be immediately cleared to 0, while a general counter is kept unchanged.

### 2.4.4 The Meter

A meter measures the rate of occurrences of a set of events over time. In Pegasus the typical application is QPS, such as `zion*profiler*RPC_RRDB_RRDB_GET.qps`, which measures the rate at which a replica server reads a single value.

It should be noted that the underlying counter of a meter is volatile, which means it will be reset to 0 immediately after the value of the meter is fetched. 

### 2.4.5 The Percentile

Like [Summary](https://prometheus.io/docs/concepts/metric_types/#summary) in Prometheus, the metric type of percentile samples observations. Periodically it calculates configurable percentiles (p50, p75, p90, p95, p99, p999, etc.) over the recent samples. 

The most common usage of the percentile is latency, such as `zion*profiler*RPC_RRDB_RRDB_PUT.latency.server.p999`, which is the 999th percentile of server-side latency between the moment the task is pushed into the queue and the point the server begins to reply to client.

## 2.5 Metric Data Sink

As it is described in the beginning of this chapter, the ultimate goal is to show the metrics in the monitoring system. Therefore, the metric data sink, as its name implies, actually is the metric model supported by each monitoring system to which the metric data are collected.

Based on this design, the base class of the data sink must be abstract. It is necessary to implement the sub class for each metric model of target, such as Prometheus or Open Falcon.

In the sub class, the metric data will be translated into the specific model. For Prometheus, a client should be created and initialized, listening on a port which is used for the Prometheus server to pull data. 

## 2.6 Clean the Stale Metrics

A table could be dropped in Pegasus. All of its metrics then will become useless if it's no longer recalled.

In the old metrics system (i.e. perf-counters) the expired metrics won't be cleared. It still exists in the memory. While the process of replica server could run for a very long term, the uncleaned outdated metrics will lead to memory leak.

Thus a mechanism should be introduced to drop the stale metrics. We can use the mechanism employed by Kudu (see `MetricEntity::RetireOldMetrics()`).

Since `ref_ptr` is used to be the type of a metric, we can just check the count of `ref_counter`. Generally if a table is in use, the count will be 2: one for a `ref_ptr` in metric entity (and the entity in metric registry), and another for a `ref_ptr` held by the user class (the table-related class). Once the user object is destructed, the count will become 1, which means the metric is not needed. This can be periodically detected. After a period of configurable time, if a metric is still unused, it will be dropped from memory.

When it comes to the table-level entity, once a table is dropped (thus the ref count will become 1), actually the whole table entity can be removed (thus all its metrics will be cleared). In a word, we can retire the metrics from memory according to its ref count.

# 3. Examples

## 3.1 Define & Instantiate Metric Entities

In this section, some examples are given to show how to define and instantiate metric entities.

### 3.1.1 Define & Instantiate Server Entity

```c++
// Define server entity
METRIC_DEFINE_entity(server);

// Instantiate server entity
auto server_entity = METRIC_ENTITY_server.instantiate(""server"");
```

### 3.1.2 Define & Instantiate Table Entity

```c++
// Define table entity
METRIC_DEFINE_entity(table);

// Instantiate table entity
std::string table_name(""test_app1"");
auto table_entity = METRIC_ENTITY_table.instantiate(
    table_name, 
    {{""table"", table_name}}
);
}
```

### 3.1.3 Define & Instantiate Replica Entity

```c++
// Define replica entity
METRIC_DEFINE_entity(replica);

// Instantiate replica entity
std::string table_name(""test_app1"");
int32_t partition_id = 2;
auto replica_id = fmt::format(""{}:{}"", table_name, partition_id);
auto replica_entity = METRIC_ENTITY_replica.instantiate(
    replica_id, 
    {{""table"", table_name}, {""partition"", std::to_string(partition_id)}}
);
```

## 3.2 Define metrics

In this section, take the put latency for example to show how to define and instantiate metrics.  

### 3.2.1 Define & Instantiate Server-level Metric

```c++
// Define server-level metric
METRIC_DEFINE_percentile(server, server_put_latency, kNanoSeconds,
    ""the server-level latency of put requests"");

// Instantiate metric
auto server_put_latency = METRIC_server_put_latency.instantiate(
    server_entity, 
    {90, 95, 99}
);
```

### 3.2.2 Define & Instantiate Table-level Metric

```c++
// Define replica-level metric
METRIC_DEFINE_percentile(table, table_put_latency, kNanoSeconds,
    ""the table-level latency of put requests"");

// Instantiate metric
auto table_get_latency = METRIC_table_put_latency.instantiate(
    table_entity, 
    {90, 99}
);
```

### 3.2.3 Define & Instantiate Replica-level Metric

```c++
// Define replica-level metric
METRIC_DEFINE_percentile(replica, replica_put_latency, kNanoSeconds,
    ""the replica-level latency of put requests"");

// Instantiate metric
auto replica_get_latency = METRIC_replica_put_latency.instantiate(
    replica_entity, 
    {50, 75, 90, 95, 99, 999}
);
```

# 4. Schedule

All of the sub tasks to implement the new framework are listed here to track the status of each of them.

## 4.1 Implement the Metric Entity & its Prototype

- [x] https://github.com/apache/incubator-pegasus/issues/925

## 4.2 Implement the Metric Registry

- [x] https://github.com/apache/incubator-pegasus/issues/927

## 4.3 Implement the Metric & its Prototype

- [x] https://github.com/apache/incubator-pegasus/issues/928

## 4.4 Implement the Metric Type of Gauge

- [x] https://github.com/apache/incubator-pegasus/issues/929
- [x] https://github.com/apache/incubator-pegasus/issues/1191

## 4.5 Implement the Metric Types of Counters (includes the Volatile Counter)

- [x] https://github.com/apache/incubator-pegasus/issues/889
- [x] https://github.com/apache/incubator-pegasus/issues/931
- [x] https://github.com/apache/incubator-pegasus/issues/933
- [x] https://github.com/apache/incubator-pegasus/issues/951

## 4.6 Implement the Metric Type of Percentile

- [x] https://github.com/apache/incubator-pegasus/issues/974
- [x] https://github.com/apache/incubator-pegasus/issues/991

## 4.7 Merge prometheus-rebased-dev into master branch

- [x] https://github.com/apache/incubator-pegasus/issues/1010

## 4.8 Collect Metrics 

- [ ] https://github.com/apache/incubator-pegasus/issues/1116
- [x] https://github.com/apache/incubator-pegasus/issues/1195
- [x] https://github.com/apache/incubator-pegasus/issues/1206

## 4.9 Implement the Metric Data Sink of Prometheus



## 4.10 Implement the Metric Data Sink of Open Falcon



## 4.11 Clean the Stale Metrics


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/922/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/923,https://api.github.com/repos/apache/incubator-pegasus/issues/923,incubator-pegasus,1159569564,923,We need a faster way to copy,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2022-03-04T11:59:27Z,2022-03-14T02:54:51Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
Our business use copy_data frequently， but it's performance is not good. I want improve it

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->
I find we can use async_multi_set improve it when single hash_key have multi data. Let's do it.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/923/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/923,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4_Io1v,incubator-pegasus,1059229039,923,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2022-03-04T14:50:56Z,2022-03-04T14:50:56Z,"The only direction to achieve high performance copy is to copy files in bulks. MultiSet still goes through heavy deserialization from disk files, rocksdb value parsing, thrift serialization, all these overheads slow down the throughput.

It's comparably heavier than simply copying the internal files.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4_Io1v/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/923,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4_jkMu,incubator-pegasus,1066287918,923,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-03-14T02:54:43Z,2022-03-14T02:54:43Z,"@neverchanje 
In some usage scenario, Pegasus users use the Shell tool to copy data from one cluster/table to another to increase partition count in lower version of Pegasus, copy files would not help.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4_jkMu/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/924,https://api.github.com/repos/apache/incubator-pegasus/issues/924,incubator-pegasus,1161279915,924,Feature: creat new app through new client interface ,cauchy1988,7292411,,,CLOSED,2022-03-07T11:17:01Z,2022-06-01T03:08:42Z,"## Feature Request
we are thinking bout adding a new client interface (mainly in java client  ) which can create a new app through only rpc request to the server side~

**Is your feature request related to a problem? Please describe:**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
our customer code will run in docker or one of some container type in k8s in the future,  in the container circumstance, customer code can not conveniently use the shell tool------""run.sh"" to create a new pegasus app; 

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->
we want to realize a client interface which can create a new app through only rpc requests to the server side

**Describe alternatives you've considered:**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->

**Teachability, Documentation, Adoption, Migration Strategy:**
<!-- If you can, explain some scenarios how users might use this, situations it would be helpful in. Any API designs, mockups, or diagrams are also helpful. -->
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/924/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/924,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4_N0PZ,incubator-pegasus,1060586457,924,NA,GiantKing,10461183,,giantkingww@163.com,NA,2022-03-07T11:33:12Z,2022-03-07T11:33:12Z,"Get/put etc. are business flow, but create/delete apps are management flow. I think mixed two flow together into client is not a good idea. We can't implement authentication in client, which is a basicly requirement for management flow.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4_N0PZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/924,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4_N3Pt,incubator-pegasus,1060598765,924,NA,cauchy1988,7292411,,,NA,2022-03-07T11:42:07Z,2022-03-07T11:42:07Z,"> Get/put etc. are business flow, but create/delete apps are management flow. I think mixed two flow together into client is not a good idea. We can't implement authentication in client, which is a basicly requirement for management flow.

I think client side can contain not only normal put/get, read/write functionality, but also can include some ""tools like"" interface for ""management usage""  you have mentioned;  authentication is another topic,  since normal read or write also need it;","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4_N3Pt/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/924,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4_Rdsz,incubator-pegasus,1061542707,924,NA,GiantKing,10461183,,giantkingww@163.com,NA,2022-03-08T08:49:18Z,2022-03-08T08:49:18Z,"> > Get/put etc. are business flow, but create/delete apps are management flow. I think mixed two flow together into client is not a good idea. We can't implement authentication in client, which is a basicly requirement for management flow.
> 
> I think client side can contain not only normal put/get, read/write functionality, but also can include some ""tools like"" interface for ""management usage"" you have mentioned; authentication is another topic, since normal read or write also need it;

How to prevent common users from calling this interface? In most scenarios, only administrators have permission to create creat a new app.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4_Rdsz/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/924,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4_Ruis,incubator-pegasus,1061611692,924,NA,cauchy1988,7292411,,,NA,2022-03-08T10:10:20Z,2022-03-08T10:10:20Z,"> > > Get/put etc. are business flow, but create/delete apps are management flow. I think mixed two flow together into client is not a good idea. We can't implement authentication in client, which is a basicly requirement for management flow.
> > 
> > 
> > I think client side can contain not only normal put/get, read/write functionality, but also can include some ""tools like"" interface for ""management usage"" you have mentioned; authentication is another topic, since normal read or write also need it;
> 
> How to prevent common users from calling this interface? In most scenarios, only administrators have permission to create creat a new app.

previously, we also cannot limit  permissions of normal users to create a app, since they can also use the shell tools;  In this case, I will add kerberos to the related rpc interface to let only the administrators to create new app;","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4_Ruis/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/924,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4_VK7O,incubator-pegasus,1062514382,924,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-03-09T03:19:11Z,2022-03-09T03:19:11Z,"Thanks to @GiantKing 's reminding, we should keep in mind not expose too many privilege to common Pegasus users.
But I also think we should achieve the goal by using authority control mechanism in Pegasus, not to provide Java API is not reasonable cause any body can use the shell tool to create table now, without any permission.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4_VK7O/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/924,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4_ZLA1,incubator-pegasus,1063563317,924,NA,levy5307,22141103,赵立伟,zlw5307@163.com,NA,2022-03-10T01:33:49Z,2022-03-10T01:33:49Z,"I think it's OK to add `createTable` interface . Many other storage systems support this interface, for example: Dynamo. And Here is the [official document](https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/services/dynamodb/DynamoDbClient.html#createTable-java.util.function.Consumer-)
And In my opinion, it's better to enable authentation to ensure safety.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM4_ZLA1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/925,https://api.github.com/repos/apache/incubator-pegasus/issues/925,incubator-pegasus,1163660144,925,Feature: implement the metric entity and its prototype,empiredan,743379,Dan Wang,,CLOSED,2022-03-09T08:56:30Z,2022-03-11T13:21:33Z,"As it has been described in https://github.com/apache/incubator-pegasus/issues/922, an entity is a *level* for some metrics, such as **server**, **table** and **replica**. It has a unique ID and some attributes, if any.

An entity prototype is a meta class of an entity. In essence it's a type and used to manufacture an entity instance, to which a metric will be attached.

The interfaces of the entities corresponding to metrics, registry and data sink will be committed later on. ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/925/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/926,https://api.github.com/repos/apache/incubator-pegasus/issues/926,incubator-pegasus,1163958090,926,Feature: support direct I/O when we download files from hdfs,GiantKing,10461183,,giantkingww@163.com,OPEN,2022-03-09T13:44:03Z,2022-03-09T13:44:03Z,"As we promote BulkLoad feature to our business, downloading files will cause a lot of negative effects on our service. I will ues direct I/O as writing files to disk, so that the cache of Linux will be bypassed. And the impact of disk writing should be reduced.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/926/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/927,https://api.github.com/repos/apache/incubator-pegasus/issues/927,incubator-pegasus,1166406933,927,Feature: implement the metric registry,empiredan,743379,Dan Wang,,CLOSED,2022-03-11T13:17:42Z,2022-03-14T02:45:34Z,"In https://github.com/apache/incubator-pegasus/issues/922, it's mentioned that while an entity will be registered in the metric registry. Thus the registry holds all of the entities; since an entity will have all of its metrics in a map as one of its members, a registry actually holds all of the metrics. 

The reason why a registry needs to have all entities and metrics, is that periodically it will take snapshots over the registry used by the monitoring systems. On the other hand, we can conveniently iterate over the registry to detect the stale entities and metrics that need to be cleaned.

Only one registry is needed for now. Thus the registry is implemented as a singleton.

In fact the entity is created by the registry, if it's not found in the registry; otherwise the original entity will not be updated except that its attributes might be changed. The reason for this design is that the user object that holds a `ref_ptr` of the entity may be destructed; after a while a new similar user object is constructed, where an entity with the same ID will also be instantiated again. The members of the original entity, such as the metrics, should be inherited by the new entity. The attributes can be updated, though in most cases they will not be changed.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/927/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/928,https://api.github.com/repos/apache/incubator-pegasus/issues/928,incubator-pegasus,1169662369,928,Feature: implement the metric and its prototype,empiredan,743379,Dan Wang,,CLOSED,2022-03-15T13:16:53Z,2022-03-21T02:45:00Z,"A metric prototype provides the meta info that are used to build an object of metric, as it's described in https://github.com/apache/incubator-pegasus/issues/922. Given the basic info, we can define a metric prototype. Once an entity is instantiated, an instance of metric can be constructed with it by the defined metric prototype. In the internal of each entity, all instances of metrics are held in a map. User can derive implementations from the base classes of metric prototype and metric.

Similar with an entity, a metric can also be reinstantiated with the same entity that it is attached to with the same metric prototype. However, the metric will not be changed if the metric prototype has existed in the entity; a metric will be created only when the metric prototype is not in that entity.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/928/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/929,https://api.github.com/repos/apache/incubator-pegasus/issues/929,incubator-pegasus,1176349423,929,Feature: implement the gauge,empiredan,743379,Dan Wang,,CLOSED,2022-03-22T06:58:58Z,2022-03-23T11:15:53Z,"As base abstractions for metrics have been supported in https://github.com/apache/incubator-pegasus/issues/928, concrete implementation can be considered for each metric type.

In https://github.com/apache/incubator-pegasus/issues/922 we've mentioned that a gauge is an instantaneous measurement which is a single numerical value that can arbitrarily go up and down. It can be typically used for current memory usage, the total capacity and available ratio of a disk, etc.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/929/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/931,https://api.github.com/repos/apache/incubator-pegasus/issues/931,incubator-pegasus,1179384860,931,Feature: implement the counter,empiredan,743379,Dan Wang,,CLOSED,2022-03-24T11:31:20Z,2022-03-28T03:04:31Z,"In https://github.com/apache/incubator-pegasus/issues/922 it has been mentioned that a counter in essence is a 64-bit integer that can be incremented and decremented. It can be used to measure the number of tasks in queues, current number of running manual compacts, etc. All counters start out at 0.

From the aspect of implementations, there are 2 kinds of counters:

- One is the general type of Counter that are implemented by `striped_long_adder`, which can achieve high performance while consuming less memory if it's not updated very frequently.
- Another uses `concurrent_long_adder` as the underlying implementation. It has higher performance while consuming more memory if it's updated very frequently.

For the details of `striped_long_adder` and `concurrent_long_adder` please see https://github.com/apache/incubator-pegasus/issues/889.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/931/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/932,https://api.github.com/repos/apache/incubator-pegasus/issues/932,incubator-pegasus,1180465218,932,support running on aarch64,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2022-03-25T08:00:17Z,2022-04-26T06:49:45Z,"## Feature Request

**Describe the feature you'd like:**
As we try to run pegasus on aarch64 machine to our business, I see some problems from thirdparty that need to be solved. Finally I passed unit test on aarch64 machine.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/932/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/932,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5ATJeh,incubator-pegasus,1078761377,932,NA,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,NA,2022-03-25T08:09:53Z,2022-03-25T08:09:53Z,Should I set it to bug is better?,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5ATJeh/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/933,https://api.github.com/repos/apache/incubator-pegasus/issues/933,incubator-pegasus,1183120828,933,Feature: implement the volatile counter,empiredan,743379,Dan Wang,,CLOSED,2022-03-28T09:04:30Z,2022-04-14T06:31:58Z,"In https://github.com/apache/incubator-pegasus/issues/931 we've implemented the counter, a type of metric. However, a counter can be volatile, whose value() function will reset the counter atomically after its value is fetched. A volatile counter can also be called as a ""recent"" counter.

Sometimes ""recent"" counters are needed, such as the number of recent failed beacons sent from replica server, the count of updating configurations of partitions recently, etc. The ""recent"" can be considered to be the accumulated count since it has been fetched last.

In most cases, a general (i.e. non-volatile) counter is enough, which means it can also work for ""recent"" counters. For example, in Prometheus, `delta()` can be used to compute ""recent"" count for a general counter. Therefore, declare a counter as volatile only when necessary.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/933/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/941,https://api.github.com/repos/apache/incubator-pegasus/issues/941,incubator-pegasus,1199538734,941,support and refactor read/write deny table env,foreverneverer,23136769,Jiashuo,js982986555@live.com,CLOSED,2022-04-11T07:53:54Z,2022-04-14T06:05:25Z,Pegasus has support `replica.deny_client_write` table env to [reject all client write](https://github.com/XiaoMi/rdsn/blob/289eb4609ae781e3bcc6bbc6bfeae64dfd8fa785/src/replica/replica_2pc.cpp#L56). we also need support `deny read request` table env,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/941/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/942,https://api.github.com/repos/apache/incubator-pegasus/issues/942,incubator-pegasus,1204109873,942,Support license header lint,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-04-14T06:51:02Z,2022-04-14T15:55:37Z,"## Feature Request
Pegasus is an Apache incubator project, all files should have a license haeder, it's needed to add a tool to do this work automically.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/942/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/945,https://api.github.com/repos/apache/incubator-pegasus/issues/945,incubator-pegasus,1205204727,945,Merge sub-projects to Pegasus main repository,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-04-15T03:06:51Z,2022-06-29T16:16:09Z,"Steps:
1. sub-project: move main source files to a sub-directory, except files like: `.github`, `.gitignore`, `.goreleaser.yml`, and etc. Some related paths may be updated, make sure all CIs passed, then create a pull request and wait it been merged.
2. pegasus-project: in a pegasus repository local directory, merge sub-project:
```
git remote add <sub-project-name> <sub-project-repo-url>
git fetch <sub-project-name>
git checkout -b <sub-project-branch-name> -t <sub-project-name>/<source-branch-in-sub-project>
git checkout master  # suppose master is your branch which track the lastest remote Pegasus repository master branch.
git pull
git checkout -b merge-<sub-project>
git merge <sub-project-branch-name> --allow-unrelated-histories
// Resolve merge conflicts if any.
git commit -m ""xxx""  // Suggest to naming the commit title like ""merge pegasus-kv/admin-cli repo""
```
3. pegasus-project: create a pull request, and wait for the CIs statuses. If any CI failed, try to fix it, and use a **new commit**, do not merge it with any previous commits. Of course, if you fix CI with multiple commits, better to merge them (but always not merge them with the commit in step 2). It would be nice for reviewers.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/945/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/945,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5DzHgG,incubator-pegasus,1137473542,945,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-05-25T15:59:32Z,2022-05-25T15:59:32Z,"There are some TODO works we have to do, see https://github.com/apache/incubator-pegasus/pull/980#issuecomment-1137438280","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5DzHgG/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/945,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5EXb10,incubator-pegasus,1146994036,945,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-06-06T03:22:27Z,2022-06-06T03:22:27Z,"# How to merge XiaoMi/rDSN
## Background

rDSN (Robust Distributed System Nucleus), is a framework for quickly
building robust distributed systems, you can see more details about
rDSN on its official site on GitHub[1].
Apache Pegasus uses rDSN as an underlying dependent layer since the
Pegasus project started in 2015 at XiaoMi, we forked the repository
and contributed to the forked repository in Pegasus usage scenarios,
including modifying files, adding files, and deleting files.
Since the forked XiaoMi rDSN repository is so different from the
original Microsoft repository, it's very hard to pick these changes
back, and the original repository is not active for years, it seems
nobody maintains it since 2019. Furthermore, the XiaoMi rDSN
repository is specially modified and only used for Apache Pegasus, to
reduce the maintenance cost of Pegasus, we are planning to merge
XiaoMi rDSN to Apache Pegasus repository for years.

## Roadmap
There are several steps we are going to take, and I will list the
indefinite points we can discuss.
@hycdong has listed the license problems we have to resolve[2], now it's
a chance to do it.

- [x] Create a pull request to add license checker in XiaoMi rDSN repository on GitHub
  - [x] It will use apache/skywalking-eyes [3] github action
  - [x] It will check the files lack of a specified license, here we use it to check the MIT License (the license of rdsb project)
  - [x] Then it will give a report with a file list which lack of MIT license
- [x] Add/Update license header for the files reported in step 1 in the rDSN project. There are some cases:
  - [x] The files lack any license header(e.g. [4]):
    - [x] we will add MIT license header for them
  - [x] The files have the MIT license header, and copyright is Microsoft (e.g. [5]):
      - [x] we will not modify these license
      - [x] remove duplicate Apache license for the files which have MIT license already
  - [x] The files has APLv2 license header, and copyright is XiaoMi (the company who donate Pegasus project to ASF, e.g. [6])
      - [x] we will add rule for the checker action to ignore these files
      - [x] we will remove the copyright and simplified Apache license header
      - [x] then add standard APLv2 header for them
  - [x] There are a few files have other type of license or copyright, we will not modify these licenses:
      - [x] APLv2 License, copyright Facebook
      - [x] zlib License, copyright Jeff Preshing
      - [x] BSD-style license, copyright The Chromium Authors
      - [x] APLv2 License, copyright The Abseil Authors
      - [x] BSD-style license, copyright Rob Jansen
      - [x] MIT license, copyright Guillaume Papin
- [x] Move source files to rdsn sub-directory
  - [x] Make sure all license problems are resolved, and there is no ongoing development work on XiaoMi rDSN repository.
  - [x] Make sure all actions are working well.
- [x] Add rdsn as subdirectory instead of submodule in the Pegasus project
  - [x] Remove rdsn submodule firstly
  - [x] Merge rdsn project to Pegasus
  - [x] Reorganize file names and job names in the combined .github/workflows directory
  - [x] Make sure all actions are working well
  - [x] Add more licenses to LICENSE file in Pegasus
  - [x] Update related URLs like https://github.com/XiaoMi/rdsn.git
- [ ] Reorganize the subdirectories, and also related CMakeList.txt files.


Links:
1. https://github.com/microsoft/rDSN
2. https://github.com/apache/incubator-pegasus/issues/823
3. https://github.com/apache/skywalking-eyes
4. https://github.com/XiaoMi/rdsn/blob/608c3cef86b8789ef728781470bff45b9bad3a09/include/dsn/utility/crc.h
5. https://github.com/XiaoMi/rdsn/blob/608c3cef86b8789ef728781470bff45b9bad3a09/include/dsn/service_api_cpp.h
6. https://github.com/XiaoMi/rdsn/blob/608c3cef86b8789ef728781470bff45b9bad3a09/src/aio/aio_task.cpp
7. https://github.com/XiaoMi/rdsn/blob/608c3cef86b8789ef728781470bff45b9bad3a09/include/dsn/utility/TokenBucket.h
8. https://github.com/XiaoMi/rdsn/blob/608c3cef86b8789ef728781470bff45b9bad3a09/include/dsn/utility/hpc_locks/autoreseteventcondvar.h
9. https://github.com/XiaoMi/rdsn/blob/608c3cef86b8789ef728781470bff45b9bad3a09/src/runtime/build_config.h
10. https://github.com/XiaoMi/rdsn/blob/608c3cef86b8789ef728781470bff45b9bad3a09/include/dsn/utility/absl/base/internal/invoke.h
11. https://github.com/XiaoMi/rdsn/blob/608c3cef86b8789ef728781470bff45b9bad3a09/bin/FindRT.cmake
12. https://github.com/XiaoMi/rdsn/blob/608c3cef86b8789ef728781470bff45b9bad3a09/scripts/linux/run-clang-format.py
13. https://github.com/apache/incubator-pegasus/issues/945","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5EXb10/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/945,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Fv7_2,incubator-pegasus,1170194422,945,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-06-29T16:16:08Z,2022-06-29T16:16:08Z,"There is a task left which aim to reorganize the subdirectories, and also related CMakeList.txt files, we will do it in another issue.
Close this one.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Fv7_2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/948,https://api.github.com/repos/apache/incubator-pegasus/issues/948,incubator-pegasus,1206784327,948,Use apache/pegasus instead of apachepegasus DockerHub,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-04-18T07:52:11Z,2022-04-24T03:56:10Z,"apache/pegasus is under ASF official organization, it's a billed plan which can provide more image pulls per day.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/948/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/951,https://api.github.com/repos/apache/incubator-pegasus/issues/951,incubator-pegasus,1207672929,951,Feature(new_metrics): make the counter increment monotonically,empiredan,743379,Dan Wang,,CLOSED,2022-04-19T03:35:01Z,2022-04-19T13:54:59Z,"In https://github.com/apache/incubator-pegasus/issues/931 we've implemented the counter that can both increase and decrease. However, in Prometheus a counter can only increase monotonically. If the counter decreases,  functions of Prometheus such as rate() and increase() cannot be used to compute QPS and ""recent"" count.

In consideration of the problem, this kind of counter is moved to the gauge. Therefore, the gauge is added with increment() and decrement() functions, which can only be used for integral types.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/951/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/952,https://api.github.com/repos/apache/incubator-pegasus/issues/952,incubator-pegasus,1207769790,952,Use action instead of App to check PR title,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-04-19T05:34:34Z,2022-04-20T05:13:38Z,"GitHub App is out of control sometimes, and GitHub actions can give us more convenient ways, we can use the later instead.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/952/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/954,https://api.github.com/repos/apache/incubator-pegasus/issues/954,incubator-pegasus,1209173742,954,Fix action yaml files syntax error,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-04-20T05:16:17Z,2022-04-24T03:55:56Z,"paths and paths-ignore used an error syntax in actions yaml files.
ref: https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#onpushpull_requestpull_request_targetpathspaths-ignore","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/954/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/959,https://api.github.com/repos/apache/incubator-pegasus/issues/959,incubator-pegasus,1215644594,959,Bug: fix BuildPegasusRegularly job,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2022-04-26T09:09:24Z,2022-04-26T15:10:55Z,"Improve the quality of code to compile by clang

job failed url: https://github.com/apache/incubator-pegasus/runs/6144099917?check_suite_focus=true

job description yaml: https://github.com/apache/incubator-pegasus/blob/master/.github/workflows/pegasus-regular-build.yml
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/959/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/961,https://api.github.com/repos/apache/incubator-pegasus/issues/961,incubator-pegasus,1220235116,961,java-client: function signature should not change for compatibility reason,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-04-29T08:34:57Z,2022-05-06T04:25:00Z,"In https://github.com/XiaoMi/pegasus-java-client/pull/177, we updated the function in src/main/java/com/xiaomi/infra/pegasus/client/HashKeyData.java from
```
public HashKeyData(byte[] hashKey, List<Pair<byte[], byte[]>> values)
```
to 
```
public HashKeyData(boolean allFetched, byte[] hashKey, List<Pair<byte[], byte[]>> values)
```
which will cause compatibility problem.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/961/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/962,https://api.github.com/repos/apache/incubator-pegasus/issues/962,incubator-pegasus,1220484039,962,Use admin-cli and pegic to replace origin c++ shell,foreverneverer,23136769,Jiashuo,js982986555@live.com,OPEN,2022-04-29T10:39:36Z,2022-07-24T14:53:37Z,"# Related-Doc

[Google：PegasusShell](https://docs.google.com/document/d/1cirCvWFstdwjD4ZMNUZSCgY--s1tBoMNDgR1gvtyK9M/edit?usp=sharing)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/962/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/962,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Cyobt,incubator-pegasus,1120569069,962,NA,neverchanje,6970676,Tao Wu,wutao.as.neverchanje@gmail.com,NA,2022-05-09T02:25:51Z,2022-05-09T02:25:51Z,Please change the privilege settings of the doc and allow it to be publicly readable. :),"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Cyobt/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/962,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Cyoow,incubator-pegasus,1120569904,962,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2022-05-09T02:27:41Z,2022-05-09T02:27:41Z,"> Please change the privilege settings of the doc and allow it to be publicly readable. :)

try again?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Cyoow/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/962,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5CyzhF,incubator-pegasus,1120614469,962,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-05-09T04:02:41Z,2022-05-09T04:02:41Z,"Copied to github:

<meta charset=""utf-8""><b style=""font-weight:normal;"" id=""docs-internal-guid-df7f3f23-7fff-8d3c-1c3b-acf62d836802""><div dir=""ltr"" style=""margin-left:0pt;"" align=""left"">

命令 | C++ shell | AdminShell(AdminCli) | UserShell(Pegic) | 备注
-- | -- | -- | -- | --
帮助 | help | help | help |   |   |   |  
版本 | version | version | - |   |   |   |  
退出 | exit | exit | - |   |   |   |  
  |   |   |   |   |   |   |  
更换连接集群 | cc | 不再支持 | 不再支持 |   |   |   |  
指定表 | use | use | use |   |   |   |  
指定超时时间 | timeout | 不再支持 | 不再支持 |   |   |   |  
  |   |   |   |   |   |   |  
集群信息 | cluster_info | cluster-info | - |   |   |   |  
节点信息 | nodes | nodes | - |   |   |   |  
服务器信息 | server_info | server-info | - |   |   |   |  
服务器指标 | server_stat | node-stat | - |   |   |   |  
远程命令 | remote_command | remote-command |   |   |   |   |  
  |   |   |   |   |   |   |  
列出所有表 | ls | ls |   |   |   |   |  
表分片信息 | app | table-partition |   |   |   |   |  
表指标信息 | app_stat | table-stat |   |   |   |   |  
强制刷LOG | flush_log | 待讨论 |   |   |   |   |  
创建表 | create | create |   |   |   |   |  
删除表 | drop | drop |   |   |   |   |  
恢复表 | recall | recall |   |   |   |   |  
获取环境变量 | get_app_env | table-env |   |   |   |   |  
设置环境变量 | set_app_env |   |   |   |   |  
删除环境变量 | del_app_env |   |   |   |   |  
清空环境变量 | clear_app_env |   |   |   |   |  
  |   |   |   |   |   |   |  
设置集群状态 | set_meta_level | meta-level |   |   |   |   |  
查看表状态 | get_meta_level |   |   |   |   |  
分片变更 | propose | 待支持 |   |   |   |   |  
分片迁移 | balance | 待支持 |   |   |   |   |  
  |   |   |   |   |   |   |  
数据恢复 | recover | 待支持 |   |   |   |   |  
分片诊断 | ddd_diagnose | 待支持 |   |   |   |   |  
  |   |   |   |   |   |   |  
冷备份 | add_backup_policy | 删除policy概念，仅支持一次性冷备份：backup |   |   |   |   |  
ls_backup_policy |   |   |   |   |  
modify_backup_policy |   |   |   |   |  
disable_backup_policy |   |   |   |   |  
enable_backup_policy |   |   |   |   |  
query_backup_policy |   |   |   |   |  
  |   |   |   |   |   |   |  
备份恢复恢复进度 | restore_app | restore |   |   |   |   |  
query_restore_status | 待支持 |   |   |   |   |  
  |   |   |   |   |   |   |  
sst文件dump | sst_dump | 待支持 |   |   |   |   |  
wal文件dump | mlog_dump | 待支持 |   |   |   |   |  
  |   |   |   |   |   |   |  
磁盘分片 | 不支持 | disk_replica |   |   |   |   |  
磁盘容量 | disk_capacity |   |   |   |   |  
磁盘迁移 | disk_migrate |   |   |   |   |  
磁盘均衡 | disk_balance |   |   |   |   |  
  |   |   |   |   |   |   |  
节点替换 | 不支持 | node_migrate |   |   |   |   |  
节点均衡 | 不支持 | 在开发 |   |   |   |   |  
  |   |   |   |   |   |   |  
数据读写 | get |   | get |   |   |   |  
  | set |   | set |   |   |   |  
  | del |   | del |   |   |   |  
  | multi_get |   | 待讨论 |   |   |   |  
  | multi_set |   | 待讨论 |   |   |   |  
  | multi_del |   | 待讨论 |   |   |   |  
  | hash_scan |   | scan |   |   |   |  
  | full_scan |   | 待讨论 |   |   |   |  
  | copy_data |   | 待支持 |   |   |   |  
  | clear_data |   | 重新实现 |   |   |   |  
  | cout_data |   | 待支持 |   |   |   |  
  |   |   |   |   |   |   |  
  |   |   |   |   |   |   |  

</div></b>","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5CyzhF/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/972,https://api.github.com/repos/apache/incubator-pegasus/issues/972,incubator-pegasus,1239362981,972,feature: add a app drop interface to the java client,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2022-05-18T02:50:36Z,2022-05-23T14:14:20Z,"[what i need?]
As Well as create app (https://github.com/XiaoMi/pegasus-java-client/issues/179), we also need drop app.

[what i will do]
i will add dropApp interface to the java client.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/972/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/974,https://api.github.com/repos/apache/incubator-pegasus/issues/974,incubator-pegasus,1239556020,974,Feature(new_metrics): support to find multiple nth elements of a sequence container at a time,empiredan,743379,Dan Wang,,CLOSED,2022-05-18T07:38:06Z,2022-07-12T12:37:59Z,"# 1. Motivation
Actually [`perf_counter_number_percentile_atomic`](https://github.com/XiaoMi/rdsn/blob/master/src/perf_counter/perf_counter_atomic.h#L185) has implemented finding multiple nth elements though independent interfaces are not provided.

The algorithm used by `perf_counter_number_percentile_atomic` to compute nth smallest element is known as
[median of medians](https://en.wikipedia.org/wiki/Median_of_medians). This algorithm is effective, however, `perf_counter_number_percentile_atomic` has some problems:

- The code is not readable, and there is not even a line of comment or any document to explain;
- Finding multiple nth elements is coupled with percentile computation;
- There is no unit test for checking correctness;
- There is no benchmark for checking performance;
- It will consume more memory, while running with lower performance since it has spent more time in allocating memory for big arrays and copying bytes for them.

# 2. Implementation
Based on the problems that `perf_counter_number_percentile_atomic` has, initially I've still tried to adopt median of medians and improve the code of `perf_counter_number_percentile_atomic`. However, after improvement the performance (execution time) increased only %5 ~ 10%.

Then, I've turned to a new solution which uses nth_element() of C++ STL to support to find multiple nth elements of a sequence container at a time, since there tends to be multiple percentiles for a metric, such as P50, P75, P90, P95, P99, P999, etc..

As a result, this new solution has better performance than `perf_counter_number_percentile_atomic`, and gets more readable. The detailed comparison is listed in next section. 

# 3. Comparison
Based on the problems described for `perf_counter_number_percentile_atomic`, we can compare it with the new method in the following table:

|   | Solution based on `perf_counter_number_percentile_atomic` | Solution based on nth_element() |
| :-------------: | :-------------: | :-------------: |
| Readability | Unreadable | Readable |
| Document | Not provided | Provided |
| Coupling | Coupled (nth element and percentile) | Decoupled |
| Unit tests  | Not provided | Provided |
| Benchmark  | Not provided | Provided (and compared with `perf_counter_number_percentile_atomic`) |
| Memory consumption | higher | lower |
| Performance | lower | higher |
# 4. Benchmark
## 4.1 Hardware configurations
``` 
cpu: Intel Xeon Processor (Cascadelake)
cores: 4
memory: 32GB
```

## 4.2 Software configurations
```
architecture: x86_64
gcc version: 7.3.1 20180303 (Red Hat 7.3.1-5)
```

## 4.3 Performance
In `perf_counter_number_percentile_atomic` the size of sampled window is 5000 (MAX_QUEUE_LENGTH), thus this size is also adopted for the benchmark. And an operation is defined as a computation to find P50, P90, P95, P99 and P999 over the sampled window.

The dataset for the benchmark is generated randomly with range size 5 (see the code of bench for details). The following table lists the comparison between the execution time of both solutions:

| Number of operations (windows size: 5000)  | Solution based on `perf_counter_number_percentile_atomic` | Solution based on nth_element() |
| :-------------: | :-------------: | :-------------: |
| 1 | 0.728 ms | 0.384 ms |
| 10 |  7.127 ms | 3.256 ms |
| 100 |  72.030 ms |  33.378 ms |
| 1000 |  0.725 s |  0.331 s |
| 10000 | 7.185 s |  3.281 s |
| 100000 | 72.358 s |  33.085 s |","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/974/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/985,https://api.github.com/repos/apache/incubator-pegasus/issues/985,incubator-pegasus,1251839824,985,collector unexpected crash,felixdae,321802,Zhangqiu Yu,felixdae@gmail.com,CLOSED,2022-05-29T09:40:54Z,2022-05-29T15:31:35Z,"当某张表 partition 为 1 时，可能导致 stat_histories_analyse 提前 return，进而 hot_points 和 _hot_points size 不一致，使得后续的 assert fail

```
D2022-05-28 06:29:48.34 (1653719388034076919 383) collector.default1.0101000000000001: compiler_depend.ts:108:stat_histories_analyse(): _partitions_stat_histories size <= 1, not enough data for calculation
F2022-05-28 06:29:48.34 (1653719388034089894 383) collector.default1.0101000000000001: compiler_depend.ts:135:update_hot_point(): assertion expression: _hot_points.size() == hot_points.size()
F2022-05-28 06:29:48.34 (1653719388034125236 383) collector.default1.0101000000000001: compiler_depend.ts:135:update_hot_point(): 1 vs 0
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/985/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/986,https://api.github.com/repos/apache/incubator-pegasus/issues/986,incubator-pegasus,1252096951,986,one doc of admin-cli is misleading,felixdae,321802,Zhangqiu Yu,felixdae@gmail.com,CLOSED,2022-05-30T02:19:31Z,2022-06-01T03:08:22Z,"```
Pegasus-AdminCli-1.2.0 » help create

Create a Pegasus table.

Please pay attention to the partition number. It usually depends on the table's on-disk storage size.
To achieve an predictable performance, you should keep the average partition size within a acceptable
range.

Usage:
  create <table> [-p|--partitions <NUM>] [-r|--replica <NUM>]

Args:
  table  string    the table name

Flags:
  -h, --help              display help
  -p, --partitions int    the number of partitions (default: 4)
  -r, --replica    int    the number of replicas (default: 3)
```

by following the usage, you just can't create a table successfully

```
Pegasus-AdminCli-1.2.0 » create test_create -p 4 -r 3
error: invalid usage of command 'create' (unconsumed input '-p 4 -r 3'), try 'help'
```

as a matter of fact, you can create tables as below, so i think the `usage` doc is wrong. it should be `create [-p|--partitions <NUM>] [-r|--replica <NUM>] <table>`

```
Pegasus-AdminCli-1.2.0 » create -p 4 -r 3 test_create1
Creating table ""test_create1"" (AppID: 3)
Available partitions:
4 / 4 [-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------] 100.00% 5 p/s 1s
Done!
Pegasus-AdminCli-1.2.0 » create -p 4 test_create2
Creating table ""test_create2"" (AppID: 4)
Available partitions:
4 / 4 [----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------] 100.00% 0 p/s 11s
Done!
Pegasus-AdminCli-1.2.0 » create test_create3
Creating table ""test_create3"" (AppID: 5)
Available partitions:
4 / 4 [----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------] 100.00% 0 p/s 13s
Done!
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/986/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/988,https://api.github.com/repos/apache/incubator-pegasus/issues/988,incubator-pegasus,1252840684,988,GitHub action uses checkstyle to replace plugins for code style checking during CI,WHBANG,38547944,,,CLOSED,2022-05-30T14:48:02Z,2022-06-21T07:22:08Z,"Benefits: 
1.  More standardized 
2.  Less dependencies","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/988/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/988,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5FOO2H,incubator-pegasus,1161358727,988,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-06-21T07:22:08Z,2022-06-21T07:22:08Z,not needed now.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5FOO2H/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/990,https://api.github.com/repos/apache/incubator-pegasus/issues/990,incubator-pegasus,1255276170,990,WaitForFlushMemTables finishing before result is committed to Manifest,daggarwal387,36129071,,,CLOSED,2022-06-01T07:55:01Z,2022-06-01T07:59:22Z,"While using the ExportColumnFamily API I observed that the result is missing the newly flushed memtable. Upon further investigation, I found that WaitForFlushMemTables is finishing before the flush has actually committed the result to the Manifest. Following are the logs for the column family that experienced it(there are some additional logs added for debugging).
`2022/05/31-16:28:39.080669 4f8a700 [ties/checkpoint/checkpoint_impl.cc:357] [0_0] export column family onto export directory /tmp/bringup/0/disks/0/scribe/newscribe/bucket_0/db_sender_sst_file
2022/05/31-16:28:39.080729 4f8a700 [_impl/db_impl_compaction_flush.cc:1310] [0_0] Manual flush start.
2022/05/31-16:28:39.095206 4f8a700 [_impl/db_impl_write.cc:1614] [0_0] New memtable created with log file: #113. Immutable memtables: 0.
2022/05/31-16:28:39.095227 4f8a700 [_impl/db_impl_compaction_flush.cc:1596] [0_0] Flush request queued.
2022/05/31-16:28:39.112425 7f5694e41700 (Original Log Time 2022/05/31-16:28:39.095402) [_impl/db_impl_compaction_flush.cc:2208] Calling FlushMemTableToOutputFile with column family [0_0], flush slots available 10, compaction slots available 1, flush slots scheduled 10, compaction slots scheduled 0
2022/05/31-16:28:39.112432 7f5694e41700 [ush_job.cc:322] [0_0] [JOB 320] Flushing memtable with next log file: 113 and mem_table_id: 10
2022/05/31-16:28:39.112450 7f5694e41700 [ush_job.cc:351] [0_0] [JOB 320] Level-0 flush table #114: started
2022/05/31-16:28:39.116592 4d70700 [_impl/db_impl_write.cc:1614] [0_0] New memtable created with log file: #115. Immutable memtables: 1.
2022/05/31-16:28:39.116620 4d70700 [_impl/db_impl_compaction_flush.cc:1596] [0_0] Flush request queued.
2022/05/31-16:28:39.127211 7f5696e45700 (Original Log Time 2022/05/31-16:28:39.116846) [_impl/db_impl_compaction_flush.cc:2208] Calling FlushMemTableToOutputFile with column family [0_0], flush slots available 10, compaction slots available 1, flush slots scheduled 10, compaction slots scheduled 0
2022/05/31-16:28:39.127218 7f5696e45700 [ush_job.cc:322] [0_0] [JOB 331] Flushing memtable with next log file: 115 and mem_table_id: 11
2022/05/31-16:28:39.127238 7f5696e45700 [ush_job.cc:351] [0_0] [JOB 331] Level-0 flush table #116: started
2022/05/31-16:28:39.130535 7f5694e41700 [ush_job.cc:392] [0_0] [JOB 320] Level-0 flush table #114: 235965 bytes OK
2022/05/31-16:28:39.140412 7f5696e45700 [ush_job.cc:392] [0_0] [JOB 331] Level-0 flush table #116: 2489 bytes OK
2022/05/31-16:28:39.154308 7f5696e45700 (Original Log Time 2022/05/31-16:28:39.150687) [_impl/db_impl_compaction_flush.cc:204] [0_0] Level summary: base level 4 level multiplier 8.00 max bytes base 4096000 files[1 0 0 0 1] max score 0.12
2022/05/31-16:28:39.154415 4f8a700 [_impl/db_impl_compaction_flush.cc:1866] [0_0]Flush finished. Num not flushed: 1, earliest_memtable_id_now: 11, mem_table_we_flushed: 10
2022/05/31-16:28:39.154423 4f8a700 [_impl/db_impl_compaction_flush.cc:1320] [0_0] Manual flush finished, status: OK
2022/05/31-16:28:39.154474 4f8a700 [ties/checkpoint/checkpoint_impl.cc:380] [0_0] HardLinking /000100.sst
2022/05/31-16:28:39.154551 4f8a700 [ties/checkpoint/checkpoint_impl.cc:380] [0_0] HardLinking /000090.sst
2022/05/31-16:28:39.166255 7f5694e41700 (Original Log Time 2022/05/31-16:28:39.137073) [mtable_list.cc:383] [0_0] Level-0 commit table #114 has started, mem_id: 10
2022/05/31-16:28:39.166260 7f5694e41700 (Original Log Time 2022/05/31-16:28:39.154210) [mtable_list.cc:439] [0_0] Level-0 commit table #114: memtable #1memtable_id: 10 done
2022/05/31-16:28:39.166263 7f5694e41700 (Original Log Time 2022/05/31-16:28:39.154215) [mtable_list.cc:383] [0_0] Level-0 commit table #116 has started, mem_id: 11
2022/05/31-16:28:39.166265 7f5694e41700 (Original Log Time 2022/05/31-16:28:39.166175) [mtable_list.cc:439] [0_0] Level-0 commit table #116: memtable #1memtable_id: 11 done
2022/05/31-16:28:39.166270 7f5694e41700 (Original Log Time 2022/05/31-16:28:39.166208) [_impl/db_impl_compaction_flush.cc:204] [0_0] Level summary: base level 4 level multiplier 8.00 max bytes base 4096000 files[3 0 0 0 1] max score 0.38
2022/05/31-16:28:39.166273 4f8a700 [ties/checkpoint/checkpoint_impl.cc:435] [0_0] Export succeeded.
2022/05/31-16:28:39.166309 4d70700 [_impl/db_impl_compaction_flush.cc:1866] [0_0]Flush finished. Num not flushed: 0, earliest_memtable_id_now: 18446744073709551615, mem_table_we_flushed: 11
2022/05/31-16:28:39.166321 4d70700 [_impl/db_impl_compaction_flush.cc:1443] [0_0] Manual compaction starting`


So if you look here, the memtable with GetID() as 10 is the one that was selected for manual flush from ExportColumnFamily. During committing the logline ""Level-0 commit table #114 has started, mem_id: 10""  the memtable is at the back of the memlist but a few lines before it ""Flush finished. Num not flushed: 1, earliest_memtable_id_now: 11, mem_table_we_flushed: 10"" the GetEarliestMemTableID gave us 11.

I am using 6.4.6 rocksdb version with some additional logging and attaching the complete LOG file also in case that helps.
[LOG.zip](https://github.com/apache/incubator-pegasus/files/8812140/LOG.zip)
.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/990/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/991,https://api.github.com/repos/apache/incubator-pegasus/issues/991,incubator-pegasus,1255820617,991,Feature(new_metrics): implement the percentile,empiredan,743379,Dan Wang,,CLOSED,2022-06-01T12:05:12Z,2022-06-17T12:41:56Z,"The percentile is a metric type that samples observations as is described in https://github.com/apache/incubator-pegasus/issues/922. The size of samples has an upper bound. Once the maximum size is reached, the earliest observations will be overwritten.

On the other hand, `kth` percentiles, such as P50, P90, P95, P99, P999, will be calculated periodically over all samples. The `kth` percentiles which are calculated are configurable provided that they are of valid `kth_percentile_type` (i.e. in `kAllKthPercentileTypes`).

The most common usage of percentile is latency, such as server-level and replica-level latencies. For example, if P99 latency is 10 ms, it means the latencies of 99% requests are less than 10 ms.

The percentile is implemented by the finder for nth elements. Each `kth` percentile is firstly converted to nth index; then, find the element corresponding to the nth index.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/991/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/992,https://api.github.com/repos/apache/incubator-pegasus/issues/992,incubator-pegasus,1257828539,992,Remove redundant dependencies in java-client,WHBANG,38547944,,,CLOSED,2022-06-02T07:54:57Z,2022-06-21T07:21:41Z,"java-client as an underlying storage service client, or a library used by business parties, java-client should remove unnecessary dependent libraries， Here are some that can be removed.
 1. Do not rely on slf4j-log4j12, only slf4j-api .
 2. Don't rely on netty-all, only the parts you need. netty-all and other netty components can exist at the same time, if the version is different, it is easy to have problems at runtime.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/992/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/994,https://api.github.com/repos/apache/incubator-pegasus/issues/994,incubator-pegasus,1257929205,994,The online environment rocksdb log size is much larger than the actual stored data,WHBANG,38547944,,,CLOSED,2022-06-02T09:26:20Z,2022-06-21T07:21:32Z,"   In the actual online production environment, the pegasus storage service cannot be stopped, so the underlying rocksdb is always running. We use pegasus to find that the disk space is insufficient, but the actual data volume is not very large (about 30G), and most of the disk space is occupied by the logs of rocksdb (500M for each partition. more than 1,000 partition in total, and finally the logs of rocksdb occupy about 500G), so I hope the log file capacity of rocksdb can be adjusted using the configuration.
some screenshots：
![wecom-temp-295466bd5672aa2fe434d3f185df1eda](https://user-images.githubusercontent.com/38547944/171599700-6225ce07-fdc4-4f9b-9464-55107cd9a5f8.png)
![image](https://user-images.githubusercontent.com/38547944/171599756-40663785-abdb-4b42-92e1-f0d47b9ca64d.png)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/994/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/995,https://api.github.com/repos/apache/incubator-pegasus/issues/995,incubator-pegasus,1258015987,995,Feature(update_replication_factor#9): merge max_replica_count_rebased_dev into master branch,empiredan,743379,Dan Wang,,CLOSED,2022-06-02T10:49:25Z,2022-06-06T06:16:36Z,"Since https://github.com/apache/incubator-pegasus/issues/865 (get/set the replication factor of each table) has been finished for [rdsn](https://github.com/XiaoMi/rdsn), its branch [max_replica_count_dev](https://github.com/XiaoMi/rdsn/tree/max_replica_count_dev) should be merged into [master branch](https://github.com/XiaoMi/rdsn/tree/master).

However, [max_replica_count_dev](https://github.com/XiaoMi/rdsn/tree/max_replica_count_dev) has not been synced to [master](https://github.com/XiaoMi/rdsn/tree/master) for a long while, some conflicts are needed to be fixed. Thanks to @acelyc111 , conflicts have been fixed and [max_replica_count_dev](https://github.com/XiaoMi/rdsn/tree/max_replica_count_dev) has been rebased to [max_replica_count_rebased_dev](https://github.com/XiaoMi/rdsn/tree/max_replica_count_rebased_dev).

After fix a clang-format error, now this PR is proposed for merging [max_replica_count_rebased_dev](https://github.com/empiredan/rdsn/tree/max_replica_count_rebased_dev) into [master](https://github.com/XiaoMi/rdsn/tree/master).","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/995/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/998,https://api.github.com/repos/apache/incubator-pegasus/issues/998,incubator-pegasus,1263066507,998,Bug: different judgement to empty hashkey ,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2022-06-07T10:21:14Z,2022-06-14T11:18:15Z,"### What appeared
When I use multi_set from c++ client, I found that hashkey couldn't be empty.  However, My table data have existed a group data (hashkey="""", sortkey="""", value=""""). 

### What problem 
I find that's different judgement to empty hashkey from client interfaces.
It's ""PERR_INVALID_HASH_KEY"" for ""multi"" prefix interfaces when hashkey is empty, but single kv operations is ok. 

### How to solve
I think empty hashkey also should be allowed on ""multi"" prefix interfaces.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/998/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1002,https://api.github.com/repos/apache/incubator-pegasus/issues/1002,incubator-pegasus,1270659755,1002,Fix: drop unused 'this' from lambda capture for unit-tests that verify replication factor,empiredan,743379,Dan Wang,,CLOSED,2022-06-14T11:28:29Z,2022-06-17T12:42:46Z,"Recently [BuildPegasusRegularly](https://github.com/apache/incubator-pegasus/actions/workflows/pegasus-regular-build.yml) has failed for several times, such as [6866926245](https://github.com/apache/incubator-pegasus/runs/6866926245?check_suite_focus=true). The error message is:
```
/root/incubator-pegasus/rdsn/src/meta/test/meta_app_operation_test.cpp:230:19: error: lambda capture 'this' is not used [-Werror,-Wunused-lambda-capture]
                [ this, expected_pid = partition_config.pid, expected_max_replica_count ](
                  ^~~~~~
/root/incubator-pegasus/rdsn/src/meta/test/meta_app_operation_test.cpp:263:14: error: lambda capture 'this' is not used [-Werror,-Wunused-lambda-capture]
            [this, app, expected_max_replica_count](error_code ec, const blob &value) {
             ^~~~~
2 errors generated.
```

The reported unit tests that verify replication factor should be fixed, since `this` in lambda capture is not used.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1002/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1003,https://api.github.com/repos/apache/incubator-pegasus/issues/1003,incubator-pegasus,1271732107,1003,Bug: copy_data error use multi_set when hashkey is empty,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2022-06-15T06:27:58Z,2022-07-14T08:52:59Z,"### What appeared
Copy data not supported empty hash key when use multi set on shell tools.
Empty hash key will be assign to different partition, so we should deal with it in addition.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1003/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1010,https://api.github.com/repos/apache/incubator-pegasus/issues/1010,incubator-pegasus,1275384405,1010,Feature(new_metrics): merge prometheus-dev into master branch,empiredan,743379,Dan Wang,,CLOSED,2022-06-17T19:24:14Z,2022-06-20T07:26:12Z,"This issue is to merge [prometheus-dev](https://github.com/XiaoMi/rdsn/tree/prometheus-dev) into [master](https://github.com/XiaoMi/rdsn/tree/master).

[prometheus-master-rebased-dev](https://github.com/empiredan/rdsn/tree/prometheus-master-rebased-dev) is firstly created based on [prometheus-dev](https://github.com/XiaoMi/rdsn/tree/prometheus-dev). Then, it is rebased onto [master](https://github.com/XiaoMi/rdsn/tree/master), with some conflicts to be resolved.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1010/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1013,https://api.github.com/repos/apache/incubator-pegasus/issues/1013,incubator-pegasus,1279885307,1013,Bug: pegasus server cann't recover by self after disk full ,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,OPEN,2022-06-22T09:59:21Z,2022-07-26T01:01:23Z,"On 1.12 version:
The disk full, and error occur after expand it.
<img width=""1076"" alt=""image"" src=""https://user-images.githubusercontent.com/48315319/175255985-4dbe055f-24b5-4981-b772-8f7ebdbd0b50.png"">


disk full node:
`on_config_proposal(): x.y@ip:port: on_config_proposal out-dated, 20 vs 21`

others:
`x.y@ip:port: replica not exists on meta server, wait to close.`

There are so much replicas cann't assign primary beacase of above reason.

I recover it by rebuild meta server data on zk. How I fix it on program?
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1013/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1015,https://api.github.com/repos/apache/incubator-pegasus/issues/1015,incubator-pegasus,1282299562,1015,thirdparty build failed on centOS7,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-06-23T12:08:27Z,2022-06-29T16:14:23Z,"https://github.com/apache/incubator-pegasus/runs/7018745976?check_suite_focus=true
https://github.com/apache/incubator-pegasus/runs/7018746726?check_suite_focus=true","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1015/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1018,https://api.github.com/repos/apache/incubator-pegasus/issues/1018,incubator-pegasus,1283340977,1018,Feature: PegasusScanner adds hasNext method,WHBANG,38547944,,,CLOSED,2022-06-24T06:55:15Z,2022-06-29T16:13:56Z,"scan is a common interface for storage components, we hope PegasusScanner has two methods, next() and hasNext() like most iterators.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1018/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1020,https://api.github.com/repos/apache/incubator-pegasus/issues/1020,incubator-pegasus,1286820508,1020,Fix: drop unused task codes of AIO  ,empiredan,743379,Dan Wang,,CLOSED,2022-06-28T06:03:18Z,2022-06-28T07:39:54Z,"We can drop the task codes of AIO that have not been used, as is listed below:
```
LPC_ASYNC_READ_COMPLETE
LPC_ASYNC_WRITE_COMPLETE
LPC_LERARN_REMOTE_DISK_STATE
LPC_REPLICA_COPY_LAST_CHECKPOINT_DONE
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1020/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1024,https://api.github.com/repos/apache/incubator-pegasus/issues/1024,incubator-pegasus,1288956887,1024,CI workflows often failed for some flaky tests,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-06-29T16:11:56Z,2022-07-11T06:37:10Z,"1. ASAN failed test
```
[ RUN      ] bulk_load_failover_test.app_info_inconsistency
=================================================================
==9760==ERROR: AddressSanitizer: heap-use-after-free on address 0x602000c838b0 at pc 0x55d6c185e647 bp 0x1478bd5831b0 sp 0x1478bd5831a0
READ of size 8 at 0x602000c838b0 thread T25 (test_meta.THREA)
    #0 0x55d6c185e646 in std::__detail::_Node_iterator_base<int, false>::_M_incr() /usr/include/c++/7/bits/hashtable_policy.h:314
    #1 0x55d6c185e646 in std::__detail::_Node_iterator<int, true, false>::operator++() /usr/include/c++/7/bits/hashtable_policy.h:369
    #2 0x55d6c185e646 in dsn::replication::bulk_load_service::try_to_continue_bulk_load() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/meta/meta_bulk_load_service.cpp:1704
    #3 0x55d6c185ea96 in dsn::replication::bulk_load_service::initialize_bulk_load_service() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/meta/meta_bulk_load_service.cpp:64
    #4 0x55d6c1389191 in dsn::replication::bulk_load_failover_test_app_info_inconsistency_Test::TestBody() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/meta/test/meta_bulk_load_service_test.cpp:1315
    #5 0x55d6c242a851 in void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/__w/incubator-pegasus/incubator-pegasus/rdsn/builder/src/meta/test/dsn.meta.test+0x202a851)
    #6 0x55d6c2424e86 in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/__w/incubator-pegasus/incubator-pegasus/rdsn/builder/src/meta/test/dsn.meta.test+0x2024e86)
    #7 0x55d6c2409095 in testing::Test::Run() (/__w/incubator-pegasus/incubator-pegasus/rdsn/builder/src/meta/test/dsn.meta.test+0x2009095)
    #8 0x55d6c24099bd in testing::TestInfo::Run() (/__w/incubator-pegasus/incubator-pegasus/rdsn/builder/src/meta/test/dsn.meta.test+0x20099bd)
    #9 0x55d6c240a035 in testing::TestCase::Run() (/__w/incubator-pegasus/incubator-pegasus/rdsn/builder/src/meta/test/dsn.meta.test+0x200a035)
    #10 0x55d6c2410ed9 in testing::internal::UnitTestImpl::RunAllTests() (/__w/incubator-pegasus/incubator-pegasus/rdsn/builder/src/meta/test/dsn.meta.test+0x2010ed9)
    #11 0x55d6c242b968 in bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/__w/incubator-pegasus/incubator-pegasus/rdsn/builder/src/meta/test/dsn.meta.test+0x202b968)
    #12 0x55d6c2425c54 in bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/__w/incubator-pegasus/incubator-pegasus/rdsn/builder/src/meta/test/dsn.meta.test+0x2025c54)
    #13 0x55d6c240fabf in testing::UnitTest::Run() (/__w/incubator-pegasus/incubator-pegasus/rdsn/builder/src/meta/test/dsn.meta.test+0x200fabf)
    #14 0x55d6c12a4537 in RUN_ALL_TESTS() /__w/incubator-pegasus/incubator-pegasus/rdsn/thirdparty/output/include/gtest/gtest.h:2233
    #15 0x55d6c12a4537 in dsn::replication::meta_service_test_app::start(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) /__w/incubator-pegasus/incubator-pegasus/rdsn/src/meta/test/main.cpp:86
    #16 0x55d6c208af14 in dsn::service_node::start_app() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/service_engine.cpp:74
    #17 0x55d6c20b6fe3 in dsn::service_control_task::exec() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/tool_api.cpp:60
    #18 0x55d6c21a824c in dsn::task::exec_internal() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/task/task.cpp:176
    #19 0x55d6c220aad5 in dsn::task_worker::loop() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/task/task_worker.cpp:224
    #20 0x55d6c220b9ac in dsn::task_worker::run_internal() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/task/task_worker.cpp:204
    #21 0x55d6c220bf43 in void std::__invoke_impl<void, void (dsn::task_worker::*&)(), dsn::task_worker*&>(std::__invoke_memfun_deref, void (dsn::task_worker::*&)(), dsn::task_worker*&) /usr/include/c++/7/bits/invoke.h:73
    #22 0x55d6c220bf43 in std::__invoke_result<void (dsn::task_worker::*&)(), dsn::task_worker*&>::type std::__invoke<void (dsn::task_worker::*&)(), dsn::task_worker*&>(void (dsn::task_worker::*&)(), dsn::task_worker*&) /usr/include/c++/7/bits/invoke.h:95
    #23 0x55d6c220bf43 in void std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>::__call<void, , 0ul>(std::tuple<>&&, std::_Index_tuple<0ul>) /usr/include/c++/7/functional:467
    #24 0x55d6c220bf43 in void std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>::operator()<, void>() /usr/include/c++/7/functional:551
    #25 0x55d6c220bf43 in void std::__invoke_impl<void, std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>>(std::__invoke_other, std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>&&) /usr/include/c++/7/bits/invoke.h:60
    #26 0x55d6c220bf43 in std::__invoke_result<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>>::type std::__invoke<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>>(std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>&&) /usr/include/c++/7/bits/invoke.h:95
    #27 0x55d6c220bf43 in decltype (__invoke((_S_declval<0ul>)())) std::thread::_Invoker<std::tuple<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()> > >::_M_invoke<0ul>(std::_Index_tuple<0ul>) /usr/include/c++/7/thread:234
    #28 0x55d6c220bf43 in std::thread::_Invoker<std::tuple<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()> > >::operator()() /usr/include/c++/7/thread:243
    #29 0x55d6c220bf43 in std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()> > > >::_M_run() /usr/include/c++/7/thread:186
    #30 0x1478c6f9d6de  (/usr/lib/x86_64-linux-gnu/libstdc++.so.6+0xbd6de)
    #31 0x1478c7d556da in start_thread (/lib/x86_64-linux-gnu/libpthread.so.0+0x76da)
    #32 0x1478c665a61e in __clone (/lib/x86_64-linux-gnu/libc.so.6+0x12161e)
0x602000c838b0 is located 0 bytes inside of 16-byte region [0x602000c838b0,0x602000c838c0)
freed by thread T33 (test_meta.THREA) here:
    #0 0x1478c95c32c0 in operator delete(void*) (/usr/lib/x86_64-linux-gnu/libasan.so.4+0xe12c0)
    #1 0x55d6c1875481 in __gnu_cxx::new_allocator<std::__detail::_Hash_node<int, false> >::deallocate(std::__detail::_Hash_node<int, false>*, unsigned long) /usr/include/c++/7/ext/new_allocator.h:125
    #2 0x55d6c1875481 in std::allocator_traits<std::allocator<std::__detail::_Hash_node<int, false> > >::deallocate(std::allocator<std::__detail::_Hash_node<int, false> >&, std::__detail::_Hash_node<int, false>*, unsigned long) /usr/include/c++/7/bits/alloc_traits.h:462
    #3 0x55d6c1875481 in std::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<int, false> > >::_M_deallocate_node(std::__detail::_Hash_node<int, false>*) /usr/include/c++/7/bits/hashtable_policy.h:2086
    #4 0x55d6c1875481 in std::_Hashtable<int, int, std::allocator<int>, std::__detail::_Identity, std::equal_to<int>, std::hash<int>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, true, true> >::_M_erase(unsigned long, std::__detail::_Hash_node_base*, std::__detail::_Hash_node<int, false>*) /usr/include/c++/7/bits/hashtable.h:1890
    #5 0x55d6c1875481 in std::_Hashtable<int, int, std::allocator<int>, std::__detail::_Identity, std::equal_to<int>, std::hash<int>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, true, true> >::_M_erase(std::integral_constant<bool, true>, int const&) /usr/include/c++/7/bits/hashtable.h:1916
    #6 0x55d6c1875481 in std::_Hashtable<int, int, std::allocator<int>, std::__detail::_Identity, std::equal_to<int>, std::hash<int>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, true, true> >::erase(int const&) /usr/include/c++/7/bits/hashtable.h:759
    #7 0x55d6c1875481 in std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> >::erase(int const&) /usr/include/c++/7/bits/unordered_set.h:544
    #8 0x55d6c1875481 in dsn::replication::bulk_load_service::reset_local_bulk_load_states_unlocked(int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool) /__w/incubator-pegasus/incubator-pegasus/rdsn/src/meta/meta_bulk_load_service.cpp:1404
    #9 0x55d6c1886b60 in dsn::replication::bulk_load_service::reset_local_bulk_load_states(int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool) /__w/incubator-pegasus/incubator-pegasus/rdsn/src/meta/meta_bulk_load_service.cpp:1424
    #10 0x55d6c1889f9a in operator() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/meta/meta_bulk_load_service.cpp:1372
    #11 0x55d6c1889f9a in _M_invoke /usr/include/c++/7/bits/std_function.h:316
    #12 0x55d6c1b14fea in std::function<void (dsn::error_code)>::operator()(dsn::error_code) const /usr/include/c++/7/bits/std_function.h:706
    #13 0x55d6c1b14fea in decltype (((declval<std::function<void (dsn::error_code)>&>)())((declval<dsn::error_code>)())) dsn::absl::base_internal::Callable::Invoke<std::function<void (dsn::error_code)>&, dsn::error_code>(std::function<void (dsn::error_code)>&, dsn::error_code&&) /__w/incubator-pegasus/incubator-pegasus/rdsn/include/dsn/utility/absl/base/internal/invoke.h:180
    #14 0x55d6c1b14fea in decltype (dsn::absl::base_internal::Invoker<std::function<void (dsn::error_code)>&, dsn::error_code>::type::Invoke((declval<std::function<void (dsn::error_code)>&>)(), (declval<dsn::error_code>)())) dsn::absl::base_internal::Invoke<std::function<void (dsn::error_code)>&, dsn::error_code>(std::function<void (dsn::error_code)>&, dsn::error_code&&) /__w/incubator-pegasus/incubator-pegasus/rdsn/include/dsn/utility/absl/base/internal/invoke.h:212
    #15 0x55d6c1b14fea in decltype (Invoke((forward<std::function<void (dsn::error_code)>&>)({parm#1}), (get<0ul>)((forward<std::tuple<dsn::error_code> >)({parm#2})))) dsn::absl::utility_internal::apply_helper<std::function<void (dsn::error_code)>&, std::tuple<dsn::error_code>, 0ul>(std::function<void (dsn::error_code)>&, std::tuple<dsn::error_code>&&, dsn::absl::integer_sequence<unsigned long, 0ul>) /__w/incubator-pegasus/incubator-pegasus/rdsn/include/dsn/utility/absl/utility/utility.h:154
    #16 0x55d6c1b14fea in _ZN3dsn4absl5applyIRSt8functionIFvNS_10error_codeEEESt5tupleIJS3_EEEEDTcl12apply_helpercl7forwardIT_Efp_Ecl7forwardIT0_Efp0_EtlNS0_16utility_internal3GenImXsrSt10tuple_sizeINSt16remove_referenceISA_E4typeEE5valueEE4typeEEEEOS9_OSA_ /__w/incubator-pegasus/incubator-pegasus/rdsn/include/dsn/utility/absl/utility/utility.h:198
    #17 0x55d6c1b14fea in dsn::future_task<dsn::error_code>::exec() /__w/incubator-pegasus/incubator-pegasus/rdsn/include/dsn/tool-api/task.h:392
    #18 0x55d6c21a824c in dsn::task::exec_internal() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/task/task.cpp:176
    #19 0x55d6c220aad5 in dsn::task_worker::loop() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/task/task_worker.cpp:224
    #20 0x55d6c220b9ac in dsn::task_worker::run_internal() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/task/task_worker.cpp:204
    #21 0x55d6c220bf43 in void std::__invoke_impl<void, void (dsn::task_worker::*&)(), dsn::task_worker*&>(std::__invoke_memfun_deref, void (dsn::task_worker::*&)(), dsn::task_worker*&) /usr/include/c++/7/bits/invoke.h:73
    #22 0x55d6c220bf43 in std::__invoke_result<void (dsn::task_worker::*&)(), dsn::task_worker*&>::type std::__invoke<void (dsn::task_worker::*&)(), dsn::task_worker*&>(void (dsn::task_worker::*&)(), dsn::task_worker*&) /usr/include/c++/7/bits/invoke.h:95
    #23 0x55d6c220bf43 in void std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>::__call<void, , 0ul>(std::tuple<>&&, std::_Index_tuple<0ul>) /usr/include/c++/7/functional:467
    #24 0x55d6c220bf43 in void std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>::operator()<, void>() /usr/include/c++/7/functional:551
    #25 0x55d6c220bf43 in void std::__invoke_impl<void, std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>>(std::__invoke_other, std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>&&) /usr/include/c++/7/bits/invoke.h:60
    #26 0x55d6c220bf43 in std::__invoke_result<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>>::type std::__invoke<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>>(std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>&&) /usr/include/c++/7/bits/invoke.h:95
    #27 0x55d6c220bf43 in decltype (__invoke((_S_declval<0ul>)())) std::thread::_Invoker<std::tuple<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()> > >::_M_invoke<0ul>(std::_Index_tuple<0ul>) /usr/include/c++/7/thread:234
    #28 0x55d6c220bf43 in std::thread::_Invoker<std::tuple<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()> > >::operator()() /usr/include/c++/7/thread:243
    #29 0x55d6c220bf43 in std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()> > > >::_M_run() /usr/include/c++/7/thread:186
    #30 0x1478c6f9d6de  (/usr/lib/x86_64-linux-gnu/libstdc++.so.6+0xbd6de)
previously allocated by thread T33 (test_meta.THREA) here:
    #0 0x1478c95c2448 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/libasan.so.4+0xe0448)
    #1 0x55d6c1840b51 in __gnu_cxx::new_allocator<std::__detail::_Hash_node<int, false> >::allocate(unsigned long, void const*) /usr/include/c++/7/ext/new_allocator.h:111
    #2 0x55d6c1840b51 in std::allocator_traits<std::allocator<std::__detail::_Hash_node<int, false> > >::allocate(std::allocator<std::__detail::_Hash_node<int, false> >&, unsigned long) /usr/include/c++/7/bits/alloc_traits.h:436
    #3 0x55d6c1840b51 in std::__detail::_Hash_node<int, false>* std::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<int, false> > >::_M_allocate_node<int const&>(int const&) /usr/include/c++/7/bits/hashtable_policy.h:2060
    #4 0x55d6c1840b51 in std::__detail::_Hash_node<int, false>* std::__detail::_AllocNode<std::allocator<std::__detail::_Hash_node<int, false> > >::operator()<int const&>(int const&) const /usr/include/c++/7/bits/hashtable_policy.h:182
    #5 0x55d6c1840b51 in std::pair<std::__detail::_Node_iterator<int, true, false>, bool> std::_Hashtable<int, int, std::allocator<int>, std::__detail::_Identity, std::equal_to<int>, std::hash<int>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, true, true> >::_M_insert<int const&, std::__detail::_AllocNode<std::allocator<std::__detail::_Hash_node<int, false> > > >(int const&, std::__detail::_AllocNode<std::allocator<std::__detail::_Hash_node<int, false> > > const&, std::integral_constant<bool, true>) /usr/include/c++/7/bits/hashtable.h:1821
    #6 0x55d6c1840b51 in std::__detail::_Insert_base<int, int, std::allocator<int>, std::__detail::_Identity, std::equal_to<int>, std::hash<int>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, true, true> >::insert(int const&) /usr/include/c++/7/bits/hashtable_policy.h:843
    #7 0x55d6c1840b51 in std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> >::insert(int const&) /usr/include/c++/7/bits/unordered_set.h:420
    #8 0x55d6c1840b51 in operator() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/meta/meta_bulk_load_service.cpp:1660
    #9 0x55d6c1af5a9e in std::function<void (dsn::error_code, dsn::blob const&)>::operator()(dsn::error_code, dsn::blob const&) const /usr/include/c++/7/bits/std_function.h:706
    #10 0x55d6c1af5a9e in operator() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/meta/meta_state_service_simple.cpp:459
    #11 0x55d6c1af5a9e in _M_invoke /usr/include/c++/7/bits/std_function.h:316
    #12 0x55d6c21a824c in dsn::task::exec_internal() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/task/task.cpp:176
    #13 0x55d6c220aad5 in dsn::task_worker::loop() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/task/task_worker.cpp:224
    #14 0x55d6c220b9ac in dsn::task_worker::run_internal() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/task/task_worker.cpp:204
    #15 0x55d6c220bf43 in void std::__invoke_impl<void, void (dsn::task_worker::*&)(), dsn::task_worker*&>(std::__invoke_memfun_deref, void (dsn::task_worker::*&)(), dsn::task_worker*&) /usr/include/c++/7/bits/invoke.h:73
    #16 0x55d6c220bf43 in std::__invoke_result<void (dsn::task_worker::*&)(), dsn::task_worker*&>::type std::__invoke<void (dsn::task_worker::*&)(), dsn::task_worker*&>(void (dsn::task_worker::*&)(), dsn::task_worker*&) /usr/include/c++/7/bits/invoke.h:95
    #17 0x55d6c220bf43 in void std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>::__call<void, , 0ul>(std::tuple<>&&, std::_Index_tuple<0ul>) /usr/include/c++/7/functional:467
    #18 0x55d6c220bf43 in void std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>::operator()<, void>() /usr/include/c++/7/functional:551
    #19 0x55d6c220bf43 in void std::__invoke_impl<void, std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>>(std::__invoke_other, std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>&&) /usr/include/c++/7/bits/invoke.h:60
    #20 0x55d6c220bf43 in std::__invoke_result<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>>::type std::__invoke<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>>(std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>&&) /usr/include/c++/7/bits/invoke.h:95
    #21 0x55d6c220bf43 in decltype (__invoke((_S_declval<0ul>)())) std::thread::_Invoker<std::tuple<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()> > >::_M_invoke<0ul>(std::_Index_tuple<0ul>) /usr/include/c++/7/thread:234
    #22 0x55d6c220bf43 in std::thread::_Invoker<std::tuple<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()> > >::operator()() /usr/include/c++/7/thread:243
    #23 0x55d6c220bf43 in std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()> > > >::_M_run() /usr/include/c++/7/thread:186
    #24 0x1478c6f9d6de  (/usr/lib/x86_64-linux-gnu/libstdc++.so.6+0xbd6de)
Thread T25 (test_meta.THREA) created by T0 here:
    #0 0x1478c9519d2f in __interceptor_pthread_create (/usr/lib/x86_64-linux-gnu/libasan.so.4+0x37d2f)
    #1 0x1478c6f9d994 in std::thread::_M_start_thread(std::unique_ptr<std::thread::_State, std::default_delete<std::thread::_State> >, void (*)()) (/usr/lib/x86_64-linux-gnu/libstdc++.so.6+0xbd994)
    #2 0x55d6c21c2f1b in dsn::task_worker_pool::start() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/task/task_engine.cpp:89
    #3 0x55d6c21c6ae0 in dsn::task_engine::start() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/task/task_engine.cpp:239
    #4 0x55d6c2093665 in dsn::service_node::start() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/service_engine.cpp:122
    #5 0x55d6c2094d1e in dsn::service_engine::start_node(dsn::service_app_spec&) /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/service_engine.cpp:247
    #6 0x55d6c207ddb3 in run /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/service_api_c.cpp:511
    #7 0x55d6c2082b63 in dsn_run_config(char const*, bool) /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/service_api_c.cpp:184
    #8 0x55d6c0ddf169 in main /__w/incubator-pegasus/incubator-pegasus/rdsn/src/meta/test/main.cpp:99
    #9 0x1478c655ac86 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x21c86)
Thread T33 (test_meta.THREA) created by T0 here:
    #0 0x1478c9519d2f in __interceptor_pthread_create (/usr/lib/x86_64-linux-gnu/libasan.so.4+0x37d2f)
    #1 0x1478c6f9d994 in std::thread::_M_start_thread(std::unique_ptr<std::thread::_State, std::default_delete<std::thread::_State> >, void (*)()) (/usr/lib/x86_64-linux-gnu/libstdc++.so.6+0xbd994)
    #2 0x55d6c21c2f1b in dsn::task_worker_pool::start() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/task/task_engine.cpp:89
    #3 0x55d6c21c6ae0 in dsn::task_engine::start() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/task/task_engine.cpp:239
    #4 0x55d6c2093665 in dsn::service_node::start() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/service_engine.cpp:122
    #5 0x55d6c2094d1e in dsn::service_engine::start_node(dsn::service_app_spec&) /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/service_engine.cpp:247
    #6 0x55d6c207ddb3 in run /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/service_api_c.cpp:511
    #7 0x55d6c2082b63 in dsn_run_config(char const*, bool) /__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/service_api_c.cpp:184
    #8 0x55d6c0ddf169 in main /__w/incubator-pegasus/incubator-pegasus/rdsn/src/meta/test/main.cpp:99
    #9 0x1478c655ac86 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x21c86)
SUMMARY: AddressSanitizer: heap-use-after-free /usr/include/c++/7/bits/hashtable_policy.h:314 in std::__detail::_Node_iterator_base<int, false>::_M_incr()
Shadow bytes around the buggy address:
  0x0c04801886c0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c04801886d0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c04801886e0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c04801886f0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0480188700: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
=>0x0c0480188710: fa fa fa fa fa fa[fd]fd fa fa fa fa fa fa fa fa
  0x0c0480188720: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0480188730: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0480188740: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0480188750: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0480188760: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07 
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
==9760==ABORTING
ERROR: run dsn.meta.test failed, return_code = 1
Error: Process completed with exit code 1.
```

2. LSAN failed
```
[ RUN      ] mutation_log_test.replay_single_file_1
E2022-06-29 12:20:50.265 (1656505250265889832 9838) replica.default0.0000265500010001: replica.cpp:569:init_disk_tag(): [1.1@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
E2022-06-29 12:20:50.266 (1656505250266339030 9838) replica.default0.0000265500010001: log_file.cpp:218:read_next_log_block(): invalid data header magic: 0x0
E2022-06-29 12:20:50.266 (1656505250266349930 9838) replica.default0.0000265500010001: log_file.cpp:102:open_read(): read first log entry of file ./test-log/log.1.0 failed, err = ERR_INVALID_DATA. Rename the file to ./test-log/log.1.0.removed
/__w/incubator-pegasus/incubator-pegasus/rdsn/src/replica/test/mutation_log_test.cpp:372: Failure
      Expected: ec
      Which is: ERR_INVALID_DATA
To be equal to: ERR_OK
ERR_INVALID_DATA
[  FAILED  ] mutation_log_test.replay_single_file_1 (1 ms)
```

3. USAN failed
```
[ RUN      ] bulk_load_failover_test.app_info_inconsistency
/usr/include/c++/7/bits/hashtable_policy.h:360:35: runtime error: member call on misaligned address 0x55415641e5894855 for type 'struct _Hash_node_value_base', which requires 8 byte alignment
0x55415641e5894855: note: pointer points here
<memory cannot be printed>
/usr/include/c++/7/bits/hashtable_policy.h:260:26: runtime error: member call on misaligned address 0x55415641e5894855 for type 'struct _Hash_node_value_base', which requires 8 byte alignment
0x55415641e5894855: note: pointer points here
<memory cannot be printed>
/usr/include/c++/7/bits/hashtable_policy.h:252:34: runtime error: member call on misaligned address 0x55415641e589485d for type 'struct __aligned_buffer', which requires 4 byte alignment
0x55415641e589485d: note: pointer points here
<memory cannot be printed>
/usr/include/c++/7/ext/aligned_buffer.h:110:41: runtime error: member call on misaligned address 0x55415641e589485d for type 'struct __aligned_buffer', which requires 4 byte alignment
0x55415641e589485d: note: pointer points here
<memory cannot be printed>
/usr/include/c++/7/bits/hashtable_policy.h:260:26: runtime error: reference binding to misaligned address 0x55415641e589485d for type 'int', which requires 4 byte alignment
0x55415641e589485d: note: pointer points here
<memory cannot be printed>
/usr/include/c++/7/bits/hashtable_policy.h:360:35: runtime error: reference binding to misaligned address 0x55415641e589485d for type 'const int', which requires 4 byte alignment
0x55415641e589485d: note: pointer points here
<memory cannot be printed>
/__w/incubator-pegasus/incubator-pegasus/rdsn/src/meta/meta_bulk_load_service.cpp:1704:30: runtime error: load of misaligned address 0x55415641e589485d for type 'const int', which requires 4 byte alignment
0x55415641e589485d: note: pointer points here
<memory cannot be printed>
E2022-06-29 12:55:14.398 (1656507314398709061 9783) test_meta.THREAD_POOL_DEFAULT0.0000261e00010001: meta_bulk_load_service.cpp:1798:validate_app(): app(bulk_load_failover_table) has different app_id or partition_count, bulk load app_id = 2, partition_count = 4, current app_id = 2, partition_count = 8
E2022-06-29 12:55:14.398 (1656507314398843462 9783) test_meta.THREAD_POOL_DEFAULT0.0000261e00010001: meta_bulk_load_service.cpp:1726:try_to_continue_app_bulk_load(): app(name=,app_id=0) is not existed or not available
E2022-06-29 12:55:14.398 (1656507314398902863 9783) test_meta.THREAD_POOL_DEFAULT0.0000261e00010001: meta_bulk_load_service.cpp:1726:try_to_continue_app_bulk_load(): app(name=,app_id=0) is not existed or not available
got signal id: 11
Segmentation fault (core dumped)
ERROR: run dsn.meta.test failed, return_code = 139
Error: Process completed with exit code 1.
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1024/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1025,https://api.github.com/repos/apache/incubator-pegasus/issues/1025,incubator-pegasus,1289588073,1025,BUG: Rocksdb options not changed even if update in Pegasus config file,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-06-30T05:00:23Z,2023-05-16T16:07:52Z,"Because we will load rocksdb options from disk when bootstrap, even if we update configs in Pegasus' config file, it seems will not take effect on these options.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1025/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1025,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5F2bvi,incubator-pegasus,1171897314,1025,NA,WHBANG,38547944,,,NA,2022-07-01T03:40:50Z,2022-07-01T03:40:50Z,i mark,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5F2bvi/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1025,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5GSSjD,incubator-pegasus,1179199683,1025,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-07-08T17:06:35Z,2022-07-08T17:06:35Z,"> i mark

@WHBANG Thanks!","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5GSSjD/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1026,https://api.github.com/repos/apache/incubator-pegasus/issues/1026,incubator-pegasus,1289887536,1026,Fix heap-use-after-free error while percentile timer is still running after percentile has already been destructed,empiredan,743379,Dan Wang,,CLOSED,2022-06-30T09:54:22Z,2022-07-06T02:59:00Z,"`heap-use-after-free` error  was found while percentile timer is executed (for complete error info, please see https://github.com/apache/incubator-pegasus/runs/7123305604?check_suite_focus=true#step:8:1770):

```
==5035==ERROR: AddressSanitizer: heap-use-after-free on address 0x6100000033d8 at pc 0x5631bbff27f1 bp 0x152a4f5e7900 sp 0x152a4f5e78f0
READ of size 8 at 0x6100000033d8 thread T131
    #0 0x5631bbff27f0 in void dsn::stl_nth_element_finder<long, std::less<long> >::operator()<__gnu_cxx::__normal_iterator<long*, std::vector<long, std::allocator<long> > > >(__gnu_cxx::__normal_iterator<long*, std::vector<long, std::allocator<long> > >, __gnu_cxx::__normal_iterator<long*, std::vector<long, std::allocator<long> > >, __gnu_cxx::__normal_iterator<long*, std::vector<long, std::allocator<long> > >) /__w/incubator-pegasus/incubator-pegasus/rdsn/include/dsn/utility/nth_element.h:94
    #1 0x5631bbff27f0 in dsn::percentile<long, dsn::stl_nth_element_finder<long, std::less<long> >, void>::find_nth_elements() /__w/incubator-pegasus/incubator-pegasus/rdsn/include/dsn/utility/metrics.h:681
    #2 0x5631bbfb5bae in void std::__invoke_impl<void, void (dsn::percentile<long, dsn::stl_nth_element_finder<long, std::less<long> >, void>::*&)(), dsn::percentile<long, dsn::stl_nth_element_finder<long, std::less<long> >, void>*&>(std::__invoke_memfun_deref, void (dsn::percentile<long, dsn::stl_nth_element_finder<long, std::less<long> >, void>::*&)(), dsn::percentile<long, dsn::stl_nth_element_finder<long, std::less<long> >, void>*&) /usr/include/c++/7/bits/invoke.h:73
    #3 0x5631bbfb5bae in std::__invoke_result<void (dsn::percentile<long, dsn::stl_nth_element_finder<long, std::less<long> >, void>::*&)(), dsn::percentile<long, dsn::stl_nth_element_finder<long, std::less<long> >, void>*&>::type std::__invoke<void (dsn::percentile<long, dsn::stl_nth_element_finder<long, std::less<long> >, void>::*&)(), dsn::percentile<long, dsn::stl_nth_element_finder<long, std::less<long> >, void>*&>(void (dsn::percentile<long, dsn::stl_nth_element_finder<long, std::less<long> >, void>::*&)(), dsn::percentile<long, dsn::stl_nth_element_finder<long, std::less<long> >, void>*&) /usr/include/c++/7/bits/invoke.h:95
    #4 0x5631bbfb5bae in void std::_Bind<void (dsn::percentile<long, dsn::stl_nth_element_finder<long, std::less<long> >, void>::*(dsn::percentile<long, dsn::stl_nth_element_finder<long, std::less<long> >, void>*))()>::__call<void, , 0ul>(std::tuple<>&&, std::_Index_tuple<0ul>) /usr/include/c++/7/functional:467
    #5 0x5631bbfb5bae in void std::_Bind<void (dsn::percentile<long, dsn::stl_nth_element_finder<long, std::less<long> >, void>::*(dsn::percentile<long, dsn::stl_nth_element_finder<long, std::less<long> >, void>*))()>::operator()<, void>() /usr/include/c++/7/functional:551
    #6 0x5631bbfb5bae in std::_Function_handler<void (), std::_Bind<void (dsn::percentile<long, dsn::stl_nth_element_finder<long, std::less<long> >, void>::*(dsn::percentile<long, dsn::stl_nth_element_finder<long, std::less<long> >, void>*))()> >::_M_invoke(std::_Any_data const&) /usr/include/c++/7/bits/std_function.h:316
    #7 0x152a55900883 in std::function<void ()>::operator()() const /usr/include/c++/7/bits/std_function.h:706
    #8 0x152a55900883 in dsn::percentile_timer::on_timer(boost::system::error_code const&) /__w/incubator-pegasus/incubator-pegasus/rdsn/src/utils/metrics.cpp:145
    #9 0x152a55924430 in void std::__invoke_impl<void, void (dsn::percentile_timer::*&)(boost::system::error_code const&), dsn::percentile_timer*&, boost::system::error_code const&>(std::__invoke_memfun_deref, void (dsn::percentile_timer::*&)(boost::system::error_code const&), dsn::percentile_timer*&, boost::system::error_code const&) /usr/include/c++/7/bits/invoke.h:73
    #10 0x152a55924430 in std::__invoke_result<void (dsn::percentile_timer::*&)(boost::system::error_code const&), dsn::percentile_timer*&, boost::system::error_code const&>::type std::__invoke<void (dsn::percentile_timer::*&)(boost::system::error_code const&), dsn::percentile_timer*&, boost::system::error_code const&>(void (dsn::percentile_timer::*&)(boost::system::error_code const&), dsn::percentile_timer*&, boost::system::error_code const&) /usr/include/c++/7/bits/invoke.h:95
    #11 0x152a55924430 in void std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>::__call<void, boost::system::error_code const&, 0ul, 1ul>(std::tuple<boost::system::error_code const&>&&, std::_Index_tuple<0ul, 1ul>) /usr/include/c++/7/functional:467
    #12 0x152a55924430 in void std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>::operator()<boost::system::error_code const&, void>(boost::system::error_code const&) /usr/include/c++/7/functional:551
    #13 0x152a55924430 in boost::asio::detail::binder1<std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>, boost::system::error_code>::operator()() /__w/incubator-pegasus/incubator-pegasus/rdsn/thirdparty/output/include/boost/asio/detail/bind_handler.hpp:65
    #14 0x152a55924430 in void boost::asio::asio_handler_invoke<boost::asio::detail::binder1<std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>, boost::system::error_code> >(boost::asio::detail::binder1<std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>, boost::system::error_code>&, ...) /__w/incubator-pegasus/incubator-pegasus/rdsn/thirdparty/output/include/boost/asio/handler_invoke_hook.hpp:69
    #15 0x152a55924430 in void boost_asio_handler_invoke_helpers::invoke<boost::asio::detail::binder1<std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>, boost::system::error_code>, std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)> >(boost::asio::detail::binder1<std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>, boost::system::error_code>&, std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>&) /__w/incubator-pegasus/incubator-pegasus/rdsn/thirdparty/output/include/boost/asio/detail/handler_invoke_helpers.hpp:37
    #16 0x152a55924430 in void boost::asio::detail::handler_work<std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>, boost::asio::system_executor>::complete<boost::asio::detail::binder1<std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>, boost::system::error_code> >(boost::asio::detail::binder1<std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>, boost::system::error_code>&, std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>&) /__w/incubator-pegasus/incubator-pegasus/rdsn/thirdparty/output/include/boost/asio/detail/handler_work.hpp:82
    #17 0x152a55924430 in boost::asio::detail::wait_handler<std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)> >::do_complete(void*, boost::asio::detail::scheduler_operation*, boost::system::error_code const&, unsigned long) /__w/incubator-pegasus/incubator-pegasus/rdsn/thirdparty/output/include/boost/asio/detail/wait_handler.hpp:72
    #18 0x152a559543d7 in boost::asio::detail::scheduler_operation::complete(void*, boost::system::error_code const&, unsigned long) /__w/incubator-pegasus/incubator-pegasus/rdsn/thirdparty/output/include/boost/asio/detail/scheduler_operation.hpp:40
    #19 0x152a559543d7 in boost::asio::detail::scheduler::do_run_one(boost::asio::detail::conditionally_enabled_mutex::scoped_lock&, boost::asio::detail::scheduler_thread_info&, boost::system::error_code const&) /__w/incubator-pegasus/incubator-pegasus/rdsn/thirdparty/output/include/boost/asio/detail/impl/scheduler.ipp:401
    #20 0x152a559543d7 in boost::asio::detail::scheduler::run(boost::system::error_code&) /__w/incubator-pegasus/incubator-pegasus/rdsn/thirdparty/output/include/boost/asio/detail/impl/scheduler.ipp:154
    #21 0x152a559543d7 in boost::asio::io_context::run() /__w/incubator-pegasus/incubator-pegasus/rdsn/thirdparty/output/include/boost/asio/impl/io_context.ipp:62
    #22 0x152a559543d7 in operator() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/utils/shared_io_service.cpp:46
    #23 0x152a559543d7 in __invoke_impl<void, dsn::tools::shared_io_service::shared_io_service()::<lambda()> > /usr/include/c++/7/bits/invoke.h:60
    #24 0x152a559543d7 in __invoke<dsn::tools::shared_io_service::shared_io_service()::<lambda()> > /usr/include/c++/7/bits/invoke.h:95
    #25 0x152a559543d7 in _M_invoke<0> /usr/include/c++/7/thread:234
    #26 0x152a559543d7 in operator() /usr/include/c++/7/thread:243
    #27 0x152a559543d7 in _M_run /usr/include/c++/7/thread:186
    #28 0x152a54d0b6de  (/usr/lib/x86_64-linux-gnu/libstdc++.so.6+0xbd6de)
    #29 0x152a554cf6da in start_thread (/lib/x86_64-linux-gnu/libpthread.so.0+0x76da)
    #30 0x152a5476661e in __clone (/lib/x86_64-linux-gnu/libc.so.6+0x12161e)
0x6100000033d8 is located 152 bytes inside of 192-byte region [0x610000003340,0x610000003400)
freed by thread T0 here:
    #0 0x152a55dbc9c8 in operator delete(void*, unsigned long) (/usr/lib/x86_64-linux-gnu/libasan.so.4+0xe19c8)
    #1 0x5631bbfc1b6e in dsn::percentile<long, dsn::stl_nth_element_finder<long, std::less<long> >, void>::~percentile() /__w/incubator-pegasus/incubator-pegasus/rdsn/include/dsn/utility/metrics.h:656
    #2 0x5631bbf9c889 in dsn::ref_counter::release_ref() /__w/incubator-pegasus/incubator-pegasus/rdsn/include/dsn/utility/autoref_ptr.h:84
    #3 0x5631bbf9c889 in dsn::ref_ptr<dsn::metric>::~ref_ptr() /__w/incubator-pegasus/incubator-pegasus/rdsn/include/dsn/utility/autoref_ptr.h:139
    #4 0x5631bbf9c889 in std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> >::~pair() /usr/include/c++/7/bits/stl_pair.h:208
    #5 0x5631bbf9c889 in void __gnu_cxx::new_allocator<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> > >::destroy<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> > >(std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> >*) /usr/include/c++/7/ext/new_allocator.h:140
    #6 0x5631bbf9c889 in void std::allocator_traits<std::allocator<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> > > >::destroy<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> > >(std::allocator<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> > >&, std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> >*) /usr/include/c++/7/bits/alloc_traits.h:487
    #7 0x5631bbf9c889 in std::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> >, false> > >::_M_deallocate_node(std::__detail::_Hash_node<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> >, false>*) /usr/include/c++/7/bits/hashtable_policy.h:2084
    #8 0x5631bbfbd353 in std::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> >, false> > >::_M_deallocate_nodes(std::__detail::_Hash_node<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> >, false>*) /usr/include/c++/7/bits/hashtable_policy.h:2097
    #9 0x5631bbfbd353 in std::_Hashtable<dsn::metric_prototype const*, std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> >, std::allocator<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> > >, std::__detail::_Select1st, std::equal_to<dsn::metric_prototype const*>, std::hash<dsn::metric_prototype const*>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, false, true> >::clear() /usr/include/c++/7/bits/hashtable.h:2032
    #10 0x152a558fbe6b in std::_Hashtable<dsn::metric_prototype const*, std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> >, std::allocator<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> > >, std::__detail::_Select1st, std::equal_to<dsn::metric_prototype const*>, std::hash<dsn::metric_prototype const*>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, false, true> >::~_Hashtable() /usr/include/c++/7/bits/hashtable.h:1358
    #11 0x152a558fbe6b in std::unordered_map<dsn::metric_prototype const*, dsn::ref_ptr<dsn::metric>, std::hash<dsn::metric_prototype const*>, std::equal_to<dsn::metric_prototype const*>, std::allocator<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> > > >::~unordered_map() /usr/include/c++/7/bits/unordered_map.h:101
    #12 0x152a558fbe6b in dsn::metric_entity::~metric_entity() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/utils/metrics.cpp:32
    #13 0x152a558fbfe0 in dsn::metric_entity::~metric_entity() /__w/incubator-pegasus/incubator-pegasus/rdsn/src/utils/metrics.cpp:32
    #14 0x5631bbfb9321 in dsn::ref_counter::release_ref() /__w/incubator-pegasus/incubator-pegasus/rdsn/include/dsn/utility/autoref_ptr.h:84
    #15 0x5631bbfb9321 in dsn::ref_ptr<dsn::metric_entity>::~ref_ptr() /__w/incubator-pegasus/incubator-pegasus/rdsn/include/dsn/utility/autoref_ptr.h:139
    #16 0x5631bbfb9321 in std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> >::~pair() /usr/include/c++/7/bits/stl_pair.h:208
    #17 0x5631bbfb9321 in void __gnu_cxx::new_allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> > >::destroy<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> > >(std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> >*) /usr/include/c++/7/ext/new_allocator.h:140
    #18 0x5631bbfb9321 in void std::allocator_traits<std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> > > >::destroy<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> > >(std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> > >&, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> >*) /usr/include/c++/7/bits/alloc_traits.h:487
    #19 0x5631bbfb9321 in std::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> >, true> > >::_M_deallocate_node(std::__detail::_Hash_node<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> >, true>*) /usr/include/c++/7/bits/hashtable_policy.h:2084
    #20 0x5631bbfb9321 in std::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> >, true> > >::_M_deallocate_nodes(std::__detail::_Hash_node<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> >, true>*) /usr/include/c++/7/bits/hashtable_policy.h:2097
    #21 0x5631bbfb9321 in std::_Hashtable<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> > >, std::__detail::_Select1st, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::clear() /usr/include/c++/7/bits/hashtable.h:2032
    #22 0x5631bbfb9321 in std::_Hashtable<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> > >, std::__detail::_Select1st, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::~_Hashtable() /usr/include/c++/7/bits/hashtable.h:1358
    #23 0x152a54688030  (/lib/x86_64-linux-gnu/libc.so.6+0x43030)
```

From the stack, we can draw a conclusion that percentile timer is still running after percentile has already been destructed. Once percentile is decided to be destructed, the timer should be cancelled firstly to ensure that there is not any member of percentile that is still be operated.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1026/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1029,https://api.github.com/repos/apache/incubator-pegasus/issues/1029,incubator-pegasus,1291799193,1029,Community chore: speed up CI on github,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-07-01T20:18:53Z,2022-07-11T06:37:38Z,"Currently, github actions will cost much time, it's too waste. we can disable `required_linear_history` to avoid re-run actions.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1029/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1032,https://api.github.com/repos/apache/incubator-pegasus/issues/1032,incubator-pegasus,1293832220,1032,Release 2.4.0,foreverneverer,23136769,Jiashuo,js982986555@live.com,CLOSED,2022-07-05T06:08:49Z,2022-11-01T03:21:09Z,"# New Module
From this version, more project module will join Apache Pegasus Project. In this version, the following projects are included:
- [RDSN](https://github.com/XiaoMi/rdsn): In the past, `rdsn` exists as a sub-project in current repository, in this release, it officially become the core module of the project.
- PegasusClient: Pegasus support multi-language client. In the past, it existed different repository, in this release, the following clients are included in the repository:
  - [Pegasus-Java-Client](https://github.com/XiaoMi/pegasus-java-client)
  - [Pegasus-Scala-Client](https://github.com/xiaomi/pegasus-scala-client)
  - [Pegasus-Golang-Client](https://github.com/XiaoMi/pegasus-go-client)
  - [Pegasus-Python-Client](https://github.com/XiaoMi/pegasus-python-client)
  - [Pegasus-NodeJS-Client](https://github.com/XiaoMi/pegasus-nodejs-client)
- [PegasusDocker](https://github.com/xiaomi/pegasus-scala-client): Pegasus support docker-build, now it is included the latest version
- PegasusShell: In the past, Pegasus use c++ shell to manage the cluster, now in the latest version, we bring new shell tools: [AdminCli](https://github.com/pegasus-kv/admin-cli) and [Pegic](https://github.com/pegasus-kv/pegic)

# New architecture
In this version, we remove the shared log to enhance the pegasus performance,  Related pull request as follow:
- https://github.com/XiaoMi/rdsn/pull/993
- https://github.com/XiaoMi/rdsn/pull/994
- https://github.com/XiaoMi/rdsn/pull/999
- https://github.com/XiaoMi/rdsn/pull/1019
- https://github.com/XiaoMi/rdsn/pull/1022
- https://github.com/XiaoMi/rdsn/pull/1048
- https://github.com/XiaoMi/rdsn/pull/1028
- https://github.com/apache/incubator-pegasus/pull/890

# New Feature
## Replica-factor update
Supporting flexible replica count. In the past, the replica factor was Immutable once one table was created. In current version, user can dynamically adjust the factor of specified table. Related pull request as follow:

- https://github.com/XiaoMi/rdsn/pull/1061
- https://github.com/XiaoMi/rdsn/pull/1072
- https://github.com/XiaoMi/rdsn/pull/1077
- https://github.com/XiaoMi/rdsn/pull/1087
- https://github.com/XiaoMi/rdsn/pull/1109
- https://github.com/XiaoMi/rdsn/pull/1110
- https://github.com/XiaoMi/rdsn/pull/1115
- https://github.com/apache/incubator-pegasus/pull/914
- https://github.com/apache/incubator-pegasus/pull/999
- https://github.com/apache/incubator-pegasus/pull/1035

## Read Request Limiter
In the past, we only support write limiter, in this version,  we add the supporting for read:
- https://github.com/XiaoMi/rdsn/pull/941
- https://github.com/XiaoMi/rdsn/pull/939
- https://github.com/apache/incubator-pegasus/pull/829
- https://github.com/XiaoMi/rdsn/pull/948
- https://github.com/XiaoMi/rdsn/pull/947
- https://github.com/XiaoMi/rdsn/pull/946

## Jemalloc Support
- https://github.com/XiaoMi/rdsn/pull/910
- https://github.com/apache/incubator-pegasus/pull/1050

## Build Feature
We have made some restrictions on the compilation environment and support `MacOS` and `aarcch64`:
- https://github.com/XiaoMi/rdsn/pull/1041
- https://github.com/XiaoMi/rdsn/pull/1034
- https://github.com/XiaoMi/rdsn/pull/1097
- https://github.com/apache/incubator-pegasus/pull/1049

## New BatchGetAPI
In the past, the `batchGet` implement based the `singleGet`, the latest version will aggregate different request first berfore sending, it will improve the performace:
- https://github.com/apache/incubator-pegasus/pull/897
- https://github.com/XiaoMi/pegasus-java-client/pull/175
## Task Queue limiter
- https://github.com/XiaoMi/rdsn/pull/902
- https://github.com/apache/incubator-pegasus/pull/831

# Feature enhancement
## Bulkload
We improve bulkload feature to reduce the io-load of downloading and ingesting, besides, we offer better interfaces and failure handling logic, the related pull request as follow:
- https://github.com/XiaoMi/rdsn/pull/952
- https://github.com/XiaoMi/rdsn/pull/958
- https://github.com/XiaoMi/rdsn/pull/959
- https://github.com/XiaoMi/rdsn/pull/960
- https://github.com/XiaoMi/rdsn/pull/962
- https://github.com/XiaoMi/rdsn/pull/964
- https://github.com/XiaoMi/rdsn/pull/967
- https://github.com/XiaoMi/rdsn/pull/1004
- https://github.com/XiaoMi/rdsn/pull/1011
- https://github.com/XiaoMi/rdsn/pull/1018
- https://github.com/XiaoMi/rdsn/pull/1027
- https://github.com/XiaoMi/rdsn/pull/1031
- https://github.com/XiaoMi/rdsn/pull/1035
- https://github.com/XiaoMi/rdsn/pull/1039
- https://github.com/XiaoMi/rdsn/pull/1102
- https://github.com/XiaoMi/rdsn/pull/1103
- https://github.com/XiaoMi/rdsn/pull/1104
- https://github.com/XiaoMi/rdsn/pull/1105
- https://github.com/XiaoMi/rdsn/pull/1069
- https://github.com/XiaoMi/rdsn/pull/1074
- https://github.com/XiaoMi/rdsn/pull/1009
- https://github.com/apache/incubator-pegasus/pull/881
- https://github.com/apache/incubator-pegasus/pull/888
- https://github.com/apache/incubator-pegasus/pull/968
- https://github.com/apache/incubator-pegasus/pull/975
- https://github.com/XiaoMi/rdsn/pull/1002
- https://github.com/apache/incubator-pegasus/pull/868

## Duplication
In the past, duplication has some shortcoming:  It depends remote filesystem to sync the checkpoint; The synchronization of plog data only sends a single mutation at each RPC. In this version, we enhance the above problem(the detail design see https://github.com/apache/incubator-pegasus/issues/892), related pull request as follows:
- https://github.com/XiaoMi/rdsn/pull/1038
- https://github.com/XiaoMi/rdsn/pull/1040
- https://github.com/XiaoMi/rdsn/pull/1045
- https://github.com/XiaoMi/rdsn/pull/1046
- https://github.com/XiaoMi/rdsn/pull/1049
- https://github.com/XiaoMi/rdsn/pull/1051
- https://github.com/XiaoMi/rdsn/pull/1053
- https://github.com/XiaoMi/rdsn/pull/1055
- https://github.com/XiaoMi/rdsn/pull/1056
- https://github.com/XiaoMi/rdsn/pull/1059
- https://github.com/XiaoMi/rdsn/pull/1060
- https://github.com/XiaoMi/rdsn/pull/1063
- https://github.com/XiaoMi/rdsn/pull/1064
- https://github.com/XiaoMi/rdsn/pull/1065
- https://github.com/apache/incubator-pegasus/pull/917
- https://github.com/XiaoMi/rdsn/pull/1066
- https://github.com/XiaoMi/rdsn/pull/1067
- https://github.com/apache/incubator-pegasus/pull/919
- https://github.com/XiaoMi/rdsn/pull/1071
- https://github.com/XiaoMi/rdsn/pull/1076
- https://github.com/XiaoMi/rdsn/pull/1080
- https://github.com/apache/incubator-pegasus/pull/930
- https://github.com/XiaoMi/rdsn/pull/1084
- https://github.com/apache/incubator-pegasus/pull/935
- https://github.com/apache/incubator-pegasus/pull/936
- https://github.com/XiaoMi/rdsn/pull/1085
- https://github.com/apache/incubator-pegasus/pull/940
- https://github.com/XiaoMi/rdsn/pull/1121
- https://github.com/apache/incubator-pegasus/pull/1007
- https://github.com/apache/incubator-pegasus/pull/1008
- https://github.com/XiaoMi/rdsn/pull/976
- https://github.com/apache/incubator-pegasus/pull/1065
- https://github.com/apache/incubator-pegasus/pull/1078

## PerfCounter
In the version, we support new metric implement to optimize performance:
- https://github.com/XiaoMi/rdsn/pull/1033
- https://github.com/XiaoMi/rdsn/pull/1070
- https://github.com/XiaoMi/rdsn/pull/1073
- https://github.com/XiaoMi/rdsn/pull/1075
- https://github.com/XiaoMi/rdsn/pull/1081
- https://github.com/apache/incubator-pegasus/pull/1074

## Manual Compaction
- https://github.com/XiaoMi/rdsn/pull/989
- https://github.com/XiaoMi/rdsn/pull/987
- https://github.com/XiaoMi/rdsn/pull/983
- https://github.com/apache/incubator-pegasus/pull/854
- https://github.com/XiaoMi/rdsn/pull/981

## Learn with NFS
To reduce the impact of data migration for IO-LOAD and ensure the migration rate, our data transmission supports disk level speed limits：
- https://github.com/XiaoMi/rdsn/pull/944
- https://github.com/XiaoMi/rdsn/pull/943
- https://github.com/XiaoMi/rdsn/pull/985

## Latency Tracer
The latest latency tracer support perf-counter and fix some bugs: 
- https://github.com/XiaoMi/rdsn/pull/1029
- https://github.com/XiaoMi/rdsn/pull/1023
- https://github.com/XiaoMi/rdsn/pull/951
- https://github.com/XiaoMi/rdsn/pull/945
- https://github.com/XiaoMi/rdsn/pull/965

## Other important
- https://github.com/XiaoMi/rdsn/pull/1086
- https://github.com/XiaoMi/rdsn/pull/988
- https://github.com/XiaoMi/rdsn/pull/979
- https://github.com/XiaoMi/rdsn/pull/978
- https://github.com/XiaoMi/rdsn/pull/963
- https://github.com/apache/incubator-pegasus/pull/870
- https://github.com/apache/incubator-pegasus/pull/852
- https://github.com/apache/incubator-pegasus/pull/907
- https://github.com/apache/incubator-pegasus/pull/1009
- https://github.com/apache/incubator-pegasus/pull/1085
- https://github.com/apache/incubator-pegasus/pull/1061

## Java Client
- https://github.com/XiaoMi/pegasus-java-client/pull/184
- https://github.com/XiaoMi/pegasus-java-client/pull/181
- https://github.com/XiaoMi/pegasus-java-client/pull/180
- https://github.com/XiaoMi/pegasus-java-client/pull/177
- https://github.com/apache/incubator-pegasus/pull/1019
- https://github.com/apache/incubator-pegasus/pull/1000
- https://github.com/apache/incubator-pegasus/pull/973

## Go Client
- https://github.com/XiaoMi/pegasus-go-client/pull/110
- https://github.com/XiaoMi/pegasus-go-client/pull/109
- https://github.com/XiaoMi/pegasus-go-client/pull/107
- https://github.com/XiaoMi/pegasus-go-client/pull/105
- https://github.com/XiaoMi/pegasus-go-client/pull/104
- https://github.com/XiaoMi/pegasus-go-client/pull/102
- https://github.com/XiaoMi/pegasus-go-client/pull/101
- https://github.com/XiaoMi/pegasus-go-client/pull/99

## Python Client
- https://github.com/apache/incubator-pegasus/pull/977

## Admin Cli
- support nodes migrator
  - https://github.com/pegasus-kv/admin-cli/pull/62
  - https://github.com/pegasus-kv/admin-cli/pull/60
  - https://github.com/pegasus-kv/admin-cli/pull/59
  - https://github.com/pegasus-kv/admin-cli/pull/54
  - https://github.com/pegasus-kv/admin-cli/pull/53
  - https://github.com/pegasus-kv/admin-cli/pull/52
  - https://github.com/pegasus-kv/admin-cli/pull/51
  - https://github.com/pegasus-kv/admin-cli/pull/50
- https://github.com/pegasus-kv/admin-cli/pull/58
- https://github.com/pegasus-kv/admin-cli/pull/57
- https://github.com/pegasus-kv/admin-cli/pull/56
- https://github.com/pegasus-kv/admin-cli/pull/55
- https://github.com/apache/incubator-pegasus/pull/987
- https://github.com/apache/incubator-pegasus/pull/969
- https://github.com/apache/incubator-pegasus/pull/958
- https://github.com/apache/incubator-pegasus/pull/1006
- https://github.com/apache/incubator-pegasus/pull/976

## Pegasus Docker
- https://github.com/apache/incubator-pegasus/pull/1011


# Code Refactor
- https://github.com/XiaoMi/rdsn/pull/1068
- https://github.com/XiaoMi/rdsn/pull/1062
- https://github.com/XiaoMi/rdsn/pull/1050
- https://github.com/XiaoMi/rdsn/pull/1032
- https://github.com/XiaoMi/rdsn/pull/1030
- https://github.com/XiaoMi/rdsn/pull/1015
- https://github.com/XiaoMi/rdsn/pull/1012
- https://github.com/XiaoMi/rdsn/pull/1010
- https://github.com/XiaoMi/rdsn/pull/1010
- https://github.com/XiaoMi/rdsn/pull/1003
- https://github.com/XiaoMi/rdsn/pull/1000
- https://github.com/XiaoMi/rdsn/pull/998
- https://github.com/XiaoMi/rdsn/pull/997
- https://github.com/XiaoMi/rdsn/pull/996
- https://github.com/XiaoMi/rdsn/pull/995
- https://github.com/XiaoMi/rdsn/pull/992
- https://github.com/XiaoMi/rdsn/pull/974
- https://github.com/XiaoMi/rdsn/pull/968
- https://github.com/XiaoMi/rdsn/pull/950
- https://github.com/XiaoMi/rdsn/pull/942
- https://github.com/apache/incubator-pegasus/pull/1021
- https://github.com/apache/incubator-pegasus/pull/1005
- https://github.com/apache/incubator-pegasus/pull/921
- https://github.com/apache/incubator-pegasus/pull/916
# Bug Fix
## Core 
- https://github.com/XiaoMi/rdsn/pull/1016
- https://github.com/XiaoMi/rdsn/pull/1008
- https://github.com/XiaoMi/rdsn/pull/1099
- https://github.com/XiaoMi/rdsn/pull/1017
- https://github.com/XiaoMi/rdsn/pull/1052

## Common
- https://github.com/XiaoMi/rdsn/pull/1047
- https://github.com/XiaoMi/rdsn/pull/1088
- https://github.com/XiaoMi/rdsn/pull/1044
- https://github.com/XiaoMi/rdsn/pull/1037
- https://github.com/XiaoMi/rdsn/pull/1036
- https://github.com/XiaoMi/rdsn/pull/1001
- https://github.com/XiaoMi/rdsn/pull/990
- https://github.com/XiaoMi/rdsn/pull/984
- https://github.com/XiaoMi/rdsn/pull/982
- https://github.com/XiaoMi/rdsn/pull/980
- https://github.com/apache/incubator-pegasus/pull/828
- https://github.com/apache/incubator-pegasus/pull/984
- https://github.com/apache/incubator-pegasus/pull/909
- https://github.com/apache/incubator-pegasus/pull/911
- https://github.com/apache/incubator-pegasus/pull/833
 
# Performance
In this benchmark, we use the new machine, for the result is more reasonable, we re-run the Pegasus Server 2.3：

- Machine parameters:  DDR4 16G * 8  | Intel Silver4210*2 2.20Ghz/3.20Ghz | SSD 480G * 8 SATA 
- Cluster Server: 3 * MetaServerNode  5 * ReplicaServerNode
- YCSB Client: 3 * ClientNode
- Request Length: 1KB(set/get)
- Centos7 5.4.54-2.0.4.std7c.el7.x86_64

## Pegasus Server 2.3
Case | client and thread | R:W | R-QPS | R-Avg | R-P99 | W-QPS | W-Avg | W-P99
-- | -- | -- | -- | -- | -- | -- | -- | --
Write Only | 3 clients * 15 threads | 0:1 | - | - | - | 48805 | 919 | 2124
Read Only | 3 clients * 50 threads | 1:0 | 370068 | 402 | 988 | - | - | -
Read Write |  3 clients * 30 threads | 1:1 | 50762 | 532 | 5859 | 50759 | 1233 | 4162
Read Write |  3 clients * 15 threads | 1:3 | 14471 | 443 | 3869 | 43425 | 884 | 1899
Read Write |  3 clients * 15 threads | 1:30 | 1583 | 473 | 3432 | 47551 | 928 | 2066
Read Write |  3 clients * 30 threads | 3:1 | 119093 | 406 | 3367 | 39693 | 1035 | 2581
Read Write |  3 clients * 50 threads | 30:1 | 322904 | 435 | 1034 | 10762 | 882 | 1392

## Pegasus Server 2.4
Case | client and thread | R:W | R-QPS | R-Avg | R-P99 | W-QPS | W-Avg | W-P99
-- | -- | -- | -- | -- | -- | -- | -- | --
Write Only | 3 clients * 15 threads | 0:1 | - | - | - | 56953 | 787 | 1786
Read Only | 3 clients * 50 threads | 1:0 | 360642 | 413 | 984 | - | - | -
Read Write |  3 clients * 30 threads | 1:1 | 62572 | 464 | 5274 | 62561 | 985 | 3764
Read Write |  3 clients * 15 threads | 1:3 | 16844 | 372 | 3980 | 50527 | 762 | 1551
Read Write |  3 clients * 15 threads | 1:30 | 1861 | 381 | 3557 | 55816 | 790 | 1688
Read Write |  3 clients * 30 threads | 3:1 | 140484 | 351 | 3277 | 46822 | 856 | 2044
Read Write |  3 clients * 50 threads | 30:1 | 336106 | 419 | 1221 | 11203 | 763 | 1276

# Config-Update
```diff
+ [pegasus.server]
+ rocksdb_max_log_file_size = 8388608
+ rocksdb_log_file_time_to_roll = 86400
+ rocksdb_keep_log_file_num = 32

+ [replication]
+ plog_force_flush = false
  
- mutation_2pc_min_replica_count = 2
+ mutation_2pc_min_replica_count = 0 # 0 means it's value based table max replica count
  
+ enable_direct_io = false # Whether to enable direct I/O when download files from hdfs, default false
+ direct_io_buffer_pages = 64 # Number of pages we need to set to direct io buffer, default 64 which is recommend in my test.
+ max_concurrent_manual_emergency_checkpointing_count = 10
  
+ enable_latency_tracer_report = false
+ latency_tracer_counter_name_prefix = trace_latency
  
+ hdfs_read_limit_rate_mb_per_sec = 200
+ hdfs_write_limit_rate_mb_per_sec = 200
  
+ duplicate_log_batch_bytes = 0 # 0 means no batch before sending
  
+ [nfs]
- max_copy_rate_megabytes = 500
+ max_copy_rate_megabytes_per_disk = 0
- max_send_rate_megabytes = 500
+ max_send_rate_megabytes_per_disk = 0
  
+ [meta_server]
+ max_reserved_dropped_replicas = 0
+ bulk_load_verify_before_ingest = false
+ bulk_load_node_max_ingesting_count = 4
+ bulk_load_node_min_disk_count = 1
+ enable_concurrent_bulk_load = false
+ max_allowed_replica_count = 5
+ min_allowed_replica_count = 1
  
+ [task.LPC_WRITE_REPLICATION_LOG_SHARED]
+ enable_trace = true # true will mark the task will be traced latency if open global trace
```
# Contributors
[acelyc111](https://github.com/acelyc111)
[cauchy1988](https://github.com/cauchy1988)
[empiredan](https://github.com/empiredan)
[foreverneverer](https://github.com/foreverneverer)
[GehaFearless](https://github.com/GehaFearless)
[GiantKing](https://github.com/GiantKing)
[happydongyaoyao](https://github.com/happydongyaoyao)
[hycdong](https://github.com/hycdong)
[levy5307](https://github.com/levy5307)
[lidingshengHHU](https://github.com/lidingshengHHU)
[neverchanje](https://github.com/neverchanje)
[padmejin](https://github.com/padmejin)
[Smityz](https://github.com/Smityz)
[totalo](https://github.com/totalo)
[WHBANG](https://github.com/WHBANG)
[xxmazha](https://github.com/xxmazha)
[ZhongChaoqiang](https://github.com/ZhongChaoqiang)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1032/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1032,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5IkfqZ,incubator-pegasus,1217526425,1032,NA,empiredan,743379,Dan Wang,,NA,2022-08-17T06:41:51Z,2022-08-17T06:41:51Z,"Several problems have been found according to the checklist for Incubator release:

- In `LICENSE` and `.licenserc.yaml`, some file paths have been not updated after refactor;
- `NOTICE` year is still the last year.

Thus some PRs have been committed to fix these problems as follows:
- [x] https://github.com/apache/incubator-pegasus/pull/1123
- [x] https://github.com/apache/incubator-pegasus/pull/1121
- [x] https://github.com/apache/incubator-pegasus/pull/1119

These PRs should be cherry-picked to [v2.4](https://github.com/apache/incubator-pegasus/tree/v2.4) to meet the requirements for Incubator release.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5IkfqZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1032,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5I8e-2,incubator-pegasus,1223815094,1032,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2022-08-23T09:35:35Z,2022-08-23T09:35:35Z,"Since we have cherry-pick more commit into v2.4, which  involved the cmake module. Suggested by @acelyc111, I re-run benchmark as follow:

- CentOS7  3.10.0-1160.1.0.el7.x86_64

## Pegasus Server 2.4
Case | client and thread | R:W | R-QPS | R-Avg | R-P99 | W-QPS | W-Avg | W-P99
-- | -- | -- | -- | -- | -- | -- | -- | --
Write Only | 3 clients * 15 threads | 0:1 | - | - | - | 55490 | 808 | 3540
Read Only | 3 clients * 50 threads | 1:0 | 361112 | 414 | 997 | - | - | -
Read Write |  3 clients * 30 threads | 1:1 | 63581 | 469 | 5447 | 63580 | 939 | 4959
Read Write |  3 clients * 15 threads | 1:3 | 16559 | 396 | 4228 | 49664 | 769 | 3987
Read Write |  3 clients * 15 threads | 1:30 | 1730 | 413 | 3669 | 51966 | 849 | 4735
Read Write |  3 clients * 30 threads | 3:1 | 135091 | 376 | 3007 | 45304 | 842 | 4753
Read Write |  3 clients * 50 threads | 30:1 | 319519 | 444 | 1442 | 10643 | 819 | 2691


For some reasons, I cannot run under  ` centos7 5.4.54-2.0.4.std7c.el7.x86_ 64 `, which may also lead to some differences in results from the last, I will retest some previous versions before the official release.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5I8e-2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1032,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5LJHx7,incubator-pegasus,1260682363,1032,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-09-28T10:06:47Z,2022-09-28T10:06:47Z,"Some more license issues have been resolved:
- https://github.com/apache/incubator-pegasus/pull/1173
- https://github.com/apache/incubator-pegasus/pull/1176","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5LJHx7/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1032,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5NXXrW,incubator-pegasus,1297971926,1032,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-11-01T03:21:00Z,2022-11-01T03:21:00Z,https://github.com/apache/incubator-pegasus/releases/tag/v2.4.0,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5NXXrW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1034,https://api.github.com/repos/apache/incubator-pegasus/issues/1034,incubator-pegasus,1295107091,1034,unit test failed `mutation_log_test.reset_from_while_writing`,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2022-07-06T04:27:18Z,2022-07-26T01:03:41Z,"ref: https://github.com/apache/incubator-pegasus/runs/7207609581?check_suite_focus=true
```
[ RUN      ] mutation_log_test.reset_from_while_writing
E2022-07-06 04:02:45.778 (1657080165778461573 106695) replica.default0.0000a0ae00010001: replica.cpp:569:init_disk_tag(): [1.1@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
E2022-07-06 04:02:46.323 (1657080166323215987 106695) replica.default0.0000a0ae00010001: log_file.cpp:218:read_next_log_block(): invalid data header magic: 0x0
E2022-07-06 04:02:46.323 (1657080166323232588 106695) replica.default0.0000a0ae00010001: log_file.cpp:102:open_read(): read first log entry of file ./test-log.test/log.1.0 failed, err = ERR_INVALID_DATA. Rename the file to ./test-log.test/log.1.0.removed
E2022-07-06 04:02:46.323 (1657080166323309593 106695) replica.default0.0000a0ae00010001: mutation_log.cpp:948:reset_from(): the log of source dir ./test-log.test is invalid:ERR_INVALID_DATA: failed to open the log file (./test-log.test/log.1.0) << open_log_file_map(log_files) << open_log_file_map(dir) << check_log_files_continuity(dir), will remove it.
/__w/incubator-pegasus/incubator-pegasus/rdsn/src/replica/test/mutation_log_test.cpp:582: Failure
      Expected: err
      Which is: ERR_INVALID_DATA
To be equal to: ERR_OK
[  FAILED  ] mutation_log_test.reset_from_while_writing (564 ms)
[----------] 14 tests from mutation_log_test (2665 ms total)
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1034/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1034,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5GVbq1,incubator-pegasus,1180023477,1034,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-07-11T06:39:30Z,2022-07-11T06:39:30Z,resolved by https://github.com/apache/incubator-pegasus/pull/1030,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5GVbq1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1034,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5G1_Qr,incubator-pegasus,1188557867,1034,NA,empiredan,743379,Dan Wang,,NA,2022-07-19T03:36:47Z,2022-07-19T03:36:47Z,"Similar error happened again as follows (https://github.com/apache/incubator-pegasus/runs/7401523538?check_suite_focus=true):
```
[ RUN      ] mutation_log_test.reset_from_while_writing
E2022-07-19 03:12:17.393 (1658200337393066268 9560) replica.default0.0000253f00010001: replica.cpp:569:init_disk_tag(): [1.1@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
E2022-07-19 03:12:17.924 (1658200337924981680 9560) replica.default0.0000253f00010001: log_file.cpp:218:read_next_log_block(): invalid data header magic: 0x0
E2022-07-19 03:12:17.924 (1658200337924994880 9560) replica.default0.0000253f00010001: log_file.cpp:102:open_read(): read first log entry of file ./test-log.test/log.1.0 failed, err = ERR_INVALID_DATA. Rename the file to ./test-log.test/log.1.0.removed
E2022-07-19 03:12:17.925 (1658200337925066279 9560) replica.default0.0000253f00010001: mutation_log.cpp:948:reset_from(): the log of source dir ./test-log.test is invalid:ERR_INVALID_DATA: failed to open the log file (./test-log.test/log.1.0) << open_log_file_map(log_files) << open_log_file_map(dir) << check_log_files_continuity(dir), will remove it.
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/replica/test/mutation_log_test.cpp:582: Failure
      Expected: err
      Which is: ERR_INVALID_DATA
To be equal to: ERR_OK
[  FAILED  ] mutation_log_test.reset_from_while_writing (549 ms)
[----------] 14 tests from mutation_log_test (2554 ms total)
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5G1_Qr/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1036,https://api.github.com/repos/apache/incubator-pegasus/issues/1036,incubator-pegasus,1296919716,1036,Unit test failed for tools_common.asio_network_provider_connection_threshold ,empiredan,743379,Dan Wang,,OPEN,2022-07-07T06:52:15Z,2022-08-19T03:07:31Z,"Unit test `tools_common.asio_network_provider_connection_threshold` sometimes failed for the following error:
```
[----------] 4 tests from tools_common
[ RUN      ] tools_common.asio_net_provider
[       OK ] tools_common.asio_net_provider (9 ms)
[ RUN      ] tools_common.asio_udp_provider
[       OK ] tools_common.asio_udp_provider (1007 ms)
[ RUN      ] tools_common.sim_net_provider
[       OK ] tools_common.sim_net_provider (6 ms)
[ RUN      ] tools_common.asio_network_provider_connection_threshold
/__w/incubator-pegasus/incubator-pegasus/rdsn/src/runtime/test/netprovider.cpp:95: Failure
Value of: ERR_TIMEOUT == ec
  Actual: false
Expected: true
[  FAILED  ] tools_common.asio_network_provider_connection_threshold (1078 ms)
[----------] 4 tests from tools_common (2100 ms total)
```


The messages from logging are as below: 
```
[  FAILED  ] 1 test, listed below:
[  FAILED  ] tools_common.asio_network_provider_connection_threshold
 1 FAILED TEST
dsn exit with code 1
run dsn_runtime_tests config-test.ini failed
---- ls ----
total 123528
drwxr-xr-x 3 root root      4096 Jul  6 07:34 CMakeFiles
-rw-r--r-- 1 root root     33775 Jul  6 07:34 Makefile
-rwxr-xr-x 1 root root      1238 Jul  6 07:34 clear.sh
-rw-r--r-- 1 root root      1320 Jul  6 07:34 cmake_install.cmake
-rw-r--r-- 1 root root       134 Jul  6 07:34 command.txt
-rw-r--r-- 1 root root      4068 Jul  6 07:34 config-test-corrupt-message.ini
-rw-r--r-- 1 root root      3416 Jul  6 07:34 config-test-sim.ini
-rw-r--r-- 1 root root      4210 Jul  6 07:34 config-test.ini
drwxr-xr-x 8 root root      4096 Jul  6 08:29 data
-rwxr-xr-x 1 root root 126408200 Jul  6 07:49 dsn_runtime_tests
-rw-r--r-- 1 root root       255 Jul  6 07:34 gtest.filter
-rwxr-xr-x 1 root root      2309 Jul  6 07:34 run.sh
./data/log/log.1.txt
---- tail -n 100 log.1.txt ----
W2022-07-06 08:29:50.739 (1657096190739289399 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: server_negotiation.cpp:137:do_challenge(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): negotiation failed, with err = ERR_TIMEOUT, msg = ERR_TIMEOUT
D2022-07-06 08:29:50.739 (1657096190739366503 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_start, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-07-06 08:29:50.739 (1657096190739423206 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_OK), frequency: 100%, max_count: -1]
W2022-07-06 08:29:50.739 (1657096190739454508 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: negotiation.cpp:65:check_status(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): get message(negotiation_select_mechanisms) while expect(negotiation_initiate)
D2022-07-06 08:29:50.739 (1657096190739507511 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_start, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-07-06 08:29:50.739 (1657096190739574915 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_TIMEOUT), frequency: 100%, max_count: -1]
D2022-07-06 08:29:50.739 (1657096190739613517 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: fail_point.cpp:155:eval(): fail on sasl_server_wrapper_start
D2022-07-06 08:29:50.739 (1657096190739629218 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: fail_point.cpp:155:eval(): fail on sasl_wrapper_retrieve_username
W2022-07-06 08:29:50.739 (1657096190739644719 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: server_negotiation.cpp:151:do_challenge(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): retrive user name failed: with err = ERR_TIMEOUT, msg = ERR_TIMEOUT
D2022-07-06 08:29:50.739 (1657096190739715122 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_start, task: Return(ERR_SASL_INCOMPLETE), frequency: 100%, max_count: -1]
D2022-07-06 08:29:50.739 (1657096190739778426 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-07-06 08:29:50.739 (1657096190739833729 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: fail_point.cpp:155:eval(): fail on sasl_server_wrapper_start
E2022-07-06 08:29:50.744 (1657096190744628594 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 14
E2022-07-06 08:29:50.744 (1657096190744640495 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 15
E2022-07-06 08:29:50.744 (1657096190744652295 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 16
E2022-07-06 08:29:50.744 (1657096190744664096 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 17
E2022-07-06 08:29:50.744 (1657096190744688497 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 18
E2022-07-06 08:29:50.744 (1657096190744709299 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 19
E2022-07-06 08:29:50.744 (1657096190744722599 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 20
E2022-07-06 08:29:50.744 (1657096190744734800 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 21
E2022-07-06 08:29:50.744 (1657096190744746701 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 22
E2022-07-06 08:29:50.744 (1657096190744765302 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 23
E2022-07-06 08:29:50.744 (1657096190744778302 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 24
E2022-07-06 08:29:50.744 (1657096190744793103 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 25
E2022-07-06 08:29:50.744 (1657096190744805704 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 26
E2022-07-06 08:29:50.744 (1657096190744[817](https://github.com/apache/incubator-pegasus/runs/7210178250?check_suite_focus=true#step:8:818)505 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 27
E2022-07-06 08:29:50.744 (1657096190744829305 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 28
E2022-07-06 08:29:50.744 (1657096190744841006 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 29
E2022-07-06 08:29:50.744 (1657096190744852807 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 30
E2022-07-06 08:29:50.744 (1657096190744870108 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 31
E2022-07-06 08:29:50.744 (16570961907448[828](https://github.com/apache/incubator-pegasus/runs/7210178250?check_suite_focus=true#step:8:829)08 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 32
E2022-07-06 08:29:50.744 (1657096190744894609 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 33
E2022-07-06 08:29:50.744 (1657096190744906310 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 34
E2022-07-06 08:29:50.744 (1657096190744992114 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 35
E2022-07-06 08:29:50.745 (1657096190745006915 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 36
E2022-07-06 08:29:50.745 (1657096190745024716 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 37
E2022-07-06 08:29:50.745 (1657096190745036717 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 38
E2022-07-06 08:29:50.745 (1657096190745059618 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 39
E2022-07-06 08:29:50.745 (1657096190745079419 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 40
E2022-07-06 08:29:50.745 (1657096190745091720 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 41
E2022-07-06 08:29:50.745 (1657096190745103420 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 42
E2022-07-06 08:29:50.745 (1657096190745115121 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 43
E2022-07-06 08:29:50.745 (1657096190745126922 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 44
E2022-07-06 08:29:50.745 (1657096190745138722 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 45
E2022-07-06 08:29:50.745 (1657096190745150523 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 46
E2022-07-06 08:29:50.745 (1657096190745166624 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 47
E2022-07-06 08:29:50.745 (1657096190745[833](https://github.com/apache/incubator-pegasus/runs/7210178250?check_suite_focus=true#step:8:834)261 4[841](https://github.com/apache/incubator-pegasus/runs/7210178250?check_suite_focus=true#step:8:842)) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:113:create_message_from_request_blob(): invalid message type: 2
E2022-07-06 08:29:50.745 (16570961[907](https://github.com/apache/incubator-pegasus/runs/7210178250?check_suite_focus=true#step:8:908)45941667 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:113:create_message_from_request_blob(): invalid message type: 65
E2022-07-06 08:29:50.746 (1657096190746741211 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: thrift_message_parser.cpp:113:create_message_from_request_blob(): invalid message type: 65
D2022-07-06 08:29:50.751 (1657096190751307364 4841) client.THREAD_POOL_DEFAULT0.000012d400010001: tracer.cpp:117:tracer_on_task_end(): LPC_CONTROL_SERVICE_APP EXEC END, task_id = 000012d400010001, err = ERR_OK
ERROR: run dsn_runtime_tests failed, return_code = 1
Error: Process completed with exit code 1.
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1036/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1036,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5IuqTY,incubator-pegasus,1220191448,1036,NA,empiredan,743379,Dan Wang,,NA,2022-08-19T03:05:04Z,2022-08-19T03:05:04Z,"https://github.com/apache/incubator-pegasus/runs/7897027671?check_suite_focus=true

```
[----------] 4 tests from tools_common
[ RUN      ] tools_common.asio_net_provider
[       OK ] tools_common.asio_net_provider (11 ms)
[ RUN      ] tools_common.asio_udp_provider
[       OK ] tools_common.asio_udp_provider (1011 ms)
[ RUN      ] tools_common.sim_net_provider
[       OK ] tools_common.sim_net_provider (6 ms)
[ RUN      ] tools_common.asio_network_provider_connection_threshold
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/test/netprovider.cpp:95: Failure
Value of: ERR_TIMEOUT == ec
  Actual: false
Expected: true
[  FAILED  ] tools_common.asio_network_provider_connection_threshold (1078 ms)
[----------] 4 tests from tools_common (2106 ms total)

......

[----------] Global test environment tear-down
[==========] 74 tests from 16 test cases ran. (6163 ms total)
[  PASSED  ] 73 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] tools_common.asio_network_provider_connection_threshold

 1 FAILED TEST
dsn exit with code 1
run dsn_runtime_tests config-test.ini failed
---- ls ----
total 124092
drwxr-xr-x 3 root root      4096 Aug 18 10:09 CMakeFiles
-rw-r--r-- 1 root root     36143 Aug 18 10:09 Makefile
-rwxr-xr-x 1 root root      1238 Aug 18 10:08 clear.sh
-rw-r--r-- 1 root root      1326 Aug 18 10:09 cmake_install.cmake
-rw-r--r-- 1 root root       134 Aug 18 10:08 command.txt
-rw-r--r-- 1 root root      4068 Aug 18 10:08 config-test-corrupt-message.ini
-rw-r--r-- 1 root root      3416 Aug 18 10:08 config-test-sim.ini
-rw-r--r-- 1 root root      4210 Aug 18 10:08 config-test.ini
drwxr-xr-x 8 root root      4096 Aug 18 11:16 data
-rwxr-xr-x 1 root root 126986272 Aug 18 10:12 dsn_runtime_tests
-rw-r--r-- 1 root root       255 Aug 18 10:08 gtest.filter
-rwxr-xr-x 1 root root      2309 Aug 18 10:08 run.sh
./data/log/log.1.txt
---- tail -n 100 log.1.txt ----
D2022-08-18 11:17:08.825 (1660821428825462404 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_server_wrapper_start
W2022-08-18 11:17:08.825 (1660821428825506904 396) client.io-thrd.00396: server_negotiation.cpp:137:do_challenge(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): negotiation failed, with err = ERR_TIMEOUT, msg = ERR_TIMEOUT
D2022-08-18 11:17:08.825 (1660821428825587105 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_start, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.825 (1660821428825640605 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_OK), frequency: 100%, max_count: -1]
W2022-08-18 11:17:08.825 (1660821428825671006 396) client.io-thrd.00396: negotiation.cpp:65:check_status(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): get message(negotiation_select_mechanisms) while expect(negotiation_initiate)
D2022-08-18 11:17:08.825 (1660821428825724906 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_start, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.825 (1660821428825773907 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_TIMEOUT), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.825 (1660821428825810007 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_server_wrapper_start
D2022-08-18 11:17:08.825 (1660821428825831707 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_wrapper_retrieve_username
W2022-08-18 11:17:08.825 (1660821428825847507 396) client.io-thrd.00396: server_negotiation.cpp:151:do_challenge(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): retrive user name failed: with err = ERR_TIMEOUT, msg = ERR_TIMEOUT
D2022-08-18 11:17:08.825 (1660821428825922908 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_start, task: Return(ERR_SASL_INCOMPLETE), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.826 (1660821428826001009 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.826 (1660821428826035109 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_server_wrapper_start
D2022-08-18 11:17:08.826 (1660821428826107410 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_start, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.826 (1660821428826174110 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.826 (1660821428826206111 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_server_wrapper_start
D2022-08-18 11:17:08.826 (1660821428826221211 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_wrapper_retrieve_username
D2022-08-18 11:17:08.826 (1660821428826233911 396) client.io-thrd.00396: server_negotiation.cpp:167:succ_negotiation(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): negotiation succeed
D2022-08-18 11:17:08.826 (1660821428826592714 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_step, task: Return(ERR_TIMEOUT), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.826 (1660821428826661715 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.826 (1660821428826718416 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_server_wrapper_step
W2022-08-18 11:17:08.826 (1660821428826754116 396) client.io-thrd.00396: server_negotiation.cpp:137:do_challenge(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): negotiation failed, with err = ERR_TIMEOUT, msg = ERR_TIMEOUT
D2022-08-18 11:17:08.826 (1660821428826821917 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_step, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.826 (1660821428826865217 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_OK), frequency: 100%, max_count: -1]
W2022-08-18 11:17:08.826 (1660821428826909517 396) client.io-thrd.00396: negotiation.cpp:65:check_status(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): get message(negotiation_select_mechanisms) while expect(negotiation_challenge_response)
D2022-08-18 11:17:08.826 (1660821428826967318 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_step, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.827 (1660821428827043419 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_TIMEOUT), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.827 (1660821428827075019 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_server_wrapper_step
D2022-08-18 11:17:08.827 (1660821428827089919 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_wrapper_retrieve_username
W2022-08-18 11:17:08.827 (1660821428827104119 396) client.io-thrd.00396: server_negotiation.cpp:151:do_challenge(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): retrive user name failed: with err = ERR_TIMEOUT, msg = ERR_TIMEOUT
D2022-08-18 11:17:08.827 (1660821428827153820 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_step, task: Return(ERR_SASL_INCOMPLETE), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.827 (1660821428827227320 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.827 (1660821428827272021 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_server_wrapper_step
D2022-08-18 11:17:08.827 (1660821428827347822 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_step, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.827 (1660821428827407822 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.827 (1660821428827438922 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_server_wrapper_step
D2022-08-18 11:17:08.827 (1660821428827453023 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_wrapper_retrieve_username
D2022-08-18 11:17:08.827 (1660821428827465023 396) client.io-thrd.00396: server_negotiation.cpp:167:succ_negotiation(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): negotiation succeed
D2022-08-18 11:17:08.827 (1660821428827767325 396) client.io-thrd.00396: tracer.cpp:146:tracer_on_aio_enqueue(): LPC_TASK_TEST AIO.ENQUEUE, task_id = 0100018c00010038, queue size = 1
D2022-08-18 11:17:08.827 (1660821428827792326 396) client.io-thrd.00396.0100018c00010038: tracer.cpp:87:tracer_on_task_begin(): LPC_TASK_TEST EXEC BEGIN, task_id = 0100018c00010038
D2022-08-18 11:17:08.827 (1660821428827823326 396) client.io-thrd.00396.0100018c00010038: tracer.cpp:117:tracer_on_task_end(): LPC_TASK_TEST EXEC END, task_id = 0100018c00010038, err = ERR_OK
D2022-08-18 11:17:08.827 (1660821428827965827 396) client.io-thrd.00396: tracer.cpp:69:tracer_on_task_create(): LPC_TASK_TEST CREATE, task_id = 0100018c00010039, type = TASK_TYPE_AIO
D2022-08-18 11:17:08.827 (1660821428827992628 396) client.io-thrd.00396: tracer.cpp:138:tracer_on_aio_call(): LPC_TASK_TEST AIO.CALL, task_id = 0100018c00010039, offset = 0, size = 128
D2022-08-18 11:17:08.828 (1660821428828007428 396) client.io-thrd.00396: tracer.cpp:69:tracer_on_task_create(): LPC_TASK_TEST CREATE, task_id = 0100018c0001003a, type = TASK_TYPE_AIO
D2022-08-18 11:17:08.828 (1660821428828034928 279) client.THREAD_POOL_DEFAULT1.0100018c0001003a: tracer.cpp:87:tracer_on_task_begin(): LPC_TASK_TEST EXEC BEGIN, task_id = 0100018c0001003a
D2022-08-18 11:17:08.828 (1660821428828057028 279) client.THREAD_POOL_DEFAULT1.0100018c0001003a: tracer.cpp:146:tracer_on_aio_enqueue(): LPC_TASK_TEST AIO.ENQUEUE, task_id = 0100018c00010039, queue size = 1
D2022-08-18 11:17:08.828 (1660821428828069528 279) client.THREAD_POOL_DEFAULT1.0100018c00010039: tracer.cpp:87:tracer_on_task_begin(): LPC_TASK_TEST EXEC BEGIN, task_id = 0100018c00010039
D2022-08-18 11:17:08.828 (1660821428828081128 279) client.THREAD_POOL_DEFAULT1.0100018c00010039: tracer.cpp:117:tracer_on_task_end(): LPC_TASK_TEST EXEC END, task_id = 0100018c00010039, err = ERR_OK
D2022-08-18 11:17:08.828 (1660821428828091429 279) client.THREAD_POOL_DEFAULT1.0100018c0001003a: tracer.cpp:117:tracer_on_task_end(): LPC_TASK_TEST EXEC END, task_id = 0100018c0001003a, err = ERR_OK
E2022-08-18 11:17:08.828 (1660821428828617434 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 0
E2022-08-18 11:17:08.828 (1660821428828647134 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 1
E2022-08-18 11:17:08.828 (1660821428828659934 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 2
E2022-08-18 11:17:08.828 (1660821428828670934 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 3
E2022-08-18 11:17:08.828 (1660821428828686034 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 4
E2022-08-18 11:17:08.828 (1660821428828698134 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 5
E2022-08-18 11:17:08.828 (1660821428828708934 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 6
E2022-08-18 11:17:08.828 (1660821428828727935 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 7
E2022-08-18 11:17:08.828 (1660821428828739935 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 8
E2022-08-18 11:17:08.828 (1660821428828750735 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 9
E2022-08-18 11:17:08.828 (1660821428828761235 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 10
E2022-08-18 11:17:08.828 (1660821428828771835 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 11
E2022-08-18 11:17:08.828 (1660821428828782535 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 12
E2022-08-18 11:17:08.828 (1660821428828793035 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 13
E2022-08-18 11:17:08.828 (1660821428828803535 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 14
E2022-08-18 11:17:08.828 (1660821428828814035 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 15
E2022-08-18 11:17:08.828 (1660821428828828836 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 16
E2022-08-18 11:17:08.828 (1660821428828840736 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 17
E2022-08-18 11:17:08.828 (1660821428828851536 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 18
E2022-08-18 11:17:08.828 (1660821428828862336 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 19
E2022-08-18 11:17:08.828 (1660821428828872736 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 20
E2022-08-18 11:17:08.828 (1660821428828887836 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 21
E2022-08-18 11:17:08.828 (1660821428828909336 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 22
E2022-08-18 11:17:08.828 (1660821428828932737 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 23
E2022-08-18 11:17:08.828 (1660821428828944737 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 24
E2022-08-18 11:17:08.828 (1660821428828955237 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 25
E2022-08-18 11:17:08.828 (1660821428828965737 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 26
E2022-08-18 11:17:08.828 (1660821428828976237 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 27
E2022-08-18 11:17:08.828 (1660821428828986637 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 28
E2022-08-18 11:17:08.828 (1660821428828997137 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 29
E2022-08-18 11:17:08.829 (1660821428829007637 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 30
E2022-08-18 11:17:08.829 (1660821428829043238 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 31
E2022-08-18 11:17:08.829 (1660821428829053938 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 32
E2022-08-18 11:17:08.829 (1660821428829074638 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 33
E2022-08-18 11:17:08.829 (1660821428829086238 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 34
E2022-08-18 11:17:08.829 (1660821428829151739 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 35
E2022-08-18 11:17:08.829 (1660821428829164639 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 36
E2022-08-18 11:17:08.829 (1660821428829179939 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 37
E2022-08-18 11:17:08.829 (1660821428829192539 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 38
E2022-08-18 11:17:08.829 (1660821428829203839 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 39
E2022-08-18 11:17:08.829 (1660821428829215239 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 40
E2022-08-18 11:17:08.829 (1660821428829226439 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 41
E2022-08-18 11:17:08.829 (1660821428829237939 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 42
E2022-08-18 11:17:08.829 (1660821428829259940 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 43
E2022-08-18 11:17:08.829 (1660821428829279240 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 44
E2022-08-18 11:17:08.829 (1660821428829291140 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 45
E2022-08-18 11:17:08.829 (1660821428829302440 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 46
E2022-08-18 11:17:08.829 (1660821428829313640 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 47
E2022-08-18 11:17:08.829 (1660821428829718544 396) client.io-thrd.00396: thrift_message_parser.cpp:113:create_message_from_request_blob(): invalid message type: 2
E2022-08-18 11:17:08.829 (1660821428829813245 396) client.io-thrd.00396: thrift_message_parser.cpp:113:create_message_from_request_blob(): invalid message type: 65
E2022-08-18 11:17:08.830 (1660821428830383750 396) client.io-thrd.00396: thrift_message_parser.cpp:113:create_message_from_request_blob(): invalid message type: 65
Error: Process completed with exit code 1.
```


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5IuqTY/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1036,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5IuqmZ,incubator-pegasus,1220192665,1036,NA,empiredan,743379,Dan Wang,,NA,2022-08-19T03:07:31Z,2022-08-19T03:07:31Z,"https://github.com/apache/incubator-pegasus/runs/7897027671?check_suite_focus=true

```
[----------] 4 tests from tools_common
[ RUN      ] tools_common.asio_net_provider
[       OK ] tools_common.asio_net_provider (11 ms)
[ RUN      ] tools_common.asio_udp_provider
[       OK ] tools_common.asio_udp_provider (1011 ms)
[ RUN      ] tools_common.sim_net_provider
[       OK ] tools_common.sim_net_provider (6 ms)
[ RUN      ] tools_common.asio_network_provider_connection_threshold
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/test/netprovider.cpp:95: Failure
Value of: ERR_TIMEOUT == ec
  Actual: false
Expected: true
[  FAILED  ] tools_common.asio_network_provider_connection_threshold (1078 ms)
[----------] 4 tests from tools_common (2106 ms total)

......

[----------] Global test environment tear-down
[==========] 74 tests from 16 test cases ran. (6163 ms total)
[  PASSED  ] 73 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] tools_common.asio_network_provider_connection_threshold

 1 FAILED TEST
dsn exit with code 1
run dsn_runtime_tests config-test.ini failed
---- ls ----
total 124092
drwxr-xr-x 3 root root      4096 Aug 18 10:09 CMakeFiles
-rw-r--r-- 1 root root     36143 Aug 18 10:09 Makefile
-rwxr-xr-x 1 root root      1238 Aug 18 10:08 clear.sh
-rw-r--r-- 1 root root      1326 Aug 18 10:09 cmake_install.cmake
-rw-r--r-- 1 root root       134 Aug 18 10:08 command.txt
-rw-r--r-- 1 root root      4068 Aug 18 10:08 config-test-corrupt-message.ini
-rw-r--r-- 1 root root      3416 Aug 18 10:08 config-test-sim.ini
-rw-r--r-- 1 root root      4210 Aug 18 10:08 config-test.ini
drwxr-xr-x 8 root root      4096 Aug 18 11:16 data
-rwxr-xr-x 1 root root 126986272 Aug 18 10:12 dsn_runtime_tests
-rw-r--r-- 1 root root       255 Aug 18 10:08 gtest.filter
-rwxr-xr-x 1 root root      2309 Aug 18 10:08 run.sh
./data/log/log.1.txt
---- tail -n 100 log.1.txt ----
D2022-08-18 11:17:08.825 (1660821428825462404 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_server_wrapper_start
W2022-08-18 11:17:08.825 (1660821428825506904 396) client.io-thrd.00396: server_negotiation.cpp:137:do_challenge(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): negotiation failed, with err = ERR_TIMEOUT, msg = ERR_TIMEOUT
D2022-08-18 11:17:08.825 (1660821428825587105 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_start, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.825 (1660821428825640605 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_OK), frequency: 100%, max_count: -1]
W2022-08-18 11:17:08.825 (1660821428825671006 396) client.io-thrd.00396: negotiation.cpp:65:check_status(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): get message(negotiation_select_mechanisms) while expect(negotiation_initiate)
D2022-08-18 11:17:08.825 (1660821428825724906 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_start, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.825 (1660821428825773907 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_TIMEOUT), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.825 (1660821428825810007 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_server_wrapper_start
D2022-08-18 11:17:08.825 (1660821428825831707 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_wrapper_retrieve_username
W2022-08-18 11:17:08.825 (1660821428825847507 396) client.io-thrd.00396: server_negotiation.cpp:151:do_challenge(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): retrive user name failed: with err = ERR_TIMEOUT, msg = ERR_TIMEOUT
D2022-08-18 11:17:08.825 (1660821428825922908 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_start, task: Return(ERR_SASL_INCOMPLETE), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.826 (1660821428826001009 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.826 (1660821428826035109 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_server_wrapper_start
D2022-08-18 11:17:08.826 (1660821428826107410 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_start, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.826 (1660821428826174110 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.826 (1660821428826206111 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_server_wrapper_start
D2022-08-18 11:17:08.826 (1660821428826221211 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_wrapper_retrieve_username
D2022-08-18 11:17:08.826 (1660821428826233911 396) client.io-thrd.00396: server_negotiation.cpp:167:succ_negotiation(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): negotiation succeed
D2022-08-18 11:17:08.826 (1660821428826592714 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_step, task: Return(ERR_TIMEOUT), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.826 (1660821428826661715 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.826 (1660821428826718416 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_server_wrapper_step
W2022-08-18 11:17:08.826 (1660821428826754116 396) client.io-thrd.00396: server_negotiation.cpp:137:do_challenge(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): negotiation failed, with err = ERR_TIMEOUT, msg = ERR_TIMEOUT
D2022-08-18 11:17:08.826 (1660821428826821917 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_step, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.826 (1660821428826865217 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_OK), frequency: 100%, max_count: -1]
W2022-08-18 11:17:08.826 (1660821428826909517 396) client.io-thrd.00396: negotiation.cpp:65:check_status(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): get message(negotiation_select_mechanisms) while expect(negotiation_challenge_response)
D2022-08-18 11:17:08.826 (1660821428826967318 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_step, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.827 (1660821428827043419 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_TIMEOUT), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.827 (1660821428827075019 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_server_wrapper_step
D2022-08-18 11:17:08.827 (1660821428827089919 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_wrapper_retrieve_username
W2022-08-18 11:17:08.827 (1660821428827104119 396) client.io-thrd.00396: server_negotiation.cpp:151:do_challenge(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): retrive user name failed: with err = ERR_TIMEOUT, msg = ERR_TIMEOUT
D2022-08-18 11:17:08.827 (1660821428827153820 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_step, task: Return(ERR_SASL_INCOMPLETE), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.827 (1660821428827227320 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.827 (1660821428827272021 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_server_wrapper_step
D2022-08-18 11:17:08.827 (1660821428827347822 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_server_wrapper_step, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.827 (1660821428827407822 396) client.io-thrd.00396: fail_point.cpp:79:cfg(): add fail_point [name: sasl_wrapper_retrieve_username, task: Return(ERR_OK), frequency: 100%, max_count: -1]
D2022-08-18 11:17:08.827 (1660821428827438922 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_server_wrapper_step
D2022-08-18 11:17:08.827 (1660821428827453023 396) client.io-thrd.00396: fail_point.cpp:155:eval(): fail on sasl_wrapper_retrieve_username
D2022-08-18 11:17:08.827 (1660821428827465023 396) client.io-thrd.00396: server_negotiation.cpp:167:succ_negotiation(): SERVER_NEGOTIATION(CLIENT=127.0.0.1:10086): negotiation succeed
D2022-08-18 11:17:08.827 (1660821428827767325 396) client.io-thrd.00396: tracer.cpp:146:tracer_on_aio_enqueue(): LPC_TASK_TEST AIO.ENQUEUE, task_id = 0100018c00010038, queue size = 1
D2022-08-18 11:17:08.827 (1660821428827792326 396) client.io-thrd.00396.0100018c00010038: tracer.cpp:87:tracer_on_task_begin(): LPC_TASK_TEST EXEC BEGIN, task_id = 0100018c00010038
D2022-08-18 11:17:08.827 (1660821428827823326 396) client.io-thrd.00396.0100018c00010038: tracer.cpp:117:tracer_on_task_end(): LPC_TASK_TEST EXEC END, task_id = 0100018c00010038, err = ERR_OK
D2022-08-18 11:17:08.827 (1660821428827965827 396) client.io-thrd.00396: tracer.cpp:69:tracer_on_task_create(): LPC_TASK_TEST CREATE, task_id = 0100018c00010039, type = TASK_TYPE_AIO
D2022-08-18 11:17:08.827 (1660821428827992628 396) client.io-thrd.00396: tracer.cpp:138:tracer_on_aio_call(): LPC_TASK_TEST AIO.CALL, task_id = 0100018c00010039, offset = 0, size = 128
D2022-08-18 11:17:08.828 (1660821428828007428 396) client.io-thrd.00396: tracer.cpp:69:tracer_on_task_create(): LPC_TASK_TEST CREATE, task_id = 0100018c0001003a, type = TASK_TYPE_AIO
D2022-08-18 11:17:08.828 (1660821428828034928 279) client.THREAD_POOL_DEFAULT1.0100018c0001003a: tracer.cpp:87:tracer_on_task_begin(): LPC_TASK_TEST EXEC BEGIN, task_id = 0100018c0001003a
D2022-08-18 11:17:08.828 (1660821428828057028 279) client.THREAD_POOL_DEFAULT1.0100018c0001003a: tracer.cpp:146:tracer_on_aio_enqueue(): LPC_TASK_TEST AIO.ENQUEUE, task_id = 0100018c00010039, queue size = 1
D2022-08-18 11:17:08.828 (1660821428828069528 279) client.THREAD_POOL_DEFAULT1.0100018c00010039: tracer.cpp:87:tracer_on_task_begin(): LPC_TASK_TEST EXEC BEGIN, task_id = 0100018c00010039
D2022-08-18 11:17:08.828 (1660821428828081128 279) client.THREAD_POOL_DEFAULT1.0100018c00010039: tracer.cpp:117:tracer_on_task_end(): LPC_TASK_TEST EXEC END, task_id = 0100018c00010039, err = ERR_OK
D2022-08-18 11:17:08.828 (1660821428828091429 279) client.THREAD_POOL_DEFAULT1.0100018c0001003a: tracer.cpp:117:tracer_on_task_end(): LPC_TASK_TEST EXEC END, task_id = 0100018c0001003a, err = ERR_OK
E2022-08-18 11:17:08.828 (1660821428828617434 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 0
E2022-08-18 11:17:08.828 (1660821428828647134 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 1
E2022-08-18 11:17:08.828 (1660821428828659934 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 2
E2022-08-18 11:17:08.828 (1660821428828670934 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 3
E2022-08-18 11:17:08.828 (1660821428828686034 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 4
E2022-08-18 11:17:08.828 (1660821428828698134 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 5
E2022-08-18 11:17:08.828 (1660821428828708934 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 6
E2022-08-18 11:17:08.828 (1660821428828727935 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 7
E2022-08-18 11:17:08.828 (1660821428828739935 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 8
E2022-08-18 11:17:08.828 (1660821428828750735 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 9
E2022-08-18 11:17:08.828 (1660821428828761235 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 10
E2022-08-18 11:17:08.828 (1660821428828771835 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 11
E2022-08-18 11:17:08.828 (1660821428828782535 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 12
E2022-08-18 11:17:08.828 (1660821428828793035 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 13
E2022-08-18 11:17:08.828 (1660821428828803535 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 14
E2022-08-18 11:17:08.828 (1660821428828814035 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 15
E2022-08-18 11:17:08.828 (1660821428828828836 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 16
E2022-08-18 11:17:08.828 (1660821428828840736 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 17
E2022-08-18 11:17:08.828 (1660821428828851536 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 18
E2022-08-18 11:17:08.828 (1660821428828862336 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 19
E2022-08-18 11:17:08.828 (1660821428828872736 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 20
E2022-08-18 11:17:08.828 (1660821428828887836 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 21
E2022-08-18 11:17:08.828 (1660821428828909336 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 22
E2022-08-18 11:17:08.828 (1660821428828932737 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 23
E2022-08-18 11:17:08.828 (1660821428828944737 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 24
E2022-08-18 11:17:08.828 (1660821428828955237 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 25
E2022-08-18 11:17:08.828 (1660821428828965737 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 26
E2022-08-18 11:17:08.828 (1660821428828976237 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 27
E2022-08-18 11:17:08.828 (1660821428828986637 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 28
E2022-08-18 11:17:08.828 (1660821428828997137 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 29
E2022-08-18 11:17:08.829 (1660821428829007637 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 30
E2022-08-18 11:17:08.829 (1660821428829043238 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 31
E2022-08-18 11:17:08.829 (1660821428829053938 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 32
E2022-08-18 11:17:08.829 (1660821428829074638 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 33
E2022-08-18 11:17:08.829 (1660821428829086238 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 34
E2022-08-18 11:17:08.829 (1660821428829151739 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 35
E2022-08-18 11:17:08.829 (1660821428829164639 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 36
E2022-08-18 11:17:08.829 (1660821428829179939 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 37
E2022-08-18 11:17:08.829 (1660821428829192539 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 38
E2022-08-18 11:17:08.829 (1660821428829203839 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 39
E2022-08-18 11:17:08.829 (1660821428829215239 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 40
E2022-08-18 11:17:08.829 (1660821428829226439 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 41
E2022-08-18 11:17:08.829 (1660821428829237939 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 42
E2022-08-18 11:17:08.829 (1660821428829259940 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 43
E2022-08-18 11:17:08.829 (1660821428829279240 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 44
E2022-08-18 11:17:08.829 (1660821428829291140 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 45
E2022-08-18 11:17:08.829 (1660821428829302440 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 46
E2022-08-18 11:17:08.829 (1660821428829313640 396) client.io-thrd.00396: thrift_message_parser.cpp:166:parse_request_header(): hdr_length should be 48, but 47
E2022-08-18 11:17:08.829 (1660821428829718544 396) client.io-thrd.00396: thrift_message_parser.cpp:113:create_message_from_request_blob(): invalid message type: 2
E2022-08-18 11:17:08.829 (1660821428829813245 396) client.io-thrd.00396: thrift_message_parser.cpp:113:create_message_from_request_blob(): invalid message type: 65
E2022-08-18 11:17:08.830 (1660821428830383750 396) client.io-thrd.00396: thrift_message_parser.cpp:113:create_message_from_request_blob(): invalid message type: 65
Error: Process completed with exit code 1.
```

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5IuqmZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1038,https://api.github.com/repos/apache/incubator-pegasus/issues/1038,incubator-pegasus,1297650861,1038,"a specific version thrift tool used by java-client, cann't download now",shenxingwuying,6360122,,,CLOSED,2022-07-07T15:33:58Z,2022-07-23T07:10:58Z,"wget http://mirrors.tuna.tsinghua.edu.cn/apache/thrift/0.11.0/thrift-0.11.0.tar.gz

--2022-07-07 23:32:07--  http://mirrors.tuna.tsinghua.edu.cn/apache/thrift/0.11.0/thrift-0.11.0.tar.gz
Resolving mirrors.tuna.tsinghua.edu.cn (mirrors.tuna.tsinghua.edu.cn)... 101.6.15.130, 2402:f000:1:400::2
Connecting to mirrors.tuna.tsinghua.edu.cn (mirrors.tuna.tsinghua.edu.cn)|101.6.15.130|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2022-07-07 23:32:07 ERROR 404: Not Found.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1038/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1040,https://api.github.com/repos/apache/incubator-pegasus/issues/1040,incubator-pegasus,1298552511,1040,"The repo contains some files generated by thrift, I think these files need not  be added into the repo.",shenxingwuying,6360122,,,CLOSED,2022-07-08T06:26:44Z,2022-07-26T01:00:14Z,"The repo contains some files generated by thrift, I think these files need not  be added into the repo.

I'll remove them from cpp and java-client, other clients(go, node...) all depends.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1040/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1040,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5GTUgd,incubator-pegasus,1179469853,1040,NA,shenxingwuying,6360122,,,NA,2022-07-09T03:26:46Z,2022-07-09T03:26:46Z,"The docker image: apache/pegasus:build-env-centos7,  include python.  
```
[root@60b4f6c98078 pegasus]# which python
/usr/bin/python
```

But in the auto checks envs: https://github.com/apache/incubator-pegasus/runs/7255552881?check_suite_focus=true

no python ? 

```
Skip building third-parties...
./run.sh: line 238: python: command not found
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5GTUgd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1043,https://api.github.com/repos/apache/incubator-pegasus/issues/1043,incubator-pegasus,1300162219,1043,Bug: clang-format action doesn't work well,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-07-11T03:38:37Z,2022-07-11T06:31:35Z,"After pull request https://github.com/apache/incubator-pegasus/pull/1022, the clang-format action doesn't work well, because it only run when code changed in non-rdsn directory.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1043/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1047,https://api.github.com/repos/apache/incubator-pegasus/issues/1047,incubator-pegasus,1302139220,1047,use official rocksdb library,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-07-12T14:23:08Z,2023-05-16T16:07:34Z,"For some historical reasons, we use the Xiaomi modified rocksdb library, since Pegasus 2.1.0, we have removed all modifications on rocksdb, only left some code to keep compatiable [1], now before 2.4.0 releasing, it's a chance to switch to the official version.


1. https://github.com/XiaoMi/pegasus-rocksdb/commits/v6.6.4-compatible","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1047/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1052,https://api.github.com/repos/apache/incubator-pegasus/issues/1052,incubator-pegasus,1306832023,1052,Build on MacOS failed because of some recent commits,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-07-16T15:10:36Z,2022-07-19T15:50:09Z,"Because there is no free MacOS based CI plan on GitHub, and some free plan (e.g. CircleCI) is not allowed in Apache repos, so we can only check it work manuallly currently.
Now, I found it build failed on MacOS, I'll try to fix them before 2.4.0 released.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1052/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1053,https://api.github.com/repos/apache/incubator-pegasus/issues/1053,incubator-pegasus,1307124016,1053,Merge rdsn and Pegasus,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-07-17T15:08:57Z,2022-11-08T08:53:56Z,"Although rdsn and Pegasus have been merged to one repo, there are still many work left to do:

- [x] unify th clang-format tools
- [x] remove run.sh in rdsn
- [x] unify the workflows
- [x] unify the CMakelists.txt
- [x] Unify build and test shell scripts to run.sh
- [x] Move some common files out of rdsn directory
- [x] reduce rebuilding time cost
- [x] Unify the cmake of rdsn and pegasus
- [x] move files in 'include' dir of rdsn to the src pf rdsn
- [x] refactor(rdsn): flatten source code directory by moving files from 'src/rdsn/src/*' to 'src/*'","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1053/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1053,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5MgYHA,incubator-pegasus,1283555776,1053,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-10-19T07:29:06Z,2022-10-19T07:29:06Z,"# Refactor directories

After the [xiaomi/rdsn](https://github.com/XiaoMi/rdsn) project has been merged into the Pegasus project, the rdsn source code directory structure is kept as before in the original repository. But it's just a temporary status, we are planning to integrate the two projects closer in the long-term plan.

The following things I'm going to do is moving source code directories from `src/rdsn/src/*` to `src/*`, for example, `src/rdsn/src/runtime/api_task.h` will be moved to `src/runtime/api_task.h`, this change will make the source structure more simple and easier to understand and accept by more new comer of Pegasus.

Of course, it's just one step of the whole refactory plan, the `src/rdsn/include/*` files have been moved to the places where they are more related to by pull request [1]. And there will be more refactor pull requests to modularize the source files in the future.

However, this change will influence hundreds of files, their paths will be changed, even though there are no functional changes. This change is presented by this [2] pull request.

Keep in mind that this is not the final view of the structure, we will do more small adjusticement in the future.

Directories in `src` before this refactor:
```
.
├── base
│   └── test
├── client_lib
├── geo
│   ├── bench
│   ├── lib
│   └── test
├── include
│   ├── pegasus
│   └── rrdb
├── rdsn
│   └── src
│       ├── aio
│       │   └── test
│       ├── block_service
│       │   ├── fds
│       │   ├── hdfs
│       │   ├── local
│       │   └── test
│       ├── client
│       ├── common
│       │   ├── serialization_helper
│       │   └── test
│       ├── failure_detector
│       │   └── test
│       ├── http
│       │   └── test
│       ├── meta
│       │   ├── duplication
│       │   └── test
│       │       ├── balancer_simulator
│       │       ├── meta_state
│       │       └── misc
│       ├── nfs
│       │   └── test
│       ├── perf_counter
│       │   └── test
│       ├── remote_cmd
│       ├── replica
│       │   ├── backup
│       │   │   └── test
│       │   ├── bulk_load
│       │   │   └── test
│       │   ├── duplication
│       │   │   └── test
│       │   ├── split
│       │   │   └── test
│       │   ├── storage
│       │   │   └── simple_kv
│       │   │       └── test
│       │   └── test
│       ├── runtime
│       │   ├── rpc
│       │   ├── security
│       │   ├── task
│       │   └── test
│       ├── tools
│       ├── utils
│       │   ├── absl
│       │   │   ├── base
│       │   │   │   └── internal
│       │   │   └── utility
│       │   ├── hpc_locks
│       │   ├── long_adder_bench
│       │   └── test
│       │       └── nth_element_bench
│       └── zookeeper
│           └── test
├── redis_protocol
│   ├── proxy
│   ├── proxy_lib
│   └── proxy_ut
├── reporter
├── sample
├── server
│   └── test
├── shell
│   ├── commands
│   ├── linenoise
│   └── sds
└── test
    ├── bench_test
    ├── function_test
    │   ├── backup_restore_test
    │   ├── base_api_test
    │   ├── bulk_load_test
    │   ├── detect_hotspot_test
    │   ├── partition_split_test
    │   ├── recovery_test
    │   ├── restore_test
    │   ├── throttle_test
    │   └── utils
    ├── kill_test
    └── pressure_test
```

Directories in `src` after this refactor:
```
.
├── aio
│   └── test
├── base
│   └── test
├── block_service
│   ├── fds
│   ├── hdfs
│   ├── local
│   └── test
├── client
├── client_lib
├── common
│   ├── serialization_helper
│   └── test
├── failure_detector
│   └── test
├── geo
│   ├── bench
│   ├── lib
│   └── test
├── http
│   └── test
├── include
│   ├── pegasus
│   └── rrdb
├── meta
│   ├── duplication
│   └── test
│       ├── balancer_simulator
│       ├── meta_state
│       └── misc
├── nfs
│   └── test
├── perf_counter
│   └── test
├── redis_protocol
│   ├── proxy
│   ├── proxy_lib
│   └── proxy_ut
├── remote_cmd
├── replica
│   ├── backup
│   │   └── test
│   ├── bulk_load
│   │   └── test
│   ├── duplication
│   │   └── test
│   ├── split
│   │   └── test
│   ├── storage
│   │   └── simple_kv
│   │       └── test
│   └── test
├── reporter
├── runtime
│   ├── rpc
│   ├── security
│   ├── task
│   └── test
├── sample
├── server
│   └── test
├── shell
│   ├── commands
│   ├── linenoise
│   └── sds
├── test
│   ├── bench_test
│   ├── function_test
│   │   ├── backup_restore_test
│   │   ├── base_api_test
│   │   ├── bulk_load_test
│   │   ├── detect_hotspot_test
│   │   ├── partition_split_test
│   │   ├── recovery_test
│   │   ├── restore_test
│   │   ├── throttle_test
│   │   └── utils
│   ├── kill_test
│   └── pressure_test
├── tools
├── utils
│   ├── absl
│   │   ├── base
│   │   │   └── internal
│   │   └── utility
│   ├── hpc_locks
│   ├── long_adder_bench
│   └── test
│       └── nth_element_bench
└── zookeeper
    └── test
```

1. https://github.com/apache/incubator-pegasus/pull/1189
2. https://github.com/apache/incubator-pegasus/pull/1190","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5MgYHA/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1053,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5N5PYj,incubator-pegasus,1306850851,1053,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-11-08T08:53:52Z,2022-11-08T08:53:52Z,"Now the ""merge"" work has been completed almostly, the following work is refactory the code.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5N5PYj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1054,https://api.github.com/repos/apache/incubator-pegasus/issues/1054,incubator-pegasus,1307440499,1054,Feature: Integrate with Apache Ranger,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-07-18T05:55:13Z,2023-05-16T16:06:49Z,"Apache Ranger™ [1] is a framework to enable, monitor and manage comprehensive data security across the Hadoop platform. There are many big data components support to integate with Ranger, like HDFS，HBase，Hive，Yarn，Kafka，Kudu.

Now Pegasus supports Kerberos and built-in ACL, but it's a bit of difficult to manage it, we can make Pegasus interact with Ranger to make it easier for management.

1. https://ranger.apache.org/","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1054/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1054,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5GvYEr,incubator-pegasus,1186824491,1054,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-07-18T06:42:18Z,2022-07-18T06:42:18Z,It is also tracked in Ranger community https://issues.apache.org/jira/browse/RANGER-3831,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5GvYEr/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1054,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Gvo5I,incubator-pegasus,1186893384,1054,NA,kirbyzhou,3401630,,,NA,2022-07-18T08:07:00Z,2022-07-18T08:07:00Z,"If I understand it correctly, the ACL model is so simple now. There are two access_controller class.  

- meta_access_controller:

Super User is allowed to do anything. 
All other users are allowed to do the things listed in FLAGS_meta_acl_rpc_allow_list. There is no per-user settings.
Default meta_acl_rpc_allow_list are 
```
RPC_CM_LIST_APPS 
RPC_CM_LIST_NODES 
RPC_CM_CLUSTER_INFO 
RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX
```

- replica_access_controller： 

Super User is allowed to do anything. 
Users in the users list are allowed to do anything. Users are set by 'replica_access_controller.allowed_users' vars in replica::update_ac_allowed_users
There seems no per-user settings too.


It seems we first have to enhance the access controller mech, add per-user / per-table support.
Such as :
{ Table1: { user1: read, user2: read+write }, Table2: { user3: read, user4: read+write } }


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Gvo5I/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1054,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HZ0NG,incubator-pegasus,1197949766,1054,NA,kirbyzhou,3401630,,,NA,2022-07-28T10:18:04Z,2022-07-28T10:18:04Z,"Add a draft of Ranger Service definition
[here](https://issues.apache.org/jira/secure/attachment/13047331/ranger-servicedef-pegasus.json)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HZ0NG/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1054,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5IaIm4,incubator-pegasus,1214810552,1054,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-08-15T09:24:07Z,2022-08-15T09:24:07Z,"The current ACL is described in https://github.com/apache/incubator-pegasus/issues/170 and [Pegasus 安全认证](https://mp.weixin.qq.com/s?__biz=MzkzNzAzNDgyNg==&mid=2247483801&idx=1&sn=0516fcdfb3667b85d03eb5a40293e6de&chksm=c294d4bcf5e35daa67a4eb3317749c7a643e09a54c5c886be407a1105d96c6607c416cb4310c&token=1577740624&lang=en_US#rd)

It can be summarized as following:


operation \ user | super user | table owner | other users
-- | -- | -- | --
query cluster basic info | √ | √ | √
table read and write | √ | √ | ×
cluster control | √ | × | ×

The extended ACL is based on the former design and detailed as following:


operation \ details | ACL symbol | rpc code | resource example | --
-- | -- | -- | -- | --
Global level | -- | -- | -- |  
query cluster,server | metadata | cluster：<br>RPC_CM_LIST_NODES<br>RPC_CM_CLUSTER_INFO<br>RPC_CM_LIST_APPS（can query all tables）<br><br>server:<br>RPC_QUERY_DISK_INFO |   | 
control cluster,server,multi tables | control | cluster+server：<br>RPC_HTTP_SERVICE(http request，has no principal currently)<br><br>set LB level:<br>RPC_CM_CONTROL_META<br><br>recover meta server though replica servers:<br>RPC_CM_START_RECOVERY<br><br>on replica server:<br>migrate replica between disks:<br>RPC_REPLICA_DISK_MIGRATE<br><br>on replica server:<br>add new disks:<br>RPC_ADD_NEW_DISK<br><br>on replica server:<br>detect hot key:<br>RPC_DETECT_HOTKEY<br><br>cluster+server: remote command(include many operations):<br>RPC_CLI_CLI_CALL<br><br>(multi-tables)backup policy's add,modify(maybe removed later):<br>RPC_CM_ADD_BACKUP_POLICY<br>RPC_CM_MODIFY_BACKUP_POLICY | -- | --
Database level | -- | -- | -- | --
query cluster,server | list | cluster:<br>RPC_CM_LIST_APPS(can only query tables in the database) | -- | --
create table | create | RPC_CM_CREATE_APP | db1 | can create tables prefixed by ""db1_""
drop/recall table | drop | RPC_CM_DROP_APP<br>RPC_CM_RECALL_APP | -- | --
manager table - query | metadata | (multi-tables)query backup policy(may be removed later):<br>RPC_CM_QUERY_BACKUP_POLICY<br><br>(single-table)query backup policy:<br>RPC_CM_QUERY_BACKUP_STATUS<br><br>(single-table)query backup from policy status:<br>RPC_CM_QUERY_RESTORE_STATUS<br><br>(single-table)query duplication:<br>RPC_CM_QUERY_DUPLICATION<br><br>(single-table)query partition split:<br>RPC_CM_QUERY_PARTITION_SPLIT<br><br>(single-table)query bulk load:<br>RPC_CM_QUERY_BULK_LOAD_STATUS<br><br>(single-table)query manual compact:<br>RPC_CM_QUERY_MANUAL_COMPACT_STATUS<br><br>(single-table)query RF of a table:<br>RPC_CM_GET_MAX_REPLICA_COUNT | -- | --
manager table - control | control | (single-table)start backup:<br>RPC_CM_START_BACKUP_APP<br>(single-table)control restore from backup:<br>RPC_CM_START_RESTORE<br><br>(single-table)migrate one replica:<br>RPC_CM_PROPOSE_BALANCER<br><br>(single-table)add or modify duplication:<br>RPC_CM_ADD_DUPLICATION<br>RPC_CM_MODIFY_DUPLICATION<br><br>(single-table)update table's envs:<br>RPC_CM_UPDATE_APP_ENV<br><br>(single-table)DDD diagnose for tables:<br>RPC_CM_DDD_DIAGNOSE<br><br>(single-table)start and control partition split:<br>RPC_CM_START_PARTITION_SPLIT<br>RPC_CM_CONTROL_PARTITION_SPLIT<br><br>(single-table)start, clear up and control bulk load:<br>RPC_CM_START_BULK_LOAD<br>RPC_CM_CONTROL_BULK_LOAD<br>RPC_CM_CLEAR_BULK_LOAD<br><br>(single-table)start manual compact:<br>RPC_CM_START_MANUAL_COMPACT<br><br>(single-table)update table's RF:<br>RPC_CM_SET_MAX_REPLICA_COUNT | -- | --
Database/Table level | -- | -- | -- | --
Read data | read | meta server:<br>route info:<br>RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX<br><br>list:<br>RPC_CM_LIST_APPS(can only query privileged tables)<br><br>replica server:<br>server level:<br>replica_stub::on_client_read<br><br>replica level:<br>replica::on_client_read | db1/table1 | can read table db1.table1
Write data | write | replica server:<br>server level:<br>replica_stub::on_client_write<br><br>replica level:<br>replica::on_client_write | db1/* | can write tables prefixed by 'db1'
server internal (not in ACL) | N/A | RPC_CM_CONFIG_SYNC<br>RPC_CM_UPDATE_PARTITION_CONFIGURATION<br>RPC_CM_REPORT_RESTORE_STATUS<br>RPC_CM_DUPLICATION_SYNC<br>RPC_CM_REGISTER_CHILD_REPLICA<br>RPC_CM_NOTIFY_STOP_SPLIT<br>RPC_CM_QUERY_CHILD_STATE<br>RPC_NEGOTIATION<br>RPC_CALL_RAW_MESSAGE<br>RPC_CALL_RAW_SESSION_DISCONNECT<br>RPC_NFS_GET_FILE_SIZE<br>RPC_NFS_COPY<br>RPC_FD_FAILURE_DETECTOR_PING<br>RPC_CALL_RAW_MESSAGE<br>RPC_CALL_RAW_SESSION_DISCONNECT<br>RPC_CONFIG_PROPOSAL<br>RPC_GROUP_CHECK<br>RPC_QUERY_PN_DECREE<br>RPC_QUERY_REPLICA_INFO<br>RPC_QUERY_LAST_CHECKPOINT_INFO<br>RPC_PREPARE<br>RPC_GROUP_CHECK<br>RPC_QUERY_APP_INFO<br>RPC_LEARN<br>RPC_LEARN_COMPLETION_NOTIFY<br>RPC_LEARN_ADD_LEARNER<br>RPC_REMOVE_REPLICARPC_COLD_BACKUP<br>RPC_CLEAR_COLD_BACKUP<br>RPC_SPLIT_NOTIFY_CATCH_UP<br>RPC_SPLIT_UPDATE_CHILD_PARTITION_COUNT<br>RPC_BULK_LOADRPC_GROUP_BULK_LOAD | -- | --

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5IaIm4/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1054,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5IaWe0,incubator-pegasus,1214867380,1054,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-08-15T10:30:59Z,2022-08-15T10:30:59Z,"1. More restrict for common query type requests.
  a. As table above, users should be granted 'metadata' before query cluster info.
2. MetaServer have to support table level ACL
  a. For example, query route info of a table (i.e. RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX)  must be in ACL
3. To support 'database' level ACL:
  **a. on MetaServer:**
    i. Handle requests on MetaServer, they are listed in the table above, 'Database level.list/create/drop/metadata/control'.
    ii. Add unordered_map<'table_prefix', unordered_set<user_name>> structure.
    iii. Parse table name from request messages when handle requests.
  **a. on ReplicaServer:**
    i. Add unordered_map<'table_prefix', unordered_set<user_name>> structure too.
    ii. Because ""table prefix"" string doesn't belong to any tables/replica, so besides replica envs, we have to add an extra server level envs for ACL.
    iii. We have to maintainance the relationship between table id and table name carefully.
    iv. Parse table id and transfer it to table name from request messages when handle requests.
4. Implemention
  a. The leader meta_server request ACL details from Apache Ranger though HTTP periodically.
  b. Parse the JSON formatted response to internal required structure.
  c. Set the structure to MetaServer and remote Zookeeper.
  d. Set to each tables.
  e. Send envs info to ReplciaServers though CconfigSync RPC.
5. The relationship between table name and database name:
  a. Use che '.' to split table name, suppose the part before '.' is database name.
  b. When bootstrap, for the tables already created, if they are not match the new naming rule, consider they are in the ""default"" database.
  c. When ACL enabled, it's not allowed to create table with name which not matched the rule.
  d. For the rename operation, it's not allowed to modify the prefix.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5IaWe0/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1054,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Iaiyw,incubator-pegasus,1214917808,1054,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-08-15T11:39:04Z,2022-08-15T11:39:04Z,"1. 需更严格的普通查看类请求：
  a. 如上表格，“Global级别权限”的“metadata”权限也需要ACL
2. meta server上增加表级的ACL：
  a. 如获取表的路由信息（RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX），也需做ACL
3. 为支持database（前缀）粒度的控制：
  **a. meta server上的：**
  i. 针对如上列表中的“Database级别权限”的在metaserver上处理的请求
  ii. 需增加unordered_map<表前缀, unordered_set<用户名>>的结构
  iii. 在各个请求响应函数中，解析到请求的”表名“之后加以判断
  **replica server上的：**
  i. 需增加与meta server上相似的数据结构（unordered_map<表前缀, unordered_set<用户名>>）
  ii. 因为”表前缀”并不属于任何表，所以除了表级的envs外，还需增加server级的envs，用于鉴权
  iii. 因为在replica server上并不维护“表名”，而只有“表id”，所以还需增加表id与表名的映射关系
  iv. 在各个请求处理函数中，解析到请求的”表id“之后，映射为表名，再加以判断
4. 实现：
  a. leader meta_server定期地通过http请求从ranger获取pegasus的ACL
  b. 解析Json格式的ACL成需要的数据结构
  c. 设置到meta server自身的acl结构中，也存储到远端存储（即zookeeper）
  d. 设置到各个表的envs中
  e. 后续通过meta server → replica server的同步，将envs下发到各个replica server上
5. 表名与database名的映射关系：
  a. 通过符号“.”来划分database名，“.”之前的即为database name
  b. 启动时，对于已创建的表，如果不符合分割方式，则他处于“default” database中
  c. 开启ACL后，再创建不符合分割规则的表则报错
  d. 对于rename操作，不允许修改前缀","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Iaiyw/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1054,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5IlWZu,incubator-pegasus,1217750638,1054,NA,kirbyzhou,3401630,,,NA,2022-08-17T09:26:11Z,2022-08-17T09:26:11Z,"We have finished the service definition in ranger according to this draft.
See https://issues.apache.org/jira/browse/RANGER-3831
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5IlWZu/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1054,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Vy3pb,incubator-pegasus,1439398491,1054,NA,WHBANG,38547944,,,NA,2023-02-22T03:45:17Z,2023-02-22T03:45:17Z,"Introduce the implementation and how to use:

1. The class diagram
![image](https://user-images.githubusercontent.com/38547944/218993763-91a7072b-086b-44fa-9041-636c5ea1d089.png)


First, you need to add ACL related configurations. The client configuration has not changed, the server has added new configurations:
```
enable_ranger_acl: indicates whether to use ranger for acl
ranger_service_url： ranger server url
ranger_service_name: use ranger policy name
mandatory_enable_acl: mandatory use range policy, currently used for testing
```
The details are as follows:
```
server
[security]
  update_ranger_policy_interval_sec
[ranger]
  ranger_service_url
  ranger_service_name
  ranger_legacy_table_database_mapping_rule
  mandatory_enable_acl
[security]
  enable_auth = true
  krb5_keytab = /root/apache/pegasus.keytab
  krb5_config = /etc/krb5.conf
  krb5_principal = XXXXX
  sasl_plugin_path = /root/apache/incubator-pegasus/thirdparty/output/lib/sasl2
  service_fqdn = XXXXX
  service_name = XXXXX
  mandatory_auth = true
  enable_acl = true
  super_users =
  meta_acl_rpc_allow_list =
  enable_ranger_acl = true
```
```
java client
java
meta_servers = 127.0.0.1:34601,127.0.0.1:34602,127.0.0.1:34603
operation_timeout = 5000
async_workers = 4
enable_perf_counter = false
perf_counter_tags = cluster=onebox,app=unit_test
push_counter_interval_secs = 10
meta_query_timeout = 5000
auth_protocol = kerberos
kerberos_service_name = XXXXX
kerberos_service_fqdn = XXXXX
kerberos_keytab = /root/apache/pegasus.keytab
kerberos_principal = XXXXX
```
```
shell
[security]
  enable_auth = true
  krb5_keytab = /root/apache/pegasus.keytab
  krb5_config = /etc/krb5.conf
  krb5_principal = XXXXX
  sasl_plugin_path = /root/apache/incubator-pegasus/thirdparty/output/lib/sasl2
  service_fqdn = XXXXX
  service_name = XXXXX
```
Second compatibility:
Retained the old ACL mode
1. Use the old ACL
```
enable_acl = true
enable_ranger_acl = false
```
2. user ranger for ACL
```
enable_acl = true
enable_ranger_acl = true
```

3. Third, define the ranger policy

- Pegasus resources can be divided into multiple types, and the operation types of each resource can also be divided. One operation type corresponds to one ACL symbol
<img width=""773"" alt=""image"" src=""https://user-images.githubusercontent.com/38547944/204430376-17e8ae9a-bdac-466c-a2a7-d4f27cae09e0.png"">

- ACLs on each type of resource correspond to specific rpc_code

<img width=""646"" alt=""image"" src=""https://user-images.githubusercontent.com/38547944/204430797-5b45e079-84c5-4f3e-a470-db063d6d87d5.png"">
<img width=""846"" alt=""image"" src=""https://user-images.githubusercontent.com/38547944/204430865-ffc16771-90ca-43c7-903a-5451cb58c86c.png"">
<img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/38547944/204430917-febf6ba1-eddc-4db0-b428-a00e7e651828.png"">

4. pegasus+ranger

After completing the integration of ranger with pegasus, you can set permissions on the ranger web page according to your own needs
<img width=""444"" alt=""image"" src=""https://user-images.githubusercontent.com/38547944/204432012-230db07c-47d0-4960-9c10-04340c19b2ff.png"">
<img width=""1382"" alt=""image"" src=""https://user-images.githubusercontent.com/38547944/204432186-344267c5-d966-4b21-99f5-cc6d4ade55db.png"">
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Vy3pb/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1054,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5VztAp,incubator-pegasus,1439617065,1054,NA,kirbyzhou,3401630,,,NA,2023-02-22T08:30:20Z,2023-02-22T08:30:20Z,"See https://issues.apache.org/jira/browse/RANGER-3831
You have to import the service definition into RANGER at first.

Using the REST API of ranger:

curl -X 'POST' \
'http://rangerhost:6080/service/public/v2/api/servicedef' \  -H
'Accept: application/json' \  -H 'Content-Type: application/json' \
-u 'admin:adminpassword' \
  -d ***@***.***'



WHBANG ***@***.***> 于2023年2月22日周三 11:45写道：

> Introduce the implementation and how to use:
>
>    1. The class diagram
>    [image: image]
>    <https://user-images.githubusercontent.com/38547944/218993763-91a7072b-086b-44fa-9041-636c5ea1d089.png>
>
> First, you need to add ACL related configurations. The client
> configuration has not changed, the server has added new configurations:
>
> enable_ranger_acl: indicates whether to use ranger for acl
>
> ranger_service_url： ranger server url
>
> ranger_service_name: use ranger policy name
>
> mandatory_enable_acl: mandatory use range policy, currently used for testing
>
>
> The details are as follows:
>
> server
>
> [security]
>
>   update_ranger_policy_interval_sec
>
> [ranger]
>
>   ranger_service_url
>
>   ranger_service_name
>
>   ranger_legacy_table_database_mapping_rule
>
>   mandatory_enable_acl
>
> [security]
>
>   enable_auth = true
>
>   krb5_keytab = /root/apache/pegasus.keytab
>
>   krb5_config = /etc/krb5.conf
>
>   krb5_principal = XXXXX
>
>   sasl_plugin_path = /root/apache/incubator-pegasus/thirdparty/output/lib/sasl2
>
>   service_fqdn = XXXXX
>
>   service_name = XXXXX
>
>   mandatory_auth = true
>
>   enable_acl = true
>
>   super_users =
>
>   meta_acl_rpc_allow_list =
>
>   enable_ranger_acl = true
>
>
> java client
>
> java
>
> meta_servers = 127.0.0.1:34601,127.0.0.1:34602,127.0.0.1:34603
>
> operation_timeout = 5000
>
> async_workers = 4
>
> enable_perf_counter = false
>
> perf_counter_tags = cluster=onebox,app=unit_test
>
> push_counter_interval_secs = 10
>
> meta_query_timeout = 5000
>
> auth_protocol = kerberos
>
> kerberos_service_name = XXXXX
>
> kerberos_service_fqdn = XXXXX
>
> kerberos_keytab = /root/apache/pegasus.keytab
>
> kerberos_principal = XXXXX
>
>
> shell
>
> [security]
>
>   enable_auth = true
>
>   krb5_keytab = /root/apache/pegasus.keytab
>
>   krb5_config = /etc/krb5.conf
>
>   krb5_principal = XXXXX
>
>   sasl_plugin_path = /root/apache/incubator-pegasus/thirdparty/output/lib/sasl2
>
>   service_fqdn = XXXXX
>
>   service_name = XXXXX
>
>
> Second compatibility:
> Retained the old ACL mode
>
>    1. Use the old ACL
>
> enable_acl = true
>
> enable_ranger_acl = false
>
>
>
>    1. user ranger for ACL
>
> enable_acl = true
>
> enable_ranger_acl = true
>
>
>
>    1. Third, define the ranger policy
>
>
>    - Pegasus resources can be divided into multiple types, and the
>    operation types of each resource can also be divided. One operation type
>    corresponds to one ACL symbol
>
> [image: image]
> <https://user-images.githubusercontent.com/38547944/204430376-17e8ae9a-bdac-466c-a2a7-d4f27cae09e0.png>
>
>    - ACLs on each type of resource correspond to specific rpc_code
>
> [image: image]
> <https://user-images.githubusercontent.com/38547944/204430797-5b45e079-84c5-4f3e-a470-db063d6d87d5.png>
>
> [image: image]
> <https://user-images.githubusercontent.com/38547944/204430865-ffc16771-90ca-43c7-903a-5451cb58c86c.png>
>
> [image: image]
> <https://user-images.githubusercontent.com/38547944/204430917-febf6ba1-eddc-4db0-b428-a00e7e651828.png>
>
>    1. pegasus+ranger
>
> After completing the integration of ranger with pegasus, you can set
> permissions on the ranger web page according to your own needs
> [image: image]
> <https://user-images.githubusercontent.com/38547944/204432012-230db07c-47d0-4960-9c10-04340c19b2ff.png>
> [image: image]
> <https://user-images.githubusercontent.com/38547944/204432186-344267c5-d966-4b21-99f5-cc6d4ade55db.png>
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/apache/incubator-pegasus/issues/1054#issuecomment-1439398491>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAZ6PHW5QXYPU2GTDNQM55LWYWDVRANCNFSM533BJVLA>
> .
> You are receiving this because you commented.Message ID:
> ***@***.***>
>
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5VztAp/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1055,https://api.github.com/repos/apache/incubator-pegasus/issues/1055,incubator-pegasus,1307456513,1055,the issue template not work,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-07-18T06:16:57Z,2022-07-26T00:59:16Z,"The github issue template not work some days ago, we should fix and improve it.
The current template files: https://github.com/apache/incubator-pegasus/tree/master/.github/ISSUE_TEMPLATE
Document of how to use: https://docs.github.com/en/communities/using-templates-to-encourage-useful-issues-and-pull-requests/about-issue-and-pull-request-templates","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1055/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1056,https://api.github.com/repos/apache/incubator-pegasus/issues/1056,incubator-pegasus,1307528493,1056,rdsn directory cannot be found while unpacking prebuilt third-parties,empiredan,743379,Dan Wang,,CLOSED,2022-07-18T07:36:01Z,2022-07-18T09:08:45Z,"While executing the **Unpack prebuilt third-parties** step for the tests `test_rdsn_Release`,  `test_rdsn_ASAN`, `test_rdsn_LSAN` and `test_rdsn_UBSAN` from `lint_and_test_cpp.yaml`, it will report the error as below:
```
Run unzip /root/thirdparties-bin.zip -d ./thirdparty
  unzip /root/thirdparties-bin.zip -d ./thirdparty
  shell: bash --noprofile --norc -e -o pipefail {0}
OCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: chdir to cwd (""/__w/incubator-pegasus/incubator-pegasus/rdsn"") set in config.json failed: no such file or directory: unknown
Error: Process completed with exit code 1[2](https://github.com/apache/incubator-pegasus/runs/7383509674?check_suite_focus=true#step:5:2)6.
```

For details please see https://github.com/apache/incubator-pegasus/runs/7383509674?check_suite_focus=true.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1056/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1058,https://api.github.com/repos/apache/incubator-pegasus/issues/1058,incubator-pegasus,1307606046,1058,Feature: the conf server_list of the meta server does not support use fqdn:port,WHBANG,38547944,,,CLOSED,2022-07-18T08:47:52Z,2022-07-26T01:02:57Z,"When the IP address of the server changes, you need to manually change the IP conf of pegasus to tell pegasus to use the new IP address. but if use fqdn instead of ip can avoid these things.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1058/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1059,https://api.github.com/repos/apache/incubator-pegasus/issues/1059,incubator-pegasus,1307626296,1059,"thirdparty/CMakeLists.txt missing """" when it detect MACOS_OPENSSL_ROOT_DIR",kirbyzhou,3401630,,,CLOSED,2022-07-18T09:06:09Z,2022-08-01T10:30:41Z,"When MACOS_OPENSSL_ROOT_DIR is not defined, it gives Inexplicable error message：

```
] cd thirdparty
] cmake .
....
CMake Error at CMakeLists.txt:31 (if):
  if given arguments:

    ""STREQUAL"" """"

  Unknown arguments specified

```



Simple FIx:

```
diff --git a/thirdparty/CMakeLists.txt b/thirdparty/CMakeLists.txt
index 1d153f82f..618305fe2 100644
--- a/thirdparty/CMakeLists.txt
+++ b/thirdparty/CMakeLists.txt
@@ -28,7 +28,7 @@ if (""${CMAKE_CXX_COMPILER_ID}"" STREQUAL ""GNU"")
 endif ()
 
 if (APPLE)
-    if (${MACOS_OPENSSL_ROOT_DIR} STREQUAL """")
+    if (""${MACOS_OPENSSL_ROOT_DIR}"" STREQUAL """")
         message(FATAL_ERROR ""OpenSSL root should be set for MacOS"")
     endif()
 endif()
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1059/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1059,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5G4fAg,incubator-pegasus,1189212192,1059,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-07-19T15:40:20Z,2022-07-19T15:40:20Z,"@kirbyzhou thanks for your report, could you help to fix it?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5G4fAg/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1060,https://api.github.com/repos/apache/incubator-pegasus/issues/1060,incubator-pegasus,1307714681,1060,Core dumped for unit test meta_bulk_load_http_test.start_compaction_test,empiredan,743379,Dan Wang,,OPEN,2022-07-18T10:08:14Z,2022-07-27T16:38:09Z,"While executing `meta_bulk_load_http_test.start_compaction_test` core dumped as follows:
```
[----------] 2 tests from meta_http_service_test
[ RUN      ] meta_http_service_test.get_app_from_primary
[       OK ] meta_http_service_test.get_app_from_primary (671 ms)
[ RUN      ] meta_http_service_test.get_app_envs
[       OK ] meta_http_service_test.get_app_envs (668 ms)
[----------] 2 tests from meta_http_service_test (1339 ms total)
[----------] 1 test from meta_backup_test_base
[ RUN      ] meta_backup_test_base.get_backup_policy
[       OK ] meta_backup_test_base.get_backup_policy (1669 ms)
[----------] 1 test from meta_backup_test_base (1669 ms total)
[----------] 4 tests from meta_bulk_load_http_test
[ RUN      ] meta_bulk_load_http_test.start_bulk_load_request
[       OK ] meta_bulk_load_http_test.start_bulk_load_request (1174 ms)
[ RUN      ] meta_bulk_load_http_test.query_bulk_load_request
E2022-07-18 09:50:29.361 (1658137829361131403 9503) test_meta.THREAD_POOL_DEFAULT0.0000250600010001: meta_bulk_load_service.cpp:1537:on_query_bulk_load_status(): app(app_not_exist) is not existed or not available
[       OK ] meta_bulk_load_http_test.query_bulk_load_request (2172 ms)
[ RUN      ] meta_bulk_load_http_test.start_compaction_test
got signal id: 11
Segmentation fault (core dumped)
ERROR: run dsn.meta.test failed, return_code = 139
Error: Process completed with exit code 1.
```

For details please see the failed workflows:
* https://github.com/apache/incubator-pegasus/runs/7386305679?check_suite_focus=true
* https://github.com/apache/incubator-pegasus/runs/7389264172?check_suite_focus=true
* https://github.com/apache/incubator-pegasus/runs/7401117011?check_suite_focus=true
* https://github.com/apache/incubator-pegasus/runs/7500806207?check_suite_focus=true","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1060/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1060,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HWKNZ,incubator-pegasus,1196991321,1060,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-07-27T16:38:09Z,2022-07-27T16:38:09Z,"Maybe also related to `meta_bulk_load_http_test.update_scenario_test` test Segmentation fault.
```
[----------] 4 tests from meta_bulk_load_http_test
[ RUN      ] meta_bulk_load_http_test.start_bulk_load_request
[       OK ] meta_bulk_load_http_test.start_bulk_load_request (1195 ms)
[ RUN      ] meta_bulk_load_http_test.query_bulk_load_request
E2022-07-27 16:28:50.253 (1658939330253401962 9510) test_meta.THREAD_POOL_DEFAULT0.0000250d00010001: meta_bulk_load_service.cpp:1537:on_query_bulk_load_status(): app(app_not_exist) is not existed or not available
[       OK ] meta_bulk_load_http_test.query_bulk_load_request (2196 ms)
[ RUN      ] meta_bulk_load_http_test.start_compaction_test
[       OK ] meta_bulk_load_http_test.start_compaction_test (1196 ms)
[ RUN      ] meta_bulk_load_http_test.update_scenario_test
got signal id: 11
Segmentation fault (core dumped)
Error: Process completed with exit code 139.
```
See https://github.com/apache/incubator-pegasus/runs/7543987315?check_suite_focus=true","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HWKNZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1063,https://api.github.com/repos/apache/incubator-pegasus/issues/1063,incubator-pegasus,1308820098,1063,Unit test failed: run dsn.zookeeper.tests,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-07-19T00:08:44Z,2022-07-19T15:37:26Z,"See more details: https://github.com/apache/incubator-pegasus/runs/7399780702?check_suite_focus=true
```
====================== run dsn.zookeeper.tests ==========================
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/bin/dsn.zookeeper.tests /__w/incubator-pegasus/incubator-pegasus
W2022-07-18 23:52:57.784 (1658188377784062016 5266) unknown.io-thrd.05266: rpc_engine.cpp:509:start(): [add_server1] network server started at port 20201, channel = RPC_CHANNEL_TCP, ...
W2022-07-18 23:52:57.784 (1658188377784194518 5266) unknown.io-thrd.05266: rpc_engine.cpp:509:start(): [add_server1] network server started at port 20201, channel = RPC_CHANNEL_UDP, ...
W2022-07-18 23:52:57.784 (1658188377784276520 5286) add_server1.io-thrd.05286: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
W2022-07-18 23:52:57.784 (1658188377784389022 5289) add_server1.io-thrd.05289: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
W2022-07-18 23:52:57.784 (1658188377784415222 5290) add_server1.io-thrd.05290: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
W2022-07-18 23:52:57.785 (1658188377785510042 5266) unknown.io-thrd.05266: rpc_engine.cpp:509:start(): [add_server2] network server started at port 20202, channel = RPC_CHANNEL_TCP, ...
W2022-07-18 23:52:57.785 (1658188377785628444 5266) unknown.io-thrd.05266: rpc_engine.cpp:509:start(): [add_server2] network server started at port 20202, channel = RPC_CHANNEL_UDP, ...
W2022-07-18 23:52:57.785 (1658188377785685245 5309) add_server2.io-thrd.05309: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
W2022-07-18 23:52:57.785 (1658188377785837647 5312) add_server2.io-thrd.05312: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
W2022-07-18 23:52:57.785 (1658188377785858848 5313) add_server2.io-thrd.05313: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
W2022-07-18 23:52:57.786 (1658188377786979668 5266) unknown.io-thrd.05266: rpc_engine.cpp:509:start(): [add_server3] network server started at port 20203, channel = RPC_CHANNEL_TCP, ...
W2022-07-18 23:52:57.787 (1658188377787113070 5266) unknown.io-thrd.05266: rpc_engine.cpp:509:start(): [add_server3] network server started at port 20203, channel = RPC_CHANNEL_UDP, ...
W2022-07-18 23:52:57.787 (1658188377787171771 5332) add_server3.io-thrd.05332: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
W2022-07-18 23:52:57.787 (1658188377787358975 5336) add_server3.io-thrd.05336: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
W2022-07-18 23:52:57.787 (1658188377787828083 5335) add_server3.io-thrd.05335: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
E2022-07-18 23:52:57.788 (1658188377788781700 5266) unknown.io-thrd.05266: asio_net_provider.cpp:357:start(): asio udp socket bind failed, port = 60615, error = Address already in use
W2022-07-18 23:52:57.788 (1658188377788960403 5351) client.io-thrd.05351: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
W2022-07-18 23:52:57.789 (1658188377789076305 5354) client.io-thrd.05354: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
W2022-07-18 23:52:57.789 (1658188377789209307 5357) client.io-thrd.05357: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
W2022-07-18 23:52:57.789 (1658188377789454712 5360) client.io-thrd.05360: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
W2022-07-18 23:52:57.789 (1658188377789590214 5363) client.io-thrd.05363: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
W2022-07-18 23:52:57.789 (1658188377789708816 5366) client.io-thrd.05366: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
W2022-07-18 23:52:57.789 (1658188377789737517 5367) client.io-thrd.05367: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
[==========] Running 2 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 2 tests from distributed_lock_service_zookeeper
[ RUN      ] distributed_lock_service_zookeeper.simple_lock_unlock
W2022-07-18 23:53:29.792 (1658188409792899270 5333) add_server3.THREAD_POOL_DEFAULT0.0000149200010003: distributed_lock_service_zookeeper.cpp:108:initialize(): attach to zookeeper session timeout, distributed lock service initialized failed
F2022-07-18 23:53:29.792 (1658188409792907070 5333) add_server3.THREAD_POOL_DEFAULT0.0000149200010003: distributed_lock_zookeeper.cpp:67:start(): assertion expression: err == ERR_OK
F2022-07-18 23:53:29.792 (1658188409792913770 5333) add_server3.THREAD_POOL_DEFAULT0.0000149200010003: distributed_lock_zookeeper.cpp:67:start(): err = ERR_TIMEOUT
./run.sh: line 34:  5266 Aborted                 (core dumped) GTEST_OUTPUT=""xml:${output_xml}"" ./dsn.zookeeper.tests config-test.ini
run dsn.zookeeper.tests failed
---- ls ----
total 200320
drwxr-xr-x 3 root root      4096 Jul 18 23:25 CMakeFiles
-rw-r--r-- 1 root root     10697 Jul 18 23:25 Makefile
-rwxr-xr-x 1 root root      1223 Jul 18 23:25 clear.sh
-rw-r--r-- 1 root root      1328 Jul 18 23:25 cmake_install.cmake
-rw-r--r-- 1 root root      3199 Jul 18 23:25 config-test.ini
drwxr-xr-x 7 root root      4096 Jul 18 23:52 data
-rwxr-xr-x 1 root root  56738504 Jul 18 23:31 dsn.zookeeper.tests
-rwxr-xr-x 1 root root      1813 Jul 18 23:25 run.sh
-rw-r--r-- 1 root root 148348868 Jul 18 23:53 zoolog.log
./data/log/log.1.txt
---- tail -n 100 log.1.txt ----
D2022-07-18 23:52:57.784 (1658188377784978532 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
D2022-07-18 23:52:57.784 (1658188377784982432 5266) unknown.io-thrd.05266: rpc_engine.cpp:480:start(): [add_server2] network client started at port 2, channel = RPC_CHANNEL_TCP, fmt = NET_HDR_THRIFT ...
I2022-07-18 23:52:57.785 (1658188377785015533 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.785 (16581883777850[2053](https://github.com/apache/incubator-pegasus/runs/7399780702?check_suite_focus=true#step:7:2054)3 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
D2022-07-18 23:52:57.785 (1658188377785110534 5266) unknown.io-thrd.05266: rpc_engine.cpp:480:start(): [add_server2] network client started at port 2, channel = RPC_CHANNEL_UDP, fmt = NET_HDR_THRIFT ...
I2022-07-18 23:52:57.785 (1658188377785210836 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.785 (1658188377785217336 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
D2022-07-18 23:52:57.785 (1658188377785221136 5266) unknown.io-thrd.05266: rpc_engine.cpp:480:start(): [add_server2] network client started at port 2, channel = RPC_CHANNEL_TCP, fmt = NET_HDR_RAW ...
I2022-07-18 23:52:57.785 (1658188377785253137 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.785 (1658188377785258237 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
D2022-07-18 23:52:57.785 (1658188377785357239 5266) unknown.io-thrd.05266: rpc_engine.cpp:480:start(): [add_server2] network client started at port 2, channel = RPC_CHANNEL_UDP, fmt = NET_HDR_RAW ...
I2022-07-18 23:52:57.785 (1658188377785460141 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.785 (1658188377785466941 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
W2022-07-18 23:52:57.785 (1658188377785505141 5266) unknown.io-thrd.05266: rpc_engine.cpp:509:start(): [add_server2] network server started at port 20202, channel = RPC_CHANNEL_TCP, ...
I2022-07-18 23:52:57.785 (1658188377785542742 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.785 (1658188377785547742 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
W2022-07-18 23:52:57.785 (1658188377785622544 5266) unknown.io-thrd.05266: rpc_engine.cpp:509:start(): [add_server2] network server started at port 20202, channel = RPC_CHANNEL_UDP, ...
D2022-07-18 23:52:57.785 (1658188377785631344 5266) unknown.io-thrd.05266: rpc_engine.cpp:517:start(): === service_node=[add_server2], primary_address=[172.18.0.2:20202] ===
W2022-07-18 23:52:57.785 (1658188377785671544 5309) add_server2.io-thrd.05309: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
D2022-07-18 23:52:57.785 (1658188377785777146 5266) unknown.io-thrd.05266: task_engine.cpp:100:start(): [add_server2]: thread pool [THREAD_POOL_DEFAULT] started, pool_code = THREAD_POOL_DEFAULT, worker_count = 2, worker_share_core = true, partitioned = false, ...
W2022-07-18 23:52:57.785 (1658188377785824847 5312) add_server2.io-thrd.05312: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
W2022-07-18 23:52:57.785 (1658188377785851848 5313) add_server2.io-thrd.05313: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
D2022-07-18 23:52:57.785 (1658188377785938449 5266) unknown.io-thrd.05266: task_engine.cpp:100:start(): [add_server2]: thread pool [THREAD_POOL_DLOCK] started, pool_code = THREAD_POOL_DLOCK, worker_count = 2, worker_share_core = true, partitioned = true, ...
D2022-07-18 23:52:57.785 (1658188377785945749 5266) unknown.io-thrd.05266: task_engine.cpp:242:start(): [add_server2]: task engine started
I2022-07-18 23:52:57.786 (1658188377786224654 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.786 (1658188377786235654 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
D2022-07-18 23:52:57.786 (1658188377786245755 5266) unknown.io-thrd.05266: rpc_engine.cpp:480:start(): [add_server3] network client started at port 3, channel = RPC_CHANNEL_TCP, fmt = NET_HDR_DSN ...
I2022-07-18 23:52:57.786 (1658188377786284155 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.786 (1658188377786289455 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
D2022-07-18 23:52:57.786 (1658188377786383057 5266) unknown.io-thrd.05266: rpc_engine.cpp:480:start(): [add_server3] network client started at port 3, channel = RPC_CHANNEL_UDP, fmt = NET_HDR_DSN ...
I2022-07-18 23:52:57.786 (1658188377786476059 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.786 (1658188377786482559 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
D2022-07-18 23:52:57.786 (1658188377786486559 5266) unknown.io-thrd.05266: rpc_engine.cpp:480:start(): [add_server3] network client started at port 3, channel = RPC_CHANNEL_TCP, fmt = NET_HDR_THRIFT ...
I2022-07-18 23:52:57.786 (1658188377786520260 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.786 (1658188377786525460 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
D2022-07-18 23:52:57.786 (1658188377786606861 5266) unknown.io-thrd.05266: rpc_engine.cpp:480:start(): [add_server3] network client started at port 3, channel = RPC_CHANNEL_UDP, fmt = NET_HDR_THRIFT ...
I2022-07-18 23:52:57.786 (1658188377786698763 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.786 (1658188377786705363 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
D2022-07-18 23:52:57.786 (1658188377786709263 5266) unknown.io-thrd.05266: rpc_engine.cpp:480:start(): [add_server3] network client started at port 3, channel = RPC_CHANNEL_TCP, fmt = NET_HDR_RAW ...
I2022-07-18 23:52:57.786 (1658188377786741564 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.786 (1658188377786746664 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
D2022-07-18 23:52:57.786 (1658188377786826265 5266) unknown.io-thrd.05266: rpc_engine.cpp:480:start(): [add_server3] network client started at port 3, channel = RPC_CHANNEL_UDP, fmt = NET_HDR_RAW ...
I2022-07-18 23:52:57.786 (1658188377786918967 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.786 (1658188377786925467 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
W2022-07-18 23:52:57.786 (1658188377786974768 5266) unknown.io-thrd.05266: rpc_engine.cpp:509:start(): [add_server3] network server started at port 20203, channel = RPC_CHANNEL_TCP, ...
I2022-07-18 23:52:57.787 (1658188377787012368 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.787 (1658188377787017268 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
W2022-07-18 23:52:57.787 (1658188377787107070 5266) unknown.io-thrd.05266: rpc_engine.cpp:509:start(): [add_server3] network server started at port 20203, channel = RPC_CHANNEL_UDP, ...
D2022-07-18 23:52:57.787 (1658188377787116070 5266) unknown.io-thrd.05266: rpc_engine.cpp:517:start(): === service_node=[add_server3], primary_address=[172.18.0.2:20203] ===
W2022-07-18 23:52:57.787 (1658188377787158971 5332) add_server3.io-thrd.05332: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
D2022-07-18 23:52:57.787 (1658188377787238772 5266) unknown.io-thrd.05266: task_engine.cpp:100:start(): [add_server3]: thread pool [THREAD_POOL_DEFAULT] started, pool_code = THREAD_POOL_DEFAULT, worker_count = 2, worker_share_core = true, partitioned = false, ...
W2022-07-18 23:52:57.787 (1658188377787348674 5336) add_server3.io-thrd.05336: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
W2022-07-18 23:52:57.787 (1658188377787810483 5335) add_server3.io-thrd.05335: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
D2022-07-18 23:52:57.787 (1658188377787855383 5266) unknown.io-thrd.05266: task_engine.cpp:100:start(): [add_server3]: thread pool [THREAD_POOL_DLOCK] started, pool_code = THREAD_POOL_DLOCK, worker_count = 2, worker_share_core = true, partitioned = true, ...
D2022-07-18 23:52:57.787 (1658188377787863884 5266) unknown.io-thrd.05266: task_engine.cpp:242:start(): [add_server3]: task engine started
I2022-07-18 23:52:57.788 (1658188377788219490 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.788 (1658188377788236590 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
D2022-07-18 23:52:57.788 (1658188377788241190 5266) unknown.io-thrd.05266: rpc_engine.cpp:480:start(): [client] network client started at port 4, channel = RPC_CHANNEL_TCP, fmt = NET_HDR_DSN ...
I2022-07-18 23:52:57.788 (1658188377788277391 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.788 (1658188377788282691 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
D2022-07-18 23:52:57.788 (1658188377788367492 5266) unknown.io-thrd.05266: rpc_engine.cpp:480:start(): [client] network client started at port 4, channel = RPC_CHANNEL_UDP, fmt = NET_HDR_DSN ...
I2022-07-18 23:52:57.788 (1658188377788464494 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.788 (1658188377788471594 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
D2022-07-18 23:52:57.788 (1658188377788475594 5266) unknown.io-thrd.05266: rpc_engine.cpp:480:start(): [client] network client started at port 4, channel = RPC_CHANNEL_TCP, fmt = NET_HDR_THRIFT ...
I2022-07-18 23:52:57.788 (1658188377788508595 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.788 (1658188377788513695 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
D2022-07-18 23:52:57.788 (1658188377788598397 5266) unknown.io-thrd.05266: rpc_engine.cpp:480:start(): [client] network client started at port 4, channel = RPC_CHANNEL_UDP, fmt = NET_HDR_THRIFT ...
I2022-07-18 23:52:57.788 (1658188377788700898 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.788 (1658188377788707599 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
D2022-07-18 23:52:57.788 (1658188377788716999 5266) unknown.io-thrd.05266: rpc_engine.cpp:480:start(): [client] network client started at port 4, channel = RPC_CHANNEL_TCP, fmt = NET_HDR_RAW ...
I2022-07-18 23:52:57.788 (1658188377788750299 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.788 (1658188377788755399 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
E2022-07-18 23:52:57.788 (1658188377788777100 5266) unknown.io-thrd.05266: asio_net_provider.cpp:357:start(): asio udp socket bind failed, port = 60615, error = Address already in use
I2022-07-18 23:52:57.788 (1658188377788816000 5266) unknown.io-thrd.05266: rpc_address.cpp:108:ipv4_from_network_interface(): skip interface(lo), address(1.0.0.127)
D2022-07-18 23:52:57.788 (16581883777888[2100](https://github.com/apache/incubator-pegasus/runs/7399780702?check_suite_focus=true#step:7:2101)1 5266) unknown.io-thrd.05266: rpc_address.cpp:121:ipv4_from_network_interface(): get ip address from network interface(eth0), addr(172.18.0.2), input interface("""")
D2022-07-18 23:52:57.788 (1658188377788895202 5266) unknown.io-thrd.05266: rpc_engine.cpp:480:start(): [client] network client started at port 4, channel = RPC_CHANNEL_UDP, fmt = NET_HDR_RAW ...
D2022-07-18 23:52:57.788 (1658188377788902302 5266) unknown.io-thrd.05266: rpc_engine.cpp:517:start(): === service_node=[client], primary_address=[172.18.0.2:4] ===
W2022-07-18 23:52:57.788 (1658188377788950603 5351) client.io-thrd.05351: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
D2022-07-18 23:52:57.789 (1658188377789026304 5266) unknown.io-thrd.05266: task_engine.cpp:100:start(): [client]: thread pool [THREAD_POOL_DEFAULT] started, pool_code = THREAD_POOL_DEFAULT, worker_count = 2, worker_share_core = true, partitioned = false, ...
W2022-07-18 23:52:57.789 (1658188377789070305 5354) client.io-thrd.05354: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
D2022-07-18 23:52:57.789 (1658188377789154707 5266) unknown.io-thrd.05266: task_engine.cpp:100:start(): [client]: thread pool [THREAD_POOL_REPLICATION] started, pool_code = THREAD_POOL_REPLICATION, worker_count = 2, worker_share_core = true, partitioned = false, ...
W2022-07-18 23:52:57.789 (1658188377789198307 5357) client.io-thrd.05357: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
D2022-07-18 23:52:57.789 (1658188377789392411 5266) unknown.io-thrd.05266: task_engine.cpp:100:start(): [client]: thread pool [THREAD_POOL_REPLICATION_LONG] started, pool_code = THREAD_POOL_REPLICATION_LONG, worker_count = 2, worker_share_core = true, partitioned = false, ...
W2022-07-18 23:52:57.789 (1658188377789444612 5360) client.io-thrd.05360: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
D2022-07-18 23:52:57.789 (1658188377789530913 5266) unknown.io-thrd.05266: task_engine.cpp:100:start(): [client]: thread pool [THREAD_POOL_SLOG] started, pool_code = THREAD_POOL_SLOG, worker_count = 2, worker_share_core = true, partitioned = false, ...
W2022-07-18 23:52:57.789 (1658188377789581614 5363) client.io-thrd.05363: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
D2022-07-18 23:52:57.789 (1658188377789656315 5266) unknown.io-thrd.05266: task_engine.cpp:100:start(): [client]: thread pool [THREAD_POOL_PLOG] started, pool_code = THREAD_POOL_PLOG, worker_count = 2, worker_share_core = true, partitioned = false, ...
W2022-07-18 23:52:57.789 (1658188377789700616 5366) client.io-thrd.05366: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
W2022-07-18 23:52:57.789 (1658188377789732517 5367) client.io-thrd.05367: task_worker.cpp:131:set_priority(): You may need priviledge to set thread priority. errno = 1
D2022-07-18 23:52:57.789 (1658188377789809218 5266) unknown.io-thrd.05266: task_engine.cpp:100:start(): [client]: thread pool [THREAD_POOL_DLOCK] started, pool_code = THREAD_POOL_DLOCK, worker_count = 2, worker_share_core = true, partitioned = true, ...
D2022-07-18 23:52:57.789 (1658188377789820418 5266) unknown.io-thrd.05266: task_engine.cpp:242:start(): [client]: task engine started
D2022-07-18 23:52:57.789 (1658188377789851019 5287) add_server1.THREAD_POOL_DEFAULT0.0000149200010001: distributed_lock_zookeeper.cpp:59:start(): name: add_server1, argc=1
D2022-07-18 23:52:57.789 (1658188377789858119 5287) add_server1.THREAD_POOL_DEFAULT0.0000149200010001: distributed_lock_zookeeper.cpp:61:start(): argv: add_server1
D2022-07-18 23:52:57.789 (1658188377789872719 5310) add_server2.THREAD_POOL_DEFAULT0.0000149200010002: distributed_lock_zookeeper.cpp:59:start(): name: add_server2, argc=1
D2022-07-18 23:52:57.789 (1658188377789877319 5310) add_server2.THREAD_POOL_DEFAULT0.0000149200010002: distributed_lock_zookeeper.cpp:61:start(): argv: add_server2
D2022-07-18 23:52:57.789 (1658188377789890220 5333) add_server3.THREAD_POOL_DEFAULT0.0000149200010003: distributed_lock_zookeeper.cpp:59:start(): name: add_server3, argc=1
D2022-07-18 23:52:57.789 (1658188377789894520 5333) add_server3.THREAD_POOL_DEFAULT0.0000149200010003: distributed_lock_zookeeper.cpp:61:start(): argv: add_server3
W2022-07-18 23:53:29.792 (1658188409792873470 5333) add_server3.THREAD_POOL_DEFAULT0.0000149200010003: distributed_lock_service_zookeeper.cpp:108:initialize(): attach to zookeeper session timeout, distributed lock service initialized failed
F2022-07-18 23:53:29.792 (1658188409792903070 5333) add_server3.THREAD_POOL_DEFAULT0.0000149200010003: distributed_lock_zookeeper.cpp:67:start(): assertion expression: err == ERR_OK
F2022-07-18 23:53:29.792 (1658188409792910170 5333) add_server3.THREAD_POOL_DEFAULT0.0000149200010003: distributed_lock_zookeeper.cpp:67:start(): err = ERR_TIMEOUT
run test ""dsn.zookeeper.tests"" in /__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/bin/dsn.zookeeper.tests failed
Error: Process completed with exit code 1.
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1063/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1063,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5G4eKn,incubator-pegasus,1189208743,1063,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-07-19T15:37:25Z,2022-07-19T15:37:25Z,"It's caused by this patch itself https://github.com/apache/incubator-pegasus/pull/1062
Because zk hasn't been started before test running.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5G4eKn/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1064,https://api.github.com/repos/apache/incubator-pegasus/issues/1064,incubator-pegasus,1308897688,1064,Fix：fix verification bug for recovery function,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2022-07-19T02:35:17Z,2022-07-20T11:55:13Z,"When zk is not useful, pegasus provides the function to recovery data from replica server to zk, meta server will collect app information from replica servers and verify their consistence. However, in release 2.1.1, 2.2.x, 2.3.x, the `duplicating` filed of `app_info` are different between primary and secondaries, which will lead to verification failed. ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1064/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1066,https://api.github.com/repos/apache/incubator-pegasus/issues/1066,incubator-pegasus,1308909190,1066,Still-running timer will lead to heap-use-after-free error after percentile is destructed,empiredan,743379,Dan Wang,,CLOSED,2022-07-19T02:57:36Z,2022-07-28T04:04:04Z,"The timer will call `cancel()` while percentile is being destructed, however it won't wait until the callback function is finished. This will lead to heap-use-after-free error as follows (https://github.com/apache/incubator-pegasus/runs/7388468584?check_suite_focus=true):
```
[----------] 11 tests from metrics_test
[ RUN      ] metrics_test.create_entity
[       OK ] metrics_test.create_entity (2 ms)
[ RUN      ] metrics_test.recreate_entity
[       OK ] metrics_test.recreate_entity (0 ms)
[ RUN      ] metrics_test.create_metric
[       OK ] metrics_test.create_metric (0 ms)
[ RUN      ] metrics_test.recreate_metric
[       OK ] metrics_test.recreate_metric (0 ms)
[ RUN      ] metrics_test.gauge_int64
[       OK ] metrics_test.gauge_int64 (0 ms)
[ RUN      ] metrics_test.gauge_double
[       OK ] metrics_test.gauge_double (0 ms)
[ RUN      ] metrics_test.gauge_increment
[       OK ] metrics_test.gauge_increment (2698 ms)
[ RUN      ] metrics_test.counter
[       OK ] metrics_test.counter (3526 ms)
[ RUN      ] metrics_test.volatile_counter
[       OK ] metrics_test.volatile_counter (11942 ms)
[ RUN      ] metrics_test.percentile_int64
[       OK ] metrics_test.percentile_int64 (5458 ms)
[ RUN      ] metrics_test.percentile_double
[       OK ] metrics_test.percentile_double (5451 ms)
[----------] 11 tests from metrics_test (29077 ms total)
[----------] 4 tests from nth_element_test
[ RUN      ] nth_element_test.basic_int64
[       OK ] nth_element_test.basic_int64 (0 ms)
[ RUN      ] nth_element_test.generated_int64
[       OK ] nth_element_test.generated_int64 (5 ms)
[ RUN      ] nth_element_test.basic_double
[       OK ] nth_element_test.basic_double (0 ms)
[ RUN      ] nth_element_test.generated_double
[       OK ] nth_element_test.generated_double (8 ms)
[----------] 4 tests from nth_element_test (13 ms total)
[----------] 4 tests from table_printer_test
[ RUN      ] table_printer_test.empty_content_test
[       OK ] table_printer_test.empty_content_test (0 ms)
[ RUN      ] table_printer_test.empty_name_test
[       OK ] table_printer_test.empty_name_test (0 ms)
[ RUN      ] table_printer_test.single_column_test
[       OK ] table_printer_test.single_column_test (0 ms)
[ RUN      ] table_printer_test.multi_columns_test
[       OK ] table_printer_test.multi_columns_test (0 ms)
[----------] 4 tests from table_printer_test (0 ms total)
[----------] 3 tests from multi_table_printer_test
[ RUN      ] multi_table_printer_test.empty_content_test
[       OK ] multi_table_printer_test.empty_content_test (0 ms)
[ RUN      ] multi_table_printer_test.single_empty_sub_test
[       OK ] multi_table_printer_test.single_empty_sub_test (0 ms)
[ RUN      ] multi_table_printer_test.multi_sub_test
[       OK ] multi_table_printer_test.multi_sub_test (0 ms)
[----------] 3 tests from multi_table_printer_test (0 ms total)
[----------] 2 tests from random
[ RUN      ] random.sanity
[       OK ] random.sanity (1 ms)
[ RUN      ] random.multi_threaded
[       OK ] random.multi_threaded (88 ms)
[----------] 2 tests from random (89 ms total)
[----------] 3 tests from MakeUniqueTest
[ RUN      ] MakeUniqueTest.Basic
[       OK ] MakeUniqueTest.Basic (0 ms)
[ RUN      ] MakeUniqueTest.MoveOnlyTypeAndValue
[       OK ] MakeUniqueTest.MoveOnlyTypeAndValue (0 ms)
[ RUN      ] MakeUniqueTest.AcceptMoveOnly
[       OK ] MakeUniqueTest.AcceptMoveOnly (0 ms)
[----------] 3 tests from MakeUniqueTest (0 ms total)
[----------] 2 tests from Make_UniqueTest
[ RUN      ] Make_UniqueTest.Array
[       OK ] Make_UniqueTest.Array (0 ms)
[ RUN      ] Make_UniqueTest.NotAmbiguousWithStdMakeUnique
[       OK ] Make_UniqueTest.NotAmbiguousWithStdMakeUnique (0 ms)
[----------] 2 tests from Make_UniqueTest (0 ms total)
[----------] 8 tests from string_conv
[ RUN      ] string_conv.buf2bool
[       OK ] string_conv.buf2bool (0 ms)
[ RUN      ] string_conv.buf2int32
[       OK ] string_conv.buf2int32 (0 ms)
[ RUN      ] string_conv.buf2int64
[       OK ] string_conv.buf2int64 (0 ms)
[ RUN      ] string_conv.buf2uint64
[       OK ] string_conv.buf2uint64 (0 ms)
[ RUN      ] string_conv.buf2uint32
[       OK ] string_conv.buf2uint32 (0 ms)
[ RUN      ] string_conv.int64_partial
[       OK ] string_conv.int64_partial (0 ms)
[ RUN      ] string_conv.uint64_partial
[       OK ] string_conv.uint64_partial (0 ms)
[ RUN      ] string_conv.buf2double
[       OK ] string_conv.buf2double (0 ms)
[----------] 8 tests from string_conv (2 ms total)
[----------] 15 tests from StringViewTest
[ RUN      ] StringViewTest.STL2
[       OK ] StringViewTest.STL2 (0 ms)
[ RUN      ] StringViewTest.STL2Substr
[       OK ] StringViewTest.STL2Substr (1 ms)
[ RUN      ] StringViewTest.Ctor
[       OK ] StringViewTest.Ctor (0 ms)
[ RUN      ] StringViewTest.Swap
[       OK ] StringViewTest.Swap (0 ms)
[ RUN      ] StringViewTest.ComparisonOperators
[       OK ] StringViewTest.ComparisonOperators (0 ms)
[ RUN      ] StringViewTest.STL1
[       OK ] StringViewTest.STL1 (0 ms)
[ RUN      ] StringViewTest.Remove
[       OK ] StringViewTest.Remove (0 ms)
[ RUN      ] StringViewTest.Set
[       OK ] StringViewTest.Set (0 ms)
[ RUN      ] StringViewTest.FrontBack
[       OK ] StringViewTest.FrontBack (0 ms)
[ RUN      ] StringViewTest.FrontBackSingleChar
[       OK ] StringViewTest.FrontBackSingleChar (0 ms)
[ RUN      ] StringViewTest.NULLInput
[       OK ] StringViewTest.NULLInput (0 ms)
[ RUN      ] StringViewTest.ExplicitConversionOperator
[       OK ] StringViewTest.ExplicitConversionOperator (0 ms)
[ RUN      ] StringViewTest.Noexcept
[       OK ] StringViewTest.Noexcept (0 ms)
[ RUN      ] StringViewTest.HeterogenousStringViewEquals
[       OK ] StringViewTest.HeterogenousStringViewEquals (0 ms)
[ RUN      ] StringViewTest.FindConformance
[       OK ] StringViewTest.FindConformance (0 ms)
[----------] 15 tests from StringViewTest (3 ms total)
[----------] 2 tests from StringViewStreamTest
[ RUN      ] StringViewStreamTest.Padding
[       OK ] StringViewStreamTest.Padding (1 ms)
[ RUN      ] StringViewStreamTest.ResetsWidth
[       OK ] StringViewStreamTest.ResetsWidth (0 ms)
[----------] 2 tests from StringViewStreamTest (1 ms total)
[----------] 5 tests from time_utils
[ RUN      ] time_utils.hh_mm_to_seconds
[       OK ] time_utils.hh_mm_to_seconds (0 ms)
[ RUN      ] time_utils.get_unix_sec_today_midnight
[       OK ] time_utils.get_unix_sec_today_midnight (0 ms)
[ RUN      ] time_utils.hh_mm_today_to_unix_sec
[       OK ] time_utils.hh_mm_today_to_unix_sec (0 ms)
[ RUN      ] time_utils.get_current_physical_time_ns
[       OK ] time_utils.get_current_physical_time_ns (0 ms)
[ RUN      ] time_utils.time_ms_to_string
[       OK ] time_utils.time_ms_to_string (0 ms)
[----------] 5 tests from time_utils (0 ms total)
[----------] 2 tests from token_bucket_throttling_controller_test
[ RUN      ] token_bucket_throttling_controller_test.test_parse_env_basic_token_bucket_throttling
[       OK ] token_bucket_throttling_controller_test.test_parse_env_basic_token_bucket_throttling (1 ms)
[ RUN      ] token_bucket_throttling_controller_test.throttle_test
[       OK ] token_bucket_throttling_controller_test.throttle_test (14583 ms)
[----------] 2 tests from token_bucket_throttling_controller_test (14584 ms total)
[----------] 1 test from token_buckets_test
[ RUN      ] token_buckets_test.test_token_buckets
[       OK ] token_buckets_test.test_token_buckets (0 ms)
[----------] 1 test from token_buckets_test (0 ms total)
[----------] 4 tests from TokenBucket/TokenBucketTest
[ RUN      ] TokenBucket/TokenBucketTest.sanity/0
[       OK ] TokenBucket/TokenBucketTest.sanity/0 (1 ms)
[ RUN      ] TokenBucket/TokenBucketTest.sanity/1
[       OK ] TokenBucket/TokenBucketTest.sanity/1 (0 ms)
[ RUN      ] TokenBucket/TokenBucketTest.sanity/2
[       OK ] TokenBucket/TokenBucketTest.sanity/2 (4 ms)
[ RUN      ] TokenBucket/TokenBucketTest.sanity/3
[       OK ] TokenBucket/TokenBucketTest.sanity/3 (1 ms)
[----------] 4 tests from TokenBucket/TokenBucketTest (6 ms total)
[----------] Global test environment tear-down
[==========] 168 tests from 32 test cases ran. (48186 ms total)
[  PASSED  ] 168 tests.
=================================================================
==4764==ERROR: AddressSanitizer: heap-use-after-free on address 0x6100000076c0 at pc 0x556c72dfa966 bp 0x15393fce8940 sp 0x15393fce8930
READ of size 8 at 0x6100000076c0 thread T129
    #0 0x556c72dfa965 in void dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >::operator()<__gnu_cxx::__normal_iterator<double*, std::vector<double, std::allocator<double> > > >(__gnu_cxx::__normal_iterator<double*, std::vector<double, std::allocator<double> > >, __gnu_cxx::__normal_iterator<double*, std::vector<double, std::allocator<double> > >, __gnu_cxx::__normal_iterator<double*, std::vector<double, std::allocator<double> > >) /__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/utility/nth_element.h:90
    #1 0x556c72dfa965 in dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>::find_nth_elements() /__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/utility/metrics.h:688
    #2 0x556c72db5c9e in void std::__invoke_impl<void, void (dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>::*&)(), dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>*&>(std::__invoke_memfun_deref, void (dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>::*&)(), dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>*&) /usr/include/c++/7/bits/invoke.h:73
    #3 0x556c72db5c9e in std::__invoke_result<void (dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>::*&)(), dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>*&>::type std::__invoke<void (dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>::*&)(), dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>*&>(void (dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>::*&)(), dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>*&) /usr/include/c++/7/bits/invoke.h:95
    #4 0x556c72db5c9e in void std::_Bind<void (dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>::*(dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>*))()>::__call<void, , 0ul>(std::tuple<>&&, std::_Index_tuple<0ul>) /usr/include/c++/7/functional:467
    #5 0x556c72db5c9e in void std::_Bind<void (dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>::*(dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>*))()>::operator()<, void>() /usr/include/c++/7/functional:551
    #6 0x556c72db5c9e in std::_Function_handler<void (), std::_Bind<void (dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>::*(dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>*))()> >::_M_invoke(std::_Any_data const&) /usr/include/c++/7/bits/std_function.h:316
    #7 0x153945de7883 in std::function<void ()>::operator()() const /usr/include/c++/7/bits/std_function.h:706
    #8 0x153945de7883 in dsn::percentile_timer::on_timer(boost::system::error_code const&) /__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/utils/metrics.cpp:145
    #9 0x153945e0dce0 in void std::__invoke_impl<void, void (dsn::percentile_timer::*&)(boost::system::error_code const&), dsn::percentile_timer*&, boost::system::error_code const&>(std::__invoke_memfun_deref, void (dsn::percentile_timer::*&)(boost::system::error_code const&), dsn::percentile_timer*&, boost::system::error_code const&) /usr/include/c++/7/bits/invoke.h:73
    #10 0x153945e0dce0 in std::__invoke_result<void (dsn::percentile_timer::*&)(boost::system::error_code const&), dsn::percentile_timer*&, boost::system::error_code const&>::type std::__invoke<void (dsn::percentile_timer::*&)(boost::system::error_code const&), dsn::percentile_timer*&, boost::system::error_code const&>(void (dsn::percentile_timer::*&)(boost::system::error_code const&), dsn::percentile_timer*&, boost::system::error_code const&) /usr/include/c++/7/bits/invoke.h:95
    #11 0x153945e0dce0 in void std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>::__call<void, boost::system::error_code const&, 0ul, 1ul>(std::tuple<boost::system::error_code const&>&&, std::_Index_tuple<0ul, 1ul>) /usr/include/c++/7/functional:467
    #12 0x153945e0dce0 in void std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>::operator()<boost::system::error_code const&, void>(boost::system::error_code const&) /usr/include/c++/7/functional:551
    #13 0x153945e0dce0 in boost::asio::detail::binder1<std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>, boost::system::error_code>::operator()() /__w/incubator-pegasus/incubator-pegasus/thirdparty/output/include/boost/asio/detail/bind_handler.hpp:65
    #14 0x153945e0dce0 in void boost::asio::asio_handler_invoke<boost::asio::detail::binder1<std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>, boost::system::error_code> >(boost::asio::detail::binder1<std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>, boost::system::error_code>&, ...) /__w/incubator-pegasus/incubator-pegasus/thirdparty/output/include/boost/asio/handler_invoke_hook.hpp:69
    #15 0x153945e0dce0 in void boost_asio_handler_invoke_helpers::invoke<boost::asio::detail::binder1<std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>, boost::system::error_code>, std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)> >(boost::asio::detail::binder1<std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>, boost::system::error_code>&, std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>&) /__w/incubator-pegasus/incubator-pegasus/thirdparty/output/include/boost/asio/detail/handler_invoke_helpers.hpp:37
    #16 0x153945e0dce0 in void boost::asio::detail::handler_work<std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>, boost::asio::system_executor>::complete<boost::asio::detail::binder1<std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>, boost::system::error_code> >(boost::asio::detail::binder1<std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>, boost::system::error_code>&, std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)>&) /__w/incubator-pegasus/incubator-pegasus/thirdparty/output/include/boost/asio/detail/handler_work.hpp:82
    #17 0x153945e0dce0 in boost::asio::detail::wait_handler<std::_Bind<void (dsn::percentile_timer::*(dsn::percentile_timer*, std::_Placeholder<1>))(boost::system::error_code const&)> >::do_complete(void*, boost::asio::detail::scheduler_operation*, boost::system::error_code const&, unsigned long) /__w/incubator-pegasus/incubator-pegasus/thirdparty/output/include/boost/asio/detail/wait_handler.hpp:72
    #18 0x153945e3b3d7 in boost::asio::detail::scheduler_operation::complete(void*, boost::system::error_code const&, unsigned long) /__w/incubator-pegasus/incubator-pegasus/thirdparty/output/include/boost/asio/detail/scheduler_operation.hpp:40
    #19 0x153945e3b3d7 in boost::asio::detail::scheduler::do_run_one(boost::asio::detail::conditionally_enabled_mutex::scoped_lock&, boost::asio::detail::scheduler_thread_info&, boost::system::error_code const&) /__w/incubator-pegasus/incubator-pegasus/thirdparty/output/include/boost/asio/detail/impl/scheduler.ipp:401
    #20 0x153945e3b3d7 in boost::asio::detail::scheduler::run(boost::system::error_code&) /__w/incubator-pegasus/incubator-pegasus/thirdparty/output/include/boost/asio/detail/impl/scheduler.ipp:154
    #21 0x153945e3b3d7 in boost::asio::io_context::run() /__w/incubator-pegasus/incubator-pegasus/thirdparty/output/include/boost/asio/impl/io_context.ipp:62
    #22 0x153945e3b3d7 in operator() /__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/utils/shared_io_service.cpp:46
    #23 0x153945e3b3d7 in __invoke_impl<void, dsn::tools::shared_io_service::shared_io_service()::<lambda()> > /usr/include/c++/7/bits/invoke.h:60
    #24 0x153945e3b3d7 in __invoke<dsn::tools::shared_io_service::shared_io_service()::<lambda()> > /usr/include/c++/7/bits/invoke.h:95
    #25 0x153945e3b3d7 in _M_invoke<0> /usr/include/c++/7/thread:234
    #26 0x153945e3b3d7 in operator() /usr/include/c++/7/thread:243
    #27 0x153945e3b3d7 in _M_run /usr/include/c++/7/thread:186
    #28 0x1539451f06de  (/usr/lib/x86_64-linux-gnu/libstdc++.so.6+0xbd6de)
    #29 0x1539459b66da in start_thread (/lib/x86_64-linux-gnu/libpthread.so.0+0x76da)
    #30 0x153944c4b61e in __clone (/lib/x86_64-linux-gnu/libc.so.6+0x12161e)
0x6100000076c0 is located 128 bytes inside of 192-byte region [0x610000007640,0x610000007700)
freed by thread T0 here:
    #0 0x1539462a39c8 in operator delete(void*, unsigned long) (/usr/lib/x86_64-linux-gnu/libasan.so.4+0xe19c8)
    #1 0x556c72dc6ace in dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>::~percentile() /__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/utility/metrics.h:663
    #2 0x556c72d9c8b9 in dsn::ref_counter::release_ref() /__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/utility/autoref_ptr.h:84
    #3 0x556c72d9c8b9 in dsn::ref_ptr<dsn::metric>::~ref_ptr() /__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/utility/autoref_ptr.h:139
    #4 0x556c72d9c8b9 in std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> >::~pair() /usr/include/c++/7/bits/stl_pair.h:208
    #5 0x556c72d9c8b9 in void __gnu_cxx::new_allocator<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> > >::destroy<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> > >(std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> >*) /usr/include/c++/7/ext/new_allocator.h:140
    #6 0x556c72d9c8b9 in void std::allocator_traits<std::allocator<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> > > >::destroy<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> > >(std::allocator<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> > >&, std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> >*) /usr/include/c++/7/bits/alloc_traits.h:487
    #7 0x556c72d9c8b9 in std::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> >, false> > >::_M_deallocate_node(std::__detail::_Hash_node<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> >, false>*) /usr/include/c++/7/bits/hashtable_policy.h:[2084](https://github.com/apache/incubator-pegasus/runs/7388468584?check_suite_focus=true#step:7:2085)
    #8 0x556c72dc1873 in std::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> >, false> > >::_M_deallocate_nodes(std::__detail::_Hash_node<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> >, false>*) /usr/include/c++/7/bits/hashtable_policy.h:[2097](https://github.com/apache/incubator-pegasus/runs/7388468584?check_suite_focus=true#step:7:2098)
    #9 0x556c72dc1873 in std::_Hashtable<dsn::metric_prototype const*, std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> >, std::allocator<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> > >, std::__detail::_Select1st, std::equal_to<dsn::metric_prototype const*>, std::hash<dsn::metric_prototype const*>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, false, true> >::clear() /usr/include/c++/7/bits/hashtable.h:2032
    #10 0x153945de2e6b in std::_Hashtable<dsn::metric_prototype const*, std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> >, std::allocator<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> > >, std::__detail::_Select1st, std::equal_to<dsn::metric_prototype const*>, std::hash<dsn::metric_prototype const*>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, false, true> >::~_Hashtable() /usr/include/c++/7/bits/hashtable.h:1358
    #11 0x153945de2e6b in std::unordered_map<dsn::metric_prototype const*, dsn::ref_ptr<dsn::metric>, std::hash<dsn::metric_prototype const*>, std::equal_to<dsn::metric_prototype const*>, std::allocator<std::pair<dsn::metric_prototype const* const, dsn::ref_ptr<dsn::metric> > > >::~unordered_map() /usr/include/c++/7/bits/unordered_map.h:101
    #12 0x153945de2e6b in dsn::metric_entity::~metric_entity() /__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/utils/metrics.cpp:32
    #13 0x153945de2fe0 in dsn::metric_entity::~metric_entity() /__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/utils/metrics.cpp:32
    #14 0x556c72dbd841 in dsn::ref_counter::release_ref() /__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/utility/autoref_ptr.h:84
    #15 0x556c72dbd841 in dsn::ref_ptr<dsn::metric_entity>::~ref_ptr() /__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/utility/autoref_ptr.h:139
    #16 0x556c72dbd841 in std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> >::~pair() /usr/include/c++/7/bits/stl_pair.h:208
    #17 0x556c72dbd841 in void __gnu_cxx::new_allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> > >::destroy<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> > >(std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> >*) /usr/include/c++/7/ext/new_allocator.h:140
    #18 0x556c72dbd841 in void std::allocator_traits<std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> > > >::destroy<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> > >(std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> > >&, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> >*) /usr/include/c++/7/bits/alloc_traits.h:487
    #19 0x556c72dbd841 in std::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> >, true> > >::_M_deallocate_node(std::__detail::_Hash_node<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> >, true>*) /usr/include/c++/7/bits/hashtable_policy.h:2084
    #20 0x556c72dbd841 in std::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> >, true> > >::_M_deallocate_nodes(std::__detail::_Hash_node<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> >, true>*) /usr/include/c++/7/bits/hashtable_policy.h:2097
    #21 0x556c72dbd841 in std::_Hashtable<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> > >, std::__detail::_Select1st, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::clear() /usr/include/c++/7/bits/hashtable.h:2032
    #22 0x556c72dbd841 in std::_Hashtable<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, dsn::ref_ptr<dsn::metric_entity> > >, std::__detail::_Select1st, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::~_Hashtable() /usr/include/c++/7/bits/hashtable.h:1358
    #23 0x153944b6d030  (/lib/x86_64-linux-gnu/libc.so.6+0x43030)
previously allocated by thread T0 here:
    #0 0x1539462a2448 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/libasan.so.4+0xe0448)
    #1 0x556c72e129b2 in dsn::ref_ptr<dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void> > dsn::metric_entity::find_or_create<dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void>, unsigned long&, std::set<dsn::kth_percentile_type, std::less<dsn::kth_percentile_type>, std::allocator<dsn::kth_percentile_type> > const&, unsigned long&>(dsn::metric_prototype const*, unsigned long&, std::set<dsn::kth_percentile_type, std::less<dsn::kth_percentile_type>, std::allocator<dsn::kth_percentile_type> > const&, unsigned long&) /__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/utility/metrics.h:158
    #2 0x556c72e129b2 in dsn::ref_ptr<dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void> > dsn::metric_prototype_with<dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void> >::instantiate<unsigned long&, std::set<dsn::kth_percentile_type, std::less<dsn::kth_percentile_type>, std::allocator<dsn::kth_percentile_type> > const&, unsigned long&>(dsn::ref_ptr<dsn::metric_entity> const&, unsigned long&, std::set<dsn::kth_percentile_type, std::less<dsn::kth_percentile_type>, std::allocator<dsn::kth_percentile_type> > const&, unsigned long&) const /__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/utility/metrics.h:283
    #3 0x556c72e129b2 in void dsn::run_percentile<double, dsn::metric_prototype_with<dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void> >, dsn::floating_checker<double> >(dsn::ref_ptr<dsn::metric_entity> const&, dsn::metric_prototype_with<dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void> > const&, std::vector<double, std::allocator<double> > const&, unsigned long, unsigned long, unsigned long, std::set<dsn::kth_percentile_type, std::less<dsn::kth_percentile_type>, std::allocator<dsn::kth_percentile_type> > const&, unsigned long, unsigned long, std::vector<double, std::allocator<double> > const&, dsn::floating_checker<double>) /__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/utils/test/metrics_test.cpp:682
    #4 0x556c72e1b88f in void dsn::run_percentile_cases<double, dsn::metric_prototype_with<dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void> >, dsn::percentile_case_generator<dsn::nth_element_case_generator<double, dsn::floating_rand_generator<double, void>, void>, void>, dsn::floating_checker<double> >(dsn::metric_prototype_with<dsn::percentile<double, dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >, void> > const&) /__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/utils/test/metrics_test.cpp:838
    #5 0x556c732cbf19 in void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/src/rdsn/src/utils/test/dsn_utils_tests+0xacbf19)
    #6 0x556c732c61cc in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/src/rdsn/src/utils/test/dsn_utils_tests+0xac61cc)
    #7 0x556c732aa13b in testing::Test::Run() (/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/src/rdsn/src/utils/test/dsn_utils_tests+0xaaa13b)
    #8 0x556c732aaa63 in testing::TestInfo::Run() (/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/src/rdsn/src/utils/test/dsn_utils_tests+0xaaaa63)
    #9 0x556c732ab0db in testing::TestCase::Run() (/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/src/rdsn/src/utils/test/dsn_utils_tests+0xaab0db)
    #10 0x556c732b1f7f in testing::internal::UnitTestImpl::RunAllTests() (/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/src/rdsn/src/utils/test/dsn_utils_tests+0xab1f7f)
    #11 0x556c732cd040 in bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/src/rdsn/src/utils/test/dsn_utils_tests+0xacd040)
    #12 0x556c732c7008 in bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/src/rdsn/src/utils/test/dsn_utils_tests+0xac7008)
    #13 0x556c732b0b65 in testing::UnitTest::Run() (/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/src/rdsn/src/utils/test/dsn_utils_tests+0xab0b65)
    #14 0x556c72ac111d in RUN_ALL_TESTS() /__w/incubator-pegasus/incubator-pegasus/thirdparty/output/include/gtest/gtest.h:[2233](https://github.com/apache/incubator-pegasus/runs/7388468584?check_suite_focus=true#step:7:2234)
    #15 0x556c72ac111d in main /__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/utils/test/main.cpp:35
    #16 0x153944b4bc86 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x21c86)
Thread T129 created by T0 here:
    #0 0x1539461f9d2f in __interceptor_pthread_create (/usr/lib/x86_64-linux-gnu/libasan.so.4+0x37d2f)
    #1 0x1539451f0994 in std::thread::_M_start_thread(std::unique_ptr<std::thread::_State, std::default_delete<std::thread::_State> >, void (*)()) (/usr/lib/x86_64-linux-gnu/libstdc++.so.6+0xbd994)
    #2 0x153945de38d2 in dsn::utils::singleton<dsn::tools::shared_io_service>::instance() /__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/utility/singleton.h:39
    #3 0x153945de38d2 in dsn::metric_registry::metric_registry() /__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/utils/metrics.cpp:77
    #4 0x153945de4b9e in dsn::utils::singleton<dsn::metric_registry>::instance() /__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/utility/singleton.h:39
    #5 0x153945de4b9e in dsn::metric_entity_prototype::instantiate(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >) const /__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/utils/metrics.cpp:58
    #6 0x153945de538d in dsn::metric_entity_prototype::instantiate(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const /__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/utils/metrics.cpp:63
    #7 0x556c72da66ed in dsn::metrics_test_create_entity_Test::TestBody() /__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/utils/test/metrics_test.cpp:161
    #8 0x556c732cbf19 in void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/src/rdsn/src/utils/test/dsn_utils_tests+0xacbf19)
    #9 0x556c732c61cc in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/src/rdsn/src/utils/test/dsn_utils_tests+0xac61cc)
    #10 0x556c732aa13b in testing::Test::Run() (/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/src/rdsn/src/utils/test/dsn_utils_tests+0xaaa13b)
    #11 0x556c732aaa63 in testing::TestInfo::Run() (/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/src/rdsn/src/utils/test/dsn_utils_tests+0xaaaa63)
    #12 0x556c732ab0db in testing::TestCase::Run() (/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/src/rdsn/src/utils/test/dsn_utils_tests+0xaab0db)
    #13 0x556c732b1f7f in testing::internal::UnitTestImpl::RunAllTests() (/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/src/rdsn/src/utils/test/dsn_utils_tests+0xab1f7f)
    #14 0x556c732cd040 in bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/src/rdsn/src/utils/test/dsn_utils_tests+0xacd040)
    #15 0x556c732c7008 in bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/src/rdsn/src/utils/test/dsn_utils_tests+0xac7008)
    #16 0x556c732b0b65 in testing::UnitTest::Run() (/__w/incubator-pegasus/incubator-pegasus/src/rdsn/builder/src/rdsn/src/utils/test/dsn_utils_tests+0xab0b65)
    #17 0x556c72ac111d in RUN_ALL_TESTS() /__w/incubator-pegasus/incubator-pegasus/thirdparty/output/include/gtest/gtest.h:2233
    #18 0x556c72ac111d in main /__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/utils/test/main.cpp:35
    #19 0x153944b4bc86 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x21c86)
SUMMARY: AddressSanitizer: heap-use-after-free /__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/utility/nth_element.h:90 in void dsn::stl_nth_element_finder<double, dsn::floating_comparator<double, void> >::operator()<__gnu_cxx::__normal_iterator<double*, std::vector<double, std::allocator<double> > > >(__gnu_cxx::__normal_iterator<double*, std::vector<double, std::allocator<double> > >, __gnu_cxx::__normal_iterator<double*, std::vector<double, std::allocator<double> > >, __gnu_cxx::__normal_iterator<double*, std::vector<double, std::allocator<double> > >)
Shadow bytes around the buggy address:
  0x0c207fff8e80: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd
  0x0c207fff8e90: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
  0x0c207fff8ea0: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd
  0x0c207fff8eb0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
  0x0c207fff8ec0: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd
=>0x0c207fff8ed0: fd fd fd fd fd fd fd fd[fd]fd fd fd fd fd fd fd
  0x0c207fff8ee0: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd
  0x0c207fff8ef0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
  0x0c207fff8f00: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd
  0x0c207fff8f10: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
  0x0c207fff8f20: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07 
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
==4764==ABORTING
ERROR: run dsn_utils_tests failed, return_code = 1
Error: Process completed with exit code 1.
```


Or just core dumped (https://github.com/apache/incubator-pegasus/runs/7386940386?check_suite_focus=true):
```
[----------] 11 tests from metrics_test
[ RUN      ] metrics_test.create_entity
[       OK ] metrics_test.create_entity (1 ms)
[ RUN      ] metrics_test.recreate_entity
[       OK ] metrics_test.recreate_entity (0 ms)
[ RUN      ] metrics_test.create_metric
[       OK ] metrics_test.create_metric (0 ms)
[ RUN      ] metrics_test.recreate_metric
[       OK ] metrics_test.recreate_metric (0 ms)
[ RUN      ] metrics_test.gauge_int64
[       OK ] metrics_test.gauge_int64 (0 ms)
[ RUN      ] metrics_test.gauge_double
[       OK ] metrics_test.gauge_double (0 ms)
[ RUN      ] metrics_test.gauge_increment
[       OK ] metrics_test.gauge_increment (1156 ms)
[ RUN      ] metrics_test.counter
[       OK ] metrics_test.counter (1521 ms)
[ RUN      ] metrics_test.volatile_counter
[       OK ] metrics_test.volatile_counter (4667 ms)
[ RUN      ] metrics_test.percentile_int64
[       OK ] metrics_test.percentile_int64 (5707 ms)
[ RUN      ] metrics_test.percentile_double
[       OK ] metrics_test.percentile_double (5487 ms)
[----------] 11 tests from metrics_test (18539 ms total)
[----------] 4 tests from nth_element_test
[ RUN      ] nth_element_test.basic_int64
[       OK ] nth_element_test.basic_int64 (0 ms)
[ RUN      ] nth_element_test.generated_int64
[       OK ] nth_element_test.generated_int64 (4 ms)
[ RUN      ] nth_element_test.basic_double
[       OK ] nth_element_test.basic_double (0 ms)
[ RUN      ] nth_element_test.generated_double
[       OK ] nth_element_test.generated_double (4 ms)
[----------] 4 tests from nth_element_test (8 ms total)
[----------] 4 tests from table_printer_test
[ RUN      ] table_printer_test.empty_content_test
[       OK ] table_printer_test.empty_content_test (0 ms)
[ RUN      ] table_printer_test.empty_name_test
[       OK ] table_printer_test.empty_name_test (0 ms)
[ RUN      ] table_printer_test.single_column_test
[       OK ] table_printer_test.single_column_test (0 ms)
[ RUN      ] table_printer_test.multi_columns_test
[       OK ] table_printer_test.multi_columns_test (0 ms)
[----------] 4 tests from table_printer_test (0 ms total)
[----------] 3 tests from multi_table_printer_test
[ RUN      ] multi_table_printer_test.empty_content_test
[       OK ] multi_table_printer_test.empty_content_test (0 ms)
[ RUN      ] multi_table_printer_test.single_empty_sub_test
[       OK ] multi_table_printer_test.single_empty_sub_test (0 ms)
[ RUN      ] multi_table_printer_test.multi_sub_test
[       OK ] multi_table_printer_test.multi_sub_test (0 ms)
[----------] 3 tests from multi_table_printer_test (0 ms total)
[----------] 2 tests from random
[ RUN      ] random.sanity
[       OK ] random.sanity (1 ms)
[ RUN      ] random.multi_threaded
[       OK ] random.multi_threaded (4 ms)
[----------] 2 tests from random (5 ms total)
[----------] 3 tests from MakeUniqueTest
[ RUN      ] MakeUniqueTest.Basic
[       OK ] MakeUniqueTest.Basic (0 ms)
[ RUN      ] MakeUniqueTest.MoveOnlyTypeAndValue
[       OK ] MakeUniqueTest.MoveOnlyTypeAndValue (0 ms)
[ RUN      ] MakeUniqueTest.AcceptMoveOnly
[       OK ] MakeUniqueTest.AcceptMoveOnly (0 ms)
[----------] 3 tests from MakeUniqueTest (1 ms total)
[----------] 2 tests from Make_UniqueTest
[ RUN      ] Make_UniqueTest.Array
[       OK ] Make_UniqueTest.Array (0 ms)
[ RUN      ] Make_UniqueTest.NotAmbiguousWithStdMakeUnique
[       OK ] Make_UniqueTest.NotAmbiguousWithStdMakeUnique (0 ms)
[----------] 2 tests from Make_UniqueTest (0 ms total)
[----------] 8 tests from string_conv
[ RUN      ] string_conv.buf2bool
[       OK ] string_conv.buf2bool (0 ms)
[ RUN      ] string_conv.buf2int32
[       OK ] string_conv.buf2int32 (0 ms)
[ RUN      ] string_conv.buf2int64
[       OK ] string_conv.buf2int64 (0 ms)
[ RUN      ] string_conv.buf2uint64
[       OK ] string_conv.buf2uint64 (0 ms)
[ RUN      ] string_conv.buf2uint32
[       OK ] string_conv.buf2uint32 (0 ms)
[ RUN      ] string_conv.int64_partial
[       OK ] string_conv.int64_partial (0 ms)
[ RUN      ] string_conv.uint64_partial
[       OK ] string_conv.uint64_partial (0 ms)
[ RUN      ] string_conv.buf2double
[       OK ] string_conv.buf2double (0 ms)
[----------] 8 tests from string_conv (1 ms total)
[----------] 15 tests from StringViewTest
[ RUN      ] StringViewTest.STL2
[       OK ] StringViewTest.STL2 (0 ms)
[ RUN      ] StringViewTest.STL2Substr
[       OK ] StringViewTest.STL2Substr (1 ms)
[ RUN      ] StringViewTest.Ctor
[       OK ] StringViewTest.Ctor (0 ms)
[ RUN      ] StringViewTest.Swap
[       OK ] StringViewTest.Swap (0 ms)
[ RUN      ] StringViewTest.ComparisonOperators
[       OK ] StringViewTest.ComparisonOperators (0 ms)
[ RUN      ] StringViewTest.STL1
[       OK ] StringViewTest.STL1 (0 ms)
[ RUN      ] StringViewTest.Remove
[       OK ] StringViewTest.Remove (0 ms)
[ RUN      ] StringViewTest.Set
[       OK ] StringViewTest.Set (0 ms)
[ RUN      ] StringViewTest.FrontBack
[       OK ] StringViewTest.FrontBack (0 ms)
[ RUN      ] StringViewTest.FrontBackSingleChar
[       OK ] StringViewTest.FrontBackSingleChar (0 ms)
[ RUN      ] StringViewTest.NULLInput
[       OK ] StringViewTest.NULLInput (0 ms)
[ RUN      ] StringViewTest.ExplicitConversionOperator
[       OK ] StringViewTest.ExplicitConversionOperator (0 ms)
[ RUN      ] StringViewTest.Noexcept
[       OK ] StringViewTest.Noexcept (0 ms)
[ RUN      ] StringViewTest.HeterogenousStringViewEquals
[       OK ] StringViewTest.HeterogenousStringViewEquals (0 ms)
[ RUN      ] StringViewTest.FindConformance
[       OK ] StringViewTest.FindConformance (0 ms)
[----------] 15 tests from StringViewTest (1 ms total)
[----------] 2 tests from StringViewStreamTest
[ RUN      ] StringViewStreamTest.Padding
[       OK ] StringViewStreamTest.Padding (0 ms)
[ RUN      ] StringViewStreamTest.ResetsWidth
[       OK ] StringViewStreamTest.ResetsWidth (0 ms)
[----------] 2 tests from StringViewStreamTest (0 ms total)
[----------] 5 tests from time_utils
[ RUN      ] time_utils.hh_mm_to_seconds
[       OK ] time_utils.hh_mm_to_seconds (0 ms)
[ RUN      ] time_utils.get_unix_sec_today_midnight
[       OK ] time_utils.get_unix_sec_today_midnight (0 ms)
[ RUN      ] time_utils.hh_mm_today_to_unix_sec
[       OK ] time_utils.hh_mm_today_to_unix_sec (0 ms)
[ RUN      ] time_utils.get_current_physical_time_ns
[       OK ] time_utils.get_current_physical_time_ns (0 ms)
[ RUN      ] time_utils.time_ms_to_string
[       OK ] time_utils.time_ms_to_string (0 ms)
[----------] 5 tests from time_utils (0 ms total)
[----------] 2 tests from token_bucket_throttling_controller_test
[ RUN      ] token_bucket_throttling_controller_test.test_parse_env_basic_token_bucket_throttling
[       OK ] token_bucket_throttling_controller_test.test_parse_env_basic_token_bucket_throttling (0 ms)
[ RUN      ] token_bucket_throttling_controller_test.throttle_test
[       OK ] token_bucket_throttling_controller_test.throttle_test (14584 ms)
[----------] 2 tests from token_bucket_throttling_controller_test (14584 ms total)
[----------] 1 test from token_buckets_test
[ RUN      ] token_buckets_test.test_token_buckets
[       OK ] token_buckets_test.test_token_buckets (0 ms)
[----------] 1 test from token_buckets_test (0 ms total)
[----------] 4 tests from TokenBucket/TokenBucketTest
[ RUN      ] TokenBucket/TokenBucketTest.sanity/0
[       OK ] TokenBucket/TokenBucketTest.sanity/0 (0 ms)
[ RUN      ] TokenBucket/TokenBucketTest.sanity/1
[       OK ] TokenBucket/TokenBucketTest.sanity/1 (1 ms)
[ RUN      ] TokenBucket/TokenBucketTest.sanity/2
[       OK ] TokenBucket/TokenBucketTest.sanity/2 (2 ms)
[ RUN      ] TokenBucket/TokenBucketTest.sanity/3
[       OK ] TokenBucket/TokenBucketTest.sanity/3 (1 ms)
[----------] 4 tests from TokenBucket/TokenBucketTest (4 ms total)
[----------] Global test environment tear-down
[==========] 168 tests from 32 test cases ran. (34928 ms total)
[  PASSED  ] 168 tests.
Segmentation fault (core dumped)
ERROR: run dsn_utils_tests failed, return_code = 139
Error: Process completed with exit code 1.
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1066/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1067,https://api.github.com/repos/apache/incubator-pegasus/issues/1067,incubator-pegasus,1308917325,1067,Unit test broken down with unhandled error for load_from_private_log_test.ignore_useless while failed to write mutation log file header ,empiredan,743379,Dan Wang,,OPEN,2022-07-19T03:12:57Z,2022-07-26T01:02:44Z,"While unit test `load_from_private_log_test.ignore_useless` was being executed, it failed to write mutation log file header then broken down with an unhandled error as follows (https://github.com/apache/incubator-pegasus/runs/7387575656?check_suite_focus=true) :
```
[----------] 9 tests from load_from_private_log_test
[ RUN      ] load_from_private_log_test.find_log_file_to_start
E2022-07-18 11:42:36.540 (1658144556540112657 9611) replica.default0.0000257e00010001: [1.1@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
E2022-07-18 11:42:36.540 (1658144556540365771 9611) replica.default0.0000257e00010001: [1.1@] unable to start duplication since no log file is available
[       OK ] load_from_private_log_test.find_log_file_to_start (7 ms)
[ RUN      ] load_from_private_log_test.start_duplication_10000_4MB
E2022-07-18 11:42:36.547 (1658144556547914109 9611) replica.default0.0000257e00010001: [1.1@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
W2022-07-18 11:42:36.586 (1658144556586599550 9615) replica.replica0.010100000000004f: [1.1@] duplication status hasn't been sync completed, try next for delay 1s, last_commit_decree=10010, confirmed_decree=-1
[       OK ] load_from_private_log_test.start_duplication_10000_4MB (1059 ms)
[ RUN      ] load_from_private_log_test.start_duplication_50000_4MB
E2022-07-18 11:42:37.607 (1658144557607002977 9611) replica.default0.0000257e00010001: [1.1@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
W2022-07-18 11:42:37.756 (1658144557756301528 9615) replica.replica0.0101000000000078: [1.1@] duplication status hasn't been sync completed, try next for delay 1s, last_commit_decree=50010, confirmed_decree=-1
E2022-07-18 11:42:38.831 (1658144558831113007 9615) replica.replica0.0104000000000015: [1.1@] ERR_FILE_OPERATION_FAILED: open_read
[       OK ] load_from_private_log_test.start_duplication_50000_4MB (1312 ms)
[ RUN      ] load_from_private_log_test.start_duplication_10000_1MB
E2022-07-18 11:42:38.918 (1658144558918401664 9611) replica.default0.0000257e00010001: [1.1@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
W2022-07-18 11:42:38.951 (1658144558951939108 9615) replica.replica0.0101000000000099: [1.1@] duplication status hasn't been sync completed, try next for delay 1s, last_commit_decree=10010, confirmed_decree=-1
[       OK ] load_from_private_log_test.start_duplication_10000_1MB (1055 ms)
[ RUN      ] load_from_private_log_test.start_duplication_50000_1MB
E2022-07-18 11:42:39.974 (1658144559974081135 9611) replica.default0.0000257e00010001: [1.1@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
W2022-07-18 11:42:40.119 (1658144560119475359 9615) replica.replica0.01010000000000ef: [1.1@] duplication status hasn't been sync completed, try next for delay 1s, last_commit_decree=50010, confirmed_decree=-1
E2022-07-18 11:42:41.121 (1658144561121544023 9615) replica.replica0.010400000000006a: [1.1@] ERR_FILE_OPERATION_FAILED: open_read
[       OK ] load_from_private_log_test.start_duplication_50000_1MB (2256 ms)
[ RUN      ] load_from_private_log_test.start_duplication_100000_4MB
E2022-07-18 11:42:42.230 (1658144562230087457 9611) replica.default0.0000257e00010001: [1.1@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
W2022-07-18 11:42:42.505 (1658144562505037289 9615) replica.replica0.0101000000000143: [1.1@] duplication status hasn't been sync completed, try next for delay 1s, last_commit_decree=100010, confirmed_decree=-1
[       OK ] load_from_private_log_test.start_duplication_100000_4MB (1478 ms)
[ RUN      ] load_from_private_log_test.handle_real_private_log
E2022-07-18 11:42:43.707 (1658144563707902946 9611) replica.default0.0000257e00010001: [1.1@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
E2022-07-18 11:42:43.711 (1658144563711313344 9611) replica.default0.0000257e00010001: [1.4@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
E2022-07-18 11:42:43.712 (1658144563712498012 9611) replica.default0.0000257e00010001: [1.4@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
E2022-07-18 11:42:43.713 (1658144563713367463 9611) replica.default0.0000257e00010001: [1.5@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
[       OK ] load_from_private_log_test.handle_real_private_log (6 ms)
[ RUN      ] load_from_private_log_test.restart_duplication
E2022-07-18 11:42:43.714 (1658144563714207711 9611) replica.default0.0000257e00010001: [1.1@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
[       OK ] load_from_private_log_test.restart_duplication (2 ms)
[ RUN      ] load_from_private_log_test.ignore_useless
E2022-07-18 11:42:43.715 (1658144563715980514 9611) replica.default0.0000257e00010001: [1.1@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
E2022-07-18 11:42:43.719 (1658144563719703030 9612) replica.default1.010100000000017f: write failed with errno=Bad file descriptor, return ERR_FILE_OPERATION_FAILED.
E2022-07-18 11:42:43.719 (1658144563719734132 9612) replica.default1.010100000000017e: write mutation log file header failed, file = ./test-log/log.1.0, err = ERR_FILE_OPERATION_FAILED
F2022-07-18 11:42:43.719 (1658144563719739932 9612) replica.default1.010100000000017e: assertion expression: false
F2022-07-18 11:42:43.719 (1658144563719745333 9612) replica.default1.010100000000017e: unhandled error
Aborted (core dumped)
I2022-07-18 11:42:43.711 (1658144563711978682 9612) replica.default1.010400010000000a: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 0104000100000009
I2022-07-18 11:42:43.711 (1658144563711983482 9612) replica.default1.010100010000006a: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 010400010000000b
I2022-07-18 11:42:43.712 (1658144563712013784 9612) replica.default1.010400010000000e: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 010400010000000d
I2022-07-18 11:42:43.712 (1658144563712023085 9612) replica.default1.010100010000006b: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 010400010000000f
D2022-07-18 11:42:43.712 (1658144563712032385 9616) replica.replica1.010400010000000c: no next log file (log.2) is found
D2022-07-18 11:42:43.712 (1658144563712465010 9611) replica.default0.0000257e00010001: [1.4@] closing duplication {""dupid"":1,""status"":""DS_PAUSE"",""remote"":""remote_address"",""confirmed"":0,""app"":""temp""}
E2022-07-18 11:42:43.712 (1658144563712493112 9611) replica.default0.0000257e00010001: [1.4@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
I2022-07-18 11:42:43.712 (1658144563712540415 9611) replica.default0.0000257e00010001: close mutation log ./test-log
I2022-07-18 11:42:43.712 (1658144563712544915 9611) replica.default0.0000257e00010001: performance counter replica*eon.replica*private.log.size(MB)@1.4 is removed, remaining reference (1)
D2022-07-18 11:42:43.712 (1658144563712547915 9611) replica.default0.0000257e00010001: 1.4@: replica closed, time_used = 0ms
I2022-07-18 11:42:43.712 (1658144563712550215 9611) replica.default0.0000257e00010001: 1.4@: replica destroyed
I2022-07-18 11:42:43.712 (1658144563712642621 9612) replica.default1.010100010000006c: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 0101000000000150
D2022-07-18 11:42:43.712 (1658144563712650821 9611) replica.default0.0000257e00010001: open private log ./test-log/log.1.0 succeed, start_offset = 0, end_offset = 426, size = 426, previous_max_decree = 0
D2022-07-18 11:42:43.712 (1658144563712657521 9611) replica.default0.0000257e00010001: start to replay mutation log ./test-log/log.1.0, offset = [0, 426), size = 426
I2022-07-18 11:42:43.712 (1658144563712685423 9612) replica.default1.0101000000000152: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 0101000000000151
I2022-07-18 11:42:43.712 (1658144563712697624 9612) replica.default1.010100010000006d: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 0101000000000153
I2022-07-18 11:42:43.712 (1658144563712716125 9612) replica.default1.0101000000000155: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 0101000000000154
I2022-07-18 11:42:43.712 (1658144563712724925 9612) replica.default1.010100010000006e: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 0101000000000156
D2022-07-18 11:42:43.712 (1658144563712731126 9611) replica.default0.0000257e00010001: finish to replay mutation log (./test-log/log.1.0) [err: ERR_HANDLE_EOF: failed to read log block]
D2022-07-18 11:42:43.712 (1658144563712741826 9611) replica.default0.0000257e00010001: [1.4@] initialize replica_duplicator[DS_PAUSE] [dupid:1, meta_confirmed_decree:0]
D2022-07-18 11:42:43.712 (1658144563712779029 9611) replica.default0.0000257e00010001: add fail_point [name: open_read, task: Return(), frequency: 25%, max_count: 1]
D2022-07-18 11:42:43.712 (1658144563712796630 9611) replica.default0.0000257e00010001: add fail_point [name: mutation_log_read_log_block, task: Return(), frequency: 25%, max_count: 1]
D2022-07-18 11:42:43.712 (1658144563712811030 9611) replica.default0.0000257e00010001: add fail_point [name: duplication_sync_complete, task: Void(), frequency: 100%, max_count: -1]
I2022-07-18 11:42:43.712 (1658144563712870434 9612) replica.default1.010100010000006f: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 0104000100000012
D2022-07-18 11:42:43.712 (1658144563712875934 9616) replica.replica1.0101000000000157: [1.4@] start loading from log file ./test-log/log.1.0
I2022-07-18 11:42:43.712 (1658144563712952939 9612) replica.default1.0104000100000017: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 0104000100000016
I2022-07-18 11:42:43.712 (1658144563712957739 9612) replica.default1.0101000100000070: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 0104000100000018
I2022-07-18 11:42:43.712 (1658144563712979140 9612) replica.default1.010400010000001b: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 010400010000001a
I2022-07-18 11:42:43.712 (1658144563712988241 9612) replica.default1.0101000100000071: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 010400010000001c
D2022-07-18 11:42:43.712 (1658144563712994041 9616) replica.replica1.0104000100000019: no next log file (log.2) is found
D2022-07-18 11:42:43.713 (1658144563713336261 9611) replica.default0.0000257e00010001: [1.4@] closing duplication {""dupid"":1,""status"":""DS_PAUSE"",""remote"":""remote_address"",""confirmed"":0,""app"":""temp""}
E2022-07-18 11:42:43.713 (1658144563713362462 9611) replica.default0.0000257e00010001: [1.5@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
I2022-07-18 11:42:43.713 (1658144563713413365 9611) replica.default0.0000257e00010001: close mutation log ./test-log
I2022-07-18 11:42:43.713 (1658144563713418166 9611) replica.default0.0000257e00010001: performance counter �چ}�U is removed, remaining reference (0)
D2022-07-18 11:42:43.713 (1658144563713420966 9611) replica.default0.0000257e00010001: 1.4@: replica closed, time_used = 0ms
I2022-07-18 11:42:43.713 (1658144563713423166 9611) replica.default0.0000257e00010001: 1.4@: replica destroyed
I2022-07-18 11:42:43.713 (1658144563713507771 9612) replica.default1.0101000100000072: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 010100000000015a
D2022-07-18 11:42:43.713 (1658144563713517071 9611) replica.default0.0000257e00010001: open private log ./test-log/log.1.0 succeed, start_offset = 0, end_offset = 405, size = 405, previous_max_decree = 0
D2022-07-18 11:42:43.713 (1658144563713523272 9611) replica.default0.0000257e00010001: start to replay mutation log ./test-log/log.1.0, offset = [0, 405), size = 405
I2022-07-18 11:42:43.713 (1658144563713554473 9612) replica.default1.010100000000015c: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 010100000000015b
I2022-07-18 11:42:43.713 (1658144563713565974 9612) replica.default1.0101000100000073: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 010100000000015d
I2022-07-18 11:42:43.713 (1658144563713581775 9612) replica.default1.010100000000015f: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 010100000000015e
I2022-07-18 11:42:43.713 (1658144563713590476 9612) replica.default1.0101000100000074: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 0101000000000160
D2022-07-18 11:42:43.713 (1658144563713596476 9611) replica.default0.0000257e00010001: finish to replay mutation log (./test-log/log.1.0) [err: ERR_HANDLE_EOF: failed to read log block]
D2022-07-18 11:42:43.713 (1658144563713606877 9611) replica.default0.0000257e00010001: [1.5@] initialize replica_duplicator[DS_PAUSE] [dupid:1, meta_confirmed_decree:0]
D2022-07-18 11:42:43.713 (1658144563713648279 9611) replica.default0.0000257e00010001: add fail_point [name: open_read, task: Return(), frequency: 25%, max_count: 1]
D2022-07-18 11:42:43.713 (1658144563713665780 9611) replica.default0.0000257e00010001: add fail_point [name: mutation_log_read_log_block, task: Return(), frequency: 25%, max_count: 1]
D2022-07-18 11:42:43.713 (1658144563713679981 9611) replica.default0.0000257e00010001: add fail_point [name: duplication_sync_complete, task: Void(), frequency: 100%, max_count: -1]
I2022-07-18 11:42:43.713 (1658144563713740984 9612) replica.default1.0101000100000075: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 01040000000000f7
D2022-07-18 11:42:43.713 (1658144563713746285 9615) replica.replica0.0101000000000161: [1.5@] start loading from log file ./test-log/log.1.0
I2022-07-18 11:42:43.713 (1658144563713845090 9612) replica.default1.01040000000000fc: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 01040000000000fb
I2022-07-18 11:42:43.713 (1658144563713850891 9612) replica.default1.0101000100000076: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 01040000000000fd
I2022-07-18 11:42:43.713 (1658144563713873792 9612) replica.default1.0104000000000100: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 01040000000000ff
I2022-07-18 11:42:43.713 (1658144563713882793 9612) replica.default1.0101000100000077: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 0104000000000101
D2022-07-18 11:42:43.713 (1658144563713889093 9615) replica.replica0.01040000000000fe: no next log file (log.2) is found
D2022-07-18 11:42:43.713 (1658144563713918795 9611) replica.default0.0000257e00010001: [1.5@] closing duplication {""dupid"":1,""status"":""DS_PAUSE"",""remote"":""remote_address"",""confirmed"":0,""app"":""temp""}
I2022-07-18 11:42:43.713 (1658144563713936196 9611) replica.default0.0000257e00010001: close mutation log ./test-log
I2022-07-18 11:42:43.713 (1658144563713940896 9611) replica.default0.0000257e00010001: performance counter �چ}�U is removed, remaining reference (0)
D2022-07-18 11:42:43.713 (1658144563713943596 9611) replica.default0.0000257e00010001: 1.5@: replica closed, time_used = 0ms
I2022-07-18 11:42:43.713 (1658144563713945796 9611) replica.default0.0000257e00010001: 1.5@: replica destroyed
D2022-07-18 11:42:43.713 (1658144563713950996 9611) replica.default0.0000257e00010001: close block service manager.
E2022-07-18 11:42:43.714 (1658144563714189910 9611) replica.default0.0000257e00010001: [1.1@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
D2022-07-18 11:42:43.715 (1658144563715030559 9611) replica.default0.0000257e00010001: [1.1@] initialize replica_duplicator[DS_PAUSE] [dupid:1, meta_confirmed_decree:-1]
D2022-07-18 11:42:43.715 (1658144563715095163 9611) replica.default0.0000257e00010001: create new log file ./test-log/log.1.0 succeed, time_used = 20201 ns
I2022-07-18 11:42:43.715 (1658144563715236171 9611) replica.default0.0000257e00010001: close mutation log ./test-log
I2022-07-18 11:42:43.715 (1658144563715308975 9612) replica.default1.0101000100000078: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 0101000000000168
D2022-07-18 11:42:43.715 (1658144563715330876 9611) replica.default0.0000257e00010001: open private log ./test-log/log.1.0 succeed, start_offset = 0, end_offset = 1192, size = 1192, previous_max_decree = 0
D2022-07-18 11:42:43.715 (1658144563715336677 9611) replica.default0.0000257e00010001: start to replay mutation log ./test-log/log.1.0, offset = [0, 1192), size = 1192
I2022-07-18 11:42:43.715 (1658144563715361578 9612) replica.default1.010100000000016a: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 0101000000000169
I2022-07-18 11:42:43.715 (1658144563715372179 9612) replica.default1.0101000100000079: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 010100000000016b
I2022-07-18 11:42:43.715 (1658144563715398780 9612) replica.default1.010100000000016d: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 010100000000016c
I2022-07-18 11:42:43.715 (1658144563715407581 9612) replica.default1.010100010000007a: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 010100000000016e
D2022-07-18 11:42:43.715 (1658144563715415881 9611) replica.default0.0000257e00010001: finish to replay mutation log (./test-log/log.1.0) [err: ERR_HANDLE_EOF: failed to read log block]
D2022-07-18 11:42:43.715 (1658144563715446583 9611) replica.default0.0000257e00010001: create new log file ./test-log/log.2.1192 succeed, time_used = 22301 ns
I2022-07-18 11:42:43.715 (1658144563715548889 9611) replica.default0.0000257e00010001: close mutation log ./test-log
I2022-07-18 11:42:43.715 (1658144563715658095 9612) replica.default1.010100010000007b: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 0101000000000177
D2022-07-18 11:42:43.715 (1658144563715670496 9611) replica.default0.0000257e00010001: open private log ./test-log/log.2.1192 succeed, start_offset = 1192, end_offset = 2400, size = 1208, previous_max_decree = 10
D2022-07-18 11:42:43.715 (1658144563715675596 9611) replica.default0.0000257e00010001: start to replay mutation log ./test-log/log.2.1192, offset = [1192, 2400), size = 1208
I2022-07-18 11:42:43.715 (1658144563715699098 9612) replica.default1.0101000000000179: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 0101000000000178
I2022-07-18 11:42:43.715 (1658144563715704698 9612) replica.default1.010100010000007c: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 010100000000017a
I2022-07-18 11:42:43.715 (1658144563715741300 9612) replica.default1.010100000000017c: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 010100000000017b
I2022-07-18 11:42:43.715 (1658144563715749901 9612) replica.default1.010100010000007d: disk operation failure with code LPC_AIO_IMMEDIATE_CALLBACK, err = ERR_HANDLE_EOF, aio_task_id = 010100000000017d
D2022-07-18 11:42:43.715 (1658144563715760101 9611) replica.default0.0000257e00010001: finish to replay mutation log (./test-log/log.2.1192) [err: ERR_HANDLE_EOF: failed to read log block]
D2022-07-18 11:42:43.715 (1658144563715768702 9611) replica.default0.0000257e00010001: [1.1@] start loading from log file ./test-log/log.2.1192
I2022-07-18 11:42:43.715 (1658144563715771402 9611) replica.default0.0000257e00010001: close mutation log ./test-log
D2022-07-18 11:42:43.715 (1658144563715793603 9611) replica.default0.0000257e00010001: [1.1@] closing duplication {""dupid"":1,""status"":""DS_PAUSE"",""remote"":""remote_address"",""confirmed"":-1,""app"":""temp""}
I2022-07-18 11:42:43.715 (1658144563715797904 9611) replica.default0.0000257e00010001: close mutation log ./test-log
I2022-07-18 11:42:43.715 (1658144563715801904 9611) replica.default0.0000257e00010001: performance counter pe�}�U is removed, remaining reference (0)
D2022-07-18 11:42:43.715 (1658144563715804904 9611) replica.default0.0000257e00010001: 1.1@: replica closed, time_used = 0ms
I2022-07-18 11:42:43.715 (1658144563715807204 9611) replica.default0.0000257e00010001: 1.1@: replica destroyed
D2022-07-18 11:42:43.715 (1658144563715812704 9611) replica.default0.0000257e00010001: close block service manager.
E2022-07-18 11:42:43.715 (1658144563715973014 9611) replica.default0.0000257e00010001: [1.1@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
D2022-07-18 11:42:43.716 (1658144563716446741 9611) replica.default0.0000257e00010001: [1.1@] initialize replica_duplicator[DS_PAUSE] [dupid:1, meta_confirmed_decree:-1]
D2022-07-18 11:42:43.716 (1658144563716862265 9611) replica.default0.0000257e00010001: create new log file ./test-log/log.1.0 succeed, time_used = 25101 ns
I2022-07-18 11:42:43.718 (1658144563718476459 9611) replica.default0.0000257e00010001: close mutation log ./test-log
E2022-07-18 11:42:43.719 (1658144563719684229 9612) replica.default1.010100000000017f: write failed with errno=Bad file descriptor, return ERR_FILE_OPERATION_FAILED.
I2022-07-18 11:42:43.719 (1658144563719706130 9612) replica.default1.010100000000017f: disk operation failure with code LPC_WRITE_REPLICATION_LOG_COMMON, err = ERR_FILE_OPERATION_FAILED, aio_task_id = 010100000000017e
E2022-07-18 11:42:43.719 (1658144563719729232 9612) replica.default1.010100000000017e: write mutation log file header failed, file = ./test-log/log.1.0, err = ERR_FILE_OPERATION_FAILED
F2022-07-18 11:42:43.719 (1658144563719736132 9612) replica.default1.010100000000017e: assertion expression: false
F2022-07-18 11:42:43.719 (1658144563719741632 9612) replica.default1.010100000000017e: unhandled error
ERROR: run dsn_replica_dup_test failed, return_code = 1
Error: Process completed with exit code 1.
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1067/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1068,https://api.github.com/repos/apache/incubator-pegasus/issues/1068,incubator-pegasus,1309742984,1068,Support more language scripts linter,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2022-07-19T16:03:41Z,2022-07-19T16:03:41Z,"There are some scripts written in Shell, Python, and etc in Pegasus project, we can add more linter actions to check the style and avoid stupid mistakes. There are some actions we can use, such as:
1. https://github.com/marketplace/actions/python-code-quality-and-lint
2. https://github.com/marketplace/actions/shell-linter","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1068/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1072,https://api.github.com/repos/apache/incubator-pegasus/issues/1072,incubator-pegasus,1315582611,1072,BuildPegasusRegularly workflow failed,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-07-23T06:53:10Z,2022-07-26T02:32:18Z,This issue will track the daily build failures.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1072/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1072,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HHN1H,incubator-pegasus,1193073991,1072,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-07-23T06:54:22Z,2022-07-23T06:54:22Z,"Build failed with clang, related to https://github.com/apache/incubator-pegasus/pull/1065, see more detailes: https://github.com/apache/incubator-pegasus/runs/7473265744?check_suite_focus=true
@hycdong help please to resolve it, thanks","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HHN1H/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1075,https://api.github.com/repos/apache/incubator-pegasus/issues/1075,incubator-pegasus,1316097530,1075,add module-labels for pull request automatically ,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-07-25T01:48:49Z,2022-07-26T00:54:57Z,it would be more convenient for developers to see what module pull request changes. ,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1075/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1077,https://api.github.com/repos/apache/incubator-pegasus/issues/1077,incubator-pegasus,1316372672,1077,Function test can not support control commands,hycdong,17868458,HeYuchen,377710264@qq.com,CLOSED,2022-07-25T07:10:06Z,2022-07-27T07:23:12Z,"I build pegasus through command `./run.sh build -c --test`, and try to run pegasus function test through command `./run.sh test -m pegasus_function_test`. When the test run not-on-travis tests, I found that function test can not support control commands, such as clear_onebox etc, the error is just like:
```
Note: Google Test filter = recovery_test.*
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from recovery_test
[ RUN      ] recovery_test.recovery
============
start global_env()
meta1 pid: 0
meta1 dir: 
project root: .
working dir: = true, partitioned = false, ...
D2022-07-25 14:58:11.605 (1658732291605899729 9335) unknown.io-thrd.09335: get ip address from network interface(enp0s31f6), addr(10.231.57.161), input interface("""")
get ip: 
Error: unknow option ""clear_onebox""
cp: 无法获取'src/server/config.min.ini' 的文件状态(stat): 没有那个文件或目录
sed: 无法读取 config-server-test-recovery.ini: 没有那个文件或目录
sed: 无法读取 config-server-test-recovery.ini: 没有那个文件或目录
sed: 无法读取 config-server-test-recovery.ini: 没有那个文件或目录
sed: 无法读取 config-server-test-recovery.ini: 没有那个文件或目录
Error: unknow option ""start_onebox""
......
```
It seems that there are something wrong with `global_env` initialization, I revert master for an old version and rerun the test, global_env is just like:
```
start global_env()
meta1 pid: 3296
meta1 dir: /home/heyuchen/work/pegasus_github/pegasus/onebox/meta1
project root: /home/heyuchen/work/pegasus_github/pegasus
working dir: /home/heyuchen/work/pegasus_github/pegasus/src/builder/src/test/function_test
```

I compare the master and old version, guess it may be caused by https://github.com/apache/incubator-pegasus/pull/1062, hoping it to be fixed.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1077/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1077,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HPVjL,incubator-pegasus,1195202763,1077,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-07-26T08:57:40Z,2022-07-26T08:57:40Z,"@hycdong Thank you to reproduce it, I will fix it.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HPVjL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1081,https://api.github.com/repos/apache/incubator-pegasus/issues/1081,incubator-pegasus,1319154707,1081,Feature: enhance cold backup and restore function,hycdong,17868458,HeYuchen,377710264@qq.com,OPEN,2022-07-27T07:44:56Z,2022-08-02T01:46:47Z,"# Background
Pegasus currently supports cold backup and restore functions, but both of them have some disadvantages. 

For cold backup, pegasus supports periodic backup through policy. Users can create a policy with backup related parameters such as provider, interval time, and apply this policy to sereval tables. Besides, pegasus also supports onetime backup since release 2.3.0. 
However, backup function has following disadvantages:
- Periodic backup can not start accurately by start time
- When Periodic backup interval time is less than 1 day, periodic backup will be triggered unexpectedly.
- User defined provider path is not supported for periodic backup.
- Once backup is started, it can not be canceled. When backup failed, it will continue to retry until succeed, even restart meta server.
- Current backup will cost heavy I/O during copying checkpoint.
- The path on provider is hard to find one table's backup.
- Backup code is not firendly to read and maintain.

For restore, pegasus supports two data_version. Tables created in release 1.x is V0, and tables created in release 2.x is V1. Restore process will create an empty table, then apply the backup checkpoint. There will be a compatible problem that release 2.x table can not apply V0 checkpoint, which will lead to coredump making cluster useless. As a result, restore need to check table data_version to make it robust.

# New backup design
The enhance version of backup, simplify backup v2, will solve all probelms above, providing a simple backup function.

## Components
Meta backup function is consist of three parts:
- Backup engine - intertact with replica server
- Periodic backup context - manage table periodic backup policy and backups
  - meta server will have a timer to check whether periodic backup should be triggered
  - for first triggered backup, server will check it by start_time whose format is like ""15:00""
  - for not-first backup, server  will compare last backup start time and periodic backup interval
  - periodic backup is not allowed to be modified, but can be deleted and recreated
- Backup service - manage cluster all tables backup, including onetime backup and periodic backup. Besides, it also expose the rpc interface to admin-cli and shell
  - add table periodic backup policy
  - query periodic backup policy
  - disable/enable periodic backup policy
  - delete periodic backup policy
  - start onetime backup
  - query backup (onetime and periodic)
  - cancel backup (onetime and periodic)
 
## Main flow
![image](https://user-images.githubusercontent.com/17868458/181185836-6eea1905-eb29-4557-bb1b-421c2b12d3a4.png)
- when receving start backup, engine will turn its backup status into `checkpointing` and send request to replica servers
- replica will turn its state into `checkpointing`, and turn to `checkpointed` after generating checkpoint succeed
- when all partitions status is `checkpointed`, meta will turn status into `uploading`
- replica will turn its state into `uploading`, and turn to `succeed` after uploading checkpoint succeed, the backup checkpoint directory will be deleted after a while
- when all partitions status is `succeed`, meta will turn status into `succeed` and consider backup succeed
- if any errors happended during whole process, backup will be failed
- if receiving cancel backup, checkpointing or uploading backup will be canceled

## Backup paths
### Path on remote storage (zk)
```
<cluster_root>/backup/<app_id>/once/<timestamp>/<backup_item>
<cluster_root>/backup/<app_id>/periodic/<policy_context>
                                       /<timestamp>/<backup_item>
```

### Path on remote backup provider (such as HDFS)
```
<root>/<cluster_name>/<app_name>_<app_id>/<timestamp>/<pidx>/chkpt_<ip>_<port>
                                                            /meta
                                                            /backup_info
```

# New restore
Restore v2 won't update design, just add data version check, refactor code and compatible for old backup path on backup provider.

# Pull request merge plan
- Add a new branch call `backup-restore-dev`, all pull reuqests will be firstly added into this branch, and finally into master branch.
- Remove all old backup and restore codes firstly because that new code is huge different from the old implementation.
- This feature is NOT planed in 2.4.0, just next release, will not block releasing process
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1081/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1083,https://api.github.com/repos/apache/incubator-pegasus/issues/1083,incubator-pegasus,1320257139,1083,What if bulk load successfully only on some nodes while failed on other nodes?,byteroll,15176699,PeakLu,pid42@foxmail.com,OPEN,2022-07-28T00:41:33Z,2022-08-01T09:16:58Z,"From this http://pegasus.incubator.apache.org/en/2020/02/18/bulk-load-design.html, it says 

> 需要说明的是，如果在bulk load在ingestion阶段失败或者在ingestion阶段执行cancel bulk load操作，可能会出现部分partition完成ingestion，而部分失败或者被cancel的情况，即部分partition成功导入了数据，部分partition没有导入数据的现象。

but it doesn't tell how to handle this situation. Is there any solution to solve this problem?

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1083/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1083,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HYRdw,incubator-pegasus,1197545328,1083,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2022-07-28T01:23:55Z,2022-07-28T01:23:55Z,"There are two cases which will cause bulk load ingestion data inconsistence:
- some partitions meet unrecoverable ingestion error during ingestion and some not
    - simple network error or replica 2pc are NOT unrecoverable error, only the ingested files can not be recognized by rocksdb is unrecoverable
- force cancel during ingestion - this is only triggered by user not by system itself

There are currently no solution to handle such situation automatically by system, because pegasus doesn't support transaction through different partitions. For example, table has 8 partitions. Partition 0 ingest succeed, but partition 1 receive wrong-format sst files which is a unrecoverable error, it can not ingest those files, bulk load failed, and partition 0 won't reset those data. Ingestion is just like batch write, different partitions won't affect others' data, they are just different partitions. If user use our client batch write interface, it also can not gurantee that data wrote into different partitions should always be consistent.

The only solution is to retry bulk load after user fix broken files, which is triggered by user manually. ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HYRdw/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1083,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HeLT6,incubator-pegasus,1199092986,1083,NA,byteroll,15176699,PeakLu,pid42@foxmail.com,NA,2022-07-29T09:58:34Z,2022-07-29T09:58:34Z,"Thanks for the explanation. By the way, is there any plan to support transaction through different partitions?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HeLT6/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1083,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HlN_f,incubator-pegasus,1200938975,1083,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2022-08-01T09:16:58Z,2022-08-01T09:16:58Z,"Transaction is not in our current plan, because we don't find out its user case, but it can be discussed and implemented if any user has strong request on transaction in future.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HlN_f/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1084,https://api.github.com/repos/apache/incubator-pegasus/issues/1084,incubator-pegasus,1320829585,1084,Use fqdn instead of ip to generate the meta_server conf In each script,WHBANG,38547944,,,CLOSED,2022-07-28T12:02:58Z,2022-08-05T09:12:35Z,"Configure meta_server to support fqdn, so replace the ip in the script with fqdn.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1084/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1090,https://api.github.com/repos/apache/incubator-pegasus/issues/1090,incubator-pegasus,1323998058,1090,improve performance of count_data,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2022-08-01T08:02:37Z,2022-09-22T06:18:07Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
When we precisely count data for a large table, it's unnecessarily return key-values from server to client. 

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->
I try to add optional section ""only_return_count"" on get_scanner_request & scan_request, also add optional  section kv_count on scan_response.
Different versions of the software will not interact with each other because of optional  section.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1090/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1092,https://api.github.com/repos/apache/incubator-pegasus/issues/1092,incubator-pegasus,1324089275,1092,zk c client 3.7 does not run well with zk server 3.4,foreverneverer,23136769,Jiashuo,js982986555@live.com,OPEN,2022-08-01T09:11:00Z,2022-08-09T08:24:11Z,"## Bug Report

https://github.com/XiaoMi/rdsn/pull/979 upgrade zk c client to 3.7(from 3.4.10) for kerberos, but it sometime can't access zk server 3.4.5

In my test, when I upgrade the meta to [latest pegasus server](https://github.com/WHBANG/incubator-pegasus/tree/746b54ad06ed12b7b3a8d8bb510e1bfb1a937175), the meta error log:
```
E2022-07-27 11:41:16.67 (1658893276067532317 139122)   meta.default0.00001f3100010001: meta_service.cpp:463:start(): initialize server state from remote storage failed, err = ERR_INCONSISTENT_STATE, retry ...
F2022-07-27 11:41:16.69 (1658893276069053850 139122)   meta.default0.00001f3100010001: server_state.cpp:709:sync_apps_from_remote_storage(): assertion expression: ERR_OK == err
F2022-07-27 11:41:16.69 (1658893276069061403 139122)   meta.default0.00001f3100010001: server_state.cpp:709:sync_apps_from_remote_storage(): can't handle this error (ERR_TIMEOUT)
```

`ERR_INCONSISTENT_STATE` is from zk error `ZRUNTIMEINCONSISTENCY`, see the zk error log:
```
2022-07-28 16:25:15,412:63056(0x7f2f8d699700):ZOO_INFO@log_env@1250: Client environment:zookeeper.version=zookeeper C client 3.7.0
2022-07-28 16:25:15,412:63056(0x7f2f8d699700):ZOO_INFO@log_env@1254: Client environment:host.name=host
2022-07-28 16:25:15,412:63056(0x7f2f8d699700):ZOO_INFO@log_env@1261: Client environment:os.name=Linux
2022-07-28 16:25:15,412:63056(0x7f2f8d699700):ZOO_INFO@log_env@1262: Client environment:os.arch=3.18.6-2.el7.centos.x86_64
2022-07-28 16:25:15,412:63056(0x7f2f8d699700):ZOO_INFO@log_env@1263: Client environment:os.version=#1 SMP Mon Oct 24 13:01:33 CST 2016
2022-07-28 16:25:15,412:63056(0x7f2f8d699700):ZOO_INFO@log_env@1271: Client environment:user.name=(null)
2022-07-28 16:25:15,412:63056(0x7f2f8d699700):ZOO_INFO@log_env@1279: Client environment:user.home=/home/work
2022-07-28 16:25:15,412:63056(0x7f2f8d699700):ZOO_INFO@log_env@1291: Client environment:user.dir=/home/work/app/pegasus/tjwqtst-msg/meta
2022-07-28 16:25:15,412:63056(0x7f2f8d699700):ZOO_INFO@zookeeper_init_internal@1344: Initiating client connection, host=host:21000 sessionTimeout=10000 watcher=0x7f2fa97f1fc0 sessionId=0 sessionPasswd=<null> context=0x2082790 flags=0
2022-07-28 16:25:15,449:63056(0x7f2f7c652700):ZOO_INFO@check_events@2988: initiated connection to server IP:21000
2022-07-28 16:25:15,467:63056(0x7f2f7c652700):ZOO_INFO@finalize_session_establishment@2868: session establishment complete on server IP:21000, sessionId=0xff82392b354205db, negotiated timeout=10000
2022-07-28 16:25:47,814:63056(0x7f2f7c652700):ZOO_WARN@zookeeper_interest@2532: Exceeded deadline by 1021ms
2022-07-28 16:26:13,404:63056(0x7f2f7c652700):ZOO_WARN@zookeeper_interest@2532: Exceeded deadline by 1021ms
2022-07-28 16:26:29,014:63056(0x7f2f7c652700):ZOO_ERROR@resolve_hosts@947: getaddrinfo: Name or service not known 
```
In server side, zk find some connection was closed by client:
```
2022-07-28,16:26:07,920 INFO org.apache.zookeeper.server.NIOServerCnxnFactory:[myid:0] Accepted socket connection 
ip:port
2022-07-28,16:26:07,924 INFO org.apache.zookeeper.server.ZooKeeperserver:[myid:0] Client attempting to renew session 0xff82392b30fa062d at ip:port
2022-07-28,16:26:07,924 INFO org.apache.zookeeper.server.quorum.Learner:[myid:@] Revalidating client: 0xff82392b30fa062d
2022-07-28,16:26:07,924 INFO org.apache.zookeeper.server.ZooKeeperServer:[myid:0] Established session 0xff82392b3ofa062d with negotiated timeout 10000 for client ip:port
2022-07-28,16:26:15,675 WARN org.apache.zookeeper.server.NIOServercnxn:[myid:0] caught end of stream exception
EndofstreamException: Unable to read additional data from client sessionid oxff82392b30fa062d, likely client has closed socket 
               at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServercnxn.java:224)）
               at org.apache.zookeeper.server.NIoServerCnxnFactory.run(NIOServercnxnFactory.java:213)
               at java.lang.Thread.run(Thread.java:748)
2022-07-28,16:26:15,676 INFO org.apache.zookeeper.server.NIOServercnxn:[myid:0] Closed socket connection for client
ip:port which had session id 0xff82392b30fa062d
```

The above bug does not always occur, I only find the bug in one cluster of our company,  The following resolution will be ok:
- revert zk client version to 3.4.10
- change zk config from host to ip

## Related jira:
https://issues.apache.org/jira/browse/ZOOKEEPER-4603","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1092/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1092,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Hlr89,incubator-pegasus,1201061693,1092,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-08-01T11:14:49Z,2022-08-01T11:14:49Z,@foreverneverer have you checked the zookeeper server side log？,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Hlr89/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1092,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Hlspj,incubator-pegasus,1201064547,1092,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-08-01T11:17:30Z,2022-08-01T11:17:30Z,"meta server log and zk client log are not at the same time, I'm not sure if they are related. Could you paste the 3 parties logs and make sure they are at the same time?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Hlspj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1092,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Hl9bj,incubator-pegasus,1201133283,1092,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-08-01T12:24:32Z,2022-08-01T12:24:32Z,"This issuse seems related to this problem, https://issues.apache.org/jira/browse/ZOOKEEPER-1998.
We can try to set a delay_ms by API `zoo_set_servers_resolution_delay`, or add a .patch file for zk thirdparty lib to revert this commit.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Hl9bj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1092,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HpF_-,incubator-pegasus,1201954814,1092,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2022-08-02T02:50:52Z,2022-08-02T02:50:52Z,"> meta server log and zk client log are not at the same time, I'm not sure if they are related. Could you paste the 3 parties logs and make sure they are at the same time?

The log was copied from different time, but the log detail is same, I don't re-produce the log","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HpF_-/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1092,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HpGCj,incubator-pegasus,1201954979,1092,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2022-08-02T02:51:12Z,2022-08-02T02:51:12Z,"> @foreverneverer have you checked the zookeeper server side log？

added","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HpGCj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1092,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HpHUL,incubator-pegasus,1201960203,1092,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2022-08-02T03:01:32Z,2022-08-02T03:01:32Z,"> This issuse seems related to this problem, https://issues.apache.org/jira/browse/ZOOKEEPER-1998.
> We can try to set a delay_ms by API `zoo_set_servers_resolution_delay`, or add a .patch file for zk thirdparty lib to revert this commit.

The issue seem say `getaddrinfo` is unconditionally in old version, and fixed in 3.7, that's also mean, it has existed in old version but not added in new version?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HpHUL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1092,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HplMh,incubator-pegasus,1202082593,1092,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-08-02T06:43:49Z,2022-08-02T06:43:49Z,"> > This issuse seems related to this problem, https://issues.apache.org/jira/browse/ZOOKEEPER-1998.
> > We can try to set a delay_ms by API `zoo_set_servers_resolution_delay`, or add a .patch file for zk thirdparty lib to revert this commit.
> 
> The issue seem say `getaddrinfo` is unconditionally in old version, and fixed in 3.7, that's also mean, it has existed in old version but not added in new version?

This issue exists in all zk client versions, this patch just add a new API (i.e. `zoo_set_servers_resolution_delay `, see https://github.com/apache/zookeeper/pull/1068/files#diff-d7c3594993d19853c9d2b312b783be085bd9f02b3f5fe8a573e7c94e0ec22734R725) to set the delay time, I'm not sure if it can resolve the problem you faced, but you can have a try.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HplMh/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1092,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HzqVx,incubator-pegasus,1204725105,1092,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2022-08-04T03:52:45Z,2022-08-04T03:52:45Z,"> This issue exists in all zk client versions, this patch just add a new API (i.e. `zoo_set_servers_resolution_delay `, see https://github.com/apache/zookeeper/pull/1068/files#diff-d7c3594993d19853c9d2b312b783be085bd9f02b3f5fe8a573e7c94e0ec22734R725) to set the delay time, I'm not sure if it can resolve the problem you faced, but you can have a try.

OK, how add .patch file? I can try it","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5HzqVx/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1092,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5H0P7K,incubator-pegasus,1204879050,1092,NA,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,NA,2022-08-04T07:34:44Z,2022-08-04T07:34:44Z,"> > This issue exists in all zk client versions, this patch just add a new API (i.e. `zoo_set_servers_resolution_delay `, see https://github.com/apache/zookeeper/pull/1068/files#diff-d7c3594993d19853c9d2b312b783be085bd9f02b3f5fe8a573e7c94e0ec22734R725) to set the delay time, I'm not sure if it can resolve the problem you faced, but you can have a try.
> 
> OK, how add .patch file? I can try it

zk c client 3.7.0 already have this patch . I add config on https://github.com/apache/incubator-pegasus/pull/1100/files, try it","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5H0P7K/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1092,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5IEOdX,incubator-pegasus,1209067351,1092,NA,foreverneverer,23136769,Jiashuo,js982986555@live.com,NA,2022-08-09T08:20:43Z,2022-08-09T08:20:43Z,"> > > This issue exists in all zk client versions, this patch just add a new API (i.e. `zoo_set_servers_resolution_delay `, see https://github.com/apache/zookeeper/pull/1068/files#diff-d7c3594993d19853c9d2b312b783be085bd9f02b3f5fe8a573e7c94e0ec22734R725) to set the delay time, I'm not sure if it can resolve the problem you faced, but you can have a try.
> > 
> > 
> > OK, how add .patch file? I can try it
> 
> zk c client 3.7.0 already have this patch . I add config on https://github.com/apache/incubator-pegasus/pull/1100/files, try it

Great！

But I have test it and it doesn't work well. the log:

Pegasus:
```
E2022-08-09 16:16:14.503 (1660032974503742256 148273)   meta.THREAD_POOL_META_SERVER0.020200020000034e: server_state.cpp:644:operator()(): get partition node failed, reason(ERR_TIMEOUT)
D2022-08-09 16:16:14.503 (1660032974503750762 148294)   meta.io-thrd.148294: zookeeper_session.cpp:323:global_watcher(): global watcher, type(session event), state(expired_session_state)
E2022-08-09 16:16:14.503 (1660032974503777787 148294)   meta.io-thrd.148294: distributed_lock_service_zookeeper.cpp:282:on_zoo_session_evt(): get zoo state: expired_session_state, which means the session is expired
E2022-08-09 16:16:14.503 (1660032974503805295 148274)   meta.THREAD_POOL_META_SERVER1.0202000000000315: server_state.cpp:644:operator()(): get partition node failed, reason(ERR_TIMEOUT)
E2022-08-09 16:16:14.503 (1660032974503820448 148274)   meta.THREAD_POOL_META_SERVER1.0202000200000350: server_state.cpp:644:operator()(): get partition node failed, reason(ERR_TIMEOUT)
E2022-08-09 16:16:14.503 (1660032974503828983 148274)   meta.THREAD_POOL_META_SERVER1.0202000300000337: server_state.cpp:644:operator()(): get partition node failed, reason(ERR_TIMEOUT)
E2022-08-09 16:16:14.503 (1660032974503836417 148274)   meta.THREAD_POOL_META_SERVER1.0202000300000338: server_state.cpp:644:operator()(): get partition node failed, reason(ERR_TIMEOUT)
E2022-08-09 16:16:14.503 (1660032974503849575 148289)   meta.     fd0.0201000000000005: meta_server_failure_detector.cpp:162:operator()(): leader lock expired callback: err(ERR_EXPIRED), owner(ip:port), version(174)
E2022-08-09 16:16:14.503 (1660032974503877813 148274)   meta.THREAD_POOL_META_SERVER1.0202000300000339: server_state.cpp:644:operator()(): get partition node failed, reason(ERR_TIMEOUT)

```

Zookeeper:
```
2022-08-09 16:15:05,844:148195(0x7faadc489700):ZOO_INFO@log_env@1250: Client environment:zookeeper.version=zookeeper C client 3.7.0
2022-08-09 16:15:05,844:148195(0x7faadc489700):ZOO_INFO@log_env@1254: Client environment:host.name=host
2022-08-09 16:15:05,844:148195(0x7faadc489700):ZOO_INFO@log_env@1261: Client environment:os.name=Linux
2022-08-09 16:15:05,844:148195(0x7faadc489700):ZOO_INFO@log_env@1262: Client environment:os.arch=3.18.6-2.el7.centos.x86_64
2022-08-09 16:15:05,844:148195(0x7faadc489700):ZOO_INFO@log_env@1263: Client environment:os.version=#1 SMP Mon Oct 24 13:01:33 CST 2016
2022-08-09 16:15:05,845:148195(0x7faadc489700):ZOO_INFO@log_env@1271: Client environment:user.name=(null)
2022-08-09 16:15:05,845:148195(0x7faadc489700):ZOO_INFO@log_env@1279: Client environment:user.home=/home/work
2022-08-09 16:15:05,845:148195(0x7faadc489700):ZOO_INFO@log_env@1291: Client environment:user.dir=/home/work/app/pegasus/tjwqtst-msg/meta
2022-08-09 16:15:05,845:148195(0x7faadc489700):ZOO_INFO@zookeeper_init_internal@1344: Initiating client connection, host=host:port sessionTimeout=10000 watcher=0x7faaf85e4460 sessionId=0 sessionPasswd=<null> context=0x3ef8790 flags=0
2022-08-09 16:15:05,852:148195(0x7faacb442700):ZOO_INFO@check_events@2988: initiated connection to server ip:port
2022-08-09 16:15:05,858:148195(0x7faacb442700):ZOO_INFO@finalize_session_establishment@2868: session establishment complete on server ip:port, sessionId=0xff82392b30fa55e5, negotiated timeout=10000
2022-08-09 16:15:44,869:148195(0x7faacb442700):ZOO_WARN@zookeeper_interest@2532: Exceeded deadline by 749ms
2022-08-09 16:15:48,962:148195(0x7faacb442700):ZOO_ERROR@resolve_hosts@947: getaddrinfo: Name or service not known

2022-08-09 16:15:48,962:148195(0x7faacb442700):ZOO_WARN@zookeeper_interest@2532: Exceeded deadline by 684ms
2022-08-09 16:16:14,488:148195(0x7faacb442700):ZOO_ERROR@handle_socket_error_msg@3007: Socket ip:port zk retcode=-4, errno=112(Host is down): failed while receiving a server response
2022-08-09 16:16:14,496:148195(0x7faacb442700):ZOO_INFO@check_events@2988: initiated connection to server ip:port
2022-08-09 16:16:14,501:148195(0x7faacb442700):ZOO_ERROR@handle_socket_error_msg@3043: Socket ip:port zk retcode=-112, errno=116(Stale file handle): sessionId=0xff82392b30fa55e5 has expired.
2022-08-09 16:16:15,594:149127(0x7ff5a74d4700):ZOO_INFO@log_env@1250: Client environment:zookeeper.version=zookeeper C client 3.7.0
2022-08-09 16:16:15,594:149127(0x7ff5a74d4700):ZOO_INFO@log_env@1254: Client environment:host.name=host
2022-08-09 16:16:15,594:149127(0x7ff5a74d4700):ZOO_INFO@log_env@1261: Client environment:os.name=Linux
2022-08-09 16:16:15,594:149127(0x7ff5a74d4700):ZOO_INFO@log_env@1262: Client environment:os.arch=3.18.6-2.el7.centos.x86_64
2022-08-09 16:16:15,594:149127(0x7ff5a74d4700):ZOO_INFO@log_env@1263: Client environment:os.version=#1 SMP Mon Oct 24 13:01:33 CST 2016
2022-08-09 16:16:15,594:149127(0x7ff5a74d4700):ZOO_INFO@log_env@1271: Client environment:user.name=(null)
2022-08-09 16:16:15,594:149127(0x7ff5a74d4700):ZOO_INFO@log_env@1279: Client environment:user.home=/home/work
2022-08-09 16:16:15,594:149127(0x7ff5a74d4700):ZOO_INFO@log_env@1291: Client environment:user.dir=/home/work/app/pegasus/tjwqtst-msg/meta
2022-08-09 16:16:15,594:149127(0x7ff5a74d4700):ZOO_INFO@zookeeper_init_internal@1344: Initiating client connection, host=host:port sessionTimeout=10000 watcher=0x7ff5c362f460 sessionId=0 sessionPasswd=<null> context=0x215a790 flags=0
2022-08-09 16:16:15,603:149127(0x7ff59648d700):ZOO_INFO@check_events@2988: initiated connection to server ip:port
2022-08-09 16:16:15,609:149127(0x7ff59648d700):ZOO_INFO@finalize_session_establishment@2868: session establishment complete on server ip:port, sessionId=0xff82392b3542553b, negotiated timeout=10000
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5IEOdX/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1093,https://api.github.com/repos/apache/incubator-pegasus/issues/1093,incubator-pegasus,1324340942,1093,Support Build on M1 MacOS,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2022-08-01T12:29:29Z,2022-08-11T03:41:59Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1093/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1095,https://api.github.com/repos/apache/incubator-pegasus/issues/1095,incubator-pegasus,1325202978,1095,Pegasus unit test failed for download hadoop failed,WHBANG,38547944,,,CLOSED,2022-08-02T02:33:12Z,2022-08-15T08:57:04Z,"## Bug Report
https://github.com/apache/incubator-pegasus/runs/7605983359?check_suite_focus=true

6650K .......... .......... .......... .......... ..........  2%  143K 9m22s
  6700K .......... .......... .......... .......... ..........  2%  137K 9m30s
  6750K .......... .......... .......... .......... ..........  2%  198K 9m35s
  6800K .......... .......... .......... .......... ..........  2%  189K 9m39s
  6850K .......... .......... .......... .......... ..........  2%  190K 9m44s
  6900K .......... .......... .......... .......... ..........  2%  189K 9m49s
  6950K .......... .......... .......... .......... ..........  2%  186K 9m53s
  7000K .......... .......... .......... .......... ..........  2%  121K 10m3s
  7050K .......... .......... .......... .......... ..........  2%  180K 10m7s
  7100K .......... .......... .......... .......... ..........  2%  180K 10m12s
  7[150](https://github.com/apache/incubator-pegasus/runs/7605983261?check_suite_focus=true#step:10:151)K .......... .......... .......... .......... ..........  2%  180K 10m17s
  7200K .......... .......... .......... .......... ..........  3% 88.3K 10m30s
  7250K .......... .......... .......... .......... ..........  3%  159K 10m36s
  7300K .......... .......... .......... .......... .....       3% 71.8K=21s

2022-08-01 07:38:16 (357 KB/s) - Read error at byte 7521757/246123562 (Success). Giving up.

ERROR: download hadoop failed
Error: Process completed with exit code 1.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1095/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1095,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Hp-83,incubator-pegasus,1202188087,1095,NA,WHBANG,38547944,,,NA,2022-08-02T08:36:20Z,2022-08-02T08:36:20Z,"https://github.com/apache/incubator-pegasus/runs/7626009179?check_suite_focus=true
Another download failure problem：

Run export LD_LIBRARY_PATH=`pwd`/thirdparty/output/lib:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server
Test start time: Tue Aug  2 07:20:24 Asia 2022
test_modules=dsn_runtime_tests,dsn_utils_tests,dsn_perf_counter_test,dsn.zookeeper.tests,dsn_aio_test,dsn.failure_detector.tests,dsn_meta_state_tests,dsn_nfs_test,dsn_block_service_test,dsn.replication.simple_kv,dsn.rep_tests.simple_kv,dsn.meta.test,dsn.replica.test,dsn_http_test,dsn_replica_dup_test,dsn_replica_backup_test,dsn_replica_bulk_load_test,dsn_replica_split_test
Downloading zookeeper...
--2022-08-02 07:20:24--  http://pegasus-thirdparty-package.oss-cn-beijing.aliyuncs.com/apache-zookeeper-3.7.0-bin.tar.gz
Resolving pegasus-thirdparty-package.oss-cn-beijing.aliyuncs.com (pegasus-thirdparty-package.oss-cn-beijing.aliyuncs.com)... 59.110.185.30
Connecting to pegasus-thirdparty-package.oss-cn-beijing.aliyuncs.com (pegasus-thirdparty-package.oss-cn-beijing.aliyuncs.com)|59.110.185.30|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 12387[6](https://github.com/apache/incubator-pegasus/runs/7626009179?check_suite_focus=true#step:8:7)14 (12M) [application/gzip]
Saving to: 'apache-zookeeper-3.[7](https://github.com/apache/incubator-pegasus/runs/7626009179?check_suite_focus=true#step:8:8).0-bin.tar.gz'

     0K .......... ....                                         0% 5.12K=2.9s

2022-0[8](https://github.com/apache/incubator-pegasus/runs/7626009179?check_suite_focus=true#step:8:9)-02 07:20:34 (5.[12](https://github.com/apache/incubator-pegasus/runs/7626009179?check_suite_focus=true#step:8:13) KB/s) - Read error at byte 15268/123876[14](https://github.com/apache/incubator-pegasus/runs/7626009179?check_suite_focus=true#step:8:15) (Connection timed out). Giving up.

ERROR: download zookeeper failed
Error: Process completed with exit code 1.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Hp-83/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1101,https://api.github.com/repos/apache/incubator-pegasus/issues/1101,incubator-pegasus,1328224350,1101,Add a gauge for the duration since the meta server has received the last beacon ,empiredan,743379,Dan Wang,,OPEN,2022-08-04T07:59:29Z,2022-08-04T07:59:45Z,"## Background

Recently a cluster on production environment was found that primary meta server had frequently disconnected the replica servers, for the reason that the duration since the last beacon from each replica server had been received by the primary meta server was often greater than the grace period (70+ seconds vs. 22 seconds).

The network latency is typically several hundreds of microseconds, which means something must have been wrong for this cluster. After trouble shooting for the root cause, it was found that there are 2 different NTP servers A and B in the configuration. A is slower than B by more than one minute. For example, meta server received a beacon at 12:05:25 from A; then the clocks jumped suddenly to 12:06:35; the meta server found that it has passed far more than the grace period, then disconnected the corresponding replica server.

## Implementation

The duration since the meta server has received the last beacon for each replica server can be added as a gauge, to find the exception in the system faster.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1101/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1103,https://api.github.com/repos/apache/incubator-pegasus/issues/1103,incubator-pegasus,1330594329,1103,used call_rpcs_sync function maybe cause crash or infinite loop,xihong08,74220545,zhuxihong,,OPEN,2022-08-06T00:49:18Z,2023-01-13T06:31:01Z,"Bug Report
We used pegasus 2.0 and pegasus_shell disk_capacity command to query cluster disk's Usage
sometimes  disk_capacity -d cause crash, sometimes cause infinite loop, it Occurs occasionally.
I tracked the pegasus_shell dead and cpu 100% when i frequent use disk_capacity -d 
the stack roughly as follows：
```
#0 in std::_Rb_tree_increment(std::_Rb_tree_node_base const*) 
#1 in std::_Rb_tree_const_iterator  gcc10/include/10.2.0/bits/stl_tree.h:376
#2 in std::_Rb_tree    stl_tree.h:2531
#3 in std::_Rb_tree    stl_tree.h:2542
#4 in std::map           stl_map.h:1069
#5 in call_rpcs_sync   replication_ddl_client.h:306
```
`rpcs.erase(rpc.first)` cause this proplem
The above bug most likely multithreading cause by Insecure operation at variable rpcs


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1103/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1103,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5INl9e,incubator-pegasus,1211522910,1103,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-08-11T03:44:16Z,2022-08-11T03:44:16Z,"Thanks for your report, we'll fix it later.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5INl9e/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1103,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SVbw6,incubator-pegasus,1381350458,1103,NA,VirendraYadav1234,95998389,Virendra Yadav,,NA,2023-01-13T05:59:38Z,2023-01-13T05:59:38Z,I want to work on this issue please assign me this issue,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SVbw6/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1103,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SVhj5,incubator-pegasus,1381374201,1103,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-01-13T06:31:00Z,2023-01-13T06:31:00Z,"> I want to work on this issue please assign me this issue

@VirendraYadav1234 Thanks, assigned.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SVhj5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1105,https://api.github.com/repos/apache/incubator-pegasus/issues/1105,incubator-pegasus,1331278942,1105,Refactor(java): refactor some code,ninsmiracle,110282526,,,CLOSED,2022-08-08T03:37:42Z,2023-01-16T16:43:08Z,"## Feature Request

**Describe the feature you'd like:**
add some detail for error log in java-client.

**Describe alternatives you've considered:**
more detail for error log will make it more clearly.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1105/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1107,https://api.github.com/repos/apache/incubator-pegasus/issues/1107,incubator-pegasus,1332946903,1107,Make cu_calculator a switch control,WHBANG,38547944,,,OPEN,2022-08-09T09:04:53Z,2022-08-15T03:34:39Z,"## Feature Request

This function is not enabled when cu_calculator is closed, which can save a little resource, if necessary.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1107/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1107,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5IZQL5,incubator-pegasus,1214579449,1107,NA,WHBANG,38547944,,,NA,2022-08-15T03:34:39Z,2022-08-15T03:34:39Z,"related link:
https://github.com/apache/incubator-pegasus/pull/318","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5IZQL5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1113,https://api.github.com/repos/apache/incubator-pegasus/issues/1113,incubator-pegasus,1336956695,1113,"Build: Release,ASAN,UBSAN build failed",WHBANG,38547944,,,CLOSED,2022-08-12T09:13:58Z,2022-08-15T08:55:01Z,"```
[ 15%] Built target dsn.security
[ 16%] Building CXX object src/rdsn/src/zookeeper/CMakeFiles/dsn.replication.zookeeper_provider.dir/distributed_lock_service_zookeeper.cpp.o
[ 16%] Building CXX object src/rdsn/src/zookeeper/CMakeFiles/dsn.replication.zookeeper_provider.dir/lock_struct.cpp.o
[ 16%] Linking CXX static library libdsn_aio.a
[ 16%] Built target dsn_aio
[ 16%] Building CXX object src/rdsn/src/zookeeper/CMakeFiles/dsn.replication.zookeeper_provider.dir/zookeeper_error.cpp.o
[ 16%] Building CXX object src/rdsn/src/zookeeper/CMakeFiles/dsn.replication.zookeeper_provider.dir/zookeeper_session.cpp.o
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/zookeeper/zookeeper_session.cpp: In member function 'int dsn::dist::zookeeper_session::attach(void*, const state_callback&)':
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/zookeeper/zookeeper_session.cpp:158:13: error: 'zoo_sasl_params_t' was not declared in this scope
             zoo_sasl_params_t sasl_params = {0};
             ^~~~~~~~~~~~~~~~~
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/zookeeper/zookeeper_session.cpp:158:13: note: suggested alternative: 'sasl_rand_t'
             zoo_sasl_params_t sasl_params = {0};
             ^~~~~~~~~~~~~~~~~
             sasl_rand_t
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/zookeeper/zookeeper_session.cpp:159:13: error: 'sasl_params' was not declared in this scope
             sasl_params.service = dsn::security::FLAGS_zookeeper_kerberos_service_name;
             ^~~~~~~~~~~
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/zookeeper/zookeeper_session.cpp:159:13: note: suggested alternative: 'sasl_rand_s'
             sasl_params.service = dsn::security::FLAGS_zookeeper_kerberos_service_name;
             ^~~~~~~~~~~
             sasl_rand_s
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/zookeeper/zookeeper_session.cpp:161:23: error: 'zookeeper_init_sasl' was not declared in this scope
             _handle = zookeeper_init_sasl(zookeeper_session_mgr::instance().zoo_hosts(),
                       ^~~~~~~~~~~~~~~~~~~
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/zookeeper/zookeeper_session.cpp:161:23: note: suggested alternative: 'zookeeper_interest'
             _handle = zookeeper_init_sasl(zookeeper_session_mgr::instance().zoo_hosts(),
                       ^~~~~~~~~~~~~~~~~~~
                       zookeeper_interest
At global scope:
cc1plus: error: unrecognized command line option '-Wno-implicit-float-conversion' [-Werror]
cc1plus: error: unrecognized command line option '-Wno-deprecated-register' [-Werror]
cc1plus: error: unrecognized command line option '-Wno-inconsistent-missing-override' [-Werror]
cc1plus: all warnings being treated as errors
src/rdsn/src/zookeeper/CMakeFiles/dsn.replication.zookeeper_provider.dir/build.make:117: recipe for target 'src/rdsn/src/zookeeper/CMakeFiles/dsn.replication.zookeeper_provider.dir/zookeeper_session.cpp.o' failed
make[2]: *** [src/rdsn/src/zookeeper/CMakeFiles/dsn.replication.zookeeper_provider.dir/zookeeper_session.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
[ 17%] Building CXX object src/rdsn/src/failure_detector/CMakeFiles/dsn.failure_detector.dir/failure_detector.cpp.o
make[1]: *** [src/rdsn/src/zookeeper/CMakeFiles/dsn.replication.zookeeper_provider.dir/all] Error 2
CMakeFiles/Makefile2:1430: recipe for target 'src/rdsn/src/zookeeper/CMakeFiles/dsn.replication.zookeeper_provider.dir/all' failed
make[1]: *** Waiting for unfinished jobs....
[ 17%] Building CXX object src/rdsn/src/failure_detector/CMakeFiles/dsn.failure_detector.dir/failure_detector_multimaster.cpp.o
[ 17%] Building CXX object src/rdsn/src/failure_detector/CMakeFiles/dsn.failure_detector.dir/__/__/__/__/thrift-gen/fd_types.cpp.o
[ 17%] Linking CXX static library libdsn.failure_detector.a
[ 17%] Built target dsn.failure_detector
make: *** [all] Error 2
Makefile:135: recipe for target 'all' failed
Error: Process completed with exit code 2.
```

Related Links：
https://github.com/apache/incubator-pegasus/runs/7803102912?check_suite_focus=true
https://github.com/apache/incubator-pegasus/runs/7803102777?check_suite_focus=true
https://github.com/apache/incubator-pegasus/runs/7803103016?check_suite_focus=true
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1113/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1116,https://api.github.com/repos/apache/incubator-pegasus/issues/1116,incubator-pegasus,1338499534,1116,Feature(new_metrics): introduce data sink to collect snapshot from each metric periodically,empiredan,743379,Dan Wang,,OPEN,2022-08-15T03:45:16Z,2022-08-15T03:45:16Z,"This is the sub-issue of https://github.com/apache/incubator-pegasus/issues/922.

As a part of the framework of new metrics, the data sink, just as its name implies, is the destination where the metric data will go. One of the most important usage of metric system, is to collect the metrics it maintains to the external monitoring system to be observed.

The implementation adopted by new metrics is to periodically take snapshot from each metric of the registry. All data needed by external monitoring systems will be used to build the snapshot. There are abstract interfaces for data sink to accept the snapshot of each metric. The concrete monitoring system must implement these interfaces to collect the snapshots. The framework supports multiple external monitoring systems.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1116/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1118,https://api.github.com/repos/apache/incubator-pegasus/issues/1118,incubator-pegasus,1340026059,1118,chore: update NOTICE year to 2022,empiredan,743379,Dan Wang,,CLOSED,2022-08-16T08:53:23Z,2022-08-17T13:42:15Z,"Update NOTICE year to this year, namely 2022.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1118/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1120,https://api.github.com/repos/apache/incubator-pegasus/issues/1120,incubator-pegasus,1340138231,1120,chore: update file paths in LICENSE,empiredan,743379,Dan Wang,,CLOSED,2022-08-16T10:24:43Z,2022-08-17T13:38:35Z,"After refactor, some file paths in LICENSE have been updated.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1120/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1122,https://api.github.com/repos/apache/incubator-pegasus/issues/1122,incubator-pegasus,1340382072,1122,Fix the conditions for file changes that trigger workflows for github,empiredan,743379,Dan Wang,,CLOSED,2022-08-16T13:44:37Z,2022-08-17T13:38:06Z,"
While only doc files such as `LICENSE` or `NOTICE` are changed and committed, cpp workflows for github will report error as follows (see https://github.com/apache/incubator-pegasus/runs/7854426641?check_suite_focus=true):
```
Run rm -rf thirdparty
  rm -rf thirdparty
  tar -zcvhf release__builder.tar DSN_ROOT/ src/builder/bin src/builder/src/server/test/config.ini  --exclude='*CMakeFiles*'
  shell: sh -e {0}
tar: DSN_ROOT: Cannot stat: No such file or directory
tar: src/builder/bin: Cannot stat: No such file or directory
tar: src/builder/src/server/test/config.ini: Cannot stat: No such file or directory
tar: Exiting with failure status due to previous errors
Error: Process completed with exit code [2](https://github.com/apache/incubator-pegasus/runs/7854426137?check_suite_focus=true#step:11:2).
```

The binaries are not built since there is not any source file changed. However, since the binaries are used to be build packages which will be uploaded as artifacts, it will report ""No such file or directory"".","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1122/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1125,https://api.github.com/repos/apache/incubator-pegasus/issues/1125,incubator-pegasus,1342736539,1125,Fix: this issue tracking the github action  bug,foreverneverer,23136769,Jiashuo,js982986555@live.com,CLOSED,2022-08-18T08:27:00Z,2023-05-16T16:05:58Z,"## Bug Report

With the code update, the github action ci may run failed, this issue is used for tracking these problem","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1125/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1131,https://api.github.com/repos/apache/incubator-pegasus/issues/1131,incubator-pegasus,1345752937,1131,Limit udp service by config,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2022-08-22T03:24:44Z,2022-12-21T10:50:30Z,"## Feature Request

**Describe the feature you'd like:**
Config `rpc_call_channel` select what channel this RPC item used. But the UDP service always start on server whether RPC use UDP or not. 

**Describe alternatives you've considered:**
Add a configuration 'enable_udp' with a default value of 'false' in the 'network' section. It gives the server control over whether to start the UDP service.

If one of RPC config `rpc_call_channel` is `RPC_CHANNEL_UDP`, should make 'enable_udp' is `true` first.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1131/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1134,https://api.github.com/repos/apache/incubator-pegasus/issues/1134,incubator-pegasus,1346074635,1134,Feature: implement administrative tools for jemalloc,empiredan,743379,Dan Wang,,CLOSED,2022-08-22T09:09:59Z,2023-10-17T16:26:43Z,"Pegasus has been supported to be built with jemalloc in https://github.com/XiaoMi/rdsn/pull/910, https://github.com/apache/incubator-pegasus/pull/1050. However, we still require some tools to administer jemalloc dynamically, including:

- dump configurations and statistics to check current state of jemalloc 
- dump heap profile for analysis
- set configurations for jemalloc dynamically
- control GC

All sub-tasks can be tracked as follows:

- [x] https://github.com/apache/incubator-pegasus/pull/1143
- [x] https://github.com/apache/incubator-pegasus/pull/1133
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1134/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1135,https://api.github.com/repos/apache/incubator-pegasus/issues/1135,incubator-pegasus,1346227681,1135,Illegal instruction for unit tests of workflows running on github,empiredan,743379,Dan Wang,,CLOSED,2022-08-22T11:11:18Z,2022-08-25T09:25:47Z,"## 1. Problem

`Illegal instruction` are sometimes found for unit tests of workflows running on github. Some failed cases are listed as follows:

[Test ASAN (pegasus_unit_test)](https://github.com/apache/incubator-pegasus/runs/7946209146?check_suite_focus=true#logs)
```
 11550K .......... .......... .......... .......... .......... 95%  345M 0s
 11600K .......... .......... .......... .......... .......... 96% 1.65M 0s
 11650K .......... .......... .......... .......... .......... 96% 51.0M 0s
 11700K .......... .......... .......... .......... .......... 97%  297M 0s
 11750K .......... .......... .......... .......... .......... 97% 64.4M 0s
 11800K .......... .......... .......... .......... .......... 97%  303M 0s
 11850K .......... .......... .......... .......... .......... 98%  335M 0s
 11900K .......... .......... .......... .......... .......... 98% 49.1M 0s
 11950K .......... .......... .......... .......... .......... 99%  330M 0s
 12000K .......... .......... .......... .......... .......... 99%  152M 0s
 12050K .......... .......... .......... .......... .......   100%  261M=4.8s

2022-08-22 07:16:05 (2.46 MB/s) - 'apache-zookeeper-3.7.0-bin.tar.gz' saved [12387614/12387614]

Decompressing zookeeper...
ZooKeeper JMX enabled by default
Using config: /__w/incubator-pegasus/incubator-pegasus/.zk_install/apache-zookeeper-3.7.0-bin/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
Zookeeper started at port 22181
ZooKeeper JMX enabled by default
Using config: /__w/incubator-pegasus/incubator-pegasus/.zk_install/apache-zookeeper-3.7.0-bin/bin/../conf/zoo.cfg
Stopping zookeeper ... STOPPED
Clearing zookeeper ... CLEARED
ZooKeeper JMX enabled by default
Using config: /__w/incubator-pegasus/incubator-pegasus/.zk_install/apache-zookeeper-3.7.0-bin/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
Zookeeper started at port 22181
starting server
cd /__w/incubator-pegasus/incubator-pegasus/onebox/meta1 && /__w/incubator-pegasus/incubator-pegasus/onebox/meta1/pegasus_server config.ini -app_list meta &>result &
root         350     181  0 07:16 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/meta1/pegasus_server config.ini -app_list meta
cd /__w/incubator-pegasus/incubator-pegasus/onebox/meta2 && /__w/incubator-pegasus/incubator-pegasus/onebox/meta2/pegasus_server config.ini -app_list meta &>result &
root         358     181  0 07:16 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/meta2/pegasus_server config.ini -app_list meta
cd /__w/incubator-pegasus/incubator-pegasus/onebox/meta3 && /__w/incubator-pegasus/incubator-pegasus/onebox/meta3/pegasus_server config.ini -app_list meta &>result &
root         366     181  0 07:16 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/meta3/pegasus_server config.ini -app_list meta
cd /__w/incubator-pegasus/incubator-pegasus/onebox/replica1 && /__w/incubator-pegasus/incubator-pegasus/onebox/replica1/pegasus_server config.ini -app_list replica &>result &
cd /__w/incubator-pegasus/incubator-pegasus/onebox/replica2 && /__w/incubator-pegasus/incubator-pegasus/onebox/replica2/pegasus_server config.ini -app_list replica &>result &
root         383     181  0 07:16 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/replica2/pegasus_server config.ini -app_list replica
cd /__w/incubator-pegasus/incubator-pegasus/onebox/replica3 && /__w/incubator-pegasus/incubator-pegasus/onebox/replica3/pegasus_server config.ini -app_list replica &>result &
root         391     181  0 07:16 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/replica3/pegasus_server config.ini -app_list replica
Wait cluster to become healthy...
Sleeped for 1 seconds
./run.sh: line 1675:   405 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
./run.sh: line 582:   350 Illegal instruction     (core dumped) $PWD/pegasus_server config.ini -app_list meta &> result  (wd: /__w/incubator-pegasus/incubator-pegasus/onebox/meta1)
Sleeped for 2 seconds
./run.sh: line 582:   358 Illegal instruction     (core dumped) $PWD/pegasus_server config.ini -app_list meta &> result  (wd: /__w/incubator-pegasus/incubator-pegasus/onebox/meta2)
./run.sh: line 582:   366 Illegal instruction     (core dumped) $PWD/pegasus_server config.ini -app_list meta &> result  (wd: /__w/incubator-pegasus/incubator-pegasus/onebox/meta3)
./run.sh: line 582:   375 Illegal instruction     (core dumped) $PWD/pegasus_server config.ini -app_list replica &> result  (wd: /__w/incubator-pegasus/incubator-pegasus/onebox/replica1)
./run.sh: line 582:   383 Illegal instruction     (core dumped) $PWD/pegasus_server config.ini -app_list replica &> result  (wd: /__w/incubator-pegasus/incubator-pegasus/onebox/replica2)
./run.sh: line 582:   391 Illegal instruction     (core dumped) $PWD/pegasus_server config.ini -app_list replica &> result  (wd: /__w/incubator-pegasus/incubator-pegasus/onebox/replica3)
./run.sh: line 1675:   416 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 3 seconds
./run.sh: line 1675:   427 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 4 seconds
./run.sh: line 1675:   438 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 5 seconds
./run.sh: line 1675:   449 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 6 seconds
./run.sh: line 1675:   460 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 7 seconds
./run.sh: line 1675:   471 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 8 seconds
./run.sh: line 1675:   482 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 9 seconds
./run.sh: line 1675:   493 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 10 seconds
./run.sh: line 1675:   504 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 11 seconds
./run.sh: line 1675:   515 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 12 seconds
./run.sh: line 1675:   526 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 13 seconds
./run.sh: line 1675:   537 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 14 seconds
./run.sh: line 1675:   548 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 15 seconds
./run.sh: line 1675:   559 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 16 seconds
```

<br>
<br>
<br>

[Test UBSAN (pegasus_rproxy_test)](https://github.com/apache/incubator-pegasus/runs/7946978039?check_suite_focus=true#logs)
```
11800K .......... .......... .......... .......... .......... 97%  149M 0s
 11850K .......... .......... .......... .......... .......... 98%  230M 0s
 11900K .......... .......... .......... .......... .......... 98%  258M 0s
 11950K .......... .......... .......... .......... .......... 99%  119M 0s
 12000K .......... .......... .......... .......... .......... 99%  250K 0s
 12050K .......... .......... .......... .......... .......   100% 48.9M=3.8s

2022-08-22 08:14:35 (3.15 MB/s) - 'apache-zookeeper-3.7.0-bin.tar.gz' saved [12387614/12387614]

Decompressing zookeeper...
ZooKeeper JMX enabled by default
Using config: /__w/incubator-pegasus/incubator-pegasus/.zk_install/apache-zookeeper-3.7.0-bin/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
Zookeeper started at port 22181
ZooKeeper JMX enabled by default
Using config: /__w/incubator-pegasus/incubator-pegasus/.zk_install/apache-zookeeper-3.7.0-bin/bin/../conf/zoo.cfg
Stopping zookeeper ... STOPPED
Clearing zookeeper ... CLEARED
ZooKeeper JMX enabled by default
Using config: /__w/incubator-pegasus/incubator-pegasus/.zk_install/apache-zookeeper-3.7.0-bin/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
Zookeeper started at port 22181
starting server
cd /__w/incubator-pegasus/incubator-pegasus/onebox/meta1 && /__w/incubator-pegasus/incubator-pegasus/onebox/meta1/pegasus_server config.ini -app_list meta &>result &
root         346     177  0 08:14 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/meta1/pegasus_server config.ini -app_list meta
cd /__w/incubator-pegasus/incubator-pegasus/onebox/meta2 && /__w/incubator-pegasus/incubator-pegasus/onebox/meta2/pegasus_server config.ini -app_list meta &>result &
root         354     177  0 08:14 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/meta2/pegasus_server config.ini -app_list meta
cd /__w/incubator-pegasus/incubator-pegasus/onebox/meta3 && /__w/incubator-pegasus/incubator-pegasus/onebox/meta3/pegasus_server config.ini -app_list meta &>result &
root         362     177  0 08:14 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/meta3/pegasus_server config.ini -app_list meta
cd /__w/incubator-pegasus/incubator-pegasus/onebox/replica1 && /__w/incubator-pegasus/incubator-pegasus/onebox/replica1/pegasus_server config.ini -app_list replica &>result &
root         371     177  0 08:14 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/replica1/pegasus_server config.ini -app_list replica
cd /__w/incubator-pegasus/incubator-pegasus/onebox/replica2 && /__w/incubator-pegasus/incubator-pegasus/onebox/replica2/pegasus_server config.ini -app_list replica &>result &
root         379     177  0 08:14 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/replica2/pegasus_server config.ini -app_list replica
cd /__w/incubator-pegasus/incubator-pegasus/onebox/replica3 && /__w/incubator-pegasus/incubator-pegasus/onebox/replica3/pegasus_server config.ini -app_list replica &>result &
root         387     177  0 08:14 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/replica3/pegasus_server config.ini -app_list replica
Wait cluster to become healthy...
Sleeped for 1 seconds
./run.sh: line 1675:   401 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
./run.sh: line 582:   346 Illegal instruction     (core dumped) $PWD/pegasus_server config.ini -app_list meta &> result  (wd: /__w/incubator-pegasus/incubator-pegasus/onebox/meta1)
./run.sh: line 582:   354 Illegal instruction     (core dumped) $PWD/pegasus_server config.ini -app_list meta &> result  (wd: /__w/incubator-pegasus/incubator-pegasus/onebox/meta2)
./run.sh: line 582:   362 Illegal instruction     (core dumped) $PWD/pegasus_server config.ini -app_list meta &> result  (wd: /__w/incubator-pegasus/incubator-pegasus/onebox/meta3)
./run.sh: line 582:   371 Illegal instruction     (core dumped) $PWD/pegasus_server config.ini -app_list replica &> result  (wd: /__w/incubator-pegasus/incubator-pegasus/onebox/replica1)
./run.sh: line 582:   379 Illegal instruction     (core dumped) $PWD/pegasus_server config.ini -app_list replica &> result  (wd: /__w/incubator-pegasus/incubator-pegasus/onebox/replica2)
./run.sh: line 582:   387 Illegal instruction     (core dumped) $PWD/pegasus_server config.ini -app_list replica &> result  (wd: /__w/incubator-pegasus/incubator-pegasus/onebox/replica3)
Sleeped for 2 seconds
./run.sh: line 1675:   412 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 3 seconds
./run.sh: line 1675:   423 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 4 seconds
./run.sh: line 1675:   434 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 5 seconds
./run.sh: line 1675:   445 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 6 seconds
./run.sh: line 1675:   456 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 7 seconds
./run.sh: line 1675:   467 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 8 seconds
./run.sh: line 1675:   478 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 9 seconds
./run.sh: line 1675:   489 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 10 seconds
./run.sh: line 1675:   500 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 11 seconds
./run.sh: line 1675:   511 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 12 seconds
./run.sh: line 1675:   522 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 13 seconds
./run.sh: line 1675:   533 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 14 seconds
./run.sh: line 1675:   544 Illegal instruction     (core dumped) ./pegasus_shell ${CONFIG} $CLUSTER_NAME
Sleeped for 15 seconds
```

## 2. Analysis

This may be caused by distributing tests running on a node that does not support instructions needed by `rocksdb`. Thus unit tests should be built with portable `rocksdb`; docker image of third-parties dedicated to unit tests should also be provided.

## 3. Task List

- [x] https://github.com/apache/incubator-pegasus/pull/1136
- [ ] https://github.com/apache/incubator-pegasus/pull/1138","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1135/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1140,https://api.github.com/repos/apache/incubator-pegasus/issues/1140,incubator-pegasus,1352184535,1140,bulkload failed when app with one replica fator and load small amount of data,xihong08,74220545,zhuxihong,,CLOSED,2022-08-26T12:12:45Z,2022-09-08T06:14:02Z,"## Bug Report
I try to use bulkload for one replica fator's and 30 partitions's app, and the app just has small amount of data.
but this bulkload failed. I found the key information in the meta's log. 
That's about it. 'partition doesn't have bulk load metadata, set bulk load failed'.  
this is at beginning of ingest status for checking if update partion metadata.
and then, i found the meta log for 'update bulk load metadata', this mean the partiton's metadata should been updated. 
and i check zookkeeper's metadata for partion, found parts of 30 partitions occur this, doesn't have bulk load metadata, 
---------------------------------------------------------------------
It's a matter of probability.
so I suspect the matadata was overwritten after the update. and check the code handle_app_downloading, 
update_partition_metadata_on_remote_storage and update_partition_info_on_remote_storage‘s Order not guaranteed for asynchronous Multithreading","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1140/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1140,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Jt7TS,incubator-pegasus,1236776146,1140,NA,hycdong,17868458,HeYuchen,377710264@qq.com,NA,2022-09-05T09:44:28Z,2022-09-05T09:44:28Z,"Thanks for your bug report~

> `update_partition_metadata_on_remote_storage` and `update_partition_info_on_remote_storage` ‘s order are not guaranteed.

Function `update_partition_metadata_on_remote_storage` and `update_partition_info_on_remote_storage` are execute in thread pool `THREAD_POOL_META_STATE`, which is single thread in current pegasus design. As a result, the order can be guaranteed, `update_partition_metadata_on_remote_storage` will always execute before `update_partition_info_on_remote_storage`. 

However, in current bulk load design, bulk load will be failed if there are some partitions don't have data. For example, if you have a table will 16 partitions, and partition[0] doesn't have data to be ingested, bulk load will be failed.

I suggest:
1. check your configuration whether threadpool THREAD_POOL_META_STATE is single thread
2. check if all partitions have data to be ingested.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Jt7TS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1140,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5J7P3w,incubator-pegasus,1240268272,1140,NA,xihong08,74220545,zhuxihong,,NA,2022-09-08T06:12:53Z,2022-09-08T06:12:53Z,"> Thanks for your bug report~
> 
> > `update_partition_metadata_on_remote_storage` and `update_partition_info_on_remote_storage` ‘s order are not guaranteed.
> 
> Function `update_partition_metadata_on_remote_storage` and `update_partition_info_on_remote_storage` are execute in thread pool `THREAD_POOL_META_STATE`, which is single thread in current pegasus design. As a result, the order can be guaranteed, `update_partition_metadata_on_remote_storage` will always execute before `update_partition_info_on_remote_storage`.
> 
> However, in current bulk load design, bulk load will be failed if there are some partitions don't have data. For example, if you have a table will 16 partitions, and partition[0] doesn't have data to be ingested, bulk load will be failed.
> 
> I suggest:
> 
> 1. check your configuration whether threadpool THREAD_POOL_META_STATE is single thread
> 2. check if all partitions have data to be ingested.


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5J7P3w/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1140,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5J7P_h,incubator-pegasus,1240268769,1140,NA,xihong08,74220545,zhuxihong,,NA,2022-09-08T06:13:08Z,2022-09-08T06:13:08Z,"> > Thanks for your bug report~
> > > `update_partition_metadata_on_remote_storage` and `update_partition_info_on_remote_storage` ‘s order are not guaranteed.
> > 
> > 
> > Function `update_partition_metadata_on_remote_storage` and `update_partition_info_on_remote_storage` are execute in thread pool `THREAD_POOL_META_STATE`, which is single thread in current pegasus design. As a result, the order can be guaranteed, `update_partition_metadata_on_remote_storage` will always execute before `update_partition_info_on_remote_storage`.
> > However, in current bulk load design, bulk load will be failed if there are some partitions don't have data. For example, if you have a table will 16 partitions, and partition[0] doesn't have data to be ingested, bulk load will be failed.
> > I suggest:
> > 
> > 1. check your configuration whether threadpool THREAD_POOL_META_STATE is single thread
> > 2. check if all partitions have data to be ingested.

THREAD_POOL_META_STATE has 2 threads
all partions have data need to ingested
update partition's metadata failed is my issue。like this,  my partition's data has 30M, but when start ingest , prompte 'partition doesn't have bulk load metadata, set bulk load failed'.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5J7P_h/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1144,https://api.github.com/repos/apache/incubator-pegasus/issues/1144,incubator-pegasus,1353098279,1144,spotbugs detect some issues should be fixed,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-08-27T17:17:44Z,2023-04-06T03:48:53Z,"This patch[1] find some issues reported by spotbugs, but it just disabled these code, we have to fix it later.


1. https://github.com/apache/incubator-pegasus/pull/1142","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1144/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1144,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5YEXgT,incubator-pegasus,1477539859,1144,NA,shalk,2435781,shalk(xiao kun),,NA,2023-03-21T09:49:41Z,2023-03-21T09:49:41Z,i can solve it ,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5YEXgT/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1145,https://api.github.com/repos/apache/incubator-pegasus/issues/1145,incubator-pegasus,1353914260,1145,Apache Pegasus 2022 Meetup,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2022-08-29T08:38:03Z,2022-11-07T08:01:26Z,"## Apache Pegasus 2022 Meetup

*活动介绍*

> ***首次活动***

2021 年 9 月，Apache Pegasus 在小米科技园举办了首次 Meetup，此次活动获得了线上线下近千名参与者的极大认可，现场获得一致好评。

![image](https://user-images.githubusercontent.com/48315319/187151455-1d65455b-3858-420c-922e-d185c05c3aa4.png)


_Apache Pegasus 首次 Meetup 圆满落幕：https://mp.weixin.qq.com/s/Uc4dcTRSyrUbhDlEvfal1g_

> ***本次活动***

一年又如许，经过一年的洗礼，Apache Pegasus 又得到了成长，为促进社区与用户之间的交流，加强开发者之间的沟通，扩大社区的技术影响力，2022 我们再次出发。去年有两位神策的同学在 meetup 上进行了分享，此前不久神策获得了 Apache Pegasus 首个非小米产生的 committer，此次神策希望完成接棒奔跑，计划 11 月 5 日（周六）于北京海淀区量子芯座（地铁知春路站f口）举办新一届 Meetup。此次活动依旧会提供线上渠道进行参与，希望不能够到场的开源用户们也能够积极响应。

![image](https://user-images.githubusercontent.com/48315319/187154555-8cc8da38-5987-4909-8fa6-e8c4d223362e.png)


*会议内容*

> 新的变化

**Apache Pegasus 2.4.0 版本正在发版筹备中**

_2.4.0版本发版 issue：https://github.com/apache/incubator-pegasus/issues/1032_

**Apache Pegasus 迎来新的 committer 成员**

_committer 王聃：https://mp.weixin.qq.com/s/AbOPa5P4MBWRdqMLvlRDGQ_


> 投稿招募

Apache Pegasus 期待各位社区参与者的投稿以及分享，内容包括不限于：

- Pegasus 的设计原理、实现细节
- Pegasus 的功能特性、使用场景
- Pegasus 的部署与维护实践
- 如何在复杂业务中使用 Pegasus
- 如何使用 Pegasus 打造高可用数据仓库
- 如何参与 Apache Pegasus 的开源建设

Apache Pegasus 2022 Meetup 欢迎各位开源用户的参与！期待金风送爽的十月与您相遇。
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1145/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1145,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5JVG16,incubator-pegasus,1230269818,1145,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-08-29T13:09:09Z,2022-08-29T13:09:09Z,Good news! Let's meet offline this October.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5JVG16/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1145,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5MOXO7,incubator-pegasus,1278833595,1145,NA,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,NA,2022-10-14T10:41:32Z,2022-10-14T10:41:32Z,非常抱歉的通知大家，由于疫情控制等多重原因，原计划于 10.22 进行的 2022 meetup，延期 2 周于 11.5 原场地进行，希望届时大家能够到场支持！,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5MOXO7/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1145,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5NBs4T,incubator-pegasus,1292291603,1145,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-10-26T16:18:22Z,2022-10-26T16:18:22Z,"![image](https://user-images.githubusercontent.com/10775040/198078540-3af40c18-b66a-4925-8606-dd6f9611bf8d.png)

## 报名方式

扫描封面二维码↑↑↑

或戳 [这里](https://sdmarketing.wjx.cn/vm/PmUZLzu.aspx)

## 时间

2022.11.5（周六） 14:00~18:00

## 地点

北京市海淀区知春路27号量子芯座大厦

## 产品介绍

Apache Pegasus (incubating) 是一个可水平扩展、高性能、强一致的分布式Key-Value数据库，在日千亿级流量下可保证 P999 毫秒级的请求延迟，吞吐量高，数据持久化存储，对用户无感知的弹性扩缩容等运维操作。同时还有热备份、冷备份、Bulk Load、Partition Split、Backup Request 等丰富的功能。Apache Pegasus 的目标是服务数据规模大，对延迟敏感高，对数据有一致性和持久化存储需求的业务。

## 活动介绍

本次活动邀请了来自多家公司的Pegasus的研发运维团队、对Pegasus有深度使用的业务系统研发团队的核心成员，来分享他们是如何设计和实现Pegasus的核心功能、探究和扩展Pegasus的使用场景、结合业务系统特点来充分利用Pegasus，以及如何在复杂多样的私有化部署场景下，保障Pegasus能够稳定高效地部署和运行在1000多个客户环境中的。

### 参与本次Meetup，你能够

- 深入理解分布式KV存储系统的设计原理、实现细节和功能特性
- 获知Pegasus最新版本2.4.0的新特性、优化等改进
- 探讨如何结合Pegasus的特性，拓展他的使用场景，提升业务系统的能力
- 学习如何在私有云下支持 1000+ 客户的部署和运维
- 交流如何参与开源社区的共建
- 参与活动领取精美纪念品

**最后，我们还邀请了Pegasus的资深开发者来介绍RisingWave这个新兴的云原生流式数据库。**

## 活动议程

### 开场致辞

时间：14:00~14:15
张铎，Apache Pegasus mentor，Apache HBase PMC chair，神策数据首席架构师

---

### Apache Pegasus 2.4.0：构建更加高效、稳定与易用的KV系统

时间：14:15~14:50
分享人：贾硕，Apache Pegasus PPMC成员，前小米工程师，目前在字节跳动从事OLAP存储引擎研发

#### 内容简介

Apache Pegasus 2.4 是2022年发布的第一个稳定大版本。在该版本中，我们把更多社区生态工具引入到了Apache Pegasus项目中，同时提供了大量的新特性以增强系统的可靠性、易用性以及读写性能，例如新的日志引擎架构、全新的批量读取等。除此之外，针对Apache Pegasus的两个重点模块：Bulkload和Dupliction，我们在实践中做了大量的改进和重构，以更好的应对复杂的业务场景。本主题将介绍Apache Pegasus 2.4.0带来的全新变化。

---

### Apache Pegasus离在线融合建设与实践

时间：14:50~15:25
分享人：王伟，小米高级软件工程师，负责KV存储系统的研发工作

#### 内容简介

Apache Pegasus作为在线存储系统，与离线系统天然隔离。随着行业发展，HTAP概念出现，离在线的边界越来越模糊。Apache Pegasus支持离线能力的需求也越来越迫切，本主题将介绍Apache Pegasus针对离在线融合方面所做的工作。

---

### 小米通用推荐算法架构及Pegasus在用户画像中的应用

时间：15:25~16:00
分享人：梁伟，18年加入小米后一直从事推荐算法架构方面的工作，核心参与了高性能特征抽取工具及大规模并行打分服务的研发，目前负责小米应用商店、游戏中心、小米视频、电视视频等业务的推荐算法架构工作。

#### 内容简介

介绍小米应用商店、游戏中心、小米视频、电视视频及zili等业务中的推荐算法架构以及Pegasus在大规模用户画像中的应用。

---

### 茶歇时间

16:00~16:15

---

### 如何在复杂toB场景下对Pegasus进行持续改进

时间：16:15~16:50
分享人：王浩，神策数据分布式研发工程师，负责Pegasus存储的研发工作，Apache Pegasus社区的新生力量

#### 内容简介

分析Apache Pegasus在神策数据各个业务场景的数据特点，结合复杂多样的客户特征介绍神策是如何工程化管理上千家客户的，并根据神策客户需求讲述了Pegasus是如何接入数据迁移、鉴权、容器化等工作。

---

### Apache Pegasus在神策广告数据流中的应用

时间：16:50~17:25
分享人：史佼明，神策数据广告架构师，负责广告投放分析系统架构研发工作

#### 内容简介

介绍神策广告数据流架构以及 Apache Pegasus 在神策广告数据流中的应用。

---

### RisingWave：如何用云、流、数据库低成本开发实时应用

时间：17:25~18:00
分享人：吴涛，Apache Pegasus PPMC成员，前小米工程师，当前在RisingWave Labs担任产品经理

#### 内容简介

介绍RisingWave是如何通过流计算架构来开发实时应用，同时利用云原生的架构来降低部署成本，并结合数据库来提供极易上手的开发体验。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5NBs4T/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1145,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5NzAX7,incubator-pegasus,1305216507,1145,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-11-07T08:00:42Z,2022-11-07T08:00:42Z,"The meetup has been held successfully, thanks everybody!
![wecom-temp-450708-1c6c04295ea6efb5c4bff4703afb7eb8](https://user-images.githubusercontent.com/10775040/200256299-2b261fc2-eb87-44de-b430-dfc712385e05.png)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5NzAX7/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1147,https://api.github.com/repos/apache/incubator-pegasus/issues/1147,incubator-pegasus,1357259811,1147,feat: expose parameter success_if_exist for interface create_app,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2022-08-31T12:23:55Z,2023-01-16T16:42:11Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
It's succeed when we create one existed table. Actually, I need a failed result remind it.

**Describe the feature you'd like:**
Option `success_if_exist ` on create_table always be true, we can make it expose.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1147/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1151,https://api.github.com/repos/apache/incubator-pegasus/issues/1151,incubator-pegasus,1364760251,1151,Table `stat` may not created when start collector,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-09-07T14:31:42Z,2022-09-14T07:15:43Z,"The role `collector` uses table `stat` to store statistics data, but maybe it is not created in the cluster, which will cause error.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1151/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1153,https://api.github.com/repos/apache/incubator-pegasus/issues/1153,incubator-pegasus,1366234313,1153,feat: make java client interface closable,shalk,2435781,shalk(xiao kun),,CLOSED,2022-09-08T12:36:11Z,2022-09-13T03:17:50Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
PegasusAdminClientInterface / PegasusClientInterface / PegasusClientInterface can not work with 
`try-with` syntax in java

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->
so we can make these interface `Closable`， and this will not add any new method 

**Describe alternatives you've considered:**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->

**Teachability, Documentation, Adoption, Migration Strategy:**
<!-- If you can, explain some scenarios how users might use this, situations it would be helpful in. Any API designs, mockups, or diagrams are also helpful. -->
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1153/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1157,https://api.github.com/repos/apache/incubator-pegasus/issues/1157,incubator-pegasus,1367808633,1157,Wrong logging format was found in meta server_state,empiredan,743379,Dan Wang,,CLOSED,2022-09-09T13:10:05Z,2022-11-05T18:44:55Z,"Some logs are found for meta server as follows:
```
F2022-09-09 17:02:17.184 (1662714137184230445 11231)   meta.meta_state0.0202000700000001: server_state.cpp:895:on_config_sync(): assertion expression: false
F2022-09-09 17:02:17.184 (1662714137184247617 11231)   meta.meta_state0.0202000700000001: server_state.cpp:895:on_config_sync(): gpid({}) on node({}) is not exist on meta server, administrator should check consistency of meta data
```

The key point is `gpid({}) on node({})` in logs of meta server rather than the fatal level, which is the result of connecting to a wrong replica server. `gpid({}) on node({})` must be caused by invalid logging format.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1157/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1157,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5NwsD5,incubator-pegasus,1304609017,1157,NA,empiredan,743379,Dan Wang,,NA,2022-11-05T18:43:04Z,2022-11-05T18:43:04Z,This issue has been fixed by https://github.com/apache/incubator-pegasus/pull/1217.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5NwsD5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1159,https://api.github.com/repos/apache/incubator-pegasus/issues/1159,incubator-pegasus,1367850026,1159,Improve image building for binary third-parties,empiredan,743379,Dan Wang,,CLOSED,2022-09-09T13:43:14Z,2022-09-22T06:17:08Z,"Several improvements can be made to reduce the time of [building images for binary third-parties](https://github.com/apache/incubator-pegasus/blob/master/.github/workflows/thirdparty-regular-push.yml):

- build all images for binary third-parties concurrently
- only build `ubuntu1804` for test images since it's the only OS version for test","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1159/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1160,https://api.github.com/repos/apache/incubator-pegasus/issues/1160,incubator-pegasus,1369291583,1160,"Unknown option ""--on_travis"" is reported for unit tests of jemalloc",empiredan,743379,Dan Wang,,CLOSED,2022-09-12T05:13:27Z,2022-09-13T03:35:38Z,"While workflow is running for unit tests of jemalloc, error is reported as follows(https://github.com/apache/incubator-pegasus/runs/8288363818?check_suite_focus=true):
```
Decompressing HDFS...
Error: unknow option ""--on_travis""

Options for subcommand 'test':
   -h|--help         print the help info
   -m|--modules      specify modules to test, split by ',',
                     e.g., ""pegasus_unit_test,dsn_runtime_tests,dsn_meta_state_tests"",
                     if not set, then run all tests
   -k|--keep_onebox  whether keep the onebox after the test[default false]
Error: Process completed with exit code 1.
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1160/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1162,https://api.github.com/repos/apache/incubator-pegasus/issues/1162,incubator-pegasus,1372281768,1162,detect_hotspot_test.write_hotspot_data failed frequently for UBSAN,empiredan,743379,Dan Wang,,OPEN,2022-09-14T03:38:38Z,2022-09-15T10:20:04Z,"
```
2022-09-13 05:54:54 (3.42 MB/s) - 'apache-zookeeper-3.7.0-bin.tar.gz' saved [12387614/12387614]

Decompressing zookeeper...
ZooKeeper JMX enabled by default
Using config: /__w/incubator-pegasus/incubator-pegasus/.zk_install/apache-zookeeper-3.7.0-bin/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
Zookeeper started at port 22181
starting server
cd /__w/incubator-pegasus/incubator-pegasus/onebox/meta1 && /__w/incubator-pegasus/incubator-pegasus/onebox/meta1/pegasus_server config.ini -app_list meta &>result &
cd /__w/incubator-pegasus/incubator-pegasus/onebox/meta2 && /__w/incubator-pegasus/incubator-pegasus/onebox/meta2/pegasus_server config.ini -app_list meta &>result &
root         282     178  0 05:54 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/meta2/pegasus_server config.ini -app_list meta
cd /__w/incubator-pegasus/incubator-pegasus/onebox/meta3 && /__w/incubator-pegasus/incubator-pegasus/onebox/meta3/pegasus_server config.ini -app_list meta &>result &
root         290     178  0 05:54 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/meta3/pegasus_server config.ini -app_list meta
cd /__w/incubator-pegasus/incubator-pegasus/onebox/replica1 && /__w/incubator-pegasus/incubator-pegasus/onebox/replica1/pegasus_server config.ini -app_list replica &>result &
cd /__w/incubator-pegasus/incubator-pegasus/onebox/replica2 && /__w/incubator-pegasus/incubator-pegasus/onebox/replica2/pegasus_server config.ini -app_list replica &>result &
root         307     178  0 05:54 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/replica2/pegasus_server config.ini -app_list replica
cd /__w/incubator-pegasus/incubator-pegasus/onebox/replica3 && /__w/incubator-pegasus/incubator-pegasus/onebox/replica3/pegasus_server config.ini -app_list replica &>result &
root         315     178  0 05:54 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/replica3/pegasus_server config.ini -app_list replica
cd /__w/incubator-pegasus/incubator-pegasus/onebox/collector && /__w/incubator-pegasus/incubator-pegasus/onebox/collector/pegasus_server config.ini -app_list collector &>result &
Wait cluster to become healthy...
Sleeped for 1 seconds
Sleeped for 2 seconds
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/dsn_message_parser.cpp:163:14: runtime error: load of misaligned address 0x1459f40010b6 for type 'uint32_t', which requires 4 byte alignment
0x1459f40010b6: note: pointer points here
 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00 00 00  00 00 0c 9d 4a aa d8 05  72 56
             ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/tool-api/rpc_message.h:157:86: runtime error: member access within misaligned address 0x1459f40010aa for type 'struct message_header', which requires 8 byte alignment
0x1459f40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/dsn_message_parser.cpp:185:17: runtime error: member access within misaligned address 0x1459f40010aa for type 'struct message_header', which requires 8 byte alignment
0x1459f40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/network.cpp:397:45: runtime error: member call on misaligned address 0x1459f400111a for type 'struct rpc_address', which requires 8 byte alignment
0x1459f400111a: note: pointer points here
 00 00  00 00 01 00 2b 87 01 00  00 7f 10 27 00 00 00 00  00 00 00 00 00 00 00 00  00 00 45 52 52 5f
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/tool-api/rpc_address.h:147:52: runtime error: member access within misaligned address 0x1459f400111a for type 'const struct rpc_address', which requires 8 byte alignment
0x1459f400111a: note: pointer points here
 00 00  00 00 01 00 2b 87 01 00  00 7f 10 27 00 00 00 00  00 00 00 00 00 00 00 00  00 00 45 52 52 5f
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/network.cpp:408:42: runtime error: member access within misaligned address 0x1459f40010aa for type 'struct message_header', which requires 8 byte alignment
0x1459f40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/network.cpp:437:32: runtime error: member access within misaligned address 0x1459f40010aa for type 'struct message_header', which requires 8 byte alignment
0x1459f40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/rpc_message.cpp:84:10: runtime error: member access within misaligned address 0x1459f40010aa for type 'struct message_header', which requires 8 byte alignment
0x1459f40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/rpc_message.cpp:89:46: runtime error: member access within misaligned address 0x1459f40010aa for type 'struct message_header', which requires 8 byte alignment
0x1459f40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/rpc_message.cpp:90:46: runtime error: member access within misaligned address 0x1459f40010aa for type 'struct message_header', which requires 8 byte alignment
0x1459f40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/rpc_engine.cpp:161:50: runtime error: member access within misaligned address 0x1459f40010aa for type 'struct message_header', which requires 8 byte alignment
0x1459f40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/cpp/serialization.h:101:78: runtime error: member access within misaligned address 0x1459f40010aa for type 'struct message_header', which requires 8 byte alignment
0x1459f40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/cpp/serialization.h:101:78: runtime error: member access within misaligned address 0x1459f400126a for type 'struct message_header', which requires 8 byte alignment
0x1459f400126a: note: pointer points here
 00 00  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 58 01 00 00 00 00  00 00 03 00 00 00
              ^ 
Sleeped for 3 seconds
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/dsn_message_parser.cpp:163:14: runtime error: load of misaligned address 0x151fa40010b6 for type 'uint32_t', which requires 4 byte alignment
0x151fa40010b6: note: pointer points here
 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00 00 00  00 00 b8 db 3b 99 87 03  b1 bd
             ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/tool-api/rpc_message.h:157:86: runtime error: member access within misaligned address 0x151fa40010aa for type 'struct message_header', which requires 8 byte alignment
0x151fa40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/dsn_message_parser.cpp:185:17: runtime error: member access within misaligned address 0x151fa40010aa for type 'struct message_header', which requires 8 byte alignment
0x151fa40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/network.cpp:397:45: runtime error: member call on misaligned address 0x151fa400111a for type 'struct rpc_address', which requires 8 byte alignment
0x151fa400111a: note: pointer points here
 00 00  00 00 01 00 2b 87 01 00  00 7f 10 27 00 00 00 00  00 00 00 00 00 00 00 00  00 00 45 52 52 5f
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/tool-api/rpc_address.h:147:52: runtime error: member access within misaligned address 0x151fa400111a for type 'const struct rpc_address', which requires 8 byte alignment
0x151fa400111a: note: pointer points here
 00 00  00 00 01 00 2b 87 01 00  00 7f 10 27 00 00 00 00  00 00 00 00 00 00 00 00  00 00 45 52 52 5f
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/network.cpp:408:42: runtime error: member access within misaligned address 0x151fa40010aa for type 'struct message_header', which requires 8 byte alignment
0x151fa40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/network.cpp:437:32: runtime error: member access within misaligned address 0x151fa40010aa for type 'struct message_header', which requires 8 byte alignment
0x151fa40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/rpc_message.cpp:84:10: runtime error: member access within misaligned address 0x151fa40010aa for type 'struct message_header', which requires 8 byte alignment
0x151fa40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/rpc_message.cpp:89:46: runtime error: member access within misaligned address 0x151fa40010aa for type 'struct message_header', which requires 8 byte alignment
0x151fa40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/rpc_message.cpp:90:46: runtime error: member access within misaligned address 0x151fa40010aa for type 'struct message_header', which requires 8 byte alignment
0x151fa40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/rpc_engine.cpp:161:50: runtime error: member access within misaligned address 0x151fa40010aa for type 'struct message_header', which requires 8 byte alignment
0x151fa40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 02 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/rpc_message.cpp:89:46: runtime error: member access within misaligned address 0x153bd4011469 for type 'struct message_header', which requires 8 byte alignment
0x153bd4011469: note: pointer points here
 00 03 00  00 52 44 53 4e 00 00 00  00 c0 00 00 00 00 00 00  00 84 02 00 00 00 00 00  00 03 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/rpc_message.cpp:90:46: runtime error: member access within misaligned address 0x153bd4011469 for type 'struct message_header', which requires 8 byte alignment
0x153bd4011469: note: pointer points here
 00 03 00  00 52 44 53 4e 00 00 00  00 c0 00 00 00 00 00 00  00 84 02 00 00 00 00 00  00 03 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/rpc_engine.cpp:161:50: runtime error: member access within misaligned address 0x153bd4011469 for type 'struct message_header', which requires 8 byte alignment
0x153bd4011469: note: pointer points here
 00 03 00  00 52 44 53 4e 00 00 00  00 c0 00 00 00 00 00 00  00 84 02 00 00 00 00 00  00 03 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/cpp/serialization.h:101:78: runtime error: member access within misaligned address 0x153bd4011469 for type 'struct message_header', which requires 8 byte alignment
0x153bd4011469: note: pointer points here
 00 03 00  00 52 44 53 4e 00 00 00  00 c0 00 00 00 00 00 00  00 84 02 00 00 00 00 00  00 03 00 00 00
              ^ 
hotspot_test not ready yet, still waiting... (0/8)
hotspot_test not ready yet, still waiting... (0/8)
hotspot_test not ready yet, still waiting... (0/8)
hotspot_test not ready yet, still waiting... (0/8)
hotspot_test not ready yet, still waiting... (3/8)
hotspot_test not ready yet, still waiting... (3/8)
hotspot_test not ready yet, still waiting... (3/8)
hotspot_test not ready yet, still waiting... (3/8)
hotspot_test not ready yet, still waiting... (3/8)
hotspot_test not ready yet, still waiting... (5/8)
hotspot_test not ready yet, still waiting... (5/8)
hotspot_test not ready yet, still waiting... (5/8)
hotspot_test not ready yet, still waiting... (5/8)
hotspot_test not ready yet, still waiting... (5/8)
hotspot_test is ready now: (8/8)
hotspot_test is ready now!
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/cpp/serialization.h:101:78: runtime error: member access within misaligned address 0x153bd4014905 for type 'struct message_header', which requires 8 byte alignment
0x153bd4014905: note: pointer points here
 00 00 00 00 52 44 53  4e 00 00 00 00 c0 00 00  00 00 00 00 00 04 03 00  00 00 00 00 00 12 00 00  00
             ^ 
start testing write hotspot data...
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/cpp/serialization.h:101:78: runtime error: member access within misaligned address 0x153bd4000c8c for type 'struct message_header', which requires 8 byte alignment
0x153bd4000c8c: note: pointer points here
  2b 00 01 00 52 44 53 4e  00 00 00 00 c0 00 00 00  00 00 00 00 0c 00 00 00  00 00 00 00 14 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/cpp/serialization.h:101:78: runtime error: member access within misaligned address 0x153bd4014cc9 for type 'struct message_header', which requires 8 byte alignment
0x153bd4014cc9: note: pointer points here
 00 00 00  00 52 44 53 4e 00 00 00  00 c0 00 00 00 00 00 00  00 04 03 00 00 00 00 00  00 15 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/cpp/serialization.h:101:78: runtime error: member access within misaligned address 0x153bd4021aab for type 'struct message_header', which requires 8 byte alignment
0x153bd4021aab: note: pointer points here
 30  32 00 00 52 44 53 4e 00  00 00 00 c0 00 00 00 00  00 00 00 3b 00 00 00 00  00 00 00 17 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/cpp/serialization.h:101:78: runtime error: member access within misaligned address 0x153bb802a83b for type 'struct message_header', which requires 8 byte alignment
0x153bb802a83b: note: pointer points here
 30  31 00 00 52 44 53 4e 00  00 00 00 c0 00 00 00 00  00 00 00 5b 00 00 00 00  00 00 00 2f 0f 02 00
              ^ 
D2022-09-13 05:56:01.16 (1663048561016213057 3329)  mimic.default7.01000cdd00030036: client session created, remote_server = 127.0.0.1:34802, current_count = 3
D2022-09-13 05:56:01.16 (1663048561016533987 3300)  mimic.io-thrd.03300: client session connected, remote_server = 127.0.0.1:34802, current_count = 3
D2022-09-13 05:56:01.18 (1663048561018[6555](https://github.com/apache/incubator-pegasus/actions/runs/3042192020/jobs/4901540660#step:7:6556)88 3293)  mimic.io-thrd.03293: client session created, remote_server = 127.0.0.1:34801, current_count = 4
D2022-09-13 05:56:01.18 (1663048561018805002 3298)  mimic.io-thrd.03298: client session connected, remote_server = 127.0.0.1:34801, current_count = 4
D2022-09-13 05:56:01.36 (1663048561036992024 3293)  mimic.io-thrd.03293: client session created, remote_server = 127.0.0.1:34803, current_count = 5
D2022-09-13 05:56:01.37 (1663048561037256049 3297)  mimic.io-thrd.03297: client session connected, remote_server = 127.0.0.1:34803, current_count = 5
write hotspot data passed.....
start testing write random data...
write random data passed.....
start testing max detection time...
max detection time passed.....
start testing read hotspot data...
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/cpp/serialization.h:101:78: runtime error: member access within misaligned address 0x153bb8021019 for type 'struct message_header', which requires 8 byte alignment
0x153bb8021019: note: pointer points here
 30 31 00  00 52 44 53 4e 00 00 00  00 c0 00 00 00 00 00 00  00 3e 00 00 00 00 00 00  00 dc 18 06 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/test/function_test/detect_hotspot_test/test_detect_hotspot.cpp:118: Failure
Value of: find_hotkey
  Actual: false
Expected: true
/__w/incubator-pegasus/incubator-pegasus/src/test/function_test/detect_hotspot_test/test_detect_hotspot.cpp:198: Failure
Expected: get_result(detection_type::read_data, key_type::hotspot_dataset) doesn't generate new fatal failures in the current thread.
  Actual: it does.
/__w/incubator-pegasus/incubator-pegasus/src/test/function_test/detect_hotspot_test/test_detect_hotspot.cpp:229: Failure
Expected: read_hotspot_data() doesn't generate new fatal failures in the current thread.
  Actual: it does.
[  FAILED  ] detect_hotspot_test.write_hotspot_data (580121 ms)
[----------] 1 test from detect_hotspot_test (580121 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (580126 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] detect_hotspot_test.write_hotspot_data

 1 FAILED TEST
dsn exit with code 1
./run.sh: line 399:   323 Segmentation fault      (core dumped) $PWD/pegasus_server config.ini -app_list collector &> result  (wd: /__w/incubator-pegasus/incubator-pegasus/onebox/collector)
Error: Process completed with exit code 1.
```

```
2022-09-13 06:12:33 (2.42 MB/s) - 'apache-zookeeper-3.7.0-bin.tar.gz' saved [12387614/12387614]

Decompressing zookeeper...
ZooKeeper JMX enabled by default
Using config: /__w/incubator-pegasus/incubator-pegasus/.zk_install/apache-zookeeper-3.7.0-bin/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
Zookeeper started at port 22181
starting server
cd /__w/incubator-pegasus/incubator-pegasus/onebox/meta1 && /__w/incubator-pegasus/incubator-pegasus/onebox/meta1/pegasus_server config.ini -app_list meta &>result &
root         278     182  0 06:12 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/meta1/pegasus_server config.ini -app_list meta
cd /__w/incubator-pegasus/incubator-pegasus/onebox/meta2 && /__w/incubator-pegasus/incubator-pegasus/onebox/meta2/pegasus_server config.ini -app_list meta &>result &
root         286     182  0 06:12 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/meta2/pegasus_server config.ini -app_list meta
cd /__w/incubator-pegasus/incubator-pegasus/onebox/meta3 && /__w/incubator-pegasus/incubator-pegasus/onebox/meta3/pegasus_server config.ini -app_list meta &>result &
root         294     182  0 06:12 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/meta3/pegasus_server config.ini -app_list meta
cd /__w/incubator-pegasus/incubator-pegasus/onebox/replica1 && /__w/incubator-pegasus/incubator-pegasus/onebox/replica1/pegasus_server config.ini -app_list replica &>result &
root         303     182  0 06:12 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/replica1/pegasus_server config.ini -app_list replica
cd /__w/incubator-pegasus/incubator-pegasus/onebox/replica2 && /__w/incubator-pegasus/incubator-pegasus/onebox/replica2/pegasus_server config.ini -app_list replica &>result &
root         311     182  0 06:12 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/replica2/pegasus_server config.ini -app_list replica
cd /__w/incubator-pegasus/incubator-pegasus/onebox/replica3 && /__w/incubator-pegasus/incubator-pegasus/onebox/replica3/pegasus_server config.ini -app_list replica &>result &
cd /__w/incubator-pegasus/incubator-pegasus/onebox/collector && /__w/incubator-pegasus/incubator-pegasus/onebox/collector/pegasus_server config.ini -app_list collector &>result &
root         327     182  0 06:12 ?        00:00:00 /__w/incubator-pegasus/incubator-pegasus/onebox/collector/pegasus_server config.ini -app_list collector
Wait cluster to become healthy...
Sleeped for 1 seconds
Sleeped for 2 seconds
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/dsn_message_parser.cpp:163:14: runtime error: load of misaligned address 0x14aae40010b6 for type 'uint32_t', which requires 4 byte alignment
0x14aae40010b6: note: pointer points here
 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 03 00 00 00 00 00  00 00 a2 af 42 a3 69 3d  66 23
             ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/tool-api/rpc_message.h:157:86: runtime error: member access within misaligned address 0x14aae40010aa for type 'struct message_header', which requires 8 byte alignment
0x14aae40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 03 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/dsn_message_parser.cpp:185:17: runtime error: member access within misaligned address 0x14aae40010aa for type 'struct message_header', which requires 8 byte alignment
0x14aae40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 03 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/network.cpp:397:45: runtime error: member call on misaligned address 0x14aae400111a for type 'struct rpc_address', which requires 8 byte alignment
0x14aae400111a: note: pointer points here
 00 00  00 00 01 00 2b 87 01 00  00 7f 10 27 00 00 00 00  00 00 00 00 00 00 00 00  00 00 45 52 52 5f
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/tool-api/rpc_address.h:147:52: runtime error: member access within misaligned address 0x14aae400111a for type 'const struct rpc_address', which requires 8 byte alignment
0x14aae400111a: note: pointer points here
 00 00  00 00 01 00 2b 87 01 00  00 7f 10 27 00 00 00 00  00 00 00 00 00 00 00 00  00 00 45 52 52 5f
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/network.cpp:408:42: runtime error: member access within misaligned address 0x14aae40010aa for type 'struct message_header', which requires 8 byte alignment
0x14aae40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 03 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/network.cpp:437:32: runtime error: member access within misaligned address 0x14aae40010aa for type 'struct message_header', which requires 8 byte alignment
0x14aae40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 03 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/rpc_message.cpp:84:10: runtime error: member access within misaligned address 0x14aae40010aa for type 'struct message_header', which requires 8 byte alignment
0x14aae40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 03 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/rpc_message.cpp:89:46: runtime error: member access within misaligned address 0x14aae40010aa for type 'struct message_header', which requires 8 byte alignment
0x14aae40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 03 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/rpc_message.cpp:90:46: runtime error: member access within misaligned address 0x14aae40010aa for type 'struct message_header', which requires 8 byte alignment
0x14aae40010aa: note: pointer points here
 6f 78  00 00 52 44 53 4e 00 00  00 00 c0 00 00 00 00 00  00 00 00 01 00 00 00 00  00 00 03 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/src/runtime/rpc/rpc_engine.cpp:161:50: runtime error: member access within misaligned address 0x14aae40010aa for type 'struct message_header', which requires 8 byte alignment
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/cpp/serialization.h:101:78: runtime error: member access within misaligned address 0x150194014ca1 for type 'struct message_header', which requires 8 byte alignment
0x150194014ca1: note: pointer points here
 00 00 00  00 52 44 53 4e 00 00 00  00 c0 00 00 00 00 00 00  00 04 03 00 00 00 00 00  00 15 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/cpp/serialization.h:101:78: runtime error: member access within misaligned address 0x150168000f2b for type 'struct message_header', which requires 8 byte alignment
0x150168000f2b: note: pointer points here
 30  32 00 00 52 44 53 4e 00  00 00 00 c0 00 00 00 00  00 00 00 3b 00 00 00 00  00 00 00 17 00 00 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/cpp/serialization.h:101:78: runtime error: member access within misaligned address 0x1501680395bf for type 'struct message_header', which requires 8 byte alignment
0x1501680395bf: note: pointer points here
 30 33 00 00 52  44 53 4e 00 00 00 00 c0  00 00 00 00 00 00 00 5b  00 00 00 00 00 00 00 45  e8 02 00
             ^ 
D2022-09-13 06:13:39.819 (1663049619819785495 3399)  mimic.default0.01000d2a00030036: client session created, remote_server = 127.0.0.1:34802, current_count = 3
D2022-09-13 06:13:39.819 (1663049619819945895 3375)  mimic.io-thrd.03375: client session connected, remote_server = 127.0.0.1:34802, current_count = 3
D2022-09-13 06:13:39.821 (1663049619821638394 3370)  mimic.io-thrd.03370: client session created, remote_server = 127.0.0.1:34803, current_count = 4
D2022-09-13 06:13:39.821 (1663049619821748494 3375)  mimic.io-thrd.03375: client session connected, remote_server = 127.0.0.1:34803, current_count = 4
D2022-09-13 06:13:39.826 (1663049619826747793 3370)  mimic.io-thrd.03370: client session created, remote_server = 127.0.0.1:34801, current_count = 5
D2022-09-13 06:13:39.826 (1663049619826867993 3377)  mimic.io-thrd.03377: client session connected, remote_server = 127.0.0.1:34801, current_count = 5
write hotspot data passed.....
start testing write random data...
write random data passed.....
start testing max detection time...
max detection time passed.....
start testing read hotspot data...
/__w/incubator-pegasus/incubator-pegasus/src/rdsn/include/dsn/cpp/serialization.h:101:78: runtime error: member access within misaligned address 0x15016800f499 for type 'struct message_header', which requires 8 byte alignment
0x15016800f499: note: pointer points here
 30 32 00  00 52 44 53 4e 00 00 00  00 c0 00 00 00 00 00 00  00 37 00 00 00 00 00 00  00 c4 a8 08 00
              ^ 
/__w/incubator-pegasus/incubator-pegasus/src/test/function_test/detect_hotspot_test/test_detect_hotspot.cpp:118: Failure
Value of: find_hotkey
  Actual: false
Expected: true
/__w/incubator-pegasus/incubator-pegasus/src/test/function_test/detect_hotspot_test/test_detect_hotspot.cpp:198: Failure
Expected: get_result(detection_type::read_data, key_type::hotspot_dataset) doesn't generate new fatal failures in the current thread.
  Actual: it does.
/__w/incubator-pegasus/incubator-pegasus/src/test/function_test/detect_hotspot_test/test_detect_hotspot.cpp:229: Failure
Expected: read_hotspot_data() doesn't generate new fatal failures in the current thread.
  Actual: it does.
[  FAILED  ] detect_hotspot_test.write_hotspot_data (579275 ms)
[----------] 1 test from detect_hotspot_test (579275 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (579278 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] detect_hotspot_test.write_hotspot_data

 1 FAILED TEST
dsn exit with code 1
./run.sh: line 399:   327 Segmentation fault      (core dumped) $PWD/pegasus_server config.ini -app_list collector &> result  (wd: /__w/incubator-pegasus/incubator-pegasus/onebox/collector)
Error: Process completed with exit code 1.
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1162/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1164,https://api.github.com/repos/apache/incubator-pegasus/issues/1164,incubator-pegasus,1375436912,1164,Download hadoop-2.8.4.tar.gz during release building or third-parties image building for unit tests,empiredan,743379,Dan Wang,,CLOSED,2022-09-16T05:39:29Z,2022-09-27T11:51:31Z,"hadoop-2.8.4.tar.gz failed to be downloaded very frequently for workflows as follows:

![image](https://user-images.githubusercontent.com/743379/190556250-8cfb80f2-05b1-46e5-bd7d-8a41c1d7d620.png)

![image](https://user-images.githubusercontent.com/743379/190556213-2082a709-4ee5-44b0-8ad7-371e02407347.png)

![image](https://user-images.githubusercontent.com/743379/190556141-be182153-aee1-470f-892a-f709dd16e183.png)

The reason could be that during a short period of time there are several tens of test cases started, all of which request hadoop-2.8.4.tar.gz from remote repository. This might trigger throttling for the downloads.

To solve this problem, hadoop-2.8.4.tar.gz can be downloaded during:

- release building: once third-parties have changed, it should be rebuilt and image would not be used, thus download hadoop-2.8.4.tar.gz; or otherwise 
- third-parties image building: none of third-parties has ever changed, image would be used, thus third-parties image should include hadoop-2.8.4.tar.gz

Thus there will be 2 pull requests for both conditions:

- [x] https://github.com/apache/incubator-pegasus/pull/1167
- [x] https://github.com/apache/incubator-pegasus/pull/1168","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1164/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1165,https://api.github.com/repos/apache/incubator-pegasus/issues/1165,incubator-pegasus,1377005616,1165,Provide container image for pegasus,liangyuanpeng,28711504,Lan,gcslyp@gmail.com,OPEN,2022-09-18T10:06:45Z,2022-10-10T01:21:31Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**

I had try to use github action to build pegasus and it in standalone action, considering that we already have a [nightly github action for build](https://github.com/apache/incubator-pegasus/blob/master/.github/workflows/regular-build.yml), it would be great if it could be include build and push docker image.

Unfortunately, it's running in a container, and I try to mount the host's docker command, which doesn't seem to work.

Some like that:

```yaml
     container:
       image: apache/pegasus:thirdparties-bin-${{ matrix.os }}-master
       volumes:
         - /var/run/docker.sock:/var/run/docker.sock 
         - /usr/bin/docker:/usr/bin/docker
``` 

Whether to consider using github action machine instead of container to build pegasus.

/cc @acelyc111 


**Describe the feature you'd like:**
Container image of pegasus.

**Describe alternatives you've considered:**


**Teachability, Documentation, Adoption, Migration Strategy:**

1. User must compile pegasus if they want to use pegasus, provide a container image of pegasus can run pegasus quick.
2. Use pegasus container instead of compile pegasus to run CI for client code. This can greatly improve efficiency.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1165/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1165,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Ksuwd,incubator-pegasus,1253239837,1165,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-09-21T05:56:14Z,2022-09-21T05:56:14Z,"@liangyuanpeng Thanks for your advice, it's reasonable. Pegasus has a docker compose solution[1], however, it's also needed to build it manually at first.
Could you help to do this work that push built Pegasus images to DockerHub?

1. https://github.com/apache/incubator-pegasus/tree/master/docker/pegasus-docker-compose","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Ksuwd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1165,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5K71bj,incubator-pegasus,1257199331,1165,NA,liangyuanpeng,28711504,Lan,gcslyp@gmail.com,NA,2022-09-25T13:54:25Z,2022-09-25T13:54:25Z,"Absolutely,   so i would change build pegasus to use command of `docker run apache/pegasus:build-env-xxxx /bin/bash -c ""./run.sh build xxx"" ` first. ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5K71bj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1165,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5L2Sq2,incubator-pegasus,1272523446,1165,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-10-09T11:41:37Z,2022-10-09T11:41:37Z,"I have updated the build docs for the master banch, see https://pegasus.apache.org/docs/build/compile-by-docker/","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5L2Sq2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1165,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5L2cP_,incubator-pegasus,1272562687,1165,NA,liangyuanpeng,28711504,Lan,gcslyp@gmail.com,NA,2022-10-09T15:09:34Z,2022-10-09T15:09:34Z,"This issue mean want to a image what can run, it isn't the problem about doc.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5L2cP_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1166,https://api.github.com/repos/apache/incubator-pegasus/issues/1166,incubator-pegasus,1377372665,1166,Pegasus code style guide,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2022-09-19T04:15:38Z,2023-01-16T16:39:29Z,"As the Pegasus project developing, more and more developer joined into the project, and there are also many code legacy from years ago, I propose to design a code style for Pegasus project.

- [ ] enable unused-result warning
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1166/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1170,https://api.github.com/repos/apache/incubator-pegasus/issues/1170,incubator-pegasus,1382650047,1170,Separete internal communicate ip address and external service ip address,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2022-09-22T15:35:53Z,2022-09-22T15:39:20Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
In some usage cases, for example, in docker, for some deployment policy, and etc, the Pegasus deployed server has more than one ip addresses, one is used for internal communication, and some other one is used for external service, it is needed to provide ip addresses for external network users. But currently, Pegasus only support one address, and it's unresolvable/unaccessable from external network.


**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->
By default, the only address is both internal communicate ip address and external service ip address, and support to provide both addresses separetly.

**Describe alternatives you've considered:**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->
none.

**Teachability, Documentation, Adoption, Migration Strategy:**
<!-- If you can, explain some scenarios how users might use this, situations it would be helpful in. Any API designs, mockups, or diagrams are also helpful. -->

Related implemention:
Kudu ""advertised addresses"":
1. https://kudu.apache.org/docs/configuration_reference.html#kudu-master_rpc_advertised_addresses
2. https://kudu.apache.org/docs/configuration_reference.html#kudu-tserver_rpc_advertised_addresses
3. https://kudu.apache.org/docs/configuration_reference.html#kudu-master_webserver_advertised_addresses
4. https://kudu.apache.org/docs/configuration_reference.html#kudu-tserver_webserver_advertised_addresses

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1170/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1177,https://api.github.com/repos/apache/incubator-pegasus/issues/1177,incubator-pegasus,1388938111,1177,Download apache-zookeeper-3.7.0-bin.tar.gz during release building or third-parties image building for unit tests,empiredan,743379,Dan Wang,,CLOSED,2022-09-28T08:22:14Z,2022-10-20T12:24:20Z,"Since https://github.com/apache/incubator-pegasus/issues/1164 had been resolved, apache-zookeeper-3.7.0-bin.tar.gz has failed to be downloaded very frequently. For example, for only one run [3140473266](https://github.com/apache/incubator-pegasus/actions/runs/3140473266/) of CI there are 3 jobs that are failed due to apache-zookeeper-3.7.0-bin.tar.gz is failed to be downloaded:

![image](https://user-images.githubusercontent.com/743379/192726370-e344a5c3-063c-47a6-b59a-c71aadbd8561.png)


![image](https://user-images.githubusercontent.com/743379/192725053-3faf99fb-8fd4-493a-9e58-87a47c1766cb.png)


![image](https://user-images.githubusercontent.com/743379/192726161-b6136a6f-57ec-4d24-9d69-ec38506b1ee0.png)


The solution to this issue is also same with https://github.com/apache/incubator-pegasus/issues/1164: 2 pull requests will be created for this issue:

- [x] https://github.com/apache/incubator-pegasus/pull/1178
- [x] https://github.com/apache/incubator-pegasus/pull/1186","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1177/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1181,https://api.github.com/repos/apache/incubator-pegasus/issues/1181,incubator-pegasus,1402116050,1181,Github workflow labeler not work,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-10-09T01:41:55Z,2022-10-09T03:02:35Z,"As shown in [1],
Github workflow labeler does not work.

It has been fixed by the lastest version[2], we have to use it.

1. https://github.com/apache/incubator-pegasus/actions/runs/3203203205/jobs/5233014549
2. https://github.com/actions/labeler/issues/446","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1181/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1183,https://api.github.com/repos/apache/incubator-pegasus/issues/1183,incubator-pegasus,1405668424,1183,update app_name on 'on_config_sync ',GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2022-10-12T06:51:30Z,2022-10-20T06:13:38Z,"## Bug Report

When I execute `recall app_id new_app_name`, the app_name on zookeeper changed but file '.app_info' for replica not. 
And If I use this 'app_info' rebuild metadata, I will get one outdated app_name.

So I want to add `update_app_name` on `on_config_sync`.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1183/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1183,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Ml3YG,incubator-pegasus,1284994566,1183,NA,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,NA,2022-10-20T06:13:34Z,2022-10-20T06:13:34Z,I will solve by this issue: https://github.com/apache/incubator-pegasus/issues/1185,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Ml3YG/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1185,https://api.github.com/repos/apache/incubator-pegasus/issues/1185,incubator-pegasus,1407242996,1185,add rename_app interface,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2022-10-13T06:47:10Z,2023-01-16T16:41:49Z,"## Feature Request

**Describe the feature you'd like:**

We should use `drop` and `recall` when we want to rename table.
Actually we don't need such a complicated process.

I want add a couple rpc to solve it.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1185/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1191,https://api.github.com/repos/apache/incubator-pegasus/issues/1191,incubator-pegasus,1414538836,1191,Refactor(metrics): drop template specialization for gauge ,empiredan,743379,Dan Wang,,CLOSED,2022-10-19T08:30:29Z,2022-10-20T12:25:29Z,"Template specialization is used at the beginning for value initialization of each data type of gauge. Since each data type of gauge must be arithmetical, the value of each type can be zero-initialized by `T()`. Thus template specialization can be dropped.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1191/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1193,https://api.github.com/repos/apache/incubator-pegasus/issues/1193,incubator-pegasus,1414680899,1193,Fix wrong comments for down_cast,empiredan,743379,Dan Wang,,CLOSED,2022-10-19T10:05:21Z,2022-10-20T10:29:24Z,"Now some comments for `down_cast` function are wrong, they should be corrected in time. ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1193/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1195,https://api.github.com/repos/apache/incubator-pegasus/issues/1195,incubator-pegasus,1414970081,1195,Feature(new_metrics): use rwlock instead of mutex to protect a metric entity or registry against few writes and numerous reads,empiredan,743379,Dan Wang,,CLOSED,2022-10-19T13:24:29Z,2022-10-21T10:50:21Z,"In the near future, the new metric framework will provide a service that process requests for queries. For example, given 2 parameters (may be regex) of which one is an entity type (say, *replica*), and another is the name of a metric that belongs to the entity, the metric framework will perform queries over the whole registry for the targeted metrics.

However, both metric entity and registry have protected their reads/writes by **mutex**. This will lead to performance problem once lots of queries (i.e. reads) are launched. The solution will be using **rwlock** which is more appropriate to support the scenario of few writes and numerous reads. ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1195/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1197,https://api.github.com/repos/apache/incubator-pegasus/issues/1197,incubator-pegasus,1416648151,1197,Support to update the configs which are used once when bootstrap,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-10-20T13:40:34Z,2023-05-16T16:05:17Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
Some configs are used only once in the process bootstrap stage, and will never use it then. Although Pegasus supports update FLAGS_* at runtime, but they will not be used after bootstrap finished.
For example, `config_sync_interval_ms` is used to init a timer task when replica server bootstrap, but even you update the config (by http) at runtime, it will not take effect. We need an method to update the timer task's schedule interval.

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->
We need an feature to update some configs even FLAGS_* only used once.

**Describe alternatives you've considered:**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->
none.

**Teachability, Documentation, Adoption, Migration Strategy:**
<!-- If you can, explain some scenarios how users might use this, situations it would be helpful in. Any API designs, mockups, or diagrams are also helpful. -->
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1197/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1199,https://api.github.com/repos/apache/incubator-pegasus/issues/1199,incubator-pegasus,1419825285,1199,Adjust the strange LOG level,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-10-23T15:07:54Z,2022-11-01T03:25:28Z,"For most logging systems, levels are ordered in:
DEBUG,INFO,WARNING,ERROR,FATAL
for example: [spdlog](https://github.com/gabime/spdlog/blob/d546201f127c306ec8a0082d57562a05a049af77/include/spdlog/common.h#L214), [glog](https://github.com/google/glog/blob/master/src/glog/log_severity.h#L53), [Log4J2](https://logging.apache.org/log4j/2.x/manual/customloglevels.html)
But in Pegasus, they are ordered in:
INFO,DEBUG,WARNING,ERROR,FATAL

It's strange, we should adjust the levels to keep consistent with well known and wildly used logging systems.

- [x] https://github.com/apache/incubator-pegasus/pull/1200
- [x] https://github.com/apache/incubator-pegasus/pull/1201
- [x] https://github.com/apache/incubator-pegasus/pull/1202","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1199/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1204,https://api.github.com/repos/apache/incubator-pegasus/issues/1204,incubator-pegasus,1423387285,1204,refactor: use CHECK* macros to replace assert macros,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2022-10-26T03:37:21Z,2022-11-08T08:55:47Z,"This refactor work contains the following motivations:
1. macros are offen recommend to use all upper case.
2. the beginning char 'd' is easy to misunderstand as 'only enable in **D**ebug version'
3. there are 2 styles, `dassert_f` and `dassert`, we can unify them

- [x] replace dassert which check a single condition
- [x] replace dcheck_eq*
- [x] replace dcheck_ge*
- [x] replace dcheck_le*
- [x] replace dcheck_gt*
- [x] replace dcheck_lt*
- [ ] remove danling-else `CHECK(false, ...)`","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1204/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1204,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5N5M8r,incubator-pegasus,1306840875,1204,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-11-08T08:45:07Z,2022-11-08T08:45:07Z,"## TODO

There are some code like
```
if (a) {
  // do sth
} else {
  CHECK(false, ...);
}
```
can be improved by
```
CHECK(a, ...)
// do sth
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5N5M8r/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1206,https://api.github.com/repos/apache/incubator-pegasus/issues/1206,incubator-pegasus,1423806115,1206,Feature(new_metrics): provide a RESTful service to query immediate values of metrics from new framework,empiredan,743379,Dan Wang,,CLOSED,2022-10-26T10:36:33Z,2023-05-04T06:41:59Z,"# 1. Motivation

The new framework should provide a service to collect metrics. Since it's hoped that the metrics are collected very conveniently, the service can be **RESTful**. With *http* protocol, the metrics can be viewed just by any browser or a `curl` command.

# 2. Data types

## 2.1 Request

Common RESTful semantics will be adopted for the service. For example, metrics can queried by `GET` method with parameters passed in [query string](https://en.wikipedia.org/wiki/Query_string).

However, there are just field pairs (name and value) in query string. In our scenario, complex data structures such as array should be supported. For example, multiple metric names may be contained in a query.

To solve this problem, the field value can be designed as such format to represent an array: comma(""`,`"") can be used as the delimiter to separate each element of the array, for example ""`put_count,put_latency,alive_node_count`""; also, comma(""`,`"") can be used safely in query component in URL according to [possible side effect using comma in query string](https://stackoverflow.com/questions/45686595/possible-side-effect-using-comma-in-querystring) and [RFC 3986](https://www.rfc-editor.org/rfc/rfc3986#section-3.4).

## 2.2 Response

The collected metrics can just be organized in *json* format. The detailed implementations will be described in the following sections.

# 3. API

## 3.1 Query metrics

### 3.1.1 Request

#### URI

```url
/metrics
```

#### Parameters

| Parameters | Data Types |Description|
|:----------:|:-----------:|:----------|
| types | array | entity types, such as `server`, `table`, `replica`, etc. |
| ids | array | entity IDs. |
| attributes | array | attributes for entity, such as table name and partition id for entity `replica`, and ""`attr_key_1,attr_val_1,attr_key_2,attr_val_2`"" means there are 2 pairs of attributes: `attr_key_1:attr_val_1` and `attr_key_2:attr_val_2`. |
| metrics | array | metric names. |
| with_metric_fields | array | includes the metric fields that are wanted by client |
| detail | boolean | `true` means all metric fields would be returned |

#### Headers

```http
Accept: application/json
```

### 3.1.2 Response

#### Headers

```http
Content-Type: application/json
```

#### Content

```json
[
    { // entity 1
        ""type"": ""<entity_type>"",
        ""id"": ""<entity_id>"",
        ""attributes"": {
            ""<attr_key_1>"": ""<attr_val_1>"",
            ""<attr_key_2>"": ""<attr_val_2>"",
            ""<attr_key_3>"": ""<attr_val_3>"",
            ...
        },
        ""metrics"": [ // metrics that belong to entity 1
            // gauge
            {
                ""name"": ""<gauge_name>"",
                ""value"": 100
            },
            // counter
            {
                ""name"": ""<counter_name>"",
                ""value"": 10000
            },
            // percentile
            {
                ""name"": ""<percentile_name>"",
                ""p50"": 1,
                ""p90"": 5,
                ""p95"": 8,
                ""p99"": 10,
                ""p999"": 15
            }
        ]
    },
    { // entity 2
        ...
    },
    ...
]
```

### 3.1.3 Examples

#### Collect all metrics

```url
/metrics
```

#### Collect all metrics from server entity

```url
/metrics?types=server
```

#### Collect all metrics from both server and replica entity

```url
/metrics?types=server,replica
```

#### Collect all metrics of the given replica entities (table_name=my_table, partitions_id=1 or 5)

```url
/metrics?types=replica&attributes=table_name,my_table,partition_id,1,partition_id,5
```

#### Collect specified metrics of server entity and the given replica entities (table_name=my_table, partitions_id=1 or 5)

```url
/metrics?types=server,replica&attributes=table_name,my_table,partition_id,1,partition_id,5,metrics=get_latency,put_count,alive_node_count
```

#### Collect all metrics with specified fields 

```
/metrics?with_metric_fields=name,value,type
```

#### Collect all metrics with all fields 

```
/metrics?detail=true
```

# 4. Task lists

- [x] https://github.com/apache/incubator-pegasus/issues/1218
- [x] https://github.com/apache/incubator-pegasus/issues/1236
- [x] https://github.com/apache/incubator-pegasus/issues/1245
- [x] https://github.com/apache/incubator-pegasus/issues/1279
- [x] https://github.com/apache/incubator-pegasus/issues/1280
- [x] https://github.com/apache/incubator-pegasus/issues/1301","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1206/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1206,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5NXb3c,incubator-pegasus,1297989084,1206,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-11-01T03:53:33Z,2022-11-01T03:53:33Z,"Some questions:
1. In the response, I know the metrics will be filtered out, but will all the `type`, `id` and `attributes` (with all attribute k-v pairs) be returned no matter what the query string is ?
2. Is it helpful to return the description of each metrics, such as `statistic the qps of PUT request` for ""put_qps"" ? We can add a more parameter to decide whether to return it, because it's verbos if return it every time.
3. Metrics type (e.g. counter, guage, percentile) is same to point 2.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5NXb3c/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1206,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5NXckb,incubator-pegasus,1297991963,1206,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-11-01T03:59:22Z,2022-11-01T03:59:22Z,"4. Return all percentiles for percentile type metrics is fast to implement, will you design some type of filter for it in the future?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5NXckb/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1206,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5NYNWn,incubator-pegasus,1298191783,1206,NA,empiredan,743379,Dan Wang,,NA,2022-11-01T08:11:31Z,2022-11-01T08:11:31Z,"> Some questions:
> 
> 1. In the response, I know the metrics will be filtered out, but will all the `type`, `id` and `attributes` (with all attribute k-v pairs) be returned no matter what the query string is ?
> 2. Is it helpful to return the description of each metrics, such as `statistic the qps of PUT request` for ""put_qps"" ? We can add a more parameter to decide whether to return it, because it's verbos if return it every time.
> 3. Metrics type (e.g. counter, guage, percentile) is same to point 2.

Good questions ! I'll answer each as follows:

1. Yes, in current design, all of `type`, `id` and `attributes` with all k-v pairs will be returned. And I think returned fields of entities can be custom.
2. Yes, actually the description for metric has been a property of `metric_prototype`, and certainly can be a field returned.  As is put in the answer for 1st question, to support custom fields, introduced a new parameter: `with_entity_fields`. For example, `with_entity_fields=type,desc` will only call for `type` and `desc` fields, is this alright ?
3. As for metric, use `with_metric_fields`. For example, `with_metric_fields=value,p99` means it will just return the `value` of gauge or counter, and `p99` of percentile, if any.
4. Just use the parameters in answer for 3rd question, percentile types can also be custom.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5NYNWn/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1206,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5NYkKz,incubator-pegasus,1298285235,1206,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-11-01T09:50:56Z,2022-11-01T09:50:56Z,"Good, let's go ahead!","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5NYkKz/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1207,https://api.github.com/repos/apache/incubator-pegasus/issues/1207,incubator-pegasus,1425004089,1207,Core dumped while testing mutation_log::reset_from,empiredan,743379,Dan Wang,,CLOSED,2022-10-27T04:17:36Z,2022-10-28T10:31:03Z,"Both [Test Release (dsn.replica.test)](https://github.com/apache/incubator-pegasus/actions/runs/3326102384/jobs/5505382914#logs) and [Test ASAN (dsn.replica.test)](https://github.com/apache/incubator-pegasus/actions/runs/3326102384/jobs/5505387446#logs) for dsn.replica.test have failed in CI workflows of https://github.com/apache/incubator-pegasus/pull/1205, with reported errors as follows:

```
[ RUN      ] mutation_log_test.reset_from
E2022-10-26 11:56:44.166 (1666785404166907303 282) replica.default0.0000010100010001: replica.cpp:569:init_disk_tag(): [1.1@] get disk tag of ./test-log failed: ERR_OBJECT_NOT_FOUND, init it to empty 
W2022-10-26 11:56:44.172 (1666785404172717263 282) replica.default0.0000010100010001: filesystem.cpp:381:rename_path(): rename from './test-log.1666785404172467160' to './test-log' failed, err = Directory not empty
F2022-10-26 11:56:44.172 (1666785404172723363 282) replica.default0.0000010100010001: mutation_log.cpp:970:operator()(): assertion expression: false
F2022-10-26 11:56:44.172 (1666785404172728763 282) replica.default0.0000010100010001: mutation_log.cpp:970:operator()(): rollback ./test-log.1666785404172[467](https://github.com/apache/incubator-pegasus/actions/runs/3326102384/jobs/5505382914#step:7:468)160 to ./test-log failed
Aborted (core dumped)
Error: Process completed with exit code 134.
```

Review the code of `mutation_log::reset_from` with the reported errors, some problems could be found.

Firstly, the reason for core dump is that `err` in capture list of the lambda function for `dsn::defer` is passed as value rather than a reference. As a result, `err` for lambda function will always be `ERR_FILE_OPERATION_FAILED`; also, `utils::filesystem::rename_path(temp_dir, _dir)` will be called mistakenly. See the following code for details:
```c++
    // define `defer` for rollback temp_dir when failed or remove temp_dir when success
    auto temp_dir_resolve = dsn::defer([this, err, temp_dir]() {
        if (err != ERR_OK) {
            if (!utils::filesystem::rename_path(temp_dir, _dir)) {
                // rollback failed means old log files are not be recovered, it may be lost if only
                // LOG_ERROR,  dassert for manual resolve it
                dassert_f(""rollback {} to {} failed"", temp_dir, _dir);
            }
        } else {
            if (!dsn::utils::filesystem::remove_path(temp_dir)) {
                // temp dir allow delete failed, it's only garbage
                LOG_ERROR_F(""remove temp dir {} failed"", temp_dir);
            }
        }
    });
```  

Secondly, from the above code, another error can be found for `dassert_f(""rollback {} to {} failed"", temp_dir, _dir);`. The first parameter for `dassert_f` should have been a condition; however, ""`rollback {} to {} failed`"" is passed as the first parameter by mistake rather than the condition. Therefore, this assertion will never failed since this string will always be converted implicitly as true.

Thirdly, the following code also renames a directory, however there is not any rollback process for this: 
```c++
    // move source dir to target dir
    if (!utils::filesystem::rename_path(dir, _dir)) {
        LOG_ERROR_F(""rename {} to {} failed"", dir, _dir);
        return err;
    }
    LOG_INFO_F(""move {} to {} as our new log directory"", dir, _dir);
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1207/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1209,https://api.github.com/repos/apache/incubator-pegasus/issues/1209,incubator-pegasus,1425635738,1209,Fix unnecessary download for zookeeper-bin,empiredan,743379,Dan Wang,,CLOSED,2022-10-27T13:25:38Z,2022-10-28T02:43:55Z,"Previously zookeeper-bin has been built in third-party images in https://github.com/apache/incubator-pegasus/issues/1177. However in recent CI workflows it's found that zookeeper-bin is still downloaded by each unit tests.

The reason is that `${INSTALL_DIR}` for zookeeper has not been created while trying to move zookeeper-bin from third-party image to `${INSTALL_DIR}`. Therefore zookeeper-bin has been renamed as `${INSTALL_DIR}` itself rather than being moved under `${INSTALL_DIR}`. ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1209/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1213,https://api.github.com/repos/apache/incubator-pegasus/issues/1213,incubator-pegasus,1426818872,1213,Building pegasus with gcc on ubuntu2004 always failed due to undefined reference,empiredan,743379,Dan Wang,,CLOSED,2022-10-28T07:30:30Z,2023-01-28T04:03:56Z,"Recently ""Build Cpp (ubuntu2004, gcc)"" for [Lint and build regularly](https://github.com/apache/incubator-pegasus/actions/workflows/regular-build.yml) always failed with the following error:

```
/usr/bin/ld: ../../base/libpegasus_base.a(rrdb_types.cpp.o): in function `dsn::apps::duplicate_entry::write(apache::thrift::protocol::TProtocol*) const':
/root/incubator-pegasus/src/base/rrdb_types.cpp:5225: undefined reference to `dsn::task_code::write(apache::thrift::protocol::TProtocol*) const'
collect2: error: ld returned 1 exit status
make[2]: *** [src/geo/bench/CMakeFiles/pegasus_geo_bench.dir/build.make:119: src/geo/bench/pegasus_geo_bench] Error 1
make[1]: *** [CMakeFiles/Makefile2:1673: src/geo/bench/CMakeFiles/pegasus_geo_bench.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1213/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1213,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5T4WWg,incubator-pegasus,1407280544,1213,NA,empiredan,743379,Dan Wang,,NA,2023-01-28T04:03:55Z,2023-01-28T04:03:55Z,This problem has been resolved by https://github.com/apache/incubator-pegasus/pull/1284.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5T4WWg/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1215,https://api.github.com/repos/apache/incubator-pegasus/issues/1215,incubator-pegasus,1427076037,1215,Feature:Add BulkLoad CU calculator and reporter,ninsmiracle,110282526,,,OPEN,2022-10-28T10:51:10Z,2023-04-17T09:06:20Z,"## Feature Request

### What problem does this PR solve? <!--add issue link with summary if exists-->
#1215 

To solve bulk load cu problem. 

### What is changed and how does it work?

#### Date structure in table stat

`hashkey:  data in format of ""2022-10-12 00:33:25""
sortkey:  ""bulkload_cu@appId:partitionID""
value: JSON in structure of {""appId:[bulkloadCU]""}`

Design reason:
-Keep the same format with CU, which is convenient for other billing system collection.

#### How replica get BulkLoad CU date?
  As we know, when RocksDB doing BulkLoad, enegine should download a file named ""meta_date"" first.For pegasus ,each partition has a meta_data file that contains the size and md5 information of each sst.This file has some date like this format:
`{""files"":[{""name"":""1.sst"",""size"":14273079,""md5"":""9ca4d0f21912eab77cf387ef9bb95eb8""},{""name"":""10.sst"",""size"":8908447,""md5"":""534b8a64b3d4659833599ebdf0719988""}}`

  The replica will first download the ""meta_date"" file and then start download sst file. During this process, the replica may cache the file locally. If the file name is detected, the file size will be checked. If the verification is successful, the download will be skipped, otherwise, the download will be re-downloaded. After the download is successful, the bulkload-related counter in replica_stub will increase the file size. This piece of logic was originally intended to support update BulkLoad download progress to update download process.
  Look at those code about update download progress.We can see that download progress is calculated by current downloaded size. THE **current downloaded size** will be reset when BulkLoad roll back or  the whole BulkLoad process failed. We can store that variable in another way to implement BulkLoad CU feature.
  If the file already exists and the verification passes, the filesize will be passed directly to cur_download_size. In this way, each replica gets a cur_download_size that is periodically updated (one update per meta request). (The logic follows the existing process report)
  Then, the secondaries converts the cur_download_size to bulkload_cu size, wraps it in the response, and sends it to the primary. The primary is aggregated into the download amount of the partition, packaged in the response and sent back to the meta.


#### How meta deal with  BulkLoad CU which replica report?
  The meta maintains two new maps, which store the table ip and the total download amount, as well as the shard gpid and shard download amount.
  When the APP_STATUS is DOWNLOADING and DOWNLOADED, we assign the download amount in the response of each primary to the meta _partitions_total_downloaded_file_size map(mapping partition info gpid to BulkLoad CU). Since it resets after an exception, we don't need to worry about data errors.
  When the  APP_STATUS turn to  FAILED state, we reset the app_bulk_load_file_size directly, and this bulkload import completely failed.
  When all app shards have completed the ingestion, we need to update related app status on ZooKeeper nodes. After receiving the reply that the ZK modification is successful, we will write _partitions_total_downloaded_file_size to the stat table.
The write method is similar with the method we record readCU,writerCU,it's a async  write method.

![bulkload_cu](https://user-images.githubusercontent.com/110282526/199875376-aba1d3d6-0f00-4615-a73f-fa556210c131.jpg)



##### Tests <!-- At least one of them must be included. -->
- Manual test :
I test it in a pegasus cluster. It run as expected when we do a BulkLoad or we do BulkLoad on two map in the same cluster at the same time.
Even we cancel BulkLoad and recover it, the program will run successfully.

FYI: BulkLoad CU will record ervery replica real downloads size.In other words,if you have to download a 100MB SST file.And  3 servers for one partition in your cluster, every partition should download 300MB total.



","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1215/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1218,https://api.github.com/repos/apache/incubator-pegasus/issues/1218,incubator-pegasus,1429842483,1218,Feature(new_metrics): take snapshot of each metric as json format,empiredan,743379,Dan Wang,,CLOSED,2022-10-31T13:22:49Z,2022-11-02T11:39:24Z,"It has been designed in https://github.com/apache/incubator-pegasus/issues/1206 that the new framework will provide a RESTful service which return current value of each metric as json. Therefore, each metric should firstly support taking snapshot as json format.

An abstract method can be added for base class of metric. And each derived class (i.e., 3 types of metrics: gauge, counter and percentile) will implement this abstract method to take snapshot.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1218/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1220,https://api.github.com/repos/apache/incubator-pegasus/issues/1220,incubator-pegasus,1434592617,1220,"Overload operator ""<<"" for customized_id to support formatting by {fmt}  ",empiredan,743379,Dan Wang,,CLOSED,2022-11-03T12:38:38Z,2022-11-07T13:14:15Z,"Some classes such as `rpc_channel` are based on template class `customized_id`. Currently they can only be formatted by [{fmt}](https://github.com/fmtlib/fmt) using `to_string()`. For example,  in `src/runtime/rpc/asio_net_provider.cpp`, there are codes as below:
```c++
    CHECK(channel == RPC_CHANNEL_TCP || channel == RPC_CHANNEL_UDP,
          ""invalid given channel {}"",
          channel.to_string());
```

In `src/runtime/rpc/network.sim.cpp`:
```c++
    CHECK(channel == RPC_CHANNEL_TCP || channel == RPC_CHANNEL_UDP,
          ""invalid given channel {}"",
          channel.to_string());
```

To simplify the formatting, operator `<<` can just be overloaded which is supported by [{fmt}](https://github.com/fmtlib/fmt). ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1220/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1224,https://api.github.com/repos/apache/incubator-pegasus/issues/1224,incubator-pegasus,1436783987,1224,Report security vulnerabilities,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2022-11-05T02:57:17Z,2022-11-05T02:58:17Z,"# Security Policy

This is a project of the [Apache Software Foundation](https://apache.org) and follows the ASF [vulnerability handling process](https://apache.org/security/#vulnerability-handling).

## Reporting a Vulnerability

To report a new vulnerability you have discovered please follow the [ASF vulnerability reporting process](https://apache.org/security/#reporting-a-vulnerability).
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1224/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1227,https://api.github.com/repos/apache/incubator-pegasus/issues/1227,incubator-pegasus,1438119487,1227,Improve checking if C string is empty,empiredan,743379,Dan Wang,,CLOSED,2022-11-07T10:25:05Z,2022-11-08T11:38:59Z,"Sometimes we want to check if a C-style string is empty. For example, in `src/common/common.cpp` there is following code:

```c++
namespace dsn {
DSN_DEFINE_string(""replication"", cluster_name, """", ""name of this cluster"");

/*extern*/ const char *get_current_cluster_name()
{
    CHECK_GT_MSG(strlen(FLAGS_cluster_name), 0, ""cluster_name is not set"");
    return FLAGS_cluster_name;
}
} // namespace dsn
```

Checking by `strlen()` will be slow since strlen will traverse the whole string especially the string is very long.

According to [Apache Commons Lang](https://commons.apache.org/proper/commons-lang/apidocs/org/apache/commons/lang3/StringUtils.html#isEmpty-java.lang.CharSequence-), firstly we can define that a C-style string is empty once the string:

- is nullptr, or
- does not include any character.

Therefore, checking if a C-style string is empty can just be implemented as:
```c++
inline bool is_empty(const char *str)
{
    return str == nullptr || *str == '\0';
}
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1227/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1231,https://api.github.com/repos/apache/incubator-pegasus/issues/1231,incubator-pegasus,1439764782,1231,Unify the IDL *.thrift files distributed everywhere,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-11-08T08:59:23Z,2023-01-16T16:40:50Z,"Now the *.thrift files are distributed in server and each client libs, they are duplicated and hard to maintenance, and have more chance to introduce inconsistency issuses.

We have to unify them to use a unique place for them.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1231/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1231,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Sgtgs,incubator-pegasus,1384306732,1231,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-01-16T16:40:49Z,2023-01-16T16:40:49Z,"duplicated with https://github.com/apache/incubator-pegasus/issues/1283, close this one.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Sgtgs/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1232,https://api.github.com/repos/apache/incubator-pegasus/issues/1232,incubator-pegasus,1439788388,1232,Support more storage engines,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2022-11-08T09:13:49Z,2022-11-08T09:13:49Z,"Now Pegasus uses RocksDB 6.6.4, we are planning to:
- upgrade RocksDB to the latest stable version
- use RocksDB API compatiable storage engines like:
  - [tikv/titan](https://github.com/tikv/titan)
  - [bytedance/terarkdb](https://github.com/bytedance/terarkdb)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1232/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1233,https://api.github.com/repos/apache/incubator-pegasus/issues/1233,incubator-pegasus,1439799586,1233,Support more Redis commands,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2022-11-08T09:19:30Z,2022-11-08T09:19:30Z,"Now Pegasus support some basic redis commands[1] by deploying a standalone `redis_proxy`, we are planning to support more commands.

Welcome community developers help to improve it!

1. https://pegasus.apache.org/zh/api/redis","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1233/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1234,https://api.github.com/repos/apache/incubator-pegasus/issues/1234,incubator-pegasus,1443417456,1234,Code refactor on rdsn module,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-11-10T07:49:34Z,2023-01-28T08:07:23Z,"Inspired by and thanks to [rdsn](https://github.com/microsoft/rDSN), the Pegasus project could be developed nowadays.

The xiaomi/rdsn repo has been merged into apache/pegasus for months, now I consider the Pegasus branch rdsn is deeply modified and only used for Pegasus, and it's very hard and maybe meaningless to contribute these changes back upstream, so I'm planning to continue the work of refactoring rdsn module, including:

- [ ] remove the C language sopporting
- [ ] make it as a internel module but not a public library
- [ ] ...","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1234/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1236,https://api.github.com/repos/apache/incubator-pegasus/issues/1236,incubator-pegasus,1443918149,1236,Feature(new_metrics): support filters for selected metric fields in response,empiredan,743379,Dan Wang,,CLOSED,2022-11-10T13:32:13Z,2022-11-18T02:51:57Z,"## 1. Motivation
According to https://github.com/apache/incubator-pegasus/issues/1206, `with_metric_fields` and `without_metric_fields` will be provided as the parameters in the RESTful API to filter the fields of metric in the response to client.

## 2. Usage for `with_metric_fields`
`with_metric_fields` will ask for the metric fields that are **not** included in the request. For example, if the request is given as below:

```url
/metrics?with_metric_fields=value,p99
```

The response will only contain the fields of ""**value**"" and ""**p99**"":

```json
[
    { // entity 1
        ""metrics"": [
            // gauge
            {
                ""value"": 100
            },
            // counter
            {
                ""value"": 10000
            },
            // percentile
            {
                ""p99"": 10
            }
        ]
    },
    ...
]
```

## 3. Usage for `without_metric_fields`

Similarly, `without_metric_fields` will ask for the metric fields that are **not** included in the request. For example, if the request is given as below:

```url
/metrics?without_metric_fields=value,p99
```

The response will contain all the fields except ""**value**"" and ""**p99**"":

```json
[
    { // entity 1
        ""metrics"": [
            // gauge
            {
                ""name"": ""<gauge_name>""
            },
            // counter
            {
                ""name"": ""<counter_name>""
            },
            // percentile
            {
                ""name"": ""<percentile_name>"",
                ""p50"": 1,
                ""p90"": 5,
                ""p95"": 8,
                ""p999"": 15
            }
        ]
    },
    ...
]
```

## 4. Wrong usage

Once both `with_metric_fields` and `without_metric_fields` are provided in the request, it will considered as invalid. We can discuss the reason in 2 conditions.

Firstly, suppose both `with_metric_fields` and `without_metric_fields` are provided, and they include the same fields. In this condition, each conflicts with another. For example, `with_metric_fields=a,b` and `without_metric_fields=b,c` will lead to contradiction: we cannot decide if `b` should be put in the response to client.

On the contrary, suppose `with_metric_fields` and `without_metric_fields` do not include any same field. In this condition, they can be simplified as a unique `with_metric_fields`. For example, `with_metric_fields=a,b` and `without_metric_fields=c,d` can be simplified as `with_metric_fields=a,b`: `without_metric_fields=c,d` is useless here since we have declared that only `a` and `b` fields are needed according to `with_metric_fields=a,b`.

Therefore, we can draw a conclusion that `with_metric_fields` and `without_metric_fields` should not be provided at the same time.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1236/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1239,https://api.github.com/repos/apache/incubator-pegasus/issues/1239,incubator-pegasus,1447537292,1239,Refactor: improve split_args for strings,empiredan,743379,Dan Wang,,CLOSED,2022-11-14T07:30:32Z,2022-12-05T07:32:26Z,"While analyzing the code of `split_args` and `trim_string`, some problems could be found.

- Both `std::string v(args)` and `v.substr(...)` will copy string, which will lead to overhead for performance.
- In `trim_string`, all trailing spaces will be updated with `\0`, which is unnecessary.

Based on the above problems, I think we can improve `split_args`.

The code of current `split_args` and `trim_string` are listed as below:

```c++
void split_args(const char *args,
                /*out*/ std::vector<std::string> &sargs,
                char splitter,
                bool keep_place_holder)
{
    sargs.clear();
    std::string v(args);
    uint64_t last_pos = 0;
    while (true) {
        auto pos = v.find(splitter, last_pos);
        if (pos != std::string::npos) {
            std::string s = trim_string((char *)v.substr(last_pos, pos - last_pos).c_str());
            if (!s.empty()) {
                sargs.push_back(s);
            } else if (keep_place_holder) {
                sargs.emplace_back("""");
            }
            last_pos = pos + 1;
        } else {
            std::string s = trim_string((char *)v.substr(last_pos).c_str());
            if (!s.empty()) {
                sargs.push_back(s);
            } else if (keep_place_holder) {
                sargs.emplace_back("""");
            }
            break;
        }
    }
}
```

```c++
char *trim_string(char *s)
{
    while (*s != '\0' && (*s == ' ' || *s == '\t')) {
        s++;
    }
    char *r = s;
    s += strlen(s);
    while (s >= r && (*s == '\0' || *s == ' ' || *s == '\t' || *s == '\r' || *s == '\n')) {
        *s = '\0';
        s--;
    }
    return r;
}
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1239/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1243,https://api.github.com/repos/apache/incubator-pegasus/issues/1243,incubator-pegasus,1451453870,1243,v2.4 onebox zookeeper directory is wrong,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2022-11-16T11:54:09Z,2022-11-17T03:23:15Z,"`clear_zk.sh` can't real clear.
It makes clear_onebox failed, also effect run test.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1243/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1243,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5OgxSq,incubator-pegasus,1317213354,1243,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-11-16T15:36:46Z,2022-11-16T15:36:46Z,I met the same error in recent days.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5OgxSq/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1245,https://api.github.com/repos/apache/incubator-pegasus/issues/1245,incubator-pegasus,1451611633,1245,Feature(new_metrics): take snapshot of each entity as json format,empiredan,743379,Dan Wang,,CLOSED,2022-11-16T13:30:58Z,2022-12-02T06:11:34Z,"As each metric has supported snapshot as json format in https://github.com/apache/incubator-pegasus/issues/1218, entity should also support this feature just like what https://github.com/apache/incubator-pegasus/issues/1206 has described.

Firstly, REST service will construct the filters in `metric_filters` according to the request by client. Then, the filters are used to select the entities and metrics conforming to the following rules:

- Empty filter means there is no need to check for this filter, just go ahead with the next filter;
- If the filter for `types` is not empty, the type of each selected entity should be included in the filter;
- If the filter for `ids` is not empty, the ID of each selected entity should be included in the filter;
- If the filter for `attributes` is not empty, at least one attribute of each selected entity should be included in the filter (both name and value of this attribute should be matched with the filter);
- If the filter for `metrics` is not empty, each selected metric should be included in the filter.

If the entity is not chosen by the filters, or none of its metrics is selected, nothing for this entity will be put in the response returned back to the client; otherwise, if at least one metric of the entity is selected, following fields will be put in the response as json format: `type`,  `id`, `attributes` and `metrics`. Even the value of any field is empty, for example, there is not any attribute for an entity, attributes will still be put in response in json format `""attributes"": {}`.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1245/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1247,https://api.github.com/repos/apache/incubator-pegasus/issues/1247,incubator-pegasus,1453215898,1247,coredump in sortkey_count,xihong08,74220545,zhuxihong,,OPEN,2022-11-17T11:55:16Z,2022-11-17T12:14:59Z,"## Bug Report
In our production environment has appeared  some times coredump. it Seems happened in sortkey_count process. from replica-server log,  coredump occurs after replica's close.  so i try reproduce in our test environment，because i cannot make replica down scenario, i use kill-partition to make it. and i reproduce this coredump
My steps
1. use sortkey count in one client such as in pegaus_shell run 'count 2'
2. at the same time use remote-command to kill-partition of replica in the first step data
coredump stack as follows
```
#0  0x00000000007c08cb in pegasus::server::capacity_unit_calculator::add_read_cu (this=0x0, read_data_size=1)
    at /incubator-pegasus/src/server/capacity_unit_calculator.cpp:84
#1  0x00000000007c0d18 in pegasus::server::capacity_unit_calculator::add_sortkey_count_cu (this=0x0, status=0)
    at /incubator-pegasus/src/server/capacity_unit_calculator.cpp:165
#2  0x000000000087d5d7 in pegasus::server::pegasus_server_impl::on_sortkey_count (this=0x3264c00, hash_key=..., reply=...)
    at /incubator-pegasus/src/server/pegasus_server_impl.cpp:1181
#3  0x00000000008135fa in dsn::apps::rrdb_service::on_sortkey_count (svc=0x3264c00, args=..., reply=...)
    at /incubator-pegasus/src/include/rrdb/rrdb.server.h:300
#4  0x0000000000814aed in dsn::replication::storage_serverlet<dsn::apps::rrdb_service>::register_async_rpc_handler<dsn::blob, dsn::apps::count_response>(dsn::task_code, char const*, void (*)(dsn::apps::rrdb_service*, dsn::blob const&, dsn::rpc_replier<dsn::apps::count_response>&))::{lambda(dsn::apps::rrdb_service*, dsn::message_ex*)#1}::operator()(dsn::apps::rrdb_service*, dsn::message_ex*) const (this=0x281b6f8, p=0x3264c00,
    r=0x588b9c9a0) at /incubator-pegasus/DSN_ROOT/include/dsn/dist/replication/storage_serverlet.h:29
#5  0x0000000000824390 in std::__invoke_impl<void, dsn::replication::storage_serverlet<dsn::apps::rrdb_service>::register_async_rpc_handler<dsn::blob, dsn::apps::count_response>(dsn::task_code, char const*, void (*)(dsn::apps::rrdb_service*, dsn::blob const&, dsn::rpc_replier<dsn::apps::count_response>&))::{lambda(dsn::apps::rrdb_service*, dsn::message_ex*)#1}&, dsn::apps::rrdb_service*, dsn::message_ex*>(std::__invoke_other, dsn::replication::storage_serverlet<dsn::apps::rrdb_service>::register_async_rpc_handler<dsn::blob, dsn::apps::count_response>(dsn::task_code, char const*, void (*)(dsn::apps::rrdb_service*, dsn::blob const&, dsn::rpc_replier<dsn::apps::count_response>&))::{lambda(dsn::apps::rrdb_service*, dsn::message_ex*)#1}&, dsn::apps::rrdb_service*&&, dsn::message_ex*&&) (__f=...) at /opt/gcc10/include/c++/10.2.0/bits/invoke.h:60
#6  0x000000000081f200 in std::__invoke_r<void, dsn::replication::storage_serverlet<dsn::apps::rrdb_service>::register_async_rpc_handler<dsn::blob, dsn::apps::count_response>(dsn::task_code, char const*, void (*)(dsn::apps::rrdb_service*, dsn::blob const&, dsn::rpc_replier<dsn::apps::count_response>&))::{lambda(dsn::apps::rrdb_service*, dsn::message_ex*)#1}&, dsn::apps::rrdb_service*, dsn::message_ex*>(dsn::replication::storage_serverlet<dsn::apps::rrdb_service>::register_async_rpc_handler<dsn::blob, dsn::apps::count_response>(dsn::task_code, char const*, void (*)(dsn::apps::rrdb_service*, dsn::blob const&, dsn::rpc_replier<dsn::apps::count_response>&))::{lambda(dsn::apps::rrdb_service*, dsn::message_ex*)#1}&, (std::__is_invocable&&)...) (__fn=...) at /opt/gcc10/include/c++/10.2.0/bits/invoke.h:153
#7  0x000000000081a219 in std::_Function_handler<void (dsn::apps::rrdb_service*, dsn::message_ex*), dsn::replication::storage_serverlet<dsn::apps::rrdb_service>::register_async_rpc_handler<dsn::blob, dsn::apps::count_response>(dsn::task_code, char const*, void (*)(dsn::apps::rrdb_service*, dsn::blob const&, dsn::rpc_replier<dsn::apps::count_response>&))::{lambda(dsn::apps::rrdb_service*, dsn::message_ex*)#1}>::_M_invoke(std::_Any_data const&, dsn::apps::rrdb_service*&&, dsn::message_ex*&&) (__functor=..., __args#0=@0x7ff78d7f7bf0: 0x3264c00
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1247/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1247,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Ol3g0,incubator-pegasus,1318549556,1247,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2022-11-17T12:14:59Z,2022-11-17T12:14:59Z,"Good find! It's a bug indeed, could you please help to fix it?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Ol3g0/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1249,https://api.github.com/repos/apache/incubator-pegasus/issues/1249,incubator-pegasus,1458121033,1249,Too many verbose logs,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-11-21T15:20:25Z,2022-11-23T12:55:19Z,"The default log level of Pegasus is DEBUG, which is most verbose level. In production env, benchmark tests, there too many logs, it's better to adjust it from DEBUG to INFO level.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1249/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1252,https://api.github.com/repos/apache/incubator-pegasus/issues/1252,incubator-pegasus,1461212059,1252,Feature: support showing true expression in message for CHECK* macro,empiredan,743379,Dan Wang,,CLOSED,2022-11-23T07:41:02Z,2022-11-23T12:09:19Z,"```c++
        CHECK_EQ_MSG(std::strcmp(prototype->name(), iter->second->prototype()->name()),
                     0,
                     ""new prototype '{}' is inconsistent with old prototype '{}' for entity '{}'"",
                     prototype->name(),
                     iter->second->prototype()->name(),
                     id);
```

While asserting failed for the above code, I found the logging had shown as:

```
F2022-11-23 14:01:47.885 (1669183307885368773 37956) : assertion expression: _v1 == _v2
F2022-11-23 14:01:47.885 (1669183307885401886 37956) : 1 vs 0 new prototype 'my_table' is inconsistent with old prototype 'my_server' for entity 'server_81'
```

I think in the line of `assertion expression:` should be shown the true expression, for example:
```
F2022-11-23 14:01:47.885 (1669183307885368773 37956) : assertion expression: std::strcmp(prototype->name(), iter->second->prototype()->name()) == 0
F2022-11-23 14:01:47.885 (1669183307885401886 37956) : 1 vs 0 new prototype 'my_table' is inconsistent with old prototype 'my_server' for entity 'server_81'
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1252/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1255,https://api.github.com/repos/apache/incubator-pegasus/issues/1255,incubator-pegasus,1466515037,1255,Use --release when building java client with java11+,Apache9,4958168,Duo Zhang,palomino219@gmail.com,CLOSED,2022-11-28T14:18:37Z,2022-11-29T01:45:36Z,"So we can make sure that when building with java11+, the generate classes are compatible with java8.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1255/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1257,https://api.github.com/repos/apache/incubator-pegasus/issues/1257,incubator-pegasus,1466663783,1257,Bump netty's version and shade it,Apache9,4958168,Duo Zhang,palomino219@gmail.com,CLOSED,2022-11-28T15:54:40Z,2022-11-30T14:30:01Z,"The current version of netty 4.1.42.Final still has some CVEs, let's upgrade it to the newest version 4.1.85.Final.

And netty is also very easy to introduce conflicts, so let's shade it too.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1257/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1259,https://api.github.com/repos/apache/incubator-pegasus/issues/1259,incubator-pegasus,1467321986,1259,Inherit apache pom,Apache9,4958168,Duo Zhang,palomino219@gmail.com,CLOSED,2022-11-29T01:45:57Z,2022-11-29T04:41:29Z,We need to inherit apache pom if we want to publish java client to repository.a.o,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1259/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1261,https://api.github.com/repos/apache/incubator-pegasus/issues/1261,incubator-pegasus,1467392747,1261,Refactor: remove zkclient dependency,Apache9,4958168,Duo Zhang,palomino219@gmail.com,CLOSED,2022-11-29T03:30:19Z,2022-11-30T07:44:56Z,"In the current java client implementation, we only need to read zk once when loading configuration. We do not need to dependency on any wrapper implementation, just use the official zookeeper client is enough.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1261/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1261,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5PYQLR,incubator-pegasus,1331757777,1261,NA,Apache9,4958168,Duo Zhang,palomino219@gmail.com,NA,2022-11-30T07:44:56Z,2022-11-30T07:44:56Z,Fixed by #1264 ,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5PYQLR/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1265,https://api.github.com/repos/apache/incubator-pegasus/issues/1265,incubator-pegasus,1468077664,1265,Remove assembly and output debug output to console when running UTs for java client,Apache9,4958168,Duo Zhang,palomino219@gmail.com,CLOSED,2022-11-29T13:35:13Z,2022-11-30T02:44:24Z,"After we publish java client to repository.a.o, we do not need to create an assembly tarball for users any more.

Let's remove it.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1265/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1265,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5PXjjp,incubator-pegasus,1331575017,1265,NA,Apache9,4958168,Duo Zhang,palomino219@gmail.com,NA,2022-11-30T02:44:24Z,2022-11-30T02:44:24Z,Fixed by #1266 ,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5PXjjp/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1267,https://api.github.com/repos/apache/incubator-pegasus/issues/1267,incubator-pegasus,1469056263,1267,Use spotless plugin to format pom file,Apache9,4958168,Duo Zhang,palomino219@gmail.com,CLOSED,2022-11-30T05:09:35Z,2022-11-30T09:14:44Z,The spotless plugin can also be used format pom files.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1267/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1268,https://api.github.com/repos/apache/incubator-pegasus/issues/1268,incubator-pegasus,1469066205,1268,Remove usage of commons-configuration,Apache9,4958168,Duo Zhang,palomino219@gmail.com,CLOSED,2022-11-30T05:24:14Z,2022-11-30T14:08:08Z,"We just need to parse a properties file so we do not need to pull in extra configuration libraries.

> And for commons-lang3, we have already use guava, most functions for commons-lang3 can be found in guava, so let's also remove it.

Updated, we have exposed `Pair` as part of public API, which is in commons-lang3, so we can not remove it any more...

As library, we should try out best to introduce less transitive dependencies, if not necessary.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1268/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1269,https://api.github.com/repos/apache/incubator-pegasus/issues/1269,incubator-pegasus,1469184460,1269,chore(action): support checking issue number prefixed with '#' in github actions for PRs,empiredan,743379,Dan Wang,,CLOSED,2022-11-30T07:39:37Z,2022-11-30T10:10:39Z,"Currently PR can pass the check whether an issue is referenced in its description only if the complete URL of the issue can be found, that is, `https://github.com/apache/incubator-pegasus/issues/<issue-number>`. However, `#<issue-number>` is also a valid format for github that can be used to generate a URL for the issue, which should also be supported.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1269/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1275,https://api.github.com/repos/apache/incubator-pegasus/issues/1275,incubator-pegasus,1469697785,1275,Use maven flatten plugin to generate resolved pom,Apache9,4958168,Duo Zhang,palomino219@gmail.com,CLOSED,2022-11-30T14:09:18Z,2022-12-01T02:22:26Z,We do not need to expose profiles to end users.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1275/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1276,https://api.github.com/repos/apache/incubator-pegasus/issues/1276,incubator-pegasus,1469701870,1276,Support publish artifacts to repository.a.o,Apache9,4958168,Duo Zhang,palomino219@gmail.com,CLOSED,2022-11-30T14:11:55Z,2022-12-01T17:48:23Z,"This should be part of our release process.

We need to add some maven plugins.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1276/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1279,https://api.github.com/repos/apache/incubator-pegasus/issues/1279,incubator-pegasus,1472281821,1279,Feature(new_metrics): traverse the whole registry to choose entities and metrics according to the filters,empiredan,743379,Dan Wang,,CLOSED,2022-12-02T04:07:41Z,2022-12-06T17:06:17Z,"In https://github.com/apache/incubator-pegasus/issues/1245, we've supported taking snapshot as json format for an entity according to the filters constructed by the client request. The next job is to traverse the whole registry, taking snapshot for each entity.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1279/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1280,https://api.github.com/repos/apache/incubator-pegasus/issues/1280,incubator-pegasus,1472302252,1280,Feature(new_metrics): provide http service to process client query for metrics,empiredan,743379,Dan Wang,,CLOSED,2022-12-02T04:36:02Z,2022-12-19T03:36:14Z,"Not that backend is enable to take snapshot for the whole registry in https://github.com/apache/incubator-pegasus/issues/1279, it's time to provide http service. The service will parse the client request, and construct the filters according to the parsed parameters.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1280/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1281,https://api.github.com/repos/apache/incubator-pegasus/issues/1281,incubator-pegasus,1473001574,1281,Bad links should be fixed,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-12-02T14:59:15Z,2022-12-04T15:56:21Z,"There are some bad links, for example:
1. java client build status
2. admin-cli build status
3. rocksdb link

Need to be fixed.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1281/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1283,https://api.github.com/repos/apache/incubator-pegasus/issues/1283,incubator-pegasus,1474847738,1283,Unify the *.thrift files use by cpp server and all clients,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-12-04T14:04:00Z,2023-01-31T08:12:09Z,"Current now, cpp server and all clients were using their own idl thrift files to generate code, it's hard to keep consistency and may introduce bugs by mistaken.
We are planning to unify the idl thrift files.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1283/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1286,https://api.github.com/repos/apache/incubator-pegasus/issues/1286,incubator-pegasus,1478462371,1286,drop bulkloading app meeting unrecoverable error,xihong08,74220545,zhuxihong,,OPEN,2022-12-06T07:41:11Z,2022-12-06T07:41:11Z,"in my case，when drop one app which is bulkloading, maybe cause meta-server's op-status never changed to meta_op_status::FREE, so this cause can not continue another bulkload task

meta-server's log
----------
meta.meta_state0.020300000002bf00: meta_bulk_load_service.cpp:xxx:check_partition_status(): app(name=xxx) is not existed, set bulk load failed
---------
meta.THREAD_POOL_META_SERVER3.02005d60004484f2: meta_bulk_load_service.cpp:xx:on_start_bulk_load(): meta server is busy now, please wait
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1286/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1288,https://api.github.com/repos/apache/incubator-pegasus/issues/1288,incubator-pegasus,1491346558,1288,Failed to Determining whether the disk capacity is available for partition split.,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2022-12-12T08:57:53Z,2023-05-16T16:01:54Z,"## Bug Report
![image](https://user-images.githubusercontent.com/48315319/207002932-5d74b3c7-4122-4912-8c5b-2cf8c96269e1.png)
![image](https://user-images.githubusercontent.com/48315319/207003127-a3953c79-a0bc-478e-bea3-0380448b2eec.png)
![image](https://user-images.githubusercontent.com/48315319/207003370-dcfaa8d9-e941-4d89-987f-ee3870d9efab.png)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1288/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1288,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5QPDAa,incubator-pegasus,1346121754,1288,NA,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,NA,2022-12-12T09:03:28Z,2022-12-12T09:03:28Z,"Also `diskThreshold := nodeDiskStats.DiskCapacity * 9 / 10` is worth considering. Cloud use a magic value like `10G`?
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5QPDAa/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1291,https://api.github.com/repos/apache/incubator-pegasus/issues/1291,incubator-pegasus,1493179175,1291,Daily regular build failures,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-12-13T02:02:58Z,2022-12-17T14:31:37Z,"There are some build failures in recent commits, as shown in
https://github.com/apache/incubator-pegasus/actions/runs/3678462538

We have to fix these issues.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1291/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1294,https://api.github.com/repos/apache/incubator-pegasus/issues/1294,incubator-pegasus,1498127267,1294,fix ubuntu-16.04 build,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2022-12-15T09:54:59Z,2022-12-16T07:12:11Z,"as https://github.com/apache/incubator-pegasus/actions/runs/3697467504/jobs/6262497111

fix build regularly on ubuntu-16.04","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1294/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1296,https://api.github.com/repos/apache/incubator-pegasus/issues/1296,incubator-pegasus,1500232608,1296,golint not work on branch v2.4,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2022-12-16T13:46:10Z,2022-12-17T14:31:19Z,"The error shows:
```
  Running [/home/runner/golangci-lint-1.29.0-linux-amd64/golangci-lint run --out-format=github-actions --path-prefix=./admin-cli] in [/home/runner/work/incubator-pegasus/incubator-pegasus/admin-cli] ...
  level=warning msg=""[runner] Can't run linter goanalysis_metalinter: bodyclose: failed prerequisites: [buildssa@github.com/apache/incubator-pegasus/admin-cli/util: analysis skipped: errors in package: [/home/runner/work/incubator-pegasus/incubator-pegasus/admin-cli/util/common_utils.go:23:2: could not import fmt (/opt/hostedtoolcache/go/1.18.9/x64/src/fmt/errors.go:7:8: could not import errors (/opt/hostedtoolcache/go/1.18.9/x64/src/errors/wrap.go:8:2: could not import internal/reflectlite (/opt/hostedtoolcache/go/1.18.9/x64/src/internal/reflectlite/swapper.go:8:2: could not import internal/goarch (-: could not load export data: cannot import \""internal/goarch\"" (unknown iexport format version 2), export data is newer version - update tool)))) /home/runner/work/incubator-pegasus/incubator-pegasus/admin-cli/util/common_utils.go:24:2: could not import reflect (/opt/hostedtoolcache/go/1.18.9/x64/src/reflect/abi.go:8:2: could not import internal/abi (/opt/hostedtoolcache/go/1.18.9/x64/src/internal/abi/abi.go:8:2: could not import internal/goarch (-: could not load export data: cannot import \""internal/goarch\"" (unknown iexport format version 2), export data is newer version - update tool))) /home/runner/work/incubator-pegasus/incubator-pegasus/admin-cli/util/common_utils.go:25:2: could not import sort (/opt/hostedtoolcache/go/1.18.9/x64/src/sort/slice_go113.go:10:8: could not import internal/reflectlite (/opt/hostedtoolcache/go/1.18.9/x64/src/internal/reflectlite/swapper.go:8:2: could not import internal/goarch (-: could not load export data: cannot import \""internal/goarch\"" (unknown iexport format version 2), export data is newer version - update tool))) /home/runner/work/incubator-pegasus/incubator-pegasus/admin-cli/util/common_utils.go:[26](https://github.com/apache/incubator-pegasus/actions/runs/3694633499/jobs/6256052995#step:3:28):2: could not import strconv (/opt/hostedtoolcache/go/1.18.9/x64/src/strconv/atof.go:13:8: could not import math (/opt/hostedtoolcache/go/1.18.9/x64/src/math/exp_amd64.go:9:8: could not import internal/cpu (-: could not load export data: cannot import \""internal/cpu\"" (unknown iexport format version 2), export data is newer version - update tool))) /home/runner/work/incubator-pegasus/incubator-pegasus/admin-cli/util/common_utils.go:[27](https://github.com/apache/incubator-pegasus/actions/runs/3694633499/jobs/6256052995#step:3:29):2: could not import strings (/opt/hostedtoolcache/go/1.18.9/x64/src/strings/builder.go:8:2: could not import unicode/utf8 (-: could not load export data: cannot import \""unicode/utf8\"" (unknown iexport format version 2), export data is newer version - update tool)) /home/runner/work/incubator-pegasus/incubator-pegasus/admin-cli/util/common_utils.go:28:2: could not import time (/opt/hostedtoolcache/go/1.18.9/x64/src/time/format.go:7:8: could not import errors (/opt/hostedtoolcache/go/1.18.9/x64/src/errors/wrap.go:8:2: could not import internal/reflectlite (/opt/hostedtoolcache/go/1.18.9/x64/src/internal/reflectlite/swapper.go:8:2: could not import internal/goarch (-: could not load export data: cannot import \""internal/goarch\"" (unknown iexport format version 2), export data is newer version - update tool)))) /home/runner/work/incubator-pegasus/incubator-pegasus/admin-cli/util/common_utils.go:30:2: could not import github.com/apache/incubator-pegasus/go-client/idl/base (/home/runner/go/pkg/mod/github.com/apache/incubator-pegasus/go-client@v0.0.0-20220617101220-e49a69d25a52/idl/base/blob.go:23:2: could not import fmt (/opt/hostedtoolcache/go/1.18.9/x64/src/fmt/errors.go:7:8: could not import errors (/opt/hostedtoolcache/go/1.18.9/x64/src/errors/wrap.go:8:2: could not import internal/reflectlite (/opt/hostedtoolcache/go/1.18.9/x64/src/internal/reflectlite/swapper.go:8:2: could not import internal/goarch (-: could not load export data: cannot import \""internal/goarch\"" (unknown iexport format version 2), export data is newer version - update tool))))) /home/runner/work/incubator-pegasus/incubator-pegasus/admin-cli/util/http_client.go:23:2: could not import context (/opt/hostedtoolcache/go/1.18.9/x64/src/context/context.go:51:2: could not import errors (/opt/hostedtoolcache/go/1.18.9/x64/src/errors/wrap.go:8:2: could not import internal/reflectlite (/opt/hostedtoolcache/go/1.18.9/x64/src/internal/reflectlite/swapper.go:8:2: could not import internal/goarch (-: could not load export data: cannot import \""internal/goarch\"" (unknown iexport format version 2), export data is newer version - update tool)))) /home/runner/work/incubator-pegasus/incubator-pegasus/admin-cli/util/http_client.go:25:2: could not import sync (/opt/hostedtoolcache/go/1.18.9/x64/src/sync/cond.go:8:2: could not import sync/atomic (/opt/hostedtoolcache/go/1.18.9/x64/src/sync/atomic/value.go:17:4: undeclared name: any)) /home/runner/work/incubator-pegasus/incubator-pegasus/admin-cli/util/http_client.go:28:2: could not import github.com/go-resty/resty/v2 (/home/runner/go/pkg/mod/github.com/go-resty/resty/v2@v2.6.0/client.go:8:2: could not import bytes (/opt/hostedtoolcache/go/1.18.9/x64/src/bytes/buffer.go:10:2: could not import errors (/opt/hostedtoolcache/go/1.18.9/x64/src/errors/wrap.go:8:2: could not import internal/reflectlite (/opt/hostedtoolcache/go/1.18.9/x64/src/internal/reflectlite/swapper.go:8:2: could not import internal/goarch (-: could not load export data: cannot import \""internal/goarch\"" (unknown iexport format version 2), export data is newer version - update tool))))) /home/runner/work/incubator-pegasus/incubator-pegasus/admin-cli/util/pegasus_node.go:24:2: could not import net (/opt/hostedtoolcache/go/1.18.9/x64/src/net/addrselect.go:11:8: could not import sort (/opt/hostedtoolcache/go/1.18.9/x64/src/sort/slice_go113.go:10:8: could not import internal/reflectlite (/opt/hostedtoolcache/go/1.18.9/x64/src/internal/reflectlite/swapper.go:8:2: could not import internal/goarch (-: could not load export data: cannot import \""internal/goarch\"" (unknown iexport format version 2), export data is newer version - update tool)))) /home/runner/work/incubator-pegasus/incubator-pegasus/admin-cli/util/pegasus_node.go:29:2: could not import github.com/apache/incubator-pegasus/go-client/session (/home/runner/go/pkg/mod/github.com/apache/incubator-pegasus/go-client@v0.0.0-20220617101220-e49a69d25a52/session/addr.go:23:2: could not import fmt (/opt/hostedtoolcache/go/1.18.9/x64/src/fmt/errors.go:7:8: could not import errors (/opt/hostedtoolcache/go/1.18.9/x64/src/errors/wrap.go:8:2: could not import internal/reflectlite (/opt/hostedtoolcache/go/1.18.9/x64/src/internal/reflectlite/swapper.go:8:2: could not import internal/goarch (-: could not load export data: cannot import \""internal/goarch\"" (unknown iexport format version 2), export data is newer version - update tool))))) /home/runner/work/incubator-pegasus/incubator-pegasus/admin-cli/util/pegasus_node.go:30:2: could not import github.com/pegasus-kv/collector/aggregate (/home/runner/go/pkg/mod/github.com/pegasus-kv/collector@v0.0.0-20220526124628-023287923c32/aggregate/aggregator.go:4:2: could not import fmt (/opt/hostedtoolcache/go/1.18.9/x64/src/fmt/errors.go:7:8: could not import errors (/opt/hostedtoolcache/go/1.18.9/x64/src/errors/wrap.go:8:2: could not import internal/reflectlite (/opt/hostedtoolcache/go/1.18.9/x64/src/internal/reflectlite/swapper.go:8:2: could not import internal/goarch (-: could not load export data: cannot import \""internal/goarch\"" (unknown iexport format version 2), export data is newer version - update tool))))) /home/runner/work/incubator-pegasus/incubator-pegasus/admin-cli/util/common_utils.go:66:2: missing return /home/runner/work/incubator-pegasus/incubator-pegasus/admin-cli/util/http_client.go:70:15: undeclared name: resty /home/runner/work/incubator-pegasus/incubator-pegasus/admin-cli/util/http_client.go:28:2: \""github.com/go-resty/resty/v2\"" imported but not used]]""
  level=warning msg=""[runner] Can't run linter unused: buildir: failed to load package goarch: could not load export data: cannot import \""internal/goarch\"" (unknown iexport format version 2), export data is newer version - update tool""
  level=error msg=""Running error: buildir: failed to load package goarch: could not load export data: cannot import \""internal/goarch\"" (unknown iexport format version 2), export data is newer version - update tool""
  
  Error: golangci-lint exit with code 3
  Ran golangci-lint in 212[28](https://github.com/apache/incubator-pegasus/actions/runs/3694633499/jobs/6256052995#step:3:30)ms
```

Link: https://github.com/apache/incubator-pegasus/actions/runs/3694633499/jobs/6256052995","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1296/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1299,https://api.github.com/repos/apache/incubator-pegasus/issues/1299,incubator-pegasus,1502459740,1299,Feature: decide whether two C strings are equal and support CHECK_STREQ* and CHECK_STRNE*,empiredan,743379,Dan Wang,,CLOSED,2022-12-19T07:42:16Z,2022-12-26T06:18:03Z,"While asserting equality between two C strings now we have to check if `strcmp` returns 0, for example:

```c++
CHECK_EQ_MSG(std::strcmp(prototype->name(), iter->second->prototype()->name()),
             0,
             ""new prototype '{}' is inconsistent with old prototype '{}' for entity '{}'"",
             prototype->name(),
             iter->second->prototype()->name(),
             id);
```

We can support CHECK_STREQ* and CHECK_STRNE* in common macros for C strings.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1299/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1301,https://api.github.com/repos/apache/incubator-pegasus/issues/1301,incubator-pegasus,1502544394,1301,Feature(new_metrics): support frequently-used combinations of metric fields in the query string,empiredan,743379,Dan Wang,,CLOSED,2022-12-19T08:50:04Z,2022-12-27T02:46:32Z,"Since we've implemented the http service for querying metrics in https://github.com/apache/incubator-pegasus/issues/1280, now we can use `with_metric_fields` to request for the fields of each metric that are returned to client.

However, it's verbose to specify each field for some common scenarios. For example, the most common scenario is that client just wants to see metric name and value without any other field.

We can design the following scenarios to simplify values for `with_metric_fields`:

- **brief** combination: return **name** and all value-related fields; for *gauge* and *counter*, it's **value**; for *percentile*, they are **p95**, **p99**, etc.
- **detail** combination: return all fields for each metric.

Since `with_metric_fields` has been used to specify each metric field, we can just introduce another keyword `detail`, where `detail=true` means **detail** combination and `detail=false` means **brief** combination. `detail=false` will be the default value.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1301/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1303,https://api.github.com/repos/apache/incubator-pegasus/issues/1303,incubator-pegasus,1521116836,1303,Feature(new_metrics): retire stale metric entities that are not used by any other object,empiredan,743379,Dan Wang,,CLOSED,2023-01-05T17:35:02Z,2023-02-07T07:45:22Z,"As has been described in https://github.com/apache/incubator-pegasus/issues/922, the framework of new metrics should check and retire the old metrics.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1303/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1305,https://api.github.com/repos/apache/incubator-pegasus/issues/1305,incubator-pegasus,1526752924,1305,Refactor log macros,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-01-10T04:37:18Z,2023-01-28T07:03:28Z,"There are two type of log macros in the code, one is using C string format specifiers, like `LOG_INFO`, the other is using `libfmt`, like `LOG_INFO_F`.

It's strange to keep the two macros in long term, I'm planning to refactor them and only leave `libfmt` style one.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1305/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1305,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SDz0D,incubator-pegasus,1376730371,1305,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-01-10T04:48:16Z,2023-01-10T04:48:16Z,"Because there are so many code to be modified, I'll separet the whole work into smaller patches to make reviewers happy 😆 
- LOG_DEBUG
  - all: https://github.com/apache/incubator-pegasus/pull/1306
- LOG_INFO
  - replica module: https://github.com/apache/incubator-pegasus/pull/1307
  - meta module: https://github.com/apache/incubator-pegasus/pull/1310
  - others: https://github.com/apache/incubator-pegasus/pull/1311
- LOG_WARN
  - replica module: https://github.com/apache/incubator-pegasus/pull/1315
  - meta module: https://github.com/apache/incubator-pegasus/pull/1316
  - others: https://github.com/apache/incubator-pegasus/pull/1319
- LOG_ERROR
  - replica module: https://github.com/apache/incubator-pegasus/pull/1318
  - others: https://github.com/apache/incubator-pegasus/pull/1320
- LOG_FATAL
  - all: https://github.com/apache/incubator-pegasus/pull/1312

After all the above work been finished, rename `LOG_*_F` to `LOG_*` for all log macros
 - https://github.com/apache/incubator-pegasus/pull/1322","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SDz0D/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1305,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SFi0m,incubator-pegasus,1377185062,1305,NA,empiredan,743379,Dan Wang,,NA,2023-01-10T12:24:39Z,2023-01-10T12:24:39Z,"After all `LOG_*` macros have been replaced with `LOG_*_F`, would `LOG_*_F` be renamed as `LOG_*`? For example, `LOG_INFO_F` is renamed as `LOG_INFO` ?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SFi0m/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1305,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SGFj2,incubator-pegasus,1377327350,1305,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-01-10T13:59:56Z,2023-01-10T13:59:56Z,"> After all `LOG_*` macros have been replaced with `LOG_*_F`, would `LOG_*_F` be renamed as `LOG_*`? For example, `LOG_INFO_F` is renamed as `LOG_INFO` ?

Yes, I'm planning to still use `LOG_*_F` before all refactor work been finished, then remove `_F` postfix at last in one patch.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SGFj2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1305,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SGLhC,incubator-pegasus,1377351746,1305,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-01-10T14:18:50Z,2023-01-10T14:18:50Z,"> > After all `LOG_*` macros have been replaced with `LOG_*_F`, would `LOG_*_F` be renamed as `LOG_*`? For example, `LOG_INFO_F` is renamed as `LOG_INFO` ?
> 
> Yes, I'm planning to still use `LOG_*_F` before all refactor work been finished, then remove `_F` postfix at last in one patch.

In this way, all macros with or without `_F` postfix are both in the same semantics in the progress.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SGLhC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1305,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SGgbl,incubator-pegasus,1377437413,1305,NA,empiredan,743379,Dan Wang,,NA,2023-01-10T15:23:22Z,2023-01-10T15:23:22Z,"> > > After all `LOG_*` macros have been replaced with `LOG_*_F`, would `LOG_*_F` be renamed as `LOG_*`? For example, `LOG_INFO_F` is renamed as `LOG_INFO` ?
> > 
> > 
> > Yes, I'm planning to still use `LOG_*_F` before all refactor work been finished, then remove `_F` postfix at last in one patch.
> 
> In this way, all macros with or without `_F` postfix are both in the same semantics in the progress.

I think it is important to keep all macros consistent without intermediate stages. Let's go ahead !","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SGgbl/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1305,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SbFM9,incubator-pegasus,1382830909,1305,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2023-01-14T16:00:13Z,2023-01-14T16:00:13Z,"Meticulous work! 
Can we add some checking of parameter count in the log function?
ref: https://github.com/fmtlib/fmt/issues/2593","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SbFM9/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1305,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Sd7OD,incubator-pegasus,1383576451,1305,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-01-16T07:03:53Z,2023-01-16T07:03:53Z,"> Meticulous work! Can we add some checking of parameter count in the log function? ref: [fmtlib/fmt#2593](https://github.com/fmtlib/fmt/issues/2593)

I assigned this work to you, you can implement it if you have time.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Sd7OD/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1305,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Sd_4U,incubator-pegasus,1383595540,1305,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-01-16T07:22:47Z,2023-01-16T07:22:47Z,"@GehaFearless Could you please to do the remain work?
- LOG_WARN
  - replica module
  - meta module
  - others

- LOG_ERROR
  - replica module
  - others","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Sd_4U/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1309,https://api.github.com/repos/apache/incubator-pegasus/issues/1309,incubator-pegasus,1528966294,1309,refactor: quotes of DSN_DEFINE_* are useless,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-01-11T12:40:25Z,2023-01-12T10:08:37Z,"The quotes of DSN_DEFINE_*'s fisrt parameter are useless, we can remove them as how does fglag do. For example,
before:
```
DSN_DEFINE_uint32(""replication"",
                  direct_io_buffer_pages,
                  64,
                  ""Number of pages we need to set to direct io buffer"");
```

after:
```
DSN_DEFINE_uint32(replication,
                  direct_io_buffer_pages,
                  64,
                  ""Number of pages we need to set to direct io buffer"");
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1309/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1313,https://api.github.com/repos/apache/incubator-pegasus/issues/1313,incubator-pegasus,1535208470,1313,Add unit test and lint workflow for python client,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-01-16T16:32:03Z,2023-01-18T05:52:07Z,"Before unify thrift files for python client (https://github.com/apache/incubator-pegasus/issues/1283), we'd better support unit test action for pythin client module.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1313/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1313,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SjTZT,incubator-pegasus,1384986195,1313,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-01-17T08:02:49Z,2023-01-17T08:02:49Z,"Lint job need to be added, anybody want to contribute it?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SjTZT/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1313,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SpLZP,incubator-pegasus,1386526287,1313,NA,empiredan,743379,Dan Wang,,NA,2023-01-18T05:51:56Z,2023-01-18T05:51:56Z,I'll add lint jobs.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5SpLZP/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1321,https://api.github.com/repos/apache/incubator-pegasus/issues/1321,incubator-pegasus,1550390990,1321,Feature(new_metrics): migrate built-in metrics,empiredan,743379,Dan Wang,,OPEN,2023-01-20T06:59:25Z,2023-01-29T06:33:44Z,"In the framework of `perf_counters`, built-in metrics are actually two metrics:

- *memused.res(MB)*, the usage of physical memory
- *memused.virt(MB)*, the size of virtual memory

They are collected in a timer each `FLAGS_perf_counter_update_interval_seconds` (typically 10 seconds).

In the framework of new metrics, we can also introduce the built-in metrics. During `start()` for both `pegasus_replication_service_app` and `pegasus_meta_service_app`, a timer will be launched to collect built-in metrics periodically. The built-in metrics will be attached to *server* entity, which is defined in `metrics.cpp`. Both of them will be *gauges*.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1321/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1323,https://api.github.com/repos/apache/incubator-pegasus/issues/1323,incubator-pegasus,1560670670,1323,Unify configuration launch method,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-01-28T03:30:18Z,2023-03-01T15:36:29Z,"There are two types of configuration launch methods in the code.
One is using gflag like macros, e.g.
```
#define DSN_DEFINE_uint64(section, name, val, desc)
```
The other is using functions, e.g.
```
uint64_t dsn_config_get_value_uint64(const char *section,
                                     const char *key,
                                     uint64_t default_value,
                                     const char *dsptr);
```

It's strange to keep the two methods in long term, We're planning to refactor them and only leave `DSN_DEFINE_*` ones.

To make the patches easy to review, I want to split the whole work into small patches, each patch only refactor one type of configuration.
- DSN_DEFINE_int32:
  - https://github.com/apache/incubator-pegasus/pull/1324
  - https://github.com/apache/incubator-pegasus/pull/1346
- DSN_DEFINE_uint32
  - https://github.com/apache/incubator-pegasus/pull/1352
- DSN_DEFINE_int64
  - https://github.com/apache/incubator-pegasus/pull/1357
- DSN_DEFINE_uint64
  - https://github.com/apache/incubator-pegasus/pull/1359
- DSN_DEFINE_double
  - https://github.com/apache/incubator-pegasus/pull/1362
- DSN_DEFINE_bool
  - https://github.com/apache/incubator-pegasus/pull/1363
- DSN_DEFINE_string
  - https://github.com/apache/incubator-pegasus/pull/1371","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1323/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1323,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5WcqWE,incubator-pegasus,1450354052,1323,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-03-01T15:36:28Z,2023-03-01T15:36:28Z,"There are some configs are launched by macros `CONFIG_BEGIN`, `CONFIG_FLD*` and `CONFIG_END`, we can refactor them in another issuse, this one closed.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5WcqWE/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1325,https://api.github.com/repos/apache/incubator-pegasus/issues/1325,incubator-pegasus,1560733808,1325,Feature(new_metrics): migrate to the new metrics system,empiredan,743379,Dan Wang,,OPEN,2023-01-28T07:21:03Z,2023-04-16T14:59:42Z,"As the new framework of metrics has nearly been completed (https://github.com/apache/incubator-pegasus/issues/922), it's time to migrate the metrics to the new framework.

This issue is used as the main entrance, tracing all migration tasks as follows:

- [ ] https://github.com/apache/incubator-pegasus/issues/1326
- [ ] https://github.com/apache/incubator-pegasus/issues/1328

Tasks which are no longer to be done:

- [ ] https://github.com/apache/incubator-pegasus/issues/1327 

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1325/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1326,https://api.github.com/repos/apache/incubator-pegasus/issues/1326,incubator-pegasus,1560746020,1326,Feature(new_metrics): migrate replica-level metrics,empiredan,743379,Dan Wang,,OPEN,2023-01-28T08:03:24Z,2023-07-06T02:12:54Z,"The replica-level entity can be bound to [replica_base](https://github.com/apache/incubator-pegasus/blob/master/src/replica/replica_base.h), since all of the classes (`pegasus_server_impl`, `replica`, etc., see classes in following sub-tasks) that hold replica-level metrics are derived from `replica_base`; Moreover, these classes also accept `replica_base *` as a parameter of their constructors. 

Therefore, the *replica* entity can just be instantiated just in the constructor of `replica_base`; then, all of the classes that hold replica-level metrics will be instantiated with the *replica* entity from `replica_base`.

All classes (except `pegasus_server_impl` and `replica`) that hold replica-level metrics are a member of `pegasus_server_impl`, which is a member of `replica` and created at `replica::init_app_and_prepare_list()`, thus the lifetime of all replica-level metrics are bound to a replica: they will be released once a replica is deleted.

The replica-level metrics are distributed on following classes and source files. The migration for them can be divided into following tasks:

- [x] https://github.com/apache/incubator-pegasus/issues/1344
- [ ] https://github.com/apache/incubator-pegasus/issues/1336
- [x] https://github.com/apache/incubator-pegasus/issues/1333
- [ ] https://github.com/apache/incubator-pegasus/issues/1334
- [ ] https://github.com/apache/incubator-pegasus/issues/1342
- [ ] https://github.com/apache/incubator-pegasus/issues/1343
- [x] https://github.com/apache/incubator-pegasus/issues/1412","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1326/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1327,https://api.github.com/repos/apache/incubator-pegasus/issues/1327,incubator-pegasus,1560755035,1327,Feature(new_metrics): migrate table-level metrics,empiredan,743379,Dan Wang,,OPEN,2023-01-28T08:26:42Z,2023-06-13T04:33:43Z,"There are following metrics all of which are in class `replica`, [replica.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/replica/replica.cpp) file:
```
_counters_table_level_latency[]
```

Therefore, table-level entity can just be defined in `replica.cpp`, where they are also instantiated in class `replica`, to which the above metrics are attached.

The table-level metrics originate from https://github.com/XiaoMi/rdsn/pull/336 which is tracked by issue https://github.com/apache/incubator-pegasus/issues/406.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1327/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1327,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5UCh5l,incubator-pegasus,1409949285,1327,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-01-31T08:23:39Z,2023-01-31T08:23:39Z,"Now that we can aggregate on collector, is it necessary to keep and aggregate these metrics on each replica server?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5UCh5l/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1327,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5UPSyU,incubator-pegasus,1413295252,1327,NA,empiredan,743379,Dan Wang,,NA,2023-02-02T07:57:45Z,2023-02-02T07:57:45Z,"> Now that we can aggregate on collector, is it necessary to keep and aggregate these metrics on each replica server?

OK, it's better to aggregate table metrics based on replica metrics, we can suspend this task.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5UPSyU/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1328,https://api.github.com/repos/apache/incubator-pegasus/issues/1328,incubator-pegasus,1560790300,1328,Feature(new_metrics): migrate metrics of server and other levels,empiredan,743379,Dan Wang,,OPEN,2023-01-28T09:43:04Z,2023-07-06T02:12:14Z,"The number of metrics that are attached to server-level entity is the largest among all kinds of entities. They are distributed very widely on classes and source files.

Thus server-level entity can just be defined in `metrics.cpp`. Once a metric is attached to the server entity, declare the entity at the header of the source file. The migration for server-level metrics can be divided into following tasks:

- [ ] https://github.com/apache/incubator-pegasus/issues/1414
- [ ] https://github.com/apache/incubator-pegasus/issues/1321
- [ ] https://github.com/apache/incubator-pegasus/issues/1329
- [x] https://github.com/apache/incubator-pegasus/issues/1425
- [ ] https://github.com/apache/incubator-pegasus/issues/1331
- [x] https://github.com/apache/incubator-pegasus/issues/1441
- [x] https://github.com/apache/incubator-pegasus/issues/1454
- [x] https://github.com/apache/incubator-pegasus/issues/1481


Following metrics are the members of `pegasus_event_listener` ([pegasus_event_listener.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/server/pegasus_event_listener.cpp)), which is created at the construction of `pegasus_server_impl`:
| Variables | Types/Computations |
| :-------: | :------------------: |
| _pfc_recent_flush_completed_count | increase(Counter) |
| _pfc_recent_flush_output_bytes | increase(Counter) |
| _pfc_recent_compaction_completed_count | increase(Counter) |
| _pfc_recent_compaction_input_bytes | increase(Counter) |
| _pfc_recent_compaction_output_bytes | increase(Counter) |
| _pfc_recent_write_change_delayed_count | increase(Counter) |
| _pfc_recent_write_change_stopped_count | increase(Counter) |




Following metrics are the members of `ship_mutation` ([duplication_pipeline.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/replica/duplication/duplication_pipeline.cpp)), which is created at `replica_duplicator::start_dup_log()`:
| Variables | Types/Computations |
| :-------: | :------------------: |
| _counter_dup_shipped_bytes_rate | rate(Counter) |","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1328/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1329,https://api.github.com/repos/apache/incubator-pegasus/issues/1329,incubator-pegasus,1561154044,1329,Feature(new_metrics): migrate server-level metrics for nfs,empiredan,743379,Dan Wang,,OPEN,2023-01-29T06:56:48Z,2023-03-29T12:00:06Z,"`nfs` metrics will be in use once `nfs_node` ([nfs_node.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/nfs/nfs_node.cpp)) is created, which is a member of `replica_stub`. Thus in new framework all of them will be attached to *server* entity. Related source files and metrics are listed as below.

<br/>

[nfs_client_impl.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/nfs/nfs_client_impl.cpp):
| Variables | Types/Computations |
| :-------: | :------------------: |
| _recent_copy_data_size | increase(Counter) |
| _recent_copy_fail_count | increase(Counter) |
| _recent_write_data_size | increase(Counter) |
| _recent_write_fail_count | increase(Counter) |

<br/>

[nfs_server_impl.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/nfs/nfs_server_impl.cpp):
| Variables | Types/Computations |
| :-------: | :------------------: |
| _recent_copy_data_size | increase(Counter) |
| _recent_copy_fail_count | increase(Counter) |","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1329/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1330,https://api.github.com/repos/apache/incubator-pegasus/issues/1330,incubator-pegasus,1561164762,1330,Add unit test workflow for node.js client,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-01-29T07:32:59Z,2023-01-30T06:33:10Z,"Before unify thrift files for node.js client (https://github.com/apache/incubator-pegasus/issues/1283), we'd better support unit test action for node.js client module.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1330/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1331,https://api.github.com/repos/apache/incubator-pegasus/issues/1331,incubator-pegasus,1561174500,1331,Feature(new_metrics): migrate meta metrics,empiredan,743379,Dan Wang,,OPEN,2023-01-29T07:58:35Z,2023-04-15T15:12:23Z,"The meta-related metrics migrated to new framework will be attached to *server* entity. All involved classes are put as below.

<br/>

Following metrics are the members of `server_state` ([server_state.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/meta/server_state.cpp)), which is created at the construction of `meta_service`:
| Variables | Types/Computations |
| :-------: | :------------------: |
| _dead_partition_count | Gauge |
| _unreadable_partition_count | Gauge |
| _unwritable_partition_count | Gauge |
| _writable_ill_partition_count | Gauge |
| _healthy_partition_count | Gauge |
| _recent_update_config_count | increase(Counter) |
| _recent_partition_change_unwritable_count | increase(Counter) |
| _recent_partition_change_writable_count | increase(Counter) |


<br/>

Following metrics are the members of `greedy_load_balancer` ([greedy_load_balancer.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/meta/greedy_load_balancer.cpp)), which is created at `meta_service::start()`:

| Variables | Types/Computations |
| :-------: | :------------------: |
| _balance_operation_count | Gauge |
| _recent_balance_move_primary_count | increase(Counter) |
| _recent_balance_copy_primary_count | increase(Counter) |
| _recent_balance_copy_secondary_count | increase(Counter) |

<br/>

Following metrics are the member of `meta_service` ([meta_service.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/meta/meta_service.cpp)), which is created at the construction of `meta_service_app`:
| Variables | Types/Computations |
| :-------: | :------------------: |
| _recent_disconnect_count | increase(Counter) |
| _unalive_nodes_count | Gauge |
| _alive_nodes_count | Gauge |

<br/>

Following metrics are the member of `policy_context` and created at `policy_context::start()` ([meta_service.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/meta/meta_backup_service.cpp)). `policy_context` is created at `meta_service::start()` once cold backup is enabled:
| Variables | Types/Computations |
| :-------: | :------------------: |
| _counter_policy_recent_backup_duration_ms | Gauge |

<br/>

Following metrics are the member of `partition_guardian` ([meta_service.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/meta/partition_guardian.cpp)), which is created at `meta_service::start()` :
| Variables | Types/Computations |
| :-------: | :------------------: |
| _recent_choose_primary_fail_count | increase(Counter) |","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1331/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1333,https://api.github.com/repos/apache/incubator-pegasus/issues/1333,incubator-pegasus,1561658901,1333,Feature(new_metrics): migrate replica-level metrics for pegasus_server_impl,empiredan,743379,Dan Wang,,CLOSED,2023-01-30T03:07:45Z,2023-07-06T02:12:54Z,"As is described in https://github.com/apache/incubator-pegasus/issues/1326, since `pegasus_server_impl` is derived from `replica_base`, replica-level metrics in `pegasus_server_impl` can be  attached directly to *replica* entity.

Following metrics are the members of `pegasus_server_impl` ([pegasus_server_impl_init.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/server/pegasus_server_impl_init.cpp)). Their variables in current source files and new types/computations in new framework are listed as below:

| Variables | Types/Computations |
| :-------: | :------------------: |
| _pfc_get_qps | rate(Counter) |
| _pfc_multi_get_qps | rate(Counter) |
| _pfc_batch_get_qps | rate(Counter) |
| _pfc_scan_qps | rate(Counter) |
| _pfc_get_latency | Percentile |
| _pfc_multi_get_latency | Percentile |
| _pfc_batch_get_latency | Percentile |
| _pfc_scan_latency | Percentile |
| _pfc_recent_expire_count | increase(Counter) |
| _pfc_recent_filter_count | increase(Counter) |
| _pfc_recent_abnormal_count | increase(Counter) |
| _pfc_rdb_sst_count | Gauge |
| _pfc_rdb_sst_size | Gauge |
| _pfc_rdb_index_and_filter_blocks_mem_usage | Gauge |
| _pfc_rdb_memtable_mem_usage | Gauge |
| _pfc_rdb_estimate_num_keys | Gauge |
| _pfc_rdb_block_cache_hit_count | Gauge |
| _pfc_rdb_block_cache_total_count | Gauge |
|_pfc_rdb_write_amplification | Gauge |
| _pfc_rdb_read_amplification | Gauge |
| _pfc_rdb_memtable_hit_count | Gauge |
| _pfc_rdb_memtable_total_count | Gauge |
| _pfc_rdb_l0_hit_count | Gauge |
| _pfc_rdb_l1_hit_count | Gauge |
| _pfc_rdb_l2andup_hit_count | Gauge |
| _pfc_rdb_bf_seek_negatives | Gauge |
| _pfc_rdb_bf_seek_total | Gauge |
| _pfc_rdb_bf_point_positive_true | Gauge |
| _pfc_rdb_bf_point_positive_total | Gauge |
| _pfc_rdb_bf_point_negatives | Gauge |
| _counter_recent_read_throttling_reject_count | Gauge |","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1333/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1334,https://api.github.com/repos/apache/incubator-pegasus/issues/1334,incubator-pegasus,1561693820,1334,Feature(new_metrics): migrate replica-level metrics for capacity_unit_calculator,empiredan,743379,Dan Wang,,OPEN,2023-01-30T03:52:43Z,2023-01-30T03:52:43Z,"As is described in https://github.com/apache/incubator-pegasus/issues/1326, the constructor of `capacity_unit_calculator` has a parameter of `replica_base`, which owns the *replica* entity that can be used to instantiate replica-level metrics for `capacity_unit_calculator`.

Following metrics are the members of `capacity_unit_calculator` ([capacity_unit_calculator.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/server/capacity_unit_calculator.cpp)), which is created at `pegasus_server_impl::start()`. Their variables in current source files and new types/computations in new framework are listed as below:

| Variables | Types/Computations |
| :--------: | :--------------------: |
| _pfc_recent_read_cu |  increase(Counter) |
| _pfc_recent_write_cu|  increase(Counter) |
| _pfc_get_bytes | rate(Counter) |
| _pfc_multi_get_bytes | rate(Counter)  |
| _pfc_batch_get_bytes | rate(Counter)  |
| _pfc_scan_bytes | rate(Counter)  |
| _pfc_put_bytes | rate(Counter)  |
| _pfc_multi_put_bytes | rate(Counter)  |
| _pfc_check_and_set_bytes | rate(Counter) |
| _pfc_check_and_mutate_bytes | rate(Counter) |
| _pfc_backup_request_bytes | rate(Counter) |","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1334/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1336,https://api.github.com/repos/apache/incubator-pegasus/issues/1336,incubator-pegasus,1561906106,1336,Feature(new_metrics): migrate replica-level metrics for write service,empiredan,743379,Dan Wang,,OPEN,2023-01-30T07:38:26Z,2023-02-23T16:11:43Z,"As is described in https://github.com/apache/incubator-pegasus/issues/1326,  since both `pegasus_server_write` and `pegasus_write_service` are derived from `replica_base`, their replica-level metrics can be attached directly to *replica* entity. `pegasus_write_service` is one of the members of `pegasus_server_write`, which is one of the members of `pegasus_server_impl`.

Following metrics are the members of `pegasus_server_write` ([pegasus_server_write.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/server/pegasus_server_write.cpp)). Their variables in current source files and new types/computations in new framework are listed as below:

| Variables | Types/Computations |
| :-------: | :------------------: |
| _pfc_recent_corrupt_write_count | increase(Counter) |

<br>

Following metrics are the members of `pegasus_write_service` ([pegasus_write_service.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/server/pegasus_write_service.cpp)). Their variables in current source files and new types/computations in new framework are listed as below:

| Variables | Types/Computations |
| :-------: | :------------------: |
| _pfc_put_qps | rate(Counter) |
| _pfc_multi_put_qps | rate(Counter) |
| _pfc_remove_qps | rate(Counter) |
| _pfc_multi_remove_qps | rate(Counter) |
| _pfc_incr_qps | rate(Counter) |
| _pfc_check_and_set_qps | rate(Counter) |
| _pfc_check_and_mutate_qps | rate(Counter) |
| _pfc_put_latency | Percentile |
| _pfc_multi_put_latency | Percentile |
| _pfc_remove_latency | Percentile |
| _pfc_multi_remove_latency | Percentile |
| _pfc_incr_latency | Percentile |
| _pfc_check_and_set_latency | Percentile |
| _pfc_check_and_mutate_latency | Percentile |
| _pfc_duplicate_qps | rate(Counter) |
| _pfc_dup_time_lag | Percentile |
| _pfc_dup_lagging_writes | increase(Counter) |","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1336/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1336,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5VkzV5,incubator-pegasus,1435710841,1336,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-02-18T16:25:53Z,2023-02-18T16:25:53Z,"@empiredan  It would be better to describe the relationship between old metric names and the new ones, then users know how to change their monitor tools.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5VkzV5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1336,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5V8-fG,incubator-pegasus,1442047942,1336,NA,empiredan,743379,Dan Wang,,NA,2023-02-23T16:11:43Z,2023-02-23T16:11:43Z,"> @empiredan It would be better to describe the relationship between old metric names and the new ones, then users know how to change their monitor tools.

OK, I'll add both old and new names into the table. Also, I'll summarize all the changes into a big table. ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5V8-fG/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1337,https://api.github.com/repos/apache/incubator-pegasus/issues/1337,incubator-pegasus,1567350131,1337,Feature(utils): Add a method to obtain the string prefix by intercepting the string with the specified character,WHBANG,38547944,,,CLOSED,2023-02-02T05:30:35Z,2023-02-03T07:15:51Z,Add a find_string_prefix method in string. h. You can specify a character to intercept the string and get the first part of the intercept.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1337/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1339,https://api.github.com/repos/apache/incubator-pegasus/issues/1339,incubator-pegasus,1573786336,1339,rocksdb build flag caused the performance to degrade,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2023-02-07T06:57:51Z,2023-05-16T16:00:35Z,"## Bug Report

Our business will introduce version 2.4 to replace 2.0，we  found  the 2.4 performance to be lower than 2.0.
For my work, I found the cause of the problem -- build type for one of thirdparty named rocksdb.
We lost flag ""CMAKE_BUILD_TYPE=Release"" on once Reconstruction.

rocksdb build type deleted by this commit: https://github.com/XiaoMi/rdsn/commit/aef76b192ea3aac21b062767f0098ccba9a74aa4#diff-74b3e476c2f3ca1dd3782cb2ef32bbf6b53adc54d8d961d829fdfd1523c433b8","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1339/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1339,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Up_yr,incubator-pegasus,1420295339,1339,NA,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,NA,2023-02-07T07:08:45Z,2023-02-07T07:08:45Z,"Performance is 25% higher than before on the same machine when I apply this commit.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Up_yr/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1342,https://api.github.com/repos/apache/incubator-pegasus/issues/1342,incubator-pegasus,1575810815,1342,Feature(new_metrics): migrate replica-level metrics for `replica` class,empiredan,743379,Dan Wang,,OPEN,2023-02-08T09:51:56Z,2023-03-21T02:57:56Z,"All of the metrics in `replica` class are replica-level. As is described in https://github.com/apache/incubator-pegasus/issues/1326, since all classes that hold replica-level metrics are sub-object to `replica` class, all of replica-level metrics are bound to `replica` class. Thus once `replica` class is destructed, *replica* entity will be retired (since reference count of entity will decrease to 1).

Following metrics are the members of `replica` ([replica.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/replica/replica.cpp)):

| New Metric Name | New Metric Type | Old Perf Counter Name | Old Perf Counter Type | Old Perf Counter Variable | Compute New Metric From Old Perf Counter |
| :-----------------: | :---------------: |  :---------------------: | :---------------------: | :-------------------------: | :------------------------------------------------: |
| private_log_size_mb | Gauge | replica*eon.replica*private.log.size(MB)@<app_id>.<partition_id> | COUNTER_TYPE_NUMBER | _counter_private_log_size | - |
| throttling_delayed_write_requests | Counter | replica\*eon.replica\*recent.write.throttling.delay.count@<app_id>.<partition_id> | COUNTER_TYPE_VOLATILE_NUMBER | _counter_recent_write_throttling_delay_count | increase(Counter) |
| throttling_rejected_write_requests | Counter | replica\*eon.replica\*recent.write.throttling.reject.count@<app_id>.<partition_id> | COUNTER_TYPE_VOLATILE_NUMBER | _counter_recent_write_throttling_reject_count | increase(Counter) |
| throttling_delayed_read_requests | Counter | replica\*eon.replica\*recent.read.throttling.delay.count@<app_id>.<partition_id> | COUNTER_TYPE_VOLATILE_NUMBER | _counter_recent_read_throttling_delay_count | increase(Counter) |
| throttling_rejected_read_requests | Counter | replica\*eon.replica\*recent.read.throttling.reject.count@<app_id>.<partition_id> | COUNTER_TYPE_VOLATILE_NUMBER | _counter_recent_read_throttling_reject_count | increase(Counter) |
| backup_requests | Counter | replica\*eon.replica\*backup_request_qps@<app_name> | COUNTER_TYPE_RATE |  _counter_backup_request_qps | rate(Counter) | 
| throttling_delayed_backup_requests | Counter | replica\*eon.replica\*recent.backup.request.throttling.delay.count@<app_name> | COUNTER_TYPE_VOLATILE_NUMBER | _counter_recent_backup_request_throttling_delay_count | increase(Counter) |
| throttling_rejected_backup_requests | Counter | replica\*eon.replica\*recent.backup.request.throttling.reject.count@<app_name> | COUNTER_TYPE_VOLATILE_NUMBER | _counter_recent_backup_request_throttling_reject_count | increase(Counter) |
| splitting_rejected_write_requests | Counter | replica\*eon.replica\*recent.write.splitting.reject.count@<app_id>.<partition_id> | COUNTER_TYPE_VOLATILE_NUMBER | _counter_recent_write_splitting_reject_count | increase(Counter) |
| splitting_rejected_read_requests | Counter | replica\*eon.replica\*recent.read.splitting.reject.count@<app_id>.<partition_id> | COUNTER_TYPE_VOLATILE_NUMBER | _counter_recent_read_splitting_reject_count | increase(Counter) |
| bulk_load_ingestion_rejected_write_requests | Counter | replica\*eon.replica\*recent.write.bulk.load.ingestion.reject.count@<app_id>.<partition_id> | COUNTER_TYPE_VOLATILE_NUMBER | _counter_recent_write_bulk_load_ingestion_reject_count | increase(Counter) |
| dup_rejected_non_idempotent_write_requests | Counter | replica\*eon.replica\*dup.disabled_non_idempotent_write_count@<app_name> | COUNTER_TYPE_VOLATILE_NUMBER | _counter_dup_disabled_non_idempotent_write_count | increase(Counter) |


Note that `increment()` invocations for `_counter_recent_*_throttling_delay_count` and `_counter_recent_*_throttling_reject_count` are in [replica_throttle.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/replica/replica_throttle.cpp).","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1342/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1343,https://api.github.com/repos/apache/incubator-pegasus/issues/1343,incubator-pegasus,1575825240,1343,Feature(new_metrics): migrate migrate replica-level metrics for pegasus_event_listener,empiredan,743379,Dan Wang,,OPEN,2023-02-08T10:01:48Z,2023-03-24T09:18:37Z,"Some classes that have few of replica-level metrics are collected into one issue to be migrated.

<br/>

Following metrics are the members of `pegasus_event_listener` ([pegasus_event_listener.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/server/pegasus_event_listener.cpp)), which is created at the construction of `pegasus_server_impl`:

| New Metric Name | New Metric Type | Old Perf Counter Name | Old Perf Counter Type | Old Perf Counter Variable | Compute New Metric From Old Perf Counter |
| :-----------------: | :---------------: |  :---------------------: | :---------------------: | :-------------------------: | :------------------------------------------------: |
| _pfc_recent_rdb_compaction_input_bytes | increase(Counter) |
| _pfc_recent_rdb_compaction_output_bytes | increase(Counter) |


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1343/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1344,https://api.github.com/repos/apache/incubator-pegasus/issues/1344,incubator-pegasus,1579524143,1344,Feature(new_metrics): add replica-level metric entity,empiredan,743379,Dan Wang,,CLOSED,2023-02-10T11:37:11Z,2023-02-13T07:48:50Z,"As is described in https://github.com/apache/incubator-pegasus/issues/1326, we can just add *replica* entity to [replica_base](https://github.com/apache/incubator-pegasus/blob/master/src/replica/replica_base.h) as a member.

All classes that hold replica-level metrics can get *replica* entity by `replica_base::replica_metric_entity()`. Since the *replica* entity has a single `id` (the format is `replica_{table_id}.{partition_id}`), all classes that hold replica-level metrics will share the single *replica* entity.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1344/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1347,https://api.github.com/repos/apache/incubator-pegasus/issues/1347,incubator-pegasus,1580861973,1347,Docker image build failed,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-02-11T14:24:09Z,2023-05-16T08:10:36Z,"The docker image build failed when tis patch [1] merged, see [2].
@GehaFearless Could you please help to check and fix it?


1. https://github.com/apache/incubator-pegasus/commit/4476219048eea20155c54f58f9d7512f9f1af4e7
2. https://github.com/apache/incubator-pegasus/actions/runs/4115308630/jobs/7103848560","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1347/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1347,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5VSs5l,incubator-pegasus,1430965861,1347,NA,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,NA,2023-02-15T08:55:59Z,2023-02-15T08:55:59Z,"
<img width=""1045"" alt=""image"" src=""https://user-images.githubusercontent.com/48315319/218980206-1615fc53-47d5-4e23-9731-267db615d852.png"">



This job has been a failure for a long time. Let me try it on local.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5VSs5l/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1348,https://api.github.com/repos/apache/incubator-pegasus/issues/1348,incubator-pegasus,1583669639,1348,Remove the supporting of EOL platforms,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-02-14T07:51:17Z,2023-10-17T16:25:17Z,"Now Pegasus supports CentOS6/7, Ubuntu 16.04/18.04/20.04, however, CentOS 6 is EOL at November 30, 2020 [1], Ubuntu 16.04 is EOL at August 13, 2020 [2].

Should we consider not support the 2 platforms too?


1. https://wiki.centos.org/FAQ/General#What_is_the_support_.27.27end_of_life.27.27_for_each_CentOS_release.3F
2. https://wiki.ubuntu.com/Releases","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1348/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1349,https://api.github.com/repos/apache/incubator-pegasus/issues/1349,incubator-pegasus,1583829921,1349,Introduce IWYU,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-02-14T09:44:12Z,2023-03-29T16:12:16Z,"# Why include what you use?

See [Why include what you use?](https://github.com/include-what-you-use/include-what-you-use/blob/master/docs/WhyIWYU.md)

# How ?

- [x] Introduce Ubuntu 22.04 to supported OS list
- [x] Use clang-14 on Ubuntu 22.04 to run IWYU, fix issues, add CI and make sure CI passed
- [ ] Use Pegasus special `fix_includes.py`
- [ ] Add mapping rules (see: https://github.com/include-what-you-use/include-what-you-use/blob/master/docs/IWYUMappings.md)
- [ ] Try to use C++ standard headers instead of C standard headers (e.g. `stdint.h` -> `cstdint`)
- [ ] Separate C standard, C++ standard and thirdparty headers
- [ ] Speed up the workflow

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1349/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1349,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5YvujC,incubator-pegasus,1488906434,1349,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-03-29T16:12:16Z,2023-03-29T16:12:16Z,The issue is open and welcome community developers to continue the works.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5YvujC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1353,https://api.github.com/repos/apache/incubator-pegasus/issues/1353,incubator-pegasus,1585691096,1353,Feature(new_metrics): migrate hotspot detection to new metrics system,empiredan,743379,Dan Wang,,OPEN,2023-02-15T11:27:19Z,2023-02-16T03:33:00Z,"In https://github.com/apache/incubator-pegasus/pull/1351, it has been found that `detect_hotspot_test` cannot be passed:
```
start testing write hotspot data...
/__w/incubator-pegasus/incubator-pegasus/src/test/function_test/detect_hotspot_test/test_detect_hotspot.cpp:105: Failure
Value of: find_hotkey
  Actual: false
Expected: true
/__w/incubator-pegasus/incubator-pegasus/src/test/function_test/detect_hotspot_test/test_detect_hotspot.cpp:138: Failure
Expected: get_result(detection_type::write_data, key_type::hotspot_dataset) doesn't generate new fatal failures in the current thread.
  Actual: it does.
/__w/incubator-pegasus/incubator-pegasus/src/test/function_test/detect_hotspot_test/test_detect_hotspot.cpp:199: Failure
Expected: write_hotspot_data() doesn't generate new fatal failures in the current thread.
  Actual: it does.
[  FAILED  ] detect_hotspot_test.write_hotspot_data_test (1312[85](https://github.com/apache/incubator-pegasus/actions/runs/4172815277/jobs/7225467127#step:7:86) ms)
[----------] 1 test from detect_hotspot_test (1312[86](https://github.com/apache/incubator-pegasus/actions/runs/4172815277/jobs/7225467127#step:7:87) ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (1312[90](https://github.com/apache/incubator-pegasus/actions/runs/4172815277/jobs/7225467127#step:7:91) ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] detect_hotspot_test.write_hotspot_data_test

 1 FAILED TEST
dsn exit with code 1
```

The reason is that the hotspot detection depends on the perf-counters system. C++ Collector collects perf-counters from each replica server, and decide if there is some partition with hotspot; once there is hotspot, C++ Collector will request the primary replica (via the replica server which the replica hosts on) to detect hotkey (`RPC_DETECT_HOTKEY`). As for the primary replica, once it has been requested to detect the hot key, it will capture each written or read hash key by `capacity_unit_calculator`.

Therefore solve this problem by following steps:

1. Temporarily disable `detect_hotspot_test` in https://github.com/apache/incubator-pegasus/pull/1351;
2. support hotspot detection by new metrics system in Go Collector, which will replace C++ Collector, detecting partitions with hotspot and requesting their primary replicas for hot key detection;
3. then, re-enable `detect_hotspot_test`.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1353/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1355,https://api.github.com/repos/apache/incubator-pegasus/issues/1355,incubator-pegasus,1587116757,1355,所有nioEventLoopGroup在进入ReplicaSession.colseSession后无限等待,kkk6285137,12046160,,,CLOSED,2023-02-16T06:43:33Z,2023-03-04T08:47:20Z,"## 现象
2月14号晚上9点半某机房3个客户端服务实例（该机房总共7个实例）突然报大量pegasus调用超时异常，持续近一个小时没有恢复。在将报异常的客户端实例重启后，客户端恢复正常。检查pegasus服务端相关的监控打点，未见QPS激增、耗时增加等情况。

## 问题追踪
保留了一个客户端实例的jstack现场，发现所有的nioEventLoopGroup在调用进入ReplicaSession.colseSession逻辑后，处于wait状态。部分线程调用栈信息如下所示。
```
""nioEventLoopGroup-2-3"" #678 prio=10 os_prio=0 tid=0x00007f85487f4000 nid=0x6fc0 in Object.wait() [0x00007f81fda9b000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:502)
	at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:252)
	- locked <0x0000000749ba8ad0> (a io.netty.channel.DefaultChannelPromise)
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:131)
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:30)
	at io.netty.util.concurrent.DefaultPromise.sync(DefaultPromise.java:403)
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:119)
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:30)
	at com.xiaomi.infra.pegasus.rpc.async.ReplicaSession.closeSession(ReplicaSession.java:121)
	at com.xiaomi.infra.pegasus.rpc.async.ReplicaSession.tryNotifyFailureWithSeqID(ReplicaSession.java:295)
	at com.xiaomi.infra.pegasus.rpc.async.ReplicaSession$4.run(ReplicaSession.java:345)
	at io.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)
	at io.netty.util.concurrent.ScheduledFutureTask.run(ScheduledFutureTask.java:170)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)

""nioEventLoopGroup-2-2"" #677 prio=10 os_prio=0 tid=0x00007f85487f3000 nid=0x6fbf in Object.wait() [0x00007f83a76f7000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:502)
	at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:252)
	- locked <0x0000000749861118> (a io.netty.channel.DefaultChannelPromise)
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:131)
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:30)
	at io.netty.util.concurrent.DefaultPromise.sync(DefaultPromise.java:403)
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:119)
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:30)
	at com.xiaomi.infra.pegasus.rpc.async.ReplicaSession.closeSession(ReplicaSession.java:121)
	at com.xiaomi.infra.pegasus.rpc.async.ReplicaSession.tryNotifyFailureWithSeqID(ReplicaSession.java:295)
	at com.xiaomi.infra.pegasus.rpc.async.ReplicaSession$4.run(ReplicaSession.java:345)
	at io.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)
	at io.netty.util.concurrent.ScheduledFutureTask.run(ScheduledFutureTask.java:170)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)

""nioEventLoopGroup-2-1"" #676 prio=10 os_prio=0 tid=0x00007f854aa4b000 nid=0x6fbe in Object.wait() [0x00007f81f6023000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:502)
	at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:252)
	- locked <0x00000007496942e0> (a io.netty.channel.DefaultChannelPromise)
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:131)
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:30)
	at io.netty.util.concurrent.DefaultPromise.sync(DefaultPromise.java:403)
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:119)
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:30)
	at com.xiaomi.infra.pegasus.rpc.async.ReplicaSession.closeSession(ReplicaSession.java:121)
	at com.xiaomi.infra.pegasus.rpc.async.ReplicaSession.tryNotifyFailureWithSeqID(ReplicaSession.java:295)
	at com.xiaomi.infra.pegasus.rpc.async.ReplicaSession$4.run(ReplicaSession.java:345)
	at io.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)
	at io.netty.util.concurrent.ScheduledFutureTask.run(ScheduledFutureTask.java:170)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)

	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
```

继续追踪当天晚上的运行日志，发现nioEventLoopGroup线程在9点24分输出

```
2023-02-14 21:24:37,147 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.136.160.9:54801): actively close the session because it's not responding for 10 seconds
```

但是未能继续输出`channel to rpc_address  %s closed ` 的日志 。表明这些线程在9点24分后，就卡在wait状态。这个时间点也与我收到故障报警的时间相同。
我比较疑惑为什么pegasus线程会进入该状态，为什么没有能从wait状态恢复正常？

部分日志信息如下：
```
2023-02-14 21:24:29,231 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005109 failed: 
2023-02-14 21:24:29,231 [nioEventLoopGroup-2-3] WARN  - passport_ptn_login_cache: replica server(rpc_address(10.114.1.15:54801)) rpc timeout for gpid(gpid(16.11)), operator(com.xiaomi.infra.pegasus.operator.rrdb_get_operator@1dfa3ab9), try(1), error_code(ERR_TIMEOUT), not retry
2023-02-14 21:24:29,231 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309853 failed: 
2023-02-14 21:24:29,231 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309853 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:29,231 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309854 failed: 
2023-02-14 21:24:29,231 [nioEventLoopGroup-2-3] WARN  - passport_ptn_login_cache: replica server(rpc_address(10.114.2.44:54801)) rpc timeout for gpid(gpid(16.4)), operator(com.xiaomi.infra.pegasus.operator.rrdb_get_operator@1b6db4a0), try(1), error_code(ERR_TIMEOUT), not retry
2023-02-14 21:24:29,231 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005110 failed: 
2023-02-14 21:24:29,231 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005110 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:29,231 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005111 failed: 
2023-02-14 21:24:29,231 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005111 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:29,231 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309855 failed: 
2023-02-14 21:24:29,231 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309855 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,133 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): actively close the session because it's not responding for 10 seconds
2023-02-14 21:24:32,160 [nioEventLoopGroup-2-3] INFO  - channel to rpc_address(10.114.1.15:54801) closed
2023-02-14 21:24:32,160 [nioEventLoopGroup-2-3] WARN  - passport_ptn_login_cache: replica server(rpc_address(10.114.1.15:54801)) doesn't serve gpid(gpid(16.12)), operator(com.xiaomi.infra.pegasus.operator.rrdb_check_and_set_operator@68bb8261), try(1), error_code(ERR_SESSION_RESET), need query meta
2023-02-14 21:24:32,160 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309856 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,169 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309857 failed: 
2023-02-14 21:24:32,169 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): actively close the session because it's not responding for 10 seconds
2023-02-14 21:24:32,169 [nioEventLoopGroup-2-3] INFO  - channel to rpc_address(10.114.2.44:54801) closed
2023-02-14 21:24:32,169 [nioEventLoopGroup-2-3] WARN  - passport_ptn_login_cache: replica server(rpc_address(10.114.2.44:54801)) doesn't serve gpid(gpid(16.4)), operator(com.xiaomi.infra.pegasus.operator.rrdb_check_and_set_operator@1c6b0b76), try(1), error_code(ERR_SESSION_RESET), need query meta
2023-02-14 21:24:32,169 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005113 failed: 
2023-02-14 21:24:32,169 [nioEventLoopGroup-2-3] WARN  - passport_ptn_login_cache: replica server(rpc_address(10.114.1.15:54801)) rpc timeout for gpid(gpid(16.12)), operator(com.xiaomi.infra.pegasus.operator.rrdb_check_and_set_operator@5088d5b0), try(1), error_code(ERR_TIMEOUT), not retry
2023-02-14 21:24:32,169 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309858 failed: 
2023-02-14 21:24:32,169 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309858 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,169 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005114 failed: 
2023-02-14 21:24:32,169 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005114 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005115 failed: 
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005115 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005116 failed: 
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005116 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309859 failed: 
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309859 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309860 failed: 
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309860 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309861 failed: 
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309861 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005117 failed: 
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005117 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005118 failed: 
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005118 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309862 failed: 
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309862 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309863 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005119 failed: 
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005119 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] WARN  - passport_ptn_login_cache: replica server(rpc_address(10.114.2.44:54801)) rpc timeout for gpid(gpid(16.3)), operator(com.xiaomi.infra.pegasus.operator.rrdb_get_operator@26b6b3b5), try(1), error_code(ERR_TIMEOUT), not retry
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309864 failed: 
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309864 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005120 failed: 
2023-02-14 21:24:32,188 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005120 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,189 [nioEventLoopGroup-2-3] WARN  - passport_ptn_login_cache: replica server(rpc_address(10.114.1.15:54801)) rpc timeout for gpid(gpid(16.12)), operator(com.xiaomi.infra.pegasus.operator.rrdb_check_and_set_operator@122a9588), try(1), error_code(ERR_TIMEOUT), not retry
2023-02-14 21:24:32,352 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005122 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,370 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005123 failed: 
2023-02-14 21:24:32,370 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005123 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,370 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309865 failed: 
2023-02-14 21:24:32,370 [nioEventLoopGroup-2-3] WARN  - passport_ptn_login_cache: replica server(rpc_address(10.114.2.44:54801)) rpc timeout for gpid(gpid(16.3)), operator(com.xiaomi.infra.pegasus.operator.rrdb_check_and_set_operator@565e1c00), try(1), error_code(ERR_TIMEOUT), not retry
2023-02-14 21:24:32,371 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309866 failed: 
2023-02-14 21:24:32,371 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309866 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,371 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005124 failed: 
2023-02-14 21:24:32,371 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005124 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,371 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309867 failed: 
2023-02-14 21:24:32,371 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309867 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,371 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309868 failed: 
2023-02-14 21:24:32,371 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309868 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,371 [nioEventLoopGroup-2-3] WARN  - passport_ptn_login_cache: replica server(rpc_address(10.114.2.44:54801)) rpc timeout for gpid(gpid(16.3)), operator(com.xiaomi.infra.pegasus.operator.rrdb_get_operator@40ed662e), try(1), error_code(ERR_TIMEOUT), not retry
2023-02-14 21:24:32,371 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309869 failed: 
2023-02-14 21:24:32,371 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309869 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:32,371 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309870 failed: 
--
2023-02-14 21:24:37,074 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005187 failed: 
2023-02-14 21:24:37,074 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005187 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,074 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005188 failed: 
2023-02-14 21:24:37,074 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005188 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,074 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005189 failed: 
2023-02-14 21:24:37,074 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005189 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,074 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309948 failed: 
2023-02-14 21:24:37,074 [nioEventLoopGroup-2-3] WARN  - passport_ptn_login_cache: replica server(rpc_address(10.114.2.44:54801)) rpc timeout for gpid(gpid(16.3)), operator(com.xiaomi.infra.pegasus.operator.rrdb_get_operator@1741089c), try(1), error_code(ERR_TIMEOUT), not retry
2023-02-14 21:24:37,074 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005190 failed: 
2023-02-14 21:24:37,074 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005190 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,074 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005191 failed: 
2023-02-14 21:24:37,074 [nioEventLoopGroup-2-3] WARN  - passport_ptn_login_cache: replica server(rpc_address(10.114.1.15:54801)) rpc timeout for gpid(gpid(16.12)), operator(com.xiaomi.infra.pegasus.operator.rrdb_get_operator@490c0e10), try(1), error_code(ERR_TIMEOUT), not retry
2023-02-14 21:24:37,075 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309949 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005192 failed: 
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005192 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309950 failed: 
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309950 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005193 failed: 
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005193 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] WARN  - passport_user_action_region: replica server(rpc_address(10.114.2.44:54801)) rpc timeout for gpid(gpid(11.19)), operator(com.xiaomi.infra.pegasus.operator.rrdb_get_operator@6f691167), try(1), error_code(ERR_TIMEOUT), not retry
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309951 failed: 
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309951 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309952 failed: 
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309952 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309953 failed: 
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309953 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005194 failed: 
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005194 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] WARN  - passport_user_action_region: replica server(rpc_address(10.114.1.15:54801)) rpc timeout for gpid(gpid(11.109)), operator(com.xiaomi.infra.pegasus.operator.rrdb_get_operator@23bbede), try(1), error_code(ERR_TIMEOUT), not retry
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005195 failed: 
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005195 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] WARN  - passport_user_action_region: replica server(rpc_address(10.114.2.44:54801)) rpc timeout for gpid(gpid(11.100)), operator(com.xiaomi.infra.pegasus.operator.rrdb_get_operator@1d2153ed), try(1), error_code(ERR_TIMEOUT), not retry
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309954 failed: 
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309954 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005196 failed: 
2023-02-14 21:24:37,120 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005196 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,138 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309955 failed: 
2023-02-14 21:24:37,138 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309955 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,138 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309956 failed: 
2023-02-14 21:24:37,138 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309956 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,138 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.1.15:54801) write seqid 514005197 failed: 
2023-02-14 21:24:37,138 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005197 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,138 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309957 failed: 
2023-02-14 21:24:37,138 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309957 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,138 [nioEventLoopGroup-2-3] WARN  - passport_user_action_region: replica server(rpc_address(10.114.1.15:54801)) rpc timeout for gpid(gpid(11.94)), operator(com.xiaomi.infra.pegasus.operator.rrdb_get_operator@34f4a2d7), try(1), error_code(ERR_TIMEOUT), not retry
2023-02-14 21:24:37,138 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.1.15:54801): 514005198 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,147 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309958 failed: 
2023-02-14 21:24:37,147 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309958 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,147 [nioEventLoopGroup-2-3] INFO  - rpc_address(10.114.2.44:54801) write seqid 514309959 failed: 
2023-02-14 21:24:37,147 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.114.2.44:54801): 514309959 is removed by others, current error ERR_TIMEOUT, isTimeoutTask false
2023-02-14 21:24:37,147 [nioEventLoopGroup-2-3] WARN  - rpc_address(10.136.160.9:54801): actively close the session because it's not responding for 10 seconds
2023-02-14 22:37:13,355 [nioEventLoopGroup-7-2] WARN  - Channel [id: 0x3acda6b5, L:/10.162.4.3:57031 ! R:/10.132.145.43:34601] for session rpc_address(10.132.145.43:34601) is inactive
2023-02-14 22:37:13,355 [nioEventLoopGroup-7-2] INFO  - rpc_address(10.132.145.43:34601): mark the session to be disconnected from state=CONNECTED
2023-02-14 22:37:13,355 [nioEventLoopGroup-7-2] WARN  - Channel [id: 0xfdf4e624, L:/10.162.4.3:60985 ! R:/10.162.62.11:35801] for session rpc_address(10.162.62.11:35801) is inactive
2023-02-14 22:37:13,355 [nioEventLoopGroup-7-2] INFO  - rpc_address(10.162.62.11:35801): mark the session to be disconnected from state=CONNECTED
2023-02-14 22:37:13,355 [nioEventLoopGroup-7-3] WARN  - Channel [id: 0xe942d2fa, L:/10.162.4.3:35711 ! R:/10.132.43.24:35801] for session rpc_address(10.132.43.24:35801) is inactive
2023-02-14 22:37:13,355 [nioEventLoopGroup-7-3] INFO  - rpc_address(10.132.43.24:35801): mark the session to be disconnected from state=CONNECTED
2023-02-14 22:37:13,355 [nioEventLoopGroup-7-1] WARN  - Channel [id: 0xc5748ec6, L:/10.162.4.3:38554 ! R:/10.162.64.31:35801] for session rpc_address(10.162.64.31:35801) is inactive
2023-02-14 22:37:13,355 [nioEventLoopGroup-7-1] INFO  - rpc_address(10.162.64.31:35801): mark the session to be disconnected from state=CONNECTED
2023-02-14 22:37:13,355 [nioEventLoopGroup-7-3] WARN  - Channel [id: 0x5a6ea942, L:/10.162.4.3:63661 ! R:/10.132.42.43:35801] for session rpc_address(10.132.42.43:35801) is inactive
2023-02-14 22:37:13,355 [nioEventLoopGroup-7-3] INFO  - rpc_address(10.132.42.43:35801): mark the session to be disconnected from state=CONNECTED
2023-02-14 22:37:13,356 [nioEventLoopGroup-7-1] WARN  - Channel [id: 0x99203515, L:/10.162.4.3:16570 ! R:/10.132.43.4:35801] for session rpc_address(10.132.43.4:35801) is inactive
2023-02-14 22:37:13,356 [nioEventLoopGroup-7-1] INFO  - rpc_address(10.132.43.4:35801): mark the session to be disconnected from state=CONNECTED
2023-02-14 22:37:13,356 [nioEventLoopGroup-7-4] WARN  - Channel [id: 0xc7b622b6, L:/10.162.4.3:61679 ! R:/10.162.62.32:35801] for session rpc_address(10.162.62.32:35801) is inactive
2023-02-14 22:37:13,356 [nioEventLoopGroup-7-4] INFO  - rpc_address(10.162.62.32:35801): mark the session to be disconnected from state=CONNECTED
2023-02-14 22:37:13,356 [nioEventLoopGroup-7-2] WARN  - Channel [id: 0x4c823fb1, L:/10.162.4.3:21723 ! R:/10.162.65.1:35801] for session rpc_address(10.162.65.1:35801) is inactive
2023-02-14 22:37:13,356 [nioEventLoopGroup-7-2] INFO  - rpc_address(10.162.65.1:35801): mark the session to be disconnected from state=CONNECTED
2023-02-14 22:37:13,356 [nioEventLoopGroup-7-4] WARN  - Channel [id: 0x64beb514, L:/10.162.4.3:59103 ! R:/10.132.42.23:35801] for session rpc_address(10.132.42.23:35801) is inactive
2023-02-14 22:37:13,356 [nioEventLoopGroup-7-4] INFO  - rpc_address(10.132.42.23:35801): mark the session to be disconnected from state=CONNECTED
2023-02-14 22:37:13,356 [nioEventLoopGroup-7-3] WARN  - Channel [id: 0x2709b68d, L:/10.162.4.3:41936 ! R:/10.132.43.2:35801] for session rpc_address(10.132.43.2:35801) is inactive
2023-02-14 22:37:13,356 [nioEventLoopGroup-7-3] INFO  - rpc_address(10.132.43.2:35801): mark the session to be disconnected from state=CONNECTED
2023-02-14 22:37:13,356 [nioEventLoopGroup-7-4] WARN  - Channel [id: 0x91f9c939, L:/10.162.4.3:30249 ! R:/10.132.42.3:35801] for session rpc_address(10.132.42.3:35801) is inactive
```

当然，这个问题看起来也很奇怪，它有一定的偶然性：客户端实例离上次重启有几个月的时间，突然报此问题，且仅有部分实例报此问题。又有一定的必然性：同时有三个实例在同一时间都出现了这一问题。
我初步怀疑可能当时同一机房的这三个实例在那个时间点出现了一定的网络抖动，导致`nioEventLoopGroup`进入了一个错误状态，未能恢复。但我没有足够的证据。
如果需要更多的日志信息，可以与我联系。


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1355/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1355,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Vk0q-,incubator-pegasus,1435716286,1355,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-02-18T16:56:26Z,2023-02-18T16:56:26Z,"@kkk6285137 Thanks for your reporting!

请问Pegasus及java client的版本是怎样的？","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Vk0q-/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1355,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Vm7Wl,incubator-pegasus,1436267941,1355,NA,kkk6285137,12046160,,,NA,2023-02-20T03:30:06Z,2023-02-20T03:30:06Z,"jkd版本：openjdk version ""1.8.0_202""
pegasus版本：https://github.com/XiaoMi/pegasus-java-client/tree/1.11-thrift-0.11.0-inlined-release","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Vm7Wl/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1355,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5WtHVL,incubator-pegasus,1454667083,1355,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-03-04T08:46:54Z,2023-03-04T08:46:54Z,"@kkk6285137 抱歉这个还没找到原因，可以加微信详聊下吗？
微信号：acelyc111","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5WtHVL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1358,https://api.github.com/repos/apache/incubator-pegasus/issues/1358,incubator-pegasus,1592691561,1358,go mod tidy报错,lihenan1993,30621077,,,CLOSED,2023-02-21T02:45:10Z,2023-02-22T08:56:45Z,"`go mod tidy

        github.com/apache/incubator-pegasus/go-client/admin imports
        github.com/apache/incubator-pegasus/go-client/session imports
        github.com/apache/incubator-pegasus/go-client/idl/radmin: module github.com/apache/incubator-pegasus/go-client@latest found (v0.0.0-20230215061652-4c76112a5ded), but does not contain package github.com/apache/incubator-pegas
us/go-client/idl/radmin
`
测试文件 session_test.go 中引用的包 idle/rrdb , idle/replication 被删除了
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1358/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1358,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Vuu4e,incubator-pegasus,1438314014,1358,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-02-21T11:24:42Z,2023-02-21T11:24:42Z,"@lihenan1993 您可以参考 https://github.com/apache/incubator-pegasus/blob/master/.github/workflows/lint_and_test_go-client.yml#L38 里面的步骤进行尝试。也就是像这里描述的一样 https://github.com/apache/incubator-pegasus/blob/master/go-client/Makefile#L18 ，需要先使用`thrift`来把 `*.thrift` 生成`*.go`到相应的目录下。
ps：这里移出之前目录文件的背景是，这些文件都是由thrift文件生成的，存在重复和过时的问题，所以编译时都现生成。","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Vuu4e/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1358,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Vz0bL,incubator-pegasus,1439647435,1358,NA,lihenan1993,30621077,,,NA,2023-02-22T08:56:45Z,2023-02-22T08:56:45Z,"您的出发点我可以理解，不过编译时现生成，会导致每个用到该库的程序，自动化部署都需要修改，对我们来说有些复杂。
感谢您的回答！","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Vz0bL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1368,https://api.github.com/repos/apache/incubator-pegasus/issues/1368,incubator-pegasus,1600473020,1368,The script tool supports specifying configuration file when using shell client,WHBANG,38547944,,,CLOSED,2023-02-27T04:58:18Z,2023-05-16T16:00:17Z,"Many script tools use the shell client through ""--cluster meta_server_list"".  we hope to add ""--config config_path"" parameter to specify the configuration file. as:
`./run.sh shell --config shell-config-path`","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1368/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1372,https://api.github.com/repos/apache/incubator-pegasus/issues/1372,incubator-pegasus,1602307270,1372,Add test bench type multi_set / multi_get,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2023-02-28T03:28:26Z,2023-03-02T15:05:09Z,"For our business, we found version 2.4 interface mutli_get & multi_get performance to be lower than 2.0.
So we need add bench type multi_get/multi_set test it.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1372/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1372,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5WSZdh,incubator-pegasus,1447663457,1372,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2023-02-28T06:39:17Z,2023-02-28T06:39:17Z,how about adding a benchmark test in the CI pipeline?,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5WSZdh/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1372,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5WSfCB,incubator-pegasus,1447686273,1372,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-02-28T07:06:55Z,2023-02-28T07:06:55Z,"> how about adding a benchmark test in the CI pipeline?

+1, and we have to make sure the performance of the machine running CI is stable.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5WSfCB/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1377,https://api.github.com/repos/apache/incubator-pegasus/issues/1377,incubator-pegasus,1603347024,1377,CLion load the project failed,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-02-28T16:00:00Z,2023-03-02T15:04:56Z,"CLion load the project failed with output like:
```
/Applications/CLion.app/Contents/bin/cmake/mac/bin/cmake -DCMAKE_BUILD_TYPE=Debug -DCMAKE_MAKE_PROGRAM=/Applications/CLion.app/Contents/bin/ninja/mac/ninja -S /Users/laiyingchun/dev/pegasus -B /Users/laiyingchun/dev/pegasus/cmake-build-debug -G Ninja -S /Users/laiyingchun/dev/pegasus -B /Users/laiyingchun/dev/pegasus/cmake-build-debug
Running CompilerInfo.cmake
Apple clang version 14.0.0 (clang-1400.0.29.202)
Target: x86_64-apple-darwin22.3.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

Selected compiler clang 14.0
-- THIRDPARTY_INSTALL_DIR = /Users/laiyingchun/dev/pegasus/thirdparty/output
-- BUILD_DIR = /Users/laiyingchun/dev/pegasus/src/builder
-- BUILD_TEST = ON
-- ENABLE_GCOV = OFF
-- ENABLE_GPERF = ON
-- USE_JEMALLOC = OFF
-- CCACHE: /usr/local/bin/ccache
-- FIND_LIBRARY_USE_LIB64_PATHS = ON
-- Found components for DL
-- DL_INCLUDES = /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX13.1.sdk/usr/include
-- DL_LIBRARIES = /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX13.1.sdk/usr/lib/libdl.tbd
-- use ccache to speed up compilation
-- CMAKE_PREFIX_PATH = /Users/laiyingchun/dev/pegasus/thirdparty/output
-- JAVA_JVM_LIBRARY=/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/jre/lib/server/libjvm.dylib
-- MACOS_OPENSSL_ROOT_DIR: 
-- THRIFT_GENERATED_FILE_PATH=/Users/laiyingchun/dev/pegasus/cmake-build-debug/thrift-gen
CMake Error at cmake_modules/ThriftUtils.cmake:49 (message):
  thrift file ../../idl/metadata.thrift does not exist
Call Stack (most recent call first):
  src/common/CMakeLists.txt:27 (thrift_generate_cpp)


-- Configuring incomplete, errors occurred!
See also ""/Users/laiyingchun/dev/pegasus/cmake-build-debug/CMakeFiles/CMakeOutput.log"".

```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1377/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1382,https://api.github.com/repos/apache/incubator-pegasus/issues/1382,incubator-pegasus,1608224104,1382,interface multi_get performance reduce,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2023-03-03T09:04:15Z,2024-05-09T06:33:59Z,"## Bug Report

We found version 2.4 interface multi_get performance to be lower than 2.0.

Through my efforts, discover thr pr on [del tls](https://github.com/XiaoMi/rdsn/commit/f9b9961e2c0b13faed157e7974e3783f5590ac53
) affects the outcome.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1382/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1382,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Wnie7,incubator-pegasus,1453205435,1382,NA,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,NA,2023-03-03T09:10:07Z,2023-03-03T09:10:07Z,Try to revert this pr back branch master.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Wnie7/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1382,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Wnnsa,incubator-pegasus,1453226778,1382,NA,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,NA,2023-03-03T09:26:49Z,2023-03-03T09:26:49Z,"I use bench test interface multi_set & multi_get compare performance on same machine .

- Firstly, start onebox and fill data.

`./run.sh bench --type multisetrandom_pegasus --num 10000000 --app_name temp --timeout 20000`

- Then, stop onebox
- Start different verison onebox and read data.

`./run.sh bench --type multigetrandom_pegasus --num 5000000 --app_name temp --timeout 20000`

Compare before and after revert :

current branch master:    1758    1780    1778   ops/sec
after revert:                       2069   2036   2018   ops/sec","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Wnnsa/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1382,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5WnoVT,incubator-pegasus,1453229395,1382,NA,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,NA,2023-03-03T09:28:59Z,2023-03-03T09:28:59Z,I think we should revert 'tls' back.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5WnoVT/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1382,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Wwfst,incubator-pegasus,1455553325,1382,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2023-03-06T06:58:34Z,2023-03-06T06:58:34Z,"how about pasting more test details, like cpu/memory usage?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Wwfst/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1383,https://api.github.com/repos/apache/incubator-pegasus/issues/1383,incubator-pegasus,1611272978,1383,RocksDB corruption leads to endless failure,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-03-06T12:09:35Z,2023-06-26T09:33:08Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
Some RocksDB instances corrupted for some reasons, maybe disk driver IO error, data corruption, etc.

2. What did you expect to see?
The cluster can recover from error automatically.

3. What did you see instead?
The replica server will close the replica when see write errors, but will start the replica again in the same place whose data is still corrupted. And then, the error occured again and again.

For read requests, the replica will not handle the error instead, then read requests will fail again and again.

Write:
![image](https://user-images.githubusercontent.com/10775040/225831222-feba4822-9f91-4418-b322-2198fafd1c86.png)

Read:
![image](https://user-images.githubusercontent.com/10775040/225831579-687c2a0d-26ae-43e4-a19f-de8fa0d9ab97.png)

4. What version of Pegasus are you using?
1.12, 2.0","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1383/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1385,https://api.github.com/repos/apache/incubator-pegasus/issues/1385,incubator-pegasus,1613341915,1385,The client does not respond after the  replication  service is restarted,aubdiy,9246111,AUB,,CLOSED,2023-03-07T12:27:32Z,2023-04-18T07:16:19Z,"## Bug Report

集群配置: 
1. 共 4 台服务器
2. 其中 3 台部署  meta , 
3. 每台服务器都部署 1 个 replic

问题场景:
1. 客户端使用 golang
2. 端从 kafka 中读取数据, 写入 pegasus
3. 当重启一台 replic  服务的时候

4 golang  客户端打印
time=""2023-03-07T07:56:34Z"" level=info msg=""session [10.218.73.124:34801(replica)] is closed by the peer"" func=""session.(*nodeSession).loopForResponse"" file=""session.go:287""

replic重启成功(1 分钟左右)

但是 client 程序一致僵死, 没有写的数据写入
拜读源码,
发现一旦发生 rpc 网络关闭问题, 
session 就关闭退出了, 
当前协程不再处理任何事情

问题出现在 session.go 的 286 行, 
```
if rpc.IsNetworkClosed(err) { // EOF
    n.logger.Printf(""session %s is closed by the peer"", n)
   return nil
}
```

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1385/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1385,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Z8lF8,incubator-pegasus,1509052796,1385,NA,PasunuriSrinidhi,93040752,Pasunuri Srinidhi,,NA,2023-04-14T18:21:39Z,2023-04-14T18:21:39Z,"I want to work on this issue ,can you please assign this to me","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Z8lF8/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1389,https://api.github.com/repos/apache/incubator-pegasus/issues/1389,incubator-pegasus,1620996563,1389,java client krb not work with  kinit,shalk,2435781,shalk(xiao kun),,CLOSED,2023-03-13T08:39:24Z,2023-03-16T10:56:15Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
If possible, provide a recipe for reproducing the error.
use keytab and principal with java client. 


2. What did you expect to see?
use keytab not fail.

4. What did you see instead?
when i use kinit ,client will auth fail.

6. What version of Pegasus are you using?
2.3.4","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1389/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1390,https://api.github.com/repos/apache/incubator-pegasus/issues/1390,incubator-pegasus,1623136844,1390,How about publish java client to the maven central repository,yx91490,8814185,CenterCode,,OPEN,2023-03-14T10:06:01Z,2023-03-21T02:49:11Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

I found that user have to build java client from the scratch, but download it from the maven central repository, it's unfriendly for a beginner. I think it's time to publish a java client to maven center repository.

Note: now that java-client have been moved to this repo(apache/incubator-pegasus), we should publish java client under the 'org.apache.pegasus' groupId.

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->

**Describe alternatives you've considered:**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->

**Teachability, Documentation, Adoption, Migration Strategy:**
<!-- If you can, explain some scenarios how users might use this, situations it would be helpful in. Any API designs, mockups, or diagrams are also helpful. -->
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1390/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1390,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5X1BR-,incubator-pegasus,1473516670,1390,NA,shalk,2435781,shalk(xiao kun),,NA,2023-03-17T09:32:24Z,2023-03-17T09:32:24Z,"ref to https://central.sonatype.org/publish/large-orgs/ 

i think this is  different from personal project. 

maybe first need publish to https://repository.apache.org/#welcome and then sync to central repo.

I don't kown if this is right way.

meanswhile,  i can not login `repository.apache.org` with  my jira account.


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5X1BR-/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1390,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5YDGea,incubator-pegasus,1477207962,1390,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-03-21T02:49:11Z,2023-03-21T02:49:11Z,"@shalk @yx91490 Pegasus client 2.4-SNAPSHOT has been published to repository.apache.org, see https://repository.apache.org/#nexus-search;quick~pegasus","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5YDGea/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1391,https://api.github.com/repos/apache/incubator-pegasus/issues/1391,incubator-pegasus,1623249559,1391,Undeclared names are reported while building alloc.h in debug mode,empiredan,743379,Dan Wang,,CLOSED,2023-03-14T11:08:33Z,2023-03-15T11:26:07Z,"While building pegasus in debug mode in command `./run.sh build --test -t debug -v -c`, errors are reported as follows:

```
[ 17%] Building CXX object src/nfs/CMakeFiles/dsn_nfs.dir/nfs_node.cpp.o
cd /data/sa_cluster/src/empiredan-pegasus-new/src/debug__builder/src/nfs && /opt/rh/devtoolset-7/root/usr/bin/g++  -DDSN_BUILD_HOSTNAME=10-120-69-237 -DDSN_BUILD_TYPE=Release -DDSN_ENABLE_GPERF -D__STDC_FORMAT_MACROS -D__FILENAME__=\""nfs_node.cpp\"" -I/dat
a/sa_cluster/src/empiredan-pegasus-new/thirdparty/output/include -I/data/sa_cluster/src/empiredan-pegasus-new/src/debug__builder/thrift-gen -I/data/sa_cluster/src/empiredan-pegasus-new/src -I/data/sa_cluster/src/empiredan-pegasus-new/src/common/serializat
ion_helper -I/data/sa_cluster/src/empiredan-pegasus-new/src/include -I/data/sa_cluster/src/empiredan-pegasus-new/src/builder/output/include  -std=c++1y -gdwarf-4 -g   -g -O2 -Wall -Werror -Wno-sign-compare -Wno-strict-aliasing -Wuninitialized -Wno-unused-
variable -Wno-deprecated-declarations -Wno-inconsistent-missing-override -Wno-attributes -fno-omit-frame-pointer -Wno-deprecated-register -Wno-implicit-float-conversion -fPIC -o CMakeFiles/dsn_nfs.dir/nfs_node.cpp.o -c /data/sa_cluster/src/empiredan-pegas
us-new/src/nfs/nfs_node.cpp
In file included from /data/sa_cluster/src/empiredan-pegasus-new/src/utils/alloc.cpp:18:0:
/data/sa_cluster/src/empiredan-pegasus-new/src/utils/alloc.h: In function ‘dsn::cacheline_aligned_ptr<T> dsn::cacheline_aligned_alloc_array(size_t)’:
/data/sa_cluster/src/empiredan-pegasus-new/src/utils/alloc.h:57:26: error: ‘fmt’ has not been declared
                          fmt::ptr(array),
                          ^~~
/data/sa_cluster/src/empiredan-pegasus-new/src/utils/alloc.h:60:26: error: ‘fmt’ has not been declared
                          fmt::ptr(elem),
                          ^~~
/data/sa_cluster/src/empiredan-pegasus-new/src/utils/alloc.h:53:13: error: there are no arguments to ‘CHECK_EQ_MSG’ that depend on a template parameter, so a declaration of ‘CHECK_EQ_MSG’ must be available [-fpermissive]
             CHECK_EQ_MSG((reinterpret_cast<const uintptr_t>(elem) & (sizeof(T) - 1)),
             ^~~~~~~~~~~~
/data/sa_cluster/src/empiredan-pegasus-new/src/utils/alloc.h:53:13: note: (if you use ‘-fpermissive’, G++ will accept your code, but allowing the use of an undeclared name is deprecated)
At global scope:
cc1plus: error: unrecognized command line option ‘-Wno-implicit-float-conversion’ [-Werror]
cc1plus: error: unrecognized command line option ‘-Wno-deprecated-register’ [-Werror]
cc1plus: error: unrecognized command line option ‘-Wno-inconsistent-missing-override’ [-Werror]
cc1plus: all warnings being treated as errors
make[2]: *** [src/utils/CMakeFiles/dsn_utils.dir/alloc.cpp.o] Error 1
make[2]: Leaving directory `/data/sa_cluster/src/empiredan-pegasus-new/src/debug__builder'
make[1]: *** [src/utils/CMakeFiles/dsn_utils.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
```

However, there is not any error while building alloc.h in release mode (for example, in github workflows) by `./run.sh build --test -t release -v -c`.

The reason is that while building `alloc.h` in debug mode, `#ifndef NDEBUG` will be true and triggered, which means `CHECK_EQ_MSG` and `fmt::ptr` will be compiled. However, since `utils/fmt_logging.h` is not included, compilation will fail and report undeclared names. Therefore, `utils/fmt_logging.h` should be included once `#ifndef NDEBUG` is true.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1391/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1394,https://api.github.com/repos/apache/incubator-pegasus/issues/1394,incubator-pegasus,1624794758,1394,ambiguous std::abs when build cpp on ubuntu1604,padmejin,89557328,,,CLOSED,2023-03-15T05:42:32Z,2023-03-15T14:41:09Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
If possible, provide a recipe for reproducing the error.

Run `Build Cpp` on ubuntu1604(docker).

2. What did you expect to see?

Build succeed.

4. What did you see instead?

Build failed with the following error.

```
In file included from /root/incubator-pegasus/src/utils/output_utils.cpp:18:0:
/root/incubator-pegasus/src/utils/output_utils.h: In member function 'std::__cxx11::string dsn::utils::table_printer::to_string(T) [with T = double; std::__cxx11::string = std::__cxx11::basic_string<char>]':
/root/incubator-pegasus/src/utils/output_utils.h:180:22: error: call of overloaded 'abs(double&)' is ambiguous
     if (std::abs(data) < 1e-6) {
                      ^
In file included from /usr/include/c++/5/cstdlib:72:0,
                 from /root/incubator-pegasus/thirdparty/output/include/rapidjson/rapidjson.h:39,
                 from /root/incubator-pegasus/thirdparty/output/include/rapidjson/stream.h:15,
                 from /root/incubator-pegasus/thirdparty/output/include/rapidjson/ostreamwrapper.h:18,
                 from /root/incubator-pegasus/src/utils/output_utils.h:20,
                 from /root/incubator-pegasus/src/utils/output_utils.cpp:18:
```

6. What version of Pegasus are you using?
master","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1394/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1398,https://api.github.com/repos/apache/incubator-pegasus/issues/1398,incubator-pegasus,1625642506,1398,"When the Pegasus cluster enables kerberos authentication, shell tool 'copy_data' will be invalid.",WHBANG,38547944,,,OPEN,2023-03-15T14:29:55Z,2023-03-16T02:27:56Z,"## Bug Report

We use the shell tool `copy_data` to migrate table `table_a` from cluster `clusterA` to table `table_b` of another cluster `clusterB`, as follows
```
>>> use table_a
>>> copy_data -c clusterB -a table_a -n -t 20000000
```
[copy_data](https://pegasus.apache.org/zh/administration/table-migration#copy_data%E8%BF%81%E7%A7%BB)

1. The `principal` of the current c++ client needs to be added to the keytab file used by the target cluster `clusterB`, otherwise the following error will be reported:
![image](https://user-images.githubusercontent.com/38547944/225493966-361e51bc-fc7b-4a03-ada4-70b9681fb1c1.png)

2. Because the conf `service_name` and `service_fqdn` of the two clusters are different, and both conf belong to global variables in the current c++ client process, which makes the client unable to access the two clusters (negotiation will fail too).
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1398/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1400,https://api.github.com/repos/apache/incubator-pegasus/issues/1400,incubator-pegasus,1628613670,1400,Make the build directory standalone,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-03-17T03:11:26Z,2023-03-21T02:22:01Z,"- Now the build directory in in `./src/builder/*`, which is in `src` sub-directory, it's not convenient to distinguish the original source code and the cmake generated code when search some key words.
- When run `run.sh build` to rebuild the project even if the codebase is not change, it will still cost a long time because thrift will re-run to generate files, and will re-generate `git_commit.h` file.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1400/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1403,https://api.github.com/repos/apache/incubator-pegasus/issues/1403,incubator-pegasus,1633301279,1403,Support for FQDN,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,OPEN,2023-03-21T06:25:27Z,2024-04-29T04:00:52Z,"Be like [PR](https://github.com/GehaFearless/incubator-pegasus/commit/f6acfceb0158dd5bdf75abfd47a562bef0d12346)，Pegasus could use FQDN generate the meta_server conf.
Actually it used ip communicate between client and server, also between server and server.

It's on trouble when we need to Deploy Pegasus on somewhere which used Network Address Translation. 
So we should support communicate with FQDN between different role for Pegasus.

That's a huge work. I will split it into multiple PR to submit.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1403/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1403,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5YDnkb,incubator-pegasus,1477343515,1403,NA,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,NA,2023-03-21T06:32:13Z,2023-03-21T06:32:13Z,"As @acelyc111 [test](https://github.com/acelyc111/pegasus/pull/52/files) , the validity of the work is verified.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5YDnkb/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1403,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM53KXfC,incubator-pegasus,1999206338,1403,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-03-15T08:58:06Z,2024-03-15T08:58:06Z,"Steps:
- Add a new host_port field to the thrift structure which has rpc_address field already.
  - The host_port field name has the same name of the related rpc_address field but with `hp_` prefix. This is meaningful to add some macros to operate on the both rpc_address and host_port fields.
    - [x] idl/backup.thrift
    - [x] idl/bulk_load.thrift https://github.com/apache/incubator-pegasus/pull/1949
    - [x] idl/command.thrift
    - [x] idl/dsn.layer2.thrift https://github.com/apache/incubator-pegasus/pull/1977
    - [x] idl/dsn.thrift
    - [x] idl/duplication.thrift https://github.com/apache/incubator-pegasus/pull/1983
    - [x] idl/meta_admin.thrift https://github.com/apache/incubator-pegasus/pull/1984
    - [x] idl/metadata.thrift https://github.com/apache/incubator-pegasus/pull/1986
    - [x] idl/partition_split.thrift https://github.com/apache/incubator-pegasus/pull/1987
    - [x] idl/replica_admin.thrift https://github.com/apache/incubator-pegasus/pull/1988
    - [x] idl/rrdb.thrift
    - [x] idl/security.thrift
    - [x] src/common/consensus.thrift https://github.com/apache/incubator-pegasus/pull/1989
    - [x] src/common/duplication_internal.thrift
    - [x] src/failure_detector/fd.thrift https://github.com/apache/incubator-pegasus/pull/1990
    - [x] src/nfs/nfs.thrift https://github.com/apache/incubator-pegasus/pull/1991
    - [x] src/replica/storage/simple_kv/simple_kv.thrift
    - [x] src/runtime/rpc/request_meta.thrift","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM53KXfC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1403,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM57s3cy,incubator-pegasus,2075359026,1403,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-04-24T16:21:31Z,2024-04-24T16:21:31Z,Need re-check `partition_configuration`,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM57s3cy/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1404,https://api.github.com/repos/apache/incubator-pegasus/issues/1404,incubator-pegasus,1633326844,1404,Add struct host_port & dns_resolver,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2023-03-21T06:56:02Z,2023-06-30T09:18:34Z,"A part of  [Support FQDN](https://github.com/apache/incubator-pegasus/issues/1403).

Implement of struct `rpc_host_port` instead of `rpc_address`. 
Also provide dns_resolver transfer `rpc_host_port` to `rpc_address.`

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1404/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1405,https://api.github.com/repos/apache/incubator-pegasus/issues/1405,incubator-pegasus,1635432692,1405,refactor method 'check_leader',WHBANG,38547944,,,CLOSED,2023-03-22T09:59:43Z,2023-05-16T15:59:12Z,"The return type of method `dsn::replication::meta_service::check_leader(dsn::message_ex *req, dsn::rpc_address *forward_address)` is int, and the return value can only be -1, 0, 1, we hope that its return value is an enumeration type","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1405/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1408,https://api.github.com/repos/apache/incubator-pegasus/issues/1408,incubator-pegasus,1635555720,1408,Chore(github): try to add some contributors into apache collaborators to allow them to use github actions,empiredan,743379,Dan Wang,,CLOSED,2023-03-22T11:13:23Z,2023-03-23T04:37:06Z,"Apache Infrastructure Team has changed default GitHub Actions behavior for outside collaborators to ""always require approval for external contributors"" on March 19th, 2023. Our contributors have found that they have to call maintainers to approve and run Github Actions for their pull requests. Thus we try to add them to try if Github Actions for their pull requests can run automatically without extra approval.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1408/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1410,https://api.github.com/repos/apache/incubator-pegasus/issues/1410,incubator-pegasus,1636746421,1410,method 'batchGetByPartitions' of java client may throw IndexOutOfBoundsException,WHBANG,38547944,,,CLOSED,2023-03-23T02:23:35Z,2023-05-16T15:58:57Z,"
<img width=""1399"" alt=""image"" src=""https://user-images.githubusercontent.com/38547944/227187173-c9c59725-0b5b-4489-9f38-75c07a9e9a94.png"">


When some partition return timeout, the size of `resultMapList` will not be equal to the size of `responseIndex`.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1410/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1412,https://api.github.com/repos/apache/incubator-pegasus/issues/1412,incubator-pegasus,1638682237,1412,Feature(new_metrics): migrate migrate replica-level metrics for pegasus_mutation_duplicator,empiredan,743379,Dan Wang,,CLOSED,2023-03-24T04:03:47Z,2023-07-06T02:12:42Z,"Following metrics are the members of `pegasus_mutation_duplicator` ([pegasus_mutation_duplicator.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/server/pegasus_mutation_duplicator.cpp)), which is created in the call chain `ship_mutation::ship_mutation()` ← `replica_duplicator::start_dup_log()` ← `replica_duplicator_manager::sync_duplication()` ← `replica::replica()` (actually the metrics in `pegasus_mutation_duplicator` of a replica share one metric instance):

| New Metric Name | New Metric Type | Old Perf Counter Name | Old Perf Counter Type | Old Perf Counter Variable | Compute New Metric From Old Perf Counter |
| :-----------------: | :---------------: |  :---------------------: | :---------------------: | :-------------------------: | :------------------------------------------------: |
| _shipped_ops | rate(Counter) |
| _failed_shipping_ops | rate(Counter) |","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1412/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1414,https://api.github.com/repos/apache/incubator-pegasus/issues/1414,incubator-pegasus,1638796585,1414,Feature(new_metrics): add server-level metric entity,empiredan,743379,Dan Wang,,OPEN,2023-03-24T06:21:32Z,2023-03-24T06:21:32Z,"As is described in https://github.com/apache/incubator-pegasus/issues/1328, the server-level entity can just be defined in [metrics.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/utils/metrics.cpp); also, convenient macros are provided to simplify operations for *server* entity.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1414/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1419,https://api.github.com/repos/apache/incubator-pegasus/issues/1419,incubator-pegasus,1642322782,1419,Regularly build failed in recent,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-03-27T15:27:27Z,2023-10-17T16:24:03Z,"In recent days, the regular build work flow failed, see https://github.com/apache/incubator-pegasus/actions/runs/4525900657

It is introduced by commit: e4fee118ca5a5ac48f57e862e6763666be22c1ce","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1419/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1419,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5YoxX3,incubator-pegasus,1487082999,1419,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-03-28T15:13:54Z,2023-03-28T15:13:54Z,"There is another build failure on Ubuntu 16.04, see https://github.com/apache/incubator-pegasus/actions/runs/4540624560/jobs/8001768228

@empiredan ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5YoxX3/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1419,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Yz0xL,incubator-pegasus,1489980491,1419,NA,empiredan,743379,Dan Wang,,NA,2023-03-30T09:24:08Z,2023-03-30T09:24:08Z,"> There is another build failure on Ubuntu 16.04, see https://github.com/apache/incubator-pegasus/actions/runs/4540624560/jobs/8001768228
> 
> @empiredan

OK, I've created an issue to track this problem: https://github.com/apache/incubator-pegasus/issues/1424.

I'll propose a mail to discuss whether to no longer support compilation on ubuntu 1604/gcc 5.4.0.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5Yz0xL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1419,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5cYlHm,incubator-pegasus,1549947366,1419,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-05-16T15:58:39Z,2023-05-16T15:58:39Z,So let's remove the supporting of Ubuntu 1604/gcc 5.4.0 ?,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5cYlHm/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1419,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5pTp81,incubator-pegasus,1766760245,1419,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-10-17T16:24:02Z,2023-10-17T16:24:02Z,"> So let's remove the supporting of Ubuntu 1604/gcc 5.4.0 ?

Done","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5pTp81/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1424,https://api.github.com/repos/apache/incubator-pegasus/issues/1424,incubator-pegasus,1647109218,1424,`std::tuple` is not implicitly convertible by member initializer lists for ubuntu 1604,empiredan,743379,Dan Wang,,CLOSED,2023-03-30T07:58:10Z,2023-07-06T02:12:27Z,"While building unit tests for new metrics on ubuntu 1604, compilation errors are reported as follows:
```
[ 97%] Building CXX object src/utils/test/CMakeFiles/dsn_utils_tests.dir/metrics_test.cpp.o
[ 97%] Building CXX object src/utils/test/CMakeFiles/dsn_utils_tests.dir/nth_element_test.cpp.o
[ 97%] Building CXX object src/utils/test/CMakeFiles/dsn_utils_tests.dir/output_utils_test.cpp.o
/root/incubator-pegasus/src/utils/test/metrics_test.cpp:3053:1: error: converting to 'std::tuple<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, bool, bool, bool, bool>' from initializer list would use explicit constructor 'constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {const char (&)[11], bool, bool, bool, bool}; <template-parameter-2-2> = void; _Elements = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, bool, bool, bool, bool}]'
 };
 ^
/root/incubator-pegasus/src/utils/test/metrics_test.cpp:3053:1: error: converting to 'std::tuple<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, bool, bool, bool, bool>' from initializer list would use explicit constructor 'constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {const char (&)[11], bool, bool, bool, bool}; <template-parameter-2-2> = void; _Elements = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, bool, bool, bool, bool}]'
/root/incubator-pegasus/src/utils/test/metrics_test.cpp:3053:1: error: converting to 'std::tuple<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, bool, bool, bool, bool>' from initializer list would use explicit constructor 'constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {const char (&)[11], bool, bool, bool, bool}; <template-parameter-2-2> = void; _Elements = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, bool, bool, bool, bool}]'
/root/incubator-pegasus/src/utils/test/metrics_test.cpp:3053:1: error: converting to 'std::tuple<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, bool, bool, bool, bool>' from initializer list would use explicit constructor 'constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {const char (&)[11], bool, bool, bool, bool}; <template-parameter-2-2> = void; _Elements = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, bool, bool, bool, bool}]'
/root/incubator-pegasus/src/utils/test/metrics_test.cpp:3053:1: error: converting to 'std::tuple<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, bool, bool, bool, bool>' from initializer list would use explicit constructor 'constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {const char (&)[11], bool, bool, bool, bool}; <template-parameter-2-2> = void; _Elements = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, bool, bool, bool, bool}]'
cc1plus: error: unrecognized command line option '-Wno-implicit-float-conversion' [-Werror]
cc1plus: error: unrecognized command line option '-Wno-deprecated-register' [-Werror]
cc1plus: error: unrecognized command line option '-Wno-inconsistent-missing-override' [-Werror]
cc1plus: error: unrecognized command line option '-Wno-dangling-else' [-Werror]
cc1plus: all warnings being treated as errors
make[2]: *** [src/utils/test/CMakeFiles/dsn_utils_tests.dir/metrics_test.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
src/utils/test/CMakeFiles/dsn_utils_tests.dir/build.make:397: recipe for target 'src/utils/test/CMakeFiles/dsn_utils_tests.dir/metrics_test.cpp.o' failed
[ 98%] Building CXX object src/failure_detector/test/CMakeFiles/dsn.failure_detector.tests.dir/failure_detector.cpp.o
make[1]: *** [src/utils/test/CMakeFiles/dsn_utils_tests.dir/all] Error 2
CMakeFiles/Makefile2:3375: recipe for target 'src/utils/test/CMakeFiles/dsn_utils_tests.dir/all' failed
make[1]: *** Waiting for unfinished jobs....
[ 98%] Building CXX object src/failure_detector/test/CMakeFiles/dsn.failure_detector.tests.dir/main.cpp.o
[ 98%] Linking CXX executable dsn.failure_detector.tests
[ 98%] Built target dsn.failure_detector.tests
make: *** [all] Error 2
Makefile:135: recipe for target 'all' failed
Error: Process completed with exit code 2.
```

The version of gcc used by ubuntu 1604 is:
```
gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.12) 
```

The reason is that `std::tuple` has not been implicitly convertible by member initializer lists for gcc 5.4.0, see [tuple](https://en.cppreference.com/w/cpp/utility/tuple/tuple) and [N4387](https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4387.html).","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1424/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1425,https://api.github.com/repos/apache/incubator-pegasus/issues/1425,incubator-pegasus,1647395848,1425,Feature(new_metrics): add disk-level metric entity and migrate disk-level metrics for `fs_manager`,empiredan,743379,Dan Wang,,CLOSED,2023-03-30T11:05:20Z,2023-07-06T02:12:13Z,"Following metrics are the members of `fs_manager` ([fs_manager.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/common/fs_manager.cpp)), which is created at the construction of `replica_stub`:
| Variables | Types/Computations |
| :-------: | :------------------: |
| _counter_total_capacity_mb | Gauge |
| _counter_total_available_mb | Gauge |
| _counter_total_available_ratio | Gauge |
| _counter_min_available_ratio | Gauge |
| _counter_max_available_ratio | Gauge |","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1425/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1426,https://api.github.com/repos/apache/incubator-pegasus/issues/1426,incubator-pegasus,1648770850,1426,New macro definitions for FQDN,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2023-03-31T06:42:53Z,2023-04-04T12:23:29Z,"relatd to [Support FQDN](https://github.com/apache/incubator-pegasus/issues/1403)

Add some new macro definitions.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1426/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1432,https://api.github.com/repos/apache/incubator-pegasus/issues/1432,incubator-pegasus,1661897504,1432,support default replica and partition,shalk,2435781,shalk(xiao kun),,OPEN,2023-04-11T07:12:50Z,2023-05-16T15:55:33Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

Every time createApp by pegasus client  should specify replica and partition,  this is relate to the deployment of server, sometime is not easy to get.

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->

Inspire by https://cwiki.apache.org/confluence/display/KAFKA/KIP-464%3A+Defaults+for+AdminClient%23createTopic

we can support default replica and partition, make createApp more simple

**Describe alternatives you've considered:**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->

**Teachability, Documentation, Adoption, Migration Strategy:**
<!-- If you can, explain some scenarios how users might use this, situations it would be helpful in. Any API designs, mockups, or diagrams are also helpful. -->
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1432/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1432,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5cYjad,incubator-pegasus,1549940381,1432,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-05-16T15:55:33Z,2023-05-16T15:55:33Z,"@shalk The new API would like this?
```
public void createApp(String appName, long timeoutMs, boolean successIfExist) throws PException
```
And the default replica count and partition count is configured on MetaServer?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5cYjad/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1441,https://api.github.com/repos/apache/incubator-pegasus/issues/1441,incubator-pegasus,1670011504,1441,Feature(new_metrics): migrate replica-level metrics for pegasus_manual_compact_service,empiredan,743379,Dan Wang,,CLOSED,2023-04-16T15:11:40Z,2023-07-06T02:12:03Z,"Following metrics are the members of `pegasus_manual_compact_service` ([pegasus_manual_compact_service.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/server/pegasus_manual_compact_service.cpp)), which is created at the construction of `pegasus_server_impl`:
| Variables | Types/Computations |
| :-------: | :------------------: |
| _pfc_manual_compact_enqueue_count | Gauge |
| _pfc_manual_compact_running_count | Gauge |","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1441/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1446,https://api.github.com/repos/apache/incubator-pegasus/issues/1446,incubator-pegasus,1670652681,1446,Add deprecate to go-client TableConnector.Close() to avoid misleading users ,littlepangdi,41042306,Yandi Lee,,CLOSED,2023-04-17T08:13:18Z,2023-04-18T07:15:38Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
I'd like to propose we add Deprecated to `TableConnector.Close()` for two reasons:

1. Action by `pegasus.Client` indicates that users  don't have to call `TableConnector.Close()`.

We use golang-client to manage pegasus tables in our project(Loki). 
Normally, after create any connection, client should manully call `defer conn.Close()` to avoid any connection leakage  if `Close()` exists.

For example:
```
table, err := c.pegasusDB.GetClient().OpenTable(goCtx, tableName)
if err != nil {
	//close anyway to make sure session released
	if table != nil {
		table.Close()
	}
	errChan <- err
	return
}
defer table.Close()
//do something with table
table.MultiSet(timeoutCtx, []byte(hashKey), entry.SortKeys, entry.Values)

```
However, in golang-client sdk, pegasus.Client takeover this job for `TableConnector` and calls it in `Client.Close()`https://github.com/apache/incubator-pegasus/blob/1beb24a765ba009499a6348f82f1ea95ca2a663d/go-client/pegasus/client.go#L82

2. This method itself indicates `TableConnector.Close()` should not be called by users.

In golang-client sdk, `TableConnector.Close()` will  kill tom and destroy all related goroutine, including `loopForAutoUpdate()`.  This causes disaster in our case, config update will be triggered but has no time to be processed before it's killed. 
Overall, it makes `TableConnector.Close()` a danger operation since if error like server crushed or restarted occurred, we'd hope config be updated other than reopen an old connection to the wrong server.




**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->
Add Deprecated to `TableConnector.Close()`


**Describe alternatives you've considered:**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->
hide `TableConnector.Close()` by changing `Close()` to `close()` since it's in-package call only.


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1446/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1446,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5aD4dZ,incubator-pegasus,1510967129,1446,NA,littlepangdi,41042306,Yandi Lee,,NA,2023-04-17T09:01:56Z,2023-04-17T09:01:56Z,"This issue is also related to #1385 , normally, when replica server is restarted , client should update config and build connect with another server ip.
I suspect that the reason config is not updated successful is that `TableConnector.Close()` is called  and `loopForAutoUpdate()` is interrupted.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5aD4dZ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1449,https://api.github.com/repos/apache/incubator-pegasus/issues/1449,incubator-pegasus,1671195604,1449,Fail to create a table due to ERR_BUSY_CREATING,empiredan,743379,Dan Wang,,CLOSED,2023-04-17T13:35:43Z,2023-05-16T15:48:32Z,"## Problem

While creating table by pegasus shell (pegasus version 2.0), `ERR_BUSY_CREATING` happened frequently as below:

![image](https://user-images.githubusercontent.com/743379/232482325-f4479816-e94a-43eb-a11b-88f1578441af.png)

## Analysis

Pegasus shell requests for creating table by DDL client as follow:

![image](https://user-images.githubusercontent.com/743379/232485119-1f4acaae-60d0-4b9f-be1a-f5f1412aef05.png)

It will attempt at most 3 times:

![image](https://user-images.githubusercontent.com/743379/232485276-53ce55ff-0a16-498f-b956-15a26c0980f0.png)

![image](https://user-images.githubusercontent.com/743379/232483771-ceb86d40-243b-406a-aa4f-d1e85b80dfdb.png)

Without receiving any response from meta server,  DDL client will wait at most 10 seconds, then it will timeout and fail:

![image](https://user-images.githubusercontent.com/743379/232484656-60c8c1d6-e9a6-4c3c-9569-639c7da90d53.png)

![image](https://user-images.githubusercontent.com/743379/232492642-45feaa47-f1e1-43ab-bdf5-91c2a7dc1db9.png)

Based on the above analysis, the process could be described as follows:

1. Pegasus shell launched request for creating a table;
2. However, after 10 seconds the meta server still did not respond with any message since the table had not been created successfully in meta server;
3. DDL client chose to attempt 2nd request for creating a table immediately;
4. Unfortunately, the table had not been created, DDL client received `ERR_BUSY_CREATING` from meta server;
5. DDL client attempted 3rd request and received `ERR_BUSY_CREATING`;
6. DDL client had attempted 3 times, thus failed.

This process could be also observed from logs of meta server:

![image](https://user-images.githubusercontent.com/743379/232498016-2451f7a9-e655-4652-b75e-85c76702187c.png)

![image](https://user-images.githubusercontent.com/743379/232498179-77831c80-ef6d-41da-8d93-898705ac3faa.png)

![image](https://user-images.githubusercontent.com/743379/232499319-331d27d1-2b9d-4cfb-b79a-9311c5c97c1b.png)

![image](https://user-images.githubusercontent.com/743379/232499433-5da8b81c-fc68-4acc-9850-7b8b10d22568.png)

![image](https://user-images.githubusercontent.com/743379/232499783-da343bad-92f1-42ee-b2ad-80073c097e43.png)

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1449/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1450,https://api.github.com/repos/apache/incubator-pegasus/issues/1450,incubator-pegasus,1671537152,1450,Start replica server failed due to incomplete created RocksDB directory,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-04-17T16:12:04Z,2023-05-16T04:21:05Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?

Construct an incomplete RocksDB directory, the column families are not completed before a crash.

2. What did you expect to see?

Replica server could start normally even if the RocksDB directory is incomplete.

3. What did you see instead?

Replica server start failed, the error logs like:
```
I2023-04-17 15:46:15.325 (1681746375325177743 1122982) replica.replica0.0301000000000003: pegasus_server_impl.cpp:1511:start(): [1.3@127.0.0.1:34801] start to open app /home/laiyingchun/data/pegasus/onebox/replica1/data/replica/reps/1.3.pegasus/data
I2023-04-17 15:46:15.325 (1681746375325185037 1122982) replica.replica0.0301000000000003: pegasus_server_impl.cpp:1555:start(): [1.3@127.0.0.1:34801] rdb is already exist, path = /home/laiyingchun/data/pegasus/onebox/replica1/data/replica/reps/1.3.pegasus/data/rdb
I2023-04-17 15:46:15.325 (1681746375325196529 1122982) replica.replica0.0301000000000003: pegasus_server_impl.cpp:1611:start(): [1.3@127.0.0.1:34801] start to open rocksDB's rdb(/home/laiyingchun/data/pegasus/onebox/replica1/data/replica/reps/1.3.pegasus/data/rdb)
F2023-04-17 15:46:15.325 (1681746375325286117 1122982) replica.replica0.0301000000000003: pegasus_server_impl.cpp:1626:start(): assertion expression: !missing_meta_cf
F2023-04-17 15:46:15.325 (1681746375325299873 1122982) replica.replica0.0301000000000003: pegasus_server_impl.cpp:1626:start(): [1.3@127.0.0.1:34801] You must upgrade Pegasus server from 2.0
```

4. What version of Pegasus are you using?

The master branch.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1450/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1450,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5aGpNQ,incubator-pegasus,1511691088,1450,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-04-17T16:18:29Z,2023-04-17T16:18:29Z,"This issuse is reported when I try to fix https://github.com/apache/incubator-pegasus/issues/1383, I injected a write error in the write path of a replica server, the replica server will try to recover (i.e. open new rocksdb instance) replicas after the ""injected corrupted"" replicas automatically closed. If the server crashed when some rocksdb instances are during creating, the instances maybe incomplete, then reproduce this issue.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5aGpNQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1450,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5aNE9w,incubator-pegasus,1513377648,1450,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-04-18T15:36:56Z,2023-04-18T15:36:56Z,"Got another incomplete RocksDB instance and crashed too, logs:
```
I2023-04-18 15:31:05.592 (1681831865592605498 1384110) replica.replica0.030100000000000b: pegasus_server_impl.cpp:1501:start(): [2.1@127.0.0.1:34801] start to open app /home/laiyingchun/data/pegasus/onebox/replica1/data/replica/reps/2.1.pegasus/data
I2023-04-18 15:31:05.592 (1681831865592610397 1384110) replica.replica0.030100000000000b: pegasus_server_impl.cpp:1545:start(): [2.1@127.0.0.1:34801] rdb is already exist, path = /home/laiyingchun/data/pegasus/onebox/replica1/data/replica/reps/2.1.pegasus/data/rdb
I2023-04-18 15:31:05.592 (1681831865592611559 1384110) replica.replica0.030100000000000b: pegasus_server_impl.cpp:1601:start(): [2.1@127.0.0.1:34801] start to open rocksDB's rdb(/home/laiyingchun/data/pegasus/onebox/replica1/data/replica/reps/2.1.pegasus/data/rdb)
E2023-04-18 15:31:05.592 (1681831865592652015 1384110) replica.replica0.030100000000000b: pegasus_server_impl.cpp:3181:check_column_families(): [2.1@127.0.0.1:34801] column family name: default
E2023-04-18 15:31:05.592 (1681831865592656654 1384110) replica.replica0.030100000000000b: pegasus_server_impl.cpp:3181:check_column_families(): [2.1@127.0.0.1:34801] column family name: pegasus_meta_cf
F2023-04-18 15:31:05.608 (1681831865608720349 1384110) replica.replica0.030100000000000b: meta_store.cpp:51:get_last_flushed_decree(): [2.1@127.0.0.1:34801] ERR_OK vs ERR_OBJECT_NOT_FOUND
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5aNE9w/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1454,https://api.github.com/repos/apache/incubator-pegasus/issues/1454,incubator-pegasus,1672478112,1454,Feature(new_metrics): migrate metrics for replica_stub,empiredan,743379,Dan Wang,,CLOSED,2023-04-18T07:00:13Z,2023-07-06T02:11:45Z,"Following metrics are the members of `replica_stub` ([replica_stub.cpp](https://github.com/apache/incubator-pegasus/blob/master/src/replica/replica_stub.cpp)), which is created at the construction of `replication_service_app`:
| Variables | Types/Computations |
| :-------: | :------------------: |
| _counter_replicas_count | Gauge |
| _counter_replicas_opening_count | Gauge |
| _counter_replicas_closing_count | Gauge |
| _counter_replicas_commit_qps | rate(Counter) |
| _counter_replicas_learning_count | Gauge |
| _counter_replicas_learning_max_duration_time_ms | Gauge |
| _counter_replicas_learning_max_copy_file_size | Gauge |
| _counter_replicas_learning_recent_start_count | increase(Counter) |
| _counter_replicas_learning_recent_round_start_count | increase(Counter) |
| _counter_replicas_learning_recent_copy_file_count | increase(Counter) |
| _counter_replicas_learning_recent_copy_file_size | increase(Counter) |
| _counter_replicas_learning_recent_copy_buffer_size | increase(Counter) |
| _counter_replicas_learning_recent_learn_cache_count | increase(Counter) |
| _counter_replicas_learning_recent_learn_app_count | increase(Counter) |
| _counter_replicas_learning_recent_learn_log_count | increase(Counter) |
| _counter_replicas_learning_recent_learn_reset_count | increase(Counter) |
| _counter_replicas_learning_recent_learn_fail_count | increase(Counter) |
| _counter_replicas_learning_recent_learn_succ_count | increase(Counter) |
| _counter_replicas_recent_prepare_fail_count | increase(Counter) |
| _counter_replicas_recent_replica_move_error_count | increase(Counter) |
| _counter_replicas_recent_replica_move_garbage_count | increase(Counter) |
| _counter_replicas_recent_replica_remove_dir_count | increase(Counter) |
| _counter_replicas_error_replica_dir_count | Gauge |
| _counter_replicas_garbage_replica_dir_count | Gauge |
| _counter_replicas_tmp_replica_dir_count | Gauge |
| _counter_replicas_origin_replica_dir_count | Gauge |
| _counter_replicas_recent_group_check_fail_count | increase(Counter) |
| _counter_shared_log_size | Gauge |
| _counter_shared_log_recent_write_size |increase(Counter) |
| _counter_recent_trigger_emergency_checkpoint_count | increase(Counter) |
| _counter_dup_confirmed_rate | rate(Counter) |
| _counter_dup_pending_mutations_count | Gauge |
| _counter_cold_backup_running_count | Gauge |
| _counter_cold_backup_recent_start_count | increase(Counter) |
| _counter_cold_backup_recent_succ_count | increase(Counter) |
| _counter_cold_backup_recent_fail_count | increase(Counter) |
| _counter_cold_backup_recent_cancel_count | increase(Counter) |
| _counter_cold_backup_recent_pause_count | increase(Counter) |
| _counter_cold_backup_recent_upload_file_succ_count | increase(Counter) |
| _counter_cold_backup_recent_upload_file_fail_count | increase(Counter) |
| _counter_cold_backup_recent_upload_file_size | increase(Counter) |
| _counter_cold_backup_max_duration_time_ms | Gauge |
| _counter_cold_backup_max_upload_file_size | Gauge |
| _counter_recent_read_fail_count | increase(Counter) |
| _counter_recent_write_fail_count | increase(Counter) |
| _counter_recent_read_busy_count | increase(Counter) |
| _counter_recent_write_busy_count | increase(Counter) |
| _counter_recent_write_size_exceed_threshold_count | increase(Counter) |
| _counter_bulk_load_running_count | Gauge |
| _counter_bulk_load_downloading_count | Gauge |
| _counter_bulk_load_ingestion_count | Gauge |
| _counter_bulk_load_succeed_count | Gauge |
| _counter_bulk_load_failed_count | Gauge |
| _counter_bulk_load_download_file_succ_count | Gauge |
| _counter_bulk_load_download_file_fail_count | Gauge |
| _counter_bulk_load_download_file_size | Gauge |
| _counter_bulk_load_max_ingestion_time_ms | Gauge |
| _counter_bulk_load_max_duration_time_ms | Gauge |
| _counter_tcmalloc_release_memory_size | Gauge |
| _counter_replicas_splitting_count | Gauge |
| _counter_replicas_splitting_max_duration_time_ms | Gauge |
| _counter_replicas_splitting_max_async_learn_time_ms | Gauge |
| _counter_replicas_splitting_max_copy_file_size | Gauge |
| _counter_replicas_splitting_recent_start_count | increase(Counter) |
| _counter_replicas_splitting_recent_copy_file_count | increase(Counter) |
| _counter_replicas_splitting_recent_copy_file_size | increase(Counter) |
| _counter_replicas_splitting_recent_copy_mutation_count | increase(Counter) |
| _counter_replicas_splitting_recent_split_succ_count | increase(Counter) |
| _counter_replicas_splitting_recent_split_fail_count | increase(Counter) |","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1454/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1467,https://api.github.com/repos/apache/incubator-pegasus/issues/1467,incubator-pegasus,1698440533,1467,garble words of time filed displayed in logs,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-05-06T04:35:12Z,2023-05-08T03:58:36Z,"Pegasus version: 2.4.0

I saw some garble words in the logs, like:
![image](https://user-images.githubusercontent.com/10775040/236602083-3d34d4b9-78b7-4538-81a5-1cbb046f459f.png)

It seems because the log buffer dosen't initialize as zero:
```
char str[24];
dsn::utils::time_ms_to_string(ts / 1000000, str);
```

The master branch has fixed this bug, but some other callers still have such bug.

Ref:

> Formats args according to specifications in fmt, writes the result to the output iterator out and returns the iterator past the end of the output range. [format_to()](https://fmt.dev/latest/api.html#_CPPv4I0DpEN3fmt9format_toE8OutputIt8OutputIt13format_stringIDp1TEDpRR1T) does not append a terminating null character.

 https://fmt.dev/latest/api.html","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1467/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1470,https://api.github.com/repos/apache/incubator-pegasus/issues/1470,incubator-pegasus,1703358317,1470,pegasus java client add listApps interface,WHBANG,38547944,,,CLOSED,2023-05-10T08:13:15Z,2023-05-27T09:33:37Z,Add an interface to the java client to get all table information in the pegasus cluster.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1470/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1479,https://api.github.com/repos/apache/incubator-pegasus/issues/1479,incubator-pegasus,1713967171,1479,Bug:some replica will never do manual compact when set a trigger time before zero o'clock,ninsmiracle,110282526,,,OPEN,2023-05-17T13:51:04Z,2023-05-17T13:51:04Z,"## Bug Report

In version 2.3.4 and 2.4.1.
We have some online cluster need to do  bulk load every day. We set a manual compact periodic trigger time like 23:50 before zero o'clock.
In our expectations,manual compact will run one by one replicas. However,some replica never do manual compact.
It will make a pegasus app's disk usage increase infinitly. 
![image](https://github.com/apache/incubator-pegasus/assets/110282526/7c6442bf-a2f2-42a5-a358-6eec77ca0ffc)


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1479/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1481,https://api.github.com/repos/apache/incubator-pegasus/issues/1481,incubator-pegasus,1715155306,1481,Feature(new_metrics): migrate metrics for some duplication class,empiredan,743379,Dan Wang,,CLOSED,2023-05-18T07:24:28Z,2023-07-06T02:11:34Z,"Migrate some duplication-related classes, including `ship_mutation`, `mutation_buffer`, `load_from_private_log`, etc.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1481/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1483,https://api.github.com/repos/apache/incubator-pegasus/issues/1483,incubator-pegasus,1718951100,1483,Feature(new_metrics): migrate metrics for task queue,empiredan,743379,Dan Wang,,CLOSED,2023-05-22T06:00:38Z,2023-06-30T09:21:39Z,Migrate all metrics of task queues to the new framework.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1483/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1485,https://api.github.com/repos/apache/incubator-pegasus/issues/1485,incubator-pegasus,1725056393,1485,Github actions run out of disk space while building releases,empiredan,743379,Dan Wang,,CLOSED,2023-05-25T04:14:59Z,2023-05-26T11:56:38Z,"While building releases for [cpp](https://github.com/apache/incubator-pegasus/actions/workflows/lint_and_test_cpp.yaml) for the branch [migrate-metrics-dev](https://github.com/apache/incubator-pegasus/tree/migrate-metrics-dev) by Github Actions, following errors are reported as:

```
You are running out of disk space. The runner will stop working when the machine runs out of disk space. Free space left: 0 MB
```

```
System.IO.IOException: No space left on device : '/home/runner/runners/2.304.0/_diag/Worker_20230523-041937-utc.log'
   at System.IO.RandomAccess.WriteAtOffset(SafeFileHandle handle, ReadOnlySpan`1 buffer, Int64 fileOffset)
   at System.IO.Strategies.BufferedFileStreamStrategy.FlushWrite()
   at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder)
   at System.Diagnostics.TextWriterTraceListener.Flush()
   at GitHub.Runner.Common.HostTraceListener.WriteHeader(String source, TraceEventType eventType, Int32 id)
   at GitHub.Runner.Common.HostTraceListener.TraceEvent(TraceEventCache eventCache, String source, TraceEventType eventType, Int32 id, String message)
   at System.Diagnostics.TraceSource.TraceEvent(TraceEventType eventType, Int32 id, String message)
   at GitHub.Runner.Worker.Worker.RunAsync(String pipeIn, String pipeOut)
   at GitHub.Runner.Worker.Program.MainAsync(IHostContext context, String[] args)
System.IO.IOException: No space left on device : '/home/runner/runners/2.304.0/_diag/Worker_20230523-041937-utc.log'
   at System.IO.RandomAccess.WriteAtOffset(SafeFileHandle handle, ReadOnlySpan`1 buffer, Int64 fileOffset)
   at System.IO.Strategies.BufferedFileStreamStrategy.FlushWrite()
   at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder)
   at System.Diagnostics.TextWriterTraceListener.Flush()
   at GitHub.Runner.Common.HostTraceListener.WriteHeader(String source, TraceEventType eventType, Int32 id)
   at GitHub.Runner.Common.HostTraceListener.TraceEvent(TraceEventCache eventCache, String source, TraceEventType eventType, Int32 id, String message)
   at System.Diagnostics.TraceSource.TraceEvent(TraceEventType eventType, Int32 id, String message)
   at GitHub.Runner.Common.Tracing.Error(Exception exception)
   at GitHub.Runner.Worker.Program.MainAsync(IHostContext context, String[] args)
Unhandled exception. System.IO.IOException: No space left on device : '/home/runner/runners/2.304.0/_diag/Worker_20230523-041937-utc.log'
   at System.IO.RandomAccess.WriteAtOffset(SafeFileHandle handle, ReadOnlySpan`1 buffer, Int64 fileOffset)
   at System.IO.Strategies.BufferedFileStreamStrategy.FlushWrite()
   at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder)
   at System.Diagnostics.TextWriterTraceListener.Flush()
   at System.Diagnostics.TraceSource.Flush()
   at GitHub.Runner.Common.TraceManager.Dispose(Boolean disposing)
   at GitHub.Runner.Common.TraceManager.Dispose()
   at GitHub.Runner.Common.HostContext.Dispose(Boolean disposing)
   at GitHub.Runner.Common.HostContext.Dispose()
   at GitHub.Runner.Worker.Program.Main(String[] args)
```

The failed jobs include `Build Release` and `Build with jemalloc`:
![image](https://github.com/apache/incubator-pegasus/assets/743379/2a58c7c5-f59f-4f35-aa99-93d7a248d3b6)

Both failed jobs would pack servers and tools, while another successful job `Build ASAN` would never do. Both jobs will generate following directories and tar packages, which consume 2.7GB disk space:
![image](https://github.com/apache/incubator-pegasus/assets/743379/c08a8189-e636-46d8-b3af-42bd068d769b)
 
Though this problem has not been found for [master](https://github.com/apache/incubator-pegasus) branch, we could also drop the generated directories and tar packages before this problem occurs.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1485/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1488,https://api.github.com/repos/apache/incubator-pegasus/issues/1488,incubator-pegasus,1725716998,1488,Feature:Online Query and Dynamic Modification of Table-level RocksDB Options,ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,OPEN,2023-05-25T12:20:18Z,2023-06-14T08:39:44Z,"### Background
Currently, in the Pegasus 2.4.1 version, dynamic querying and modification of RocksDB options at the table level are not supported. While the Pegasus shell provides commands like `set_app_envs `and `create` for configuring certain table-level options, it does not include RocksDB options. Consequently, adding the  configure table-level RocksDB options ability would enhance the app configuration flexibility in Pegasus.
### Goals
This issue aims to achieve the following goals:
1. Provide online modification functionality for table-level RocksDB configuration.
2. Provide online query functionality for table-level RocksDB configuration.
3. Research on the principles, interdependencies, and modification risks of the rocksdb options used in Pegasus with rocksdb-6.6.4 version, and compile a comprehensive documentation on the usage of table-level rocksdb options.
4. By modifying the logic of the Usage Scenario feature, unify the approach for setting table-level RocksDB options.
### Solution
**Here are the solutions for goals 1 and 2.**
The storage location of rocksdb option is as follows:
1. Utilize the config.ini file to provide default global RocksDB options information.
2.  Store table-specific RocksDB options in the envs section of the app_info in ZooKeeper (zk).

Three scenarios to set specific rocksdb option, and the solution as follows：
1. [Create a replica with specific rocksdb option]
Set specify option by `create appname -e rocksdb.options ` command
If a replica is not created, use the app envs rocksdb option from metaserver to create. For a  already created replica, because the app envs is not transmitted from the metaserver, use options from the RocksDB OPTION file as the reference.

2. [Modify a specific rocksdb option of an online replica]
 Set specify option by `set_app_envs rocksdb.options` command

3. [Query the modified option online]
Get  specify option by `get_app_envs` command

To validate the feasibility of the proposed solution, I will use rocksdb.write_buffer_size as a dynamically modifiable parameter and rocksdb.num_levels as a non-dynamically modifiable parameter to test my idea.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1488/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1488,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5e0Mj2,incubator-pegasus,1590741238,1488,NA,ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,NA,2023-06-14T08:38:34Z,2023-06-14T08:38:34Z,"This is the app creation process, I show the app envs passing through the num_levels option.

![App创建流程 (2)](https://github.com/apache/incubator-pegasus/assets/93246280/c1abed5a-1211-42de-b726-918c522e8998)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5e0Mj2/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1488,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5e0M2d,incubator-pegasus,1590742429,1488,NA,ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,NA,2023-06-14T08:39:24Z,2023-06-14T08:39:24Z,"![replica具体创建过程 (2)](https://github.com/apache/incubator-pegasus/assets/93246280/01a542e0-8359-4891-9c6e-44fbe4fe6c01)

This is the specific creation process of replica, including my two modifications. Respectively deal with creating a new rocksdb and restarting rocksdb.
### Create a replica key process：
1. `new replica` ： Create a replica, which contains env information num_levels=5, and the actual rocksdb has not been created
2. `store_app_info(_app_info)` ： Persist app_info information to .app_info file
3. `_app.reset(replication_app_base::new_storage_instance(_app_info.app_type, this));` ：Construct pegasus_server_impl. Parameter this pointer is replica*, which contains env information num_levels=5.
4. `pegasus_server_impl::pegasus_server_impl `：Construct pegasus_server_impl. Initialize rocksdb option via FLAGS_rocksdb_*
5. `update_app_envs_before_open_db(envs);` ：Make the env information effective before creating the rocksdb instance.
6. `reset_usage_scenario_options` ： Load rocksdb option from OPTION-XXXXX file.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5e0M2d/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1488,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5e0M-Z,incubator-pegasus,1590742937,1488,NA,ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,NA,2023-06-14T08:39:44Z,2023-06-14T08:39:44Z,"I show the set_app_envs procedure using the write_buffer_size option. My third modification solves the online modification of dynamic rocksdb options.
![app envupdate (2)](https://github.com/apache/incubator-pegasus/assets/93246280/54436e90-5a7f-4edb-9970-3a01cdbf24b2)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5e0M-Z/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1490,https://api.github.com/repos/apache/incubator-pegasus/issues/1490,incubator-pegasus,1727162059,1490,"Units showed ""Unknown"" for some metrics of new framework",empiredan,743379,Dan Wang,,CLOSED,2023-05-26T08:35:39Z,2023-06-30T09:17:57Z,"While querying the metrics of new framework, some metric units showed ""Unknown"" as follows:
```json
    {
        ""type"": ""replica"",
        ""id"": ""replica_6.1"",
        ""attributes"": {
            ""partition_id"": ""1"",
            ""table_id"": ""6""
        },
        ""metrics"": [
            {
                ""type"": ""counter"",
                ""name"": ""corrupt_writes"",
                ""unit"": ""requests"",
                ""desc"": ""The number of corrupt writes for each replica"",
                ""value"": 0
            },
            {
                ""type"": ""counter"",
                ""name"": ""backup_request_bytes"",
                ""unit"": ""Unknown"",
                ""desc"": ""The number of bytes for backup requests"",
                ""value"": 0
            },
            {
                ""type"": ""counter"",
                ""name"": ""check_and_mutate_bytes"",
                ""unit"": ""Unknown"",
                ""desc"": ""The number of bytes for CHECK_AND_MUTATE requests"",
                ""value"": 0
            },
            {
                ""type"": ""counter"",
                ""name"": ""dup_lagging_writes"",
                ""unit"": ""requests"",
                ""desc"": ""the number of lagging writes (time lag larger than `dup_lagging_write_threshold_ms`)"",
                ""value"": 0
            },
            {
                ""type"": ""counter"",
                ""name"": ""check_and_set_bytes"",
                ""unit"": ""Unknown"",
                ""desc"": ""The number of bytes for CHECK_AND_SET requests"",
                ""value"": 0
            },
            {
                ""type"": ""percentile"",
                ""name"": ""dup_time_lag_ms"",
                ""unit"": ""milliseconds"",
                ""desc"": ""the time lag (in ms) between master and slave in the duplication"",
                ""p50"": 0,
                ""p90"": 0,
                ""p95"": 0,
                ""p99"": 0,
                ""p999"": 0
            },
            {
                ""type"": ""counter"",
                ""name"": ""multi_put_bytes"",
                ""unit"": ""Unknown"",
                ""desc"": ""The number of bytes for MULTI_PUT requests"",
                ""value"": 0
            },
            ...
        ],
        ...
    }
```

The reason is that `enum_to_string` are forgotten to be added for some metric units.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1490/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1492,https://api.github.com/repos/apache/incubator-pegasus/issues/1492,incubator-pegasus,1727375606,1492,Support for Bitmap like Redis Bitmap,shijiaoming,24383803,,,OPEN,2023-05-26T10:49:17Z,2023-05-27T09:32:59Z,"## Feature Request

Our usage scenario is to filter `user_id` in   [Ad_exchange](https://en.wikipedia.org/wiki/Ad_exchange)

**Describe the feature you'd like:**

1. Create a new bloom filter by adding a new item
2. Find out whether an item exists in the filter
3. Adding and checking multiple items

**Teachability, Documentation, Adoption, Migration Strategy:**

[RedisBloom](https://github.com/RedisBloom/RedisBloom)
[RedisBloom Quick Start](https://redis.io/docs/stack/bloom/quick_start/)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1492/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1492,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5dTKuc,incubator-pegasus,1565305756,1492,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-05-27T09:32:59Z,2023-05-27T09:32:59Z,"@shijiaoming Thanks for the report!

We'll consider to support this feature.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5dTKuc/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1493,https://api.github.com/repos/apache/incubator-pegasus/issues/1493,incubator-pegasus,1728741148,1493,Start onebox failed,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-05-27T13:58:46Z,2023-06-02T09:15:19Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
Start onebox after Pegasus has been built.

2. What did you expect to see?
The onebox start failed, the error message is:
```
/home/laiyingchun/data/pegasus_2.4/onebox/meta1/pegasus_server: error while loading shared libraries: libjvm.so: cannot open shared object file: No such file or directory
```

3. What did you see instead?
The cluster should start normally.

4. What version of Pegasus are you using?
Branch v2.4/master
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1493/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1495,https://api.github.com/repos/apache/incubator-pegasus/issues/1495,incubator-pegasus,1730157595,1495,feat(FQDN): serialization format rpc_host_port ,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2023-05-29T06:24:01Z,2023-06-09T08:46:03Z,"about https://github.com/apache/incubator-pegasus/issues/1404

Implement thrift protocol functon `read` and `write` for class `rpc_host_port`.
Both about binary protocol and json.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1495/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1497,https://api.github.com/repos/apache/incubator-pegasus/issues/1497,incubator-pegasus,1730659635,1497,Github actions run out of disk space while building ASAN ,empiredan,743379,Dan Wang,,CLOSED,2023-05-29T12:12:36Z,2023-06-02T09:15:10Z,"Previously `Build Release` and `Build with jemalloc` failed due to running out of disk space (see https://github.com/apache/incubator-pegasus/issues/1485). Recently, `Build ASAN` also failed due to the same reason (Unhandled exception. System.IO.IOException: No space left on device):

![image](https://github.com/apache/incubator-pegasus/assets/743379/7ef98720-74fc-46e0-a28e-4a45e37f4292)

This means we have to spare more space. And actually `CMakeFiles` occupied much disk space. By running `find ./build/latest/src/ -name '*CMakeFiles*' -type d -exec du -csh ""{}"" +` we found that typically it could consume 3.4GB:
```
8.0K	./build/latest/src/CMakeFiles
25M	./build/latest/src/aio/CMakeFiles
9.4M	./build/latest/src/aio/test/CMakeFiles
19M	./build/latest/src/base/CMakeFiles
3.6M	./build/latest/src/base/test/CMakeFiles
9.4M	./build/latest/src/block_service/CMakeFiles
9.1M	./build/latest/src/block_service/fds/CMakeFiles
11M	./build/latest/src/block_service/hdfs/CMakeFiles
11M	./build/latest/src/block_service/local/CMakeFiles
35M	./build/latest/src/block_service/test/CMakeFiles
45M	./build/latest/src/client/CMakeFiles
6.9M	./build/latest/src/client/test/CMakeFiles
29M	./build/latest/src/client_lib/CMakeFiles
80M	./build/latest/src/common/CMakeFiles
68M	./build/latest/src/common/test/CMakeFiles
16M	./build/latest/src/failure_detector/CMakeFiles
16M	./build/latest/src/failure_detector/test/CMakeFiles
8.0K	./build/latest/src/geo/CMakeFiles
7.6M	./build/latest/src/geo/lib/CMakeFiles
12M	./build/latest/src/geo/test/CMakeFiles
2.4M	./build/latest/src/geo/bench/CMakeFiles
26M	./build/latest/src/http/CMakeFiles
11M	./build/latest/src/http/test/CMakeFiles
306M	./build/latest/src/meta/CMakeFiles
294M	./build/latest/src/meta/test/CMakeFiles
19M	./build/latest/src/meta/test/balancer_simulator/CMakeFiles
8.9M	./build/latest/src/meta/test/meta_state/CMakeFiles
45M	./build/latest/src/nfs/CMakeFiles
6.9M	./build/latest/src/nfs/test/CMakeFiles
17M	./build/latest/src/perf_counter/CMakeFiles
12M	./build/latest/src/perf_counter/test/CMakeFiles
8.0K	./build/latest/src/redis_protocol/CMakeFiles
7.4M	./build/latest/src/redis_protocol/proxy/CMakeFiles
17M	./build/latest/src/redis_protocol/proxy_lib/CMakeFiles
8.0M	./build/latest/src/redis_protocol/proxy_ut/CMakeFiles
6.1M	./build/latest/src/remote_cmd/CMakeFiles
411M	./build/latest/src/replica/CMakeFiles
90M	./build/latest/src/replica/duplication/test/CMakeFiles
13M	./build/latest/src/replica/backup/test/CMakeFiles
14M	./build/latest/src/replica/bulk_load/test/CMakeFiles
15M	./build/latest/src/replica/split/test/CMakeFiles
8.0K	./build/latest/src/replica/storage/CMakeFiles
22M	./build/latest/src/replica/storage/simple_kv/CMakeFiles
74M	./build/latest/src/replica/storage/simple_kv/test/CMakeFiles
123M	./build/latest/src/replica/test/CMakeFiles
11M	./build/latest/src/reporter/CMakeFiles
113M	./build/latest/src/runtime/CMakeFiles
131M	./build/latest/src/runtime/test/CMakeFiles
78M	./build/latest/src/runtime/rpc/CMakeFiles
73M	./build/latest/src/runtime/task/CMakeFiles
57M	./build/latest/src/runtime/security/CMakeFiles
19M	./build/latest/src/runtime/ranger/CMakeFiles
344K	./build/latest/src/sample/CMakeFiles
213M	./build/latest/src/server/CMakeFiles
345M	./build/latest/src/server/test/CMakeFiles
105M	./build/latest/src/shell/CMakeFiles
2.9M	./build/latest/src/test_util/CMakeFiles
4.6M	./build/latest/src/test/bench_test/CMakeFiles
8.0K	./build/latest/src/test/function_test/CMakeFiles
13M	./build/latest/src/test/function_test/utils/CMakeFiles
7.5M	./build/latest/src/test/function_test/backup_restore_test/CMakeFiles
64M	./build/latest/src/test/function_test/base_api_test/CMakeFiles
5.6M	./build/latest/src/test/function_test/bulk_load_test/CMakeFiles
3.5M	./build/latest/src/test/function_test/detect_hotspot_test/CMakeFiles
6.0M	./build/latest/src/test/function_test/partition_split_test/CMakeFiles
5.3M	./build/latest/src/test/function_test/recovery_test/CMakeFiles
8.1M	./build/latest/src/test/function_test/restore_test/CMakeFiles
6.4M	./build/latest/src/test/function_test/throttle_test/CMakeFiles
24M	./build/latest/src/test/kill_test/CMakeFiles
4.7M	./build/latest/src/test/pressure_test/CMakeFiles
6.3M	./build/latest/src/tools/CMakeFiles
87M	./build/latest/src/utils/CMakeFiles
1.3M	./build/latest/src/utils/long_adder_bench/CMakeFiles
94M	./build/latest/src/utils/test/CMakeFiles
5.4M	./build/latest/src/utils/test/nth_element_bench/CMakeFiles
16M	./build/latest/src/zookeeper/CMakeFiles
7.9M	./build/latest/src/zookeeper/test/CMakeFiles
3.4G	total
```

Therefore we could drop `CMakeFiles` directories to spare more disk space.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1497/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1501,https://api.github.com/repos/apache/incubator-pegasus/issues/1501,incubator-pegasus,1734665769,1501,Feature(new_metrics): migrate metrics for failure detector,empiredan,743379,Dan Wang,,CLOSED,2023-05-31T17:22:48Z,2023-06-30T09:17:13Z,Migrate all metrics of failure detector to the new framework.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1501/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1503,https://api.github.com/repos/apache/incubator-pegasus/issues/1503,incubator-pegasus,1735550518,1503,Feature(new_metrics): migrate metrics for network,empiredan,743379,Dan Wang,,CLOSED,2023-06-01T06:11:39Z,2023-06-30T09:16:58Z,Migrate all metrics of network to the new framework.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1503/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1505,https://api.github.com/repos/apache/incubator-pegasus/issues/1505,incubator-pegasus,1737427415,1505,Feature(new_metrics): migrate server-level metrics of rocksdb,empiredan,743379,Dan Wang,,CLOSED,2023-06-02T04:04:08Z,2023-06-30T09:17:27Z,Migrate server-level metrics of rocksdb to the new framework.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1505/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1508,https://api.github.com/repos/apache/incubator-pegasus/issues/1508,incubator-pegasus,1737848658,1508,Refactor: remove counter_info and counter_info_ptr,empiredan,743379,Dan Wang,,CLOSED,2023-06-02T09:46:20Z,2023-06-30T09:17:06Z,"Since we've removed profiler-related remote commands in https://github.com/XiaoMi/rdsn/pull/810, `counter_info` and `counter_info_ptr` have no longer been used since then. Thus could remove both of them.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1508/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1513,https://api.github.com/repos/apache/incubator-pegasus/issues/1513,incubator-pegasus,1740790009,1513,fix install zk in branch v2.4,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2023-06-05T02:48:51Z,2023-06-05T08:00:36Z,"<img width=""505"" alt=""image"" src=""https://github.com/apache/incubator-pegasus/assets/48315319/da273263-4d60-4767-a33e-7a94ab4e4891"">","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1513/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1513,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5d8mwm,incubator-pegasus,1576168486,1513,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-06-05T07:07:39Z,2023-06-05T07:07:39Z,"error details:
![image](https://github.com/apache/incubator-pegasus/assets/10775040/a6327ab7-3c12-4f18-8b34-b33c97a47bbe)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5d8mwm/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1515,https://api.github.com/repos/apache/incubator-pegasus/issues/1515,incubator-pegasus,1741054611,1515,chore: update rocksdb install url on thirdparty,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2023-06-05T06:14:25Z,2023-06-08T03:57:00Z,change url on our oss to speed up the download,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1515/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1520,https://api.github.com/repos/apache/incubator-pegasus/issues/1520,incubator-pegasus,1747036565,1520,fix invaild build type,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2023-06-08T03:55:56Z,2023-06-30T09:16:50Z,"`./run.sh `build_type not working.

I found always build release server even though I use param `-t debug.`

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1520/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1523,https://api.github.com/repos/apache/incubator-pegasus/issues/1523,incubator-pegasus,1749629521,1523,Feature(new_metrics): migrate metrics for profiler,empiredan,743379,Dan Wang,,CLOSED,2023-06-09T10:33:26Z,2023-06-30T09:16:43Z,Migrate all metrics of profiler to the new framework.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1523/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1525,https://api.github.com/repos/apache/incubator-pegasus/issues/1525,incubator-pegasus,1751783298,1525,Bug:command ./run.sh test is not working,ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,CLOSED,2023-06-12T02:43:50Z,2023-06-30T09:16:22Z,"## Bug Report
I want to run all test modules, but I found that the command `./run.sh test` is not working.

1. What did you do?
```
git checkout  86181aac93de065dddb18589fab22dfa3a4f5ead
./run.sh build -c --test
./run.sh test
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1525/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1525,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5ekX2J,incubator-pegasus,1586593161,1525,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-06-12T05:13:48Z,2023-06-12T05:13:48Z,"What does ""not working"" mean? Is the test running, what did you see and what you expect to see?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5ekX2J/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1525,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5ekanU,incubator-pegasus,1586604500,1525,NA,ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,NA,2023-06-12T05:23:59Z,2023-06-12T05:23:59Z,"> What does ""not working"" mean? Is the test running, what did you see and what you expect to see?
I expect `./run.sh test` to execute every test normally. But it doesn't work properly. Next is the error I see. 

```shell
root@ubuntu2004:~/pegasus/incubator-pegasus# ./run.sh test
Test start time: Mon 12 Jun 2023 05:16:03 AM UTC
test_modules=backup_restore_test,base_api_test,base_test,bulk_load_test,detect_hotspot_test,dsn_aio_test,dsn_block_service_test,dsn_client_test,dsn.failure_detector.tests,dsn_http_test,dsn_meta_state_tests,dsn.meta.test,dsn_nfs_test,dsn_perf_counter_test,dsn_replica_backup_test,dsn_replica_bulk_load_test,dsn_replica_dup_test,dsn_replica_split_test,dsn.replica.test,dsn_replication_common_test,dsn.replication.simple_kv,dsn.rep_tests.simple_kv,dsn_runtime_tests,dsn_utils_tests,dsn.zookeeper.tests,partition_split_test,pegasus_geo_test,pegasus_rproxy_test,pegasus_unit_test,recovery_test,restore_test,throttle_test
====================== run backup_restore_test ==========================
ZooKeeper JMX enabled by default
Using config: /root/pegasus/incubator-pegasus/.zk_install/zookeeper-bin/bin/../conf/zoo.cfg
Stopping zookeeper ... STOPPED
TARGET_PATH zookeeper-bin has existed, thus do not try to download apache-zookeeper-3.7.0-bin
ZooKeeper JMX enabled by default
Using config: /root/pegasus/incubator-pegasus/.zk_install/zookeeper-bin/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
Zookeeper started at port 22181
~/pegasus/incubator-pegasus/build/latest/bin/backup_restore_test ~/pegasus/incubator-pegasus
[==========] Running 2 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 2 tests from backup_restore_test
W2023-06-12 05:16:07.845 (1686546967845285840 25727) unknown.io-thrd.25727: rdsn engine already started, ignore the config file 'config.ini'
[ RUN      ] backup_restore_test.test_backup_and_restore
E2023-06-12 05:16:07.847 (1686546967847099144 25734)  mimic.io-thrd.25734: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.847 (1686546967847164028 25734)  mimic.io-thrd.25734: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.847 (1686546967847439718 25734)  mimic.io-thrd.25734: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.847 (1686546967847490742 25734)  mimic.io-thrd.25734: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.847 (1686546967847829809 25733)  mimic.io-thrd.25733: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.847 (1686546967847879616 25733)  mimic.io-thrd.25733: asio socket shutdown failed, error = Transport endpoint is not connected
create app test_app failed: [create] call server error: ERR_NETWORK_FAILURE
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:202: Failure
      Expected: ERR_OK
To be equal to: err
      Which is: ERR_NETWORK_FAILURE
E2023-06-12 05:16:07.848 (1686546967848299581 25731)  mimic.io-thrd.25731: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.848 (1686546967848350341 25731)  mimic.io-thrd.25731: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.848 (1686546967848489295 25731)  mimic.io-thrd.25731: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.848 (1686546967848527529 25731)  mimic.io-thrd.25731: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.848 (1686546967848656928 25734)  mimic.io-thrd.25734: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.848 (1686546967848710685 25734)  mimic.io-thrd.25734: asio socket shutdown failed, error = Transport endpoint is not connected
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:75: Failure
      Expected: ERR_OK
To be equal to: _ddl_client->drop_app(_old_app_name, 0)
      Which is: ERR_NETWORK_FAILURE
[  FAILED  ] backup_restore_test.test_backup_and_restore (4 ms)
[ RUN      ] backup_restore_test.test_backup_and_restore_with_user_specified_path
E2023-06-12 05:16:07.849 (1686546967849176370 25732)  mimic.io-thrd.25732: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.849 (1686546967849238315 25732)  mimic.io-thrd.25732: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.849 (1686546967849692713 25733)  mimic.io-thrd.25733: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.849 (1686546967849754504 25733)  mimic.io-thrd.25733: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.849 (1686546967849864133 25731)  mimic.io-thrd.25731: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.849 (1686546967849898383 25731)  mimic.io-thrd.25731: asio socket shutdown failed, error = Transport endpoint is not connected
create app test_app failed: [create] call server error: ERR_NETWORK_FAILURE
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:202: Failure
      Expected: ERR_OK
To be equal to: err
      Which is: ERR_NETWORK_FAILURE
E2023-06-12 05:16:07.850 (1686546967850085801 25731)  mimic.io-thrd.25731: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.850 (1686546967850137425 25731)  mimic.io-thrd.25731: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.850 (1686546967850246187 25733)  mimic.io-thrd.25733: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.850 (1686546967850296096 25733)  mimic.io-thrd.25733: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.850 (1686546967850393640 25734)  mimic.io-thrd.25734: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.850 (1686546967850444248 25734)  mimic.io-thrd.25734: asio socket shutdown failed, error = Transport endpoint is not connected
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:75: Failure
      Expected: ERR_OK
To be equal to: _ddl_client->drop_app(_old_app_name, 0)
      Which is: ERR_NETWORK_FAILURE
[  FAILED  ] backup_restore_test.test_backup_and_restore_with_user_specified_path (1 ms)
[----------] 2 tests from backup_restore_test (5 ms total)

[----------] Global test environment tear-down
[==========] 2 tests from 1 test case ran. (6 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 2 tests, listed below:
[  FAILED  ] backup_restore_test.test_backup_and_restore
[  FAILED  ] backup_restore_test.test_backup_and_restore_with_user_specified_path

 2 FAILED TESTS
dsn exit with code 1
[==========] Running 2 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 2 tests from backup_restore_test
W2023-06-12 05:16:07.870 (1686546967870032584 25778) unknown.io-thrd.25778: rdsn engine already started, ignore the config file 'config.ini'
[ RUN      ] backup_restore_test.test_backup_and_restore
E2023-06-12 05:16:07.870 (1686546967870539421 25784)  mimic.io-thrd.25784: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.870 (1686546967870604459 25784)  mimic.io-thrd.25784: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.870 (1686546967870747076 25785)  mimic.io-thrd.25785: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.870 (1686546967870807622 25785)  mimic.io-thrd.25785: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.870 (1686546967870976348 25782)  mimic.io-thrd.25782: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.871 (1686546967871027715 25782)  mimic.io-thrd.25782: asio socket shutdown failed, error = Transport endpoint is not connected
create app test_app failed: [create] call server error: ERR_NETWORK_FAILURE
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:202: Failure
      Expected: ERR_OK
To be equal to: err
      Which is: ERR_NETWORK_FAILURE
E2023-06-12 05:16:07.871 (1686546967871282474 25785)  mimic.io-thrd.25785: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.871 (1686546967871319809 25785)  mimic.io-thrd.25785: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.871 (1686546967871437613 25784)  mimic.io-thrd.25784: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.871 (1686546967871488638 25784)  mimic.io-thrd.25784: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.871 (1686546967871598067 25785)  mimic.io-thrd.25785: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.871 (1686546967871643552 25785)  mimic.io-thrd.25785: asio socket shutdown failed, error = Transport endpoint is not connected
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:75: Failure
      Expected: ERR_OK
To be equal to: _ddl_client->drop_app(_old_app_name, 0)
      Which is: ERR_NETWORK_FAILURE
[  FAILED  ] backup_restore_test.test_backup_and_restore (1 ms)
[ RUN      ] backup_restore_test.test_backup_and_restore_with_user_specified_path
E2023-06-12 05:16:07.871 (1686546967871936001 25784)  mimic.io-thrd.25784: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.871 (1686546967871983802 25784)  mimic.io-thrd.25784: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.872 (1686546967872085130 25785)  mimic.io-thrd.25785: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.872 (1686546967872122173 25785)  mimic.io-thrd.25785: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.872 (1686546967872221998 25783)  mimic.io-thrd.25783: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.872 (1686546967872271545 25783)  mimic.io-thrd.25783: asio socket shutdown failed, error = Transport endpoint is not connected
create app test_app failed: [create] call server error: ERR_NETWORK_FAILURE
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:202: Failure
      Expected: ERR_OK
To be equal to: err
      Which is: ERR_NETWORK_FAILURE
E2023-06-12 05:16:07.872 (1686546967872487083 25785)  mimic.io-thrd.25785: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.872 (1686546967872523456 25785)  mimic.io-thrd.25785: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.872 (1686546967872622906 25782)  mimic.io-thrd.25782: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.872 (1686546967872670883 25782)  mimic.io-thrd.25782: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.872 (1686546967872846012 25785)  mimic.io-thrd.25785: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.872 (1686546967872883217 25785)  mimic.io-thrd.25785: asio socket shutdown failed, error = Transport endpoint is not connected
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:75: Failure
      Expected: ERR_OK
To be equal to: _ddl_client->drop_app(_old_app_name, 0)
      Which is: ERR_NETWORK_FAILURE
[  FAILED  ] backup_restore_test.test_backup_and_restore_with_user_specified_path (2 ms)
[----------] 2 tests from backup_restore_test (3 ms total)

[----------] Global test environment tear-down
[==========] 2 tests from 1 test case ran. (4 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 2 tests, listed below:
[  FAILED  ] backup_restore_test.test_backup_and_restore
[  FAILED  ] backup_restore_test.test_backup_and_restore_with_user_specified_path

 2 FAILED TESTS
dsn exit with code 1
[==========] Running 2 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 2 tests from backup_restore_test
W2023-06-12 05:16:07.891 (1686546967891812007 25829) unknown.io-thrd.25829: rdsn engine already started, ignore the config file 'config.ini'
[ RUN      ] backup_restore_test.test_backup_and_restore
E2023-06-12 05:16:07.892 (1686546967892018208 25833)  mimic.io-thrd.25833: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.892 (1686546967892083640 25833)  mimic.io-thrd.25833: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.892 (1686546967892407817 25835)  mimic.io-thrd.25835: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.892 (1686546967892493120 25835)  mimic.io-thrd.25835: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.892 (1686546967892644591 25833)  mimic.io-thrd.25833: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.892 (1686546967892697740 25833)  mimic.io-thrd.25833: asio socket shutdown failed, error = Transport endpoint is not connected
create app test_app failed: [create] call server error: ERR_NETWORK_FAILURE
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:202: Failure
      Expected: ERR_OK
To be equal to: err
      Which is: ERR_NETWORK_FAILURE
E2023-06-12 05:16:07.892 (1686546967892990293 25833)  mimic.io-thrd.25833: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.893 (1686546967893039253 25833)  mimic.io-thrd.25833: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.893 (1686546967893222498 25834)  mimic.io-thrd.25834: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.893 (1686546967893272750 25834)  mimic.io-thrd.25834: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.893 (1686546967893412684 25835)  mimic.io-thrd.25835: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.893 (1686546967893468272 25835)  mimic.io-thrd.25835: asio socket shutdown failed, error = Transport endpoint is not connected
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:75: Failure
      Expected: ERR_OK
To be equal to: _ddl_client->drop_app(_old_app_name, 0)
      Which is: ERR_NETWORK_FAILURE
[  FAILED  ] backup_restore_test.test_backup_and_restore (2 ms)
[ RUN      ] backup_restore_test.test_backup_and_restore_with_user_specified_path
E2023-06-12 05:16:07.893 (1686546967893735171 25834)  mimic.io-thrd.25834: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.893 (1686546967893782549 25834)  mimic.io-thrd.25834: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.893 (1686546967893920744 25834)  mimic.io-thrd.25834: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.893 (1686546967893971706 25834)  mimic.io-thrd.25834: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.894 (1686546967894111085 25834)  mimic.io-thrd.25834: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.894 (1686546967894161937 25834)  mimic.io-thrd.25834: asio socket shutdown failed, error = Transport endpoint is not connected
create app test_app failed: [create] call server error: ERR_NETWORK_FAILURE
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:202: Failure
      Expected: ERR_OK
To be equal to: err
      Which is: ERR_NETWORK_FAILURE
E2023-06-12 05:16:07.894 (1686546967894402649 25835)  mimic.io-thrd.25835: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.894 (1686546967894449830 25835)  mimic.io-thrd.25835: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.894 (1686546967894606218 25836)  mimic.io-thrd.25836: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.894 (1686546967894678424 25836)  mimic.io-thrd.25836: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.894 (1686546967894816070 25833)  mimic.io-thrd.25833: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.894 (1686546967894891045 25833)  mimic.io-thrd.25833: asio socket shutdown failed, error = Transport endpoint is not connected
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:75: Failure
      Expected: ERR_OK
To be equal to: _ddl_client->drop_app(_old_app_name, 0)
      Which is: ERR_NETWORK_FAILURE
[  FAILED  ] backup_restore_test.test_backup_and_restore_with_user_specified_path (2 ms)
[----------] 2 tests from backup_restore_test (4 ms total)

[----------] Global test environment tear-down
[==========] 2 tests from 1 test case ran. (4 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 2 tests, listed below:
[  FAILED  ] backup_restore_test.test_backup_and_restore
[  FAILED  ] backup_restore_test.test_backup_and_restore_with_user_specified_path

 2 FAILED TESTS
dsn exit with code 1
[==========] Running 2 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 2 tests from backup_restore_test
W2023-06-12 05:16:07.912 (1686546967912174083 25880) unknown.io-thrd.25880: rdsn engine already started, ignore the config file 'config.ini'
[ RUN      ] backup_restore_test.test_backup_and_restore
E2023-06-12 05:16:07.912 (1686546967912495819 25886)  mimic.io-thrd.25886: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.912 (1686546967912576984 25886)  mimic.io-thrd.25886: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.912 (1686546967912825660 25884)  mimic.io-thrd.25884: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.912 (1686546967912909424 25884)  mimic.io-thrd.25884: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.913 (1686546967913048383 25886)  mimic.io-thrd.25886: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.913 (1686546967913115331 25886)  mimic.io-thrd.25886: asio socket shutdown failed, error = Transport endpoint is not connected
create app test_app failed: [create] call server error: ERR_NETWORK_FAILURE
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:202: Failure
      Expected: ERR_OK
To be equal to: err
      Which is: ERR_NETWORK_FAILURE
E2023-06-12 05:16:07.913 (1686546967913626080 25887)  mimic.io-thrd.25887: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.913 (1686546967913698695 25887)  mimic.io-thrd.25887: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.913 (1686546967913922912 25885)  mimic.io-thrd.25885: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.913 (1686546967913979411 25885)  mimic.io-thrd.25885: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.914 (1686546967914135983 25887)  mimic.io-thrd.25887: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.914 (1686546967914201661 25887)  mimic.io-thrd.25887: asio socket shutdown failed, error = Transport endpoint is not connected
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:75: Failure
      Expected: ERR_OK
To be equal to: _ddl_client->drop_app(_old_app_name, 0)
      Which is: ERR_NETWORK_FAILURE
[  FAILED  ] backup_restore_test.test_backup_and_restore (2 ms)
[ RUN      ] backup_restore_test.test_backup_and_restore_with_user_specified_path
E2023-06-12 05:16:07.914 (1686546967914509168 25887)  mimic.io-thrd.25887: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.914 (1686546967914549777 25887)  mimic.io-thrd.25887: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.914 (1686546967914660888 25887)  mimic.io-thrd.25887: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.914 (1686546967914701180 25887)  mimic.io-thrd.25887: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.914 (1686546967914822961 25887)  mimic.io-thrd.25887: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.914 (1686546967914860754 25887)  mimic.io-thrd.25887: asio socket shutdown failed, error = Transport endpoint is not connected
create app test_app failed: [create] call server error: ERR_NETWORK_FAILURE
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:202: Failure
      Expected: ERR_OK
To be equal to: err
      Which is: ERR_NETWORK_FAILURE
E2023-06-12 05:16:07.915 (1686546967915138706 25886)  mimic.io-thrd.25886: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.915 (1686546967915192515 25886)  mimic.io-thrd.25886: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.915 (1686546967915366678 25887)  mimic.io-thrd.25887: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.915 (1686546967915457500 25887)  mimic.io-thrd.25887: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.915 (1686546967915587485 25884)  mimic.io-thrd.25884: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.915 (1686546967915642891 25884)  mimic.io-thrd.25884: asio socket shutdown failed, error = Transport endpoint is not connected
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:75: Failure
      Expected: ERR_OK
To be equal to: _ddl_client->drop_app(_old_app_name, 0)
      Which is: ERR_NETWORK_FAILURE
[  FAILED  ] backup_restore_test.test_backup_and_restore_with_user_specified_path (1 ms)
[----------] 2 tests from backup_restore_test (3 ms total)

[----------] Global test environment tear-down
[==========] 2 tests from 1 test case ran. (3 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 2 tests, listed below:
[  FAILED  ] backup_restore_test.test_backup_and_restore
[  FAILED  ] backup_restore_test.test_backup_and_restore_with_user_specified_path

 2 FAILED TESTS
dsn exit with code 1
[==========] Running 2 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 2 tests from backup_restore_test
W2023-06-12 05:16:07.933 (1686546967933391086 25931) unknown.io-thrd.25931: rdsn engine already started, ignore the config file 'config.ini'
[ RUN      ] backup_restore_test.test_backup_and_restore
E2023-06-12 05:16:07.933 (1686546967933665656 25935)  mimic.io-thrd.25935: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.933 (1686546967933726291 25935)  mimic.io-thrd.25935: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.933 (1686546967933893935 25935)  mimic.io-thrd.25935: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.933 (1686546967933941211 25935)  mimic.io-thrd.25935: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.934 (1686546967934076816 25936)  mimic.io-thrd.25936: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.934 (1686546967934124074 25936)  mimic.io-thrd.25936: asio socket shutdown failed, error = Transport endpoint is not connected
create app test_app failed: [create] call server error: ERR_NETWORK_FAILURE
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:202: Failure
      Expected: ERR_OK
To be equal to: err
      Which is: ERR_NETWORK_FAILURE
E2023-06-12 05:16:07.934 (1686546967934427754 25937)  mimic.io-thrd.25937: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.934 (1686546967934473632 25937)  mimic.io-thrd.25937: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.934 (1686546967934647138 25938)  mimic.io-thrd.25938: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.934 (1686546967934693260 25938)  mimic.io-thrd.25938: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.934 (1686546967934795385 25937)  mimic.io-thrd.25937: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.934 (1686546967934853048 25937)  mimic.io-thrd.25937: asio socket shutdown failed, error = Transport endpoint is not connected
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:75: Failure
      Expected: ERR_OK
To be equal to: _ddl_client->drop_app(_old_app_name, 0)
      Which is: ERR_NETWORK_FAILURE
[  FAILED  ] backup_restore_test.test_backup_and_restore (1 ms)
[ RUN      ] backup_restore_test.test_backup_and_restore_with_user_specified_path
E2023-06-12 05:16:07.935 (1686546967935068218 25938)  mimic.io-thrd.25938: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.935 (1686546967935113738 25938)  mimic.io-thrd.25938: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.935 (1686546967935271548 25935)  mimic.io-thrd.25935: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.935 (1686546967935319227 25935)  mimic.io-thrd.25935: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.935 (1686546967935414702 25937)  mimic.io-thrd.25937: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.935 (1686546967935463948 25937)  mimic.io-thrd.25937: asio socket shutdown failed, error = Transport endpoint is not connected
create app test_app failed: [create] call server error: ERR_NETWORK_FAILURE
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:202: Failure
      Expected: ERR_OK
To be equal to: err
      Which is: ERR_NETWORK_FAILURE
E2023-06-12 05:16:07.935 (1686546967935663761 25936)  mimic.io-thrd.25936: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.935 (1686546967935723150 25936)  mimic.io-thrd.25936: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.935 (1686546967935860556 25938)  mimic.io-thrd.25938: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.935 (1686546967935908462 25938)  mimic.io-thrd.25938: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.936 (1686546967936009620 25938)  mimic.io-thrd.25938: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.936 (1686546967936056348 25938)  mimic.io-thrd.25938: asio socket shutdown failed, error = Transport endpoint is not connected
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:75: Failure
      Expected: ERR_OK
To be equal to: _ddl_client->drop_app(_old_app_name, 0)
      Which is: ERR_NETWORK_FAILURE
[  FAILED  ] backup_restore_test.test_backup_and_restore_with_user_specified_path (2 ms)
[----------] 2 tests from backup_restore_test (3 ms total)

[----------] Global test environment tear-down
[==========] 2 tests from 1 test case ran. (3 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 2 tests, listed below:
[  FAILED  ] backup_restore_test.test_backup_and_restore
[  FAILED  ] backup_restore_test.test_backup_and_restore_with_user_specified_path

 2 FAILED TESTS
dsn exit with code 1
[==========] Running 2 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 2 tests from backup_restore_test
W2023-06-12 05:16:07.952 (1686546967952318543 25982) unknown.io-thrd.25982: rdsn engine already started, ignore the config file 'config.ini'
[ RUN      ] backup_restore_test.test_backup_and_restore
E2023-06-12 05:16:07.952 (1686546967952650377 25987)  mimic.io-thrd.25987: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.952 (1686546967952712485 25987)  mimic.io-thrd.25987: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.952 (1686546967952882067 25989)  mimic.io-thrd.25989: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.952 (1686546967952929268 25989)  mimic.io-thrd.25989: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.953 (1686546967953103498 25987)  mimic.io-thrd.25987: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.953 (1686546967953153344 25987)  mimic.io-thrd.25987: asio socket shutdown failed, error = Transport endpoint is not connected
create app test_app failed: [create] call server error: ERR_NETWORK_FAILURE
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:202: Failure
      Expected: ERR_OK
To be equal to: err
      Which is: ERR_NETWORK_FAILURE
E2023-06-12 05:16:07.953 (1686546967953434304 25986)  mimic.io-thrd.25986: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.953 (1686546967953486937 25986)  mimic.io-thrd.25986: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.953 (1686546967953641376 25987)  mimic.io-thrd.25987: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.953 (1686546967953691011 25987)  mimic.io-thrd.25987: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.953 (1686546967953846289 25986)  mimic.io-thrd.25986: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.953 (1686546967953903622 25986)  mimic.io-thrd.25986: asio socket shutdown failed, error = Transport endpoint is not connected
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:75: Failure
      Expected: ERR_OK
To be equal to: _ddl_client->drop_app(_old_app_name, 0)
      Which is: ERR_NETWORK_FAILURE
[  FAILED  ] backup_restore_test.test_backup_and_restore (2 ms)
[ RUN      ] backup_restore_test.test_backup_and_restore_with_user_specified_path
E2023-06-12 05:16:07.954 (1686546967954212470 25989)  mimic.io-thrd.25989: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.954 (1686546967954256171 25989)  mimic.io-thrd.25989: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.954 (1686546967954540023 25988)  mimic.io-thrd.25988: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.954 (1686546967954584155 25988)  mimic.io-thrd.25988: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.954 (1686546967954792215 25986)  mimic.io-thrd.25986: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.954 (1686546967954845711 25986)  mimic.io-thrd.25986: asio socket shutdown failed, error = Transport endpoint is not connected
create app test_app failed: [create] call server error: ERR_NETWORK_FAILURE
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:202: Failure
      Expected: ERR_OK
To be equal to: err
      Which is: ERR_NETWORK_FAILURE
E2023-06-12 05:16:07.955 (1686546967955279697 25986)  mimic.io-thrd.25986: client session connect to 127.0.0.1:34602 failed, error = Connection refused
W2023-06-12 05:16:07.955 (1686546967955328902 25986)  mimic.io-thrd.25986: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.955 (1686546967955457400 25987)  mimic.io-thrd.25987: client session connect to 127.0.0.1:34603 failed, error = Connection refused
W2023-06-12 05:16:07.955 (1686546967955508497 25987)  mimic.io-thrd.25987: asio socket shutdown failed, error = Transport endpoint is not connected
E2023-06-12 05:16:07.955 (1686546967955640012 25989)  mimic.io-thrd.25989: client session connect to 127.0.0.1:34601 failed, error = Connection refused
W2023-06-12 05:16:07.955 (1686546967955713510 25989)  mimic.io-thrd.25989: asio socket shutdown failed, error = Transport endpoint is not connected
/root/pegasus/incubator-pegasus/src/test/function_test/backup_restore_test/test_backup_and_restore.cpp:75: Failure
      Expected: ERR_OK
To be equal to: _ddl_client->drop_app(_old_app_name, 0)
      Which is: ERR_NETWORK_FAILURE
[  FAILED  ] backup_restore_test.test_backup_and_restore_with_user_specified_path (2 ms)
[----------] 2 tests from backup_restore_test (4 ms total)

[----------] Global test environment tear-down
[==========] 2 tests from 1 test case ran. (4 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 2 tests, listed below:
[  FAILED  ] backup_restore_test.test_backup_and_restore
[  FAILED  ] backup_restore_test.test_backup_and_restore_with_user_specified_path

 2 FAILED TESTS
dsn exit with code 1
---- ls ----
total 195464
-rwxr-xr-x 1 root root 200117328 Jun 12 02:01 backup_restore_test
drwxr-xr-x 3 root root      4096 Jun  9 06:10 CMakeFiles
-rw-r--r-- 1 root root      1337 Jun  9 06:10 cmake_install.cmake
-rw-r--r-- 1 root root      2350 Jun  5 02:27 config.ini
drwxr-xr-x 4 root root      4096 Jun 12 02:29 data
-rw-r--r-- 1 root root     10742 Jun  9 06:10 Makefile
-rwxr-xr-x 1 root root      1317 Jun  5 02:27 run.sh
root@ubuntu2004:~/pegasus/incubator-pegasus# 

``` ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5ekanU/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1525,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5ekf3h,incubator-pegasus,1586626017,1525,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-06-12T05:49:49Z,2023-06-12T05:49:49Z,"I see, thanks!","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5ekf3h/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1527,https://api.github.com/repos/apache/incubator-pegasus/issues/1527,incubator-pegasus,1752374910,1527,Profiled tasks are measured by the wrong metrics,empiredan,743379,Dan Wang,,CLOSED,2023-06-12T10:01:23Z,2023-06-30T09:15:41Z,"While using [bench](https://github.com/apache/incubator-pegasus/tree/master/src/test/bench_test) to do `fillrandom_pegasus` or `readrandom_pegasus`, it is found that the value the metrics of `RPC_RRDB_RRDB_PUT_ACK` and `RPC_RRDB_RRDB_GET_ACK`, such as `profiler_executed_tasks` and `profiler_execute_latency_ns`, are non-zero, while `RPC_RRDB_RRDB_PUT` and `RPC_RRDB_RRDB_GET` are still zero.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1527/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1529,https://api.github.com/repos/apache/incubator-pegasus/issues/1529,incubator-pegasus,1752888358,1529,`throttle_test` failed frequently insufficient disk space,empiredan,743379,Dan Wang,,CLOSED,2023-06-12T14:24:23Z,2023-06-30T09:15:29Z,"`throttle_test` failed frequently as following error:

```
start test, on set test / throttle by size / 50kb value size
/__w/incubator-pegasus/incubator-pegasus/src/test/function_test/throttle_test/test_throttle.cpp:309: Failure
Value of: last_error == PERR_OK || last_error == PERR_APP_BUSY
  Actual: false
Expected: true
-107
/__w/incubator-pegasus/incubator-pegasus/src/test_util/test_util.cpp:92: Failure
Failed
Timed out waiting for assertion to pass.
/__w/incubator-pegasus/incubator-pegasus/src/test/function_test/throttle_test/test_throttle.cpp:523: Failure
Expected: start_test(plan) doesn't generate new fatal failures in the current thread.
  Actual: it does.
[  FAILED  ] throttle_test.test (625101 ms)
[----------] 1 test from throttle_test (625101 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (625101 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] throttle_test.test

 1 FAILED TEST
dsn exit with code 1
```

`-107` is defined in `src/include/pegasus/error_def.h`, meaning `disk insufficient`:
```c++
PEGASUS_ERR_CODE(PERR_DISK_INSUFFICIENT, -107, ""disk insufficient"");
```

This resulted from insufficient disk space:
```
2023-06-12 20:21:38.28 (1686572498028082022 60602) replica.default0.0000ec3900010001: fs_manager.cpp:188:update_disk_stat(): update disk space succeed: dir = /data/onebox/replica1/data/replica/reps, capacity_mb = 204700, available_mb = 7750, available_ratio = 4%, disk_status = replication::disk_status::SPACE_INSUFFICIENT
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1529/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1532,https://api.github.com/repos/apache/incubator-pegasus/issues/1532,incubator-pegasus,1756185863,1532,Refactor(new_metrics): remove all perf-counters referenced in shared log,empiredan,743379,Dan Wang,,CLOSED,2023-06-14T06:52:54Z,2023-06-30T09:16:34Z,"In https://github.com/apache/incubator-pegasus/pull/1462 all shared-log-related metrics have been removed. However, there's still some perf-counters referenced in shared log. All of them should be removed.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1532/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1534,https://api.github.com/repos/apache/incubator-pegasus/issues/1534,incubator-pegasus,1756327285,1534,Refactor throttle_test and reduce the consumption of disk space,empiredan,743379,Dan Wang,,CLOSED,2023-06-14T08:18:51Z,2023-06-30T09:15:20Z,"- refactor the code of throttle_test;
- at the end of each test case, stop onebox for each replica server and clear the data to release the disk space.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1534/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1536,https://api.github.com/repos/apache/incubator-pegasus/issues/1536,incubator-pegasus,1758127696,1536,Feature(new_metrics): migrate metrics for latency tracer,empiredan,743379,Dan Wang,,CLOSED,2023-06-15T06:26:11Z,2023-06-30T09:16:03Z,Migrate all metrics of latency tracer to the new framework.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1536/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1538,https://api.github.com/repos/apache/incubator-pegasus/issues/1538,incubator-pegasus,1758159626,1538,Feature: support app load balance base on replicas disk usage,ninsmiracle,110282526,,,OPEN,2023-06-15T06:51:29Z,2023-06-15T13:01:31Z,"## Feature Request

***Is your feature request related to a problem? Please describe:***
The current load balancing strategy in Pegasus is relatively simplistic, where the balance is determined based on the number of replicas a node hosts for a particular table. The load balancing strategy treats the weights of these tables as uniform, but in reality, the disk occupancy of these shards is different. This inevitably leads to some nodes having higher disk loads while others have lower disk loads, potentially resulting in inefficient utilization of disk resources.

Therefore, our goal is to achieve disk-based load balancing criteria and adjust the distribution of shards at the table level, aiming to achieve a more balanced distribution of shards across nodes for that table.

***Describe the feature you'd like:***
There are 4 new feature to finish disk usage balance:
1.add a balance function switch,let user chose balance policy--base on replica number **OR** replica disk usage.
2.new way to collect every replica disk usage
3.new app envs feature to control balance completion
4.the corresponding new load balancing calculation method

To ensure consistent functionality,load balance base on replicas disk usage need to support 'only_copy_primary'.

***Describe alternatives you've considered:***

**Function Switch and Dynamic Modification of Stop Threshold**

This document describes a strategy parallel to the existing replica_num. Therefore, the greedy_load_balancer::greedy_balancer in greedy_balancer needs to use gflag to set the switch method for users to choose balance_policy.

The disk load balancing of table dimensions can only achieve relative balance because the weight scores of all shards are no longer 1. Therefore, the final stop threshold should be that the skewness of disk occupancy for all shards in the table is less than our preset value, or the load on each disk directly reaches the desired average value. This preset value should be dynamically modifiable at the table level and is designed in the app_evns of the table.

**Data Acquisition and Reporting**
Each replica instance obtains the locally held replica disk occupancy information, and the acquisition method is to use the existing pegasus interface get_disk_space_info to obtain the disk usage of the path. This part of the collected information is divided into two parts: the occupancy of each replica and the real physical occupancy of the disk where the replica is located (there may be replicas of other tables or even other clusters). It is reported to meta through on_config_sync, so the information acquisition is not real-time and has a default delay of 90 seconds.

**Data Aggregation and Load Calculation**
Meta aggregates and organizes the replica disk load data based on tables. The aggregated data is stored in the global_view structure and is periodically synchronized to meta through on_config_sync, and then passed as parameters through check_all_partition to the balance function.
In the load balancing logic based on replica_num, copy_primary has two parts: move_primary. Its preliminary steps include building a maximum flow graph and calculating the shortest path. However, in the load balancing strategy based on disk capacity, role switching does not improve the disk of the replica, so I have removed the logic of move_primary here.
That is, in the table-level implementation strategy of disk load balancing, there are only two steps: copy_primary and copy_secondary. The only difference between the two is the scope of calculation. Copy_primary is prioritized because the primary_replica in Pegasus plays the role of the first point of contact for read and write operations, and it is important to prioritize the balance of the primary replica.


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1538/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1538,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5e633V,incubator-pegasus,1592491477,1538,NA,ninsmiracle,110282526,,,NA,2023-06-15T07:14:31Z,2023-06-15T07:14:31Z,"![disk_usage_load_balance](https://github.com/apache/incubator-pegasus/assets/110282526/780e65e9-45b2-4660-bed6-2897e33d2666)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5e633V/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1538,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5e80f9,incubator-pegasus,1593001981,1538,NA,ninsmiracle,110282526,,,NA,2023-06-15T13:01:30Z,2023-06-15T13:01:30Z,"![balance_policy_graph drawio](https://github.com/apache/incubator-pegasus/assets/110282526/2d207513-e963-4545-9334-135f54bfdb92)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5e80f9/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1539,https://api.github.com/repos/apache/incubator-pegasus/issues/1539,incubator-pegasus,1766939189,1539,Feature(new_metrics): remove http service for perf counters,empiredan,743379,Dan Wang,,CLOSED,2023-06-21T07:29:16Z,2023-06-30T09:15:11Z,"In https://github.com/XiaoMi/rdsn/pull/349 we've supported getting value of counter by http request. Now since perf counter will be replaced by new metrics, it can also be removed. ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1539/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1541,https://api.github.com/repos/apache/incubator-pegasus/issues/1541,incubator-pegasus,1767182078,1541,total_capacity_mb/total_available_mb are not atomic,empiredan,743379,Dan Wang,,CLOSED,2023-06-21T09:32:27Z,2023-06-30T09:15:03Z,"After rebasing from [master](https://github.com/apache/incubator-pegasus/tree/master) onto [migrate-metrics-dev](https://github.com/apache/incubator-pegasus/tree/migrate-metrics-dev), there are some problems for `total_capacity_mb`/`total_available_mb` of `fs_manager`:

- both are still referencing the value of perf-counters;
```
[ 68%] Building CXX object src/replica/CMakeFiles/dsn_replica_server.dir/replica_stub.cpp.o
/__w/incubator-pegasus/incubator-pegasus/src/replica/replica_stub.cpp: In member function 'void dsn::replication::replica_stub::on_query_disk_info(dsn::replication::query_disk_info_rpc)':
/__w/incubator-pegasus/incubator-pegasus/src/replica/replica_stub.cpp:913:42: error: 'class dsn::replication::fs_manager' has no member named '_counter_total_capacity_mb'; did you mean '_total_capacity_mb'?
  913 |     resp.total_capacity_mb = _fs_manager._counter_total_capacity_mb->get_integer_value();
      |                                          ^~~~~~~~~~~~~~~~~~~~~~~~~~
      |                                          _total_capacity_mb
/__w/incubator-pegasus/incubator-pegasus/src/replica/replica_stub.cpp:914:43: error: 'class dsn::replication::fs_manager' has no member named '_counter_total_available_mb'; did you mean '_total_available_mb'?
  914 |     resp.total_available_mb = _fs_manager._counter_total_available_mb->get_integer_value();
      |                                           ^~~~~~~~~~~~~~~~~~~~~~~~~~~
      |                                           _total_available_mb
At global scope:
cc1plus: note: unrecognized command-line option '-Wno-implicit-float-conversion' may have been intended to silence earlier diagnostics
cc1plus: note: unrecognized command-line option '-Wno-deprecated-register' may have been intended to silence earlier diagnostics
cc1plus: note: unrecognized command-line option '-Wno-inconsistent-missing-override' may have been intended to silence earlier diagnostics
make[2]: *** [src/replica/CMakeFiles/dsn_replica_server.dir/build.make:398: src/replica/CMakeFiles/dsn_replica_server.dir/replica_stub.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:2398: src/replica/CMakeFiles/dsn_replica_server.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
```
- setting/getting both are not atomic.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1541/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1543,https://api.github.com/repos/apache/incubator-pegasus/issues/1543,incubator-pegasus,1767198844,1543,About Backup and Restore,jerryHo123,53845481,hehao,,OPEN,2023-06-21T09:42:49Z,2023-06-26T08:37:53Z,"1.By admin-cli,after using the backup command, it shows that the backup is successful, and the backup directory has also been generated, but the .sst data file is not found in the directory. Why?
![image](https://github.com/apache/incubator-pegasus/assets/53845481/df77531e-5592-45f0-8d4d-a66ffb5dfdb3)

The following is the backup directory generated by the pegasus-shell command
![image](https://github.com/apache/incubator-pegasus/assets/53845481/7cf6d699-2ee8-499b-80cd-a3bd526f37ef)

2.By pegasus-shell,after using the add_backup_policy command, it shows that the backup is successful, and the backup directory has also been generated, and each partition has a directory containing the .sst data file, but the restore_app command is displayed as follows, why is this?
![cd9f2d71-8ee8-4b70-87b9-3aab51574fbe](https://github.com/apache/incubator-pegasus/assets/53845481/96b4621b-e04b-415f-a389-79400a91d62c)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1543/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1543,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5fyJJc,incubator-pegasus,1606980188,1543,NA,Smityz,22953824,Smilencer,smityz@qq.com,NA,2023-06-26T08:37:53Z,2023-06-26T08:37:53Z,"Can this operation be reproduced under the onebox environment?
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5fyJJc/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1545,https://api.github.com/repos/apache/incubator-pegasus/issues/1545,incubator-pegasus,1773163766,1545,About cold-backup,jerryHo123,53845481,hehao,,OPEN,2023-06-25T09:05:27Z,2023-06-25T09:52:22Z,"In the cluster mode with version 2.4.0, the command to add_backup_policy shows success,backup to local.
![image](https://github.com/apache/incubator-pegasus/assets/53845481/aaa4a593-8375-49c4-afcb-ef4a91c6e454)
Querying the cold backup strategy shows the start and end time of cold backup, but the corresponding directory file is not found locally.
![bf76bae6-7b4b-40b3-b38d-5fb53ec18eab](https://github.com/apache/incubator-pegasus/assets/53845481/c6853e6f-5303-4fa1-a869-a3816fa53822)
How to solve this？
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1545/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1545,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5fuZKc,incubator-pegasus,1605997212,1545,NA,jerryHo123,53845481,hehao,,NA,2023-06-25T09:52:21Z,2023-06-25T09:52:21Z,"After a while, there will be a directory file, but it seems that some information is missing, and the following error will be reported when running the restore_appcommand.
![image](https://github.com/apache/incubator-pegasus/assets/53845481/efa7af43-9407-4dfb-8b0c-92cdf89d4535)

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5fuZKc/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1546,https://api.github.com/repos/apache/incubator-pegasus/issues/1546,incubator-pegasus,1774244833,1546,Bug:restore bug,jerryHo123,53845481,hehao,,OPEN,2023-06-26T08:39:37Z,2023-06-30T03:35:24Z,"Version:2.4.0
Cluster info:2 meta,4 replica

Add_backup_policy command succeed,and restore_app command also succeed.But an error occurred with the query_restore_status command:
![image](https://github.com/apache/incubator-pegasus/assets/53845481/db3d0497-91e5-408e-9797-a7996ac14609)
View log found:the x.x.x.195 replica node actually stores the partitions 0 and 4, but when restoring, it reads the partition data of 2 and 6. The partitions 2 and 6 actually exist on the  x.x.x.226 replica node, so it will report read file failed
![image](https://github.com/apache/incubator-pegasus/assets/53845481/22521d3d-58fc-4fd4-9906-4b4c446f7a4a)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1546/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1547,https://api.github.com/repos/apache/incubator-pegasus/issues/1547,incubator-pegasus,1774283675,1547,Feature(new_metrics): remove `pegasus_counter_reporter`,empiredan,743379,Dan Wang,,CLOSED,2023-06-26T09:01:06Z,2023-06-30T09:17:36Z,"Since perf counters would be removed, `pegasus_counter_reporter` would not be used to collect metrics to target monitoring system, thus could be removed.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1547/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1549,https://api.github.com/repos/apache/incubator-pegasus/issues/1549,incubator-pegasus,1774882240,1549,Add a dev container configure,Smityz,22953824,Smilencer,smityz@qq.com,CLOSED,2023-06-26T14:07:15Z,2023-06-30T09:14:47Z,"It's not easy for a beginner to compile a C++ program, I think dev container can help to build pegasus","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1549/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1550,https://api.github.com/repos/apache/incubator-pegasus/issues/1550,incubator-pegasus,1775998231,1550,"When primary meta server changes,this new function will throw an Index0utOfBoundsException.",COLDMANYU,46128522,,,CLOSED,2023-06-27T02:53:55Z,2023-06-30T08:22:03Z,"## Bug Report

I use pegasus-2.4.

When primary meta server changes,this new function will throw an Index0utOfBoundsException.

If meta_a was killed,meta_b will be switched to primary meta immediately.Pegasus are available during this time,but this new function will throw an Index0utOfBoundsException and stop our program.

![企业微信截图_16877787986626](https://github.com/apache/incubator-pegasus/assets/46128522/fdffa2a7-9f15-4b25-831e-86b72dea104c)

![企业微信截图_16877788105516](https://github.com/apache/incubator-pegasus/assets/46128522/f61ac6df-65f1-457c-8f85-c3ef9b9c267c)

I use this method to connect pegasus.

![企业微信截图_16877788284157](https://github.com/apache/incubator-pegasus/assets/46128522/d6c6f4db-8f77-462e-8fce-86ab9698c706)

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1550/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1550,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5gN0CQ,incubator-pegasus,1614233744,1550,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-06-30T07:16:05Z,2023-06-30T07:16:05Z,"@asunyu1a It seems duplicate with https://github.com/apache/incubator-pegasus/issues/1410, it has been resolved in the lastest master branch.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5gN0CQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1551,https://api.github.com/repos/apache/incubator-pegasus/issues/1551,incubator-pegasus,1776176785,1551,Checking issue for github always failed,empiredan,743379,Dan Wang,,OPEN,2023-06-27T06:24:12Z,2023-06-29T07:39:32Z,"`neofinancial/ticket-check-action@v1.3.0` always failed as below: 

```
Run neofinancial/ticket-check-action@v1.3.0
  with:
    token: ***
    titleFormat: %title%
    quiet: true
    bodyRegex: #(\d+)
    bodyURLRegex: http(s?):\/\/(github.com)(\/apache)(\/incubator-pegasus)(\/issues)\/\d+
    ticketPrefix: #
    titleRegex: ^(CH)(-?)(\d{3,})
    titleRegexFlags: gi
    branchRegex: ^(CH)(-?)(\d{3,})
    branchRegexFlags: gi
    bodyRegexFlags: gim
    bodyURLRegexFlags: gim
/home/runner/work/_actions/neofinancial/ticket-check-action/v1.3.0/build/index.js:1
module.exports=function(e,t){""use strict"";var r={};function __webpack_require__(t){if(r[t]){return r[t].exports}var i=r[t]={i:t,l:false,exports:{}};e[t].call(i.exports,i,i.exports,__webpack_require__);i.l=true;return i.exports}__webpack_require__.ab=__dirname+""/"";function startup(){return __webpack_require__(3[2](https://github.com/apache/incubator-pegasus/actions/runs/5385704346/jobs/9774987825?pr=1548#step:2:2)5)}return startup()}({0:function(e,t,r){const{requestLog:i}=r(916);const{restEndpointMethods:n}=r(842);const s=r(529);const o=[r(190),r(19),i,r(148),n,r(4[3](https://github.com/apache/incubator-pegasus/actions/runs/5385704346/jobs/9774987825?pr=1548#step:2:3)0),r(850)];const a=s.plugin(o);function DeprecatedOctokit(e){const t=e&&e.log&&e.log.warn?e.log.warn:console.warn;t('[@octokit/rest] `const Octokit = require(""@octokit/rest"")` is deprecated. Use `const { Octokit } = require(""@octokit/rest"")` instead');return new a(e)}const u=Object.assign(DeprecatedOctokit,{Octokit:a});Object.keys(a).forEach(e=>{if(a.hasOwnProperty(e)){u[e]=a[e]}});e.exports=u},2:function(e,t,r){""use strict"";const i=r(87);const n=r(118);const s=r([4](https://github.com/apache/incubator-pegasus/actions/runs/5385704346/jobs/9774987825?pr=1548#step:2:4)9);const o=(e,t)=>{if(!e&&t){throw new Error(""You can't specify a `release` without specifying `platform`"")}e=e||i.platform();let r;if(e===""darwin""){if(!t&&i.platform()===""darwin""){t=i.release()}const e=t?Number(t.split(""."")[0])>1[5](https://github.com/apache/incubator-pegasus/actions/runs/5385704346/jobs/9774987825?pr=1548#step:2:5)?""macOS"":""OS X"":""macOS"";r=t?n(t).name:"""";return e+(r?"" ""+r:"""")}if(e===""linux""){if(!t&&i.platform()===""linux""){t=i.release()}r=t?t.replace(/^(\d+\.\d+).*/,""$1""):"""";return""Linux""+(r?"" ""+r:"""")}if(e===""win32""){if(!t&&i.platform()===""win32""){t=i.release()}r=t?s(t):"""";return""Windows""+(r?"" ""+r:"""")}return e};e.exports=o},9:function(e,t,r){var i=r(9[6](https://github.com/apache/incubator-pegasus/actions/runs/5385704346/jobs/9774987825?pr=1548#step:2:6)9);var n=function(){};var s=function(e){return e.setHeader&&typeof e.abort===""function""};var o=function(e){return e.stdio&&Array.isArray(e.stdio)&&e.stdio.length===3};var a=function(e,t,r){if(typeof t===""function"")return a(e,null,t);if(!t)t={};r=i(r||n);var u=e._writableState;var p=e._readableState;var c=t.readable||t.readable!==false&&e.readable;var d=t.writable||t.writable!==false&&e.writable;var l=false;var g=function(){if(!e.writable)m()};var m=function(){d=false;if(!c)r.call(e)};var h=function(){c=false;if(!d)r.call(e)};var y=function(t){r.call(e,t?new Error(""exited with error code: ""+t):null)};var f=function(t){r.call(e,t)};var b=function(){process.nextTick(_)};var _=function(){if(l)return;if(c&&!(p&&(p.ended&&!p.destroyed)))return r.call(e,new Error(""premature close""));if(d&&!(u&&(u.ended&&!u.destroyed)))return r.call(e,new Error(""premature close""))};var q=function(){e.req.on(""finish"",m)};if(s(e)){e.on(""complete"",m);e.on(""abort"",b);if(e.req)q();else e.on(""request"",q)}else if(d&&!u){e.on(""end"",g);e.on(""close"",g)}if(o(e))e.on(""exit"",y);e.on(""end"",h);e.on(""finish"",m);if(t.error!==false)e.on(""error"",f);e.on(""close"",b);return function()

...... (omit since it's too long)

RequestError [HttpError]: Resource not accessible by integration
    at /home/runner/work/_actions/neofinancial/ticket-check-action/v1.3.0/build/index.js:1:117831
    at processTicksAndRejections (node:internal/process/task_queues:96:5) {
  status: 403,
  headers: {
    'access-control-allow-origin': '*',
    'access-control-expose-headers': 'ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset',
    connection: 'close',
    'content-encoding': 'gzip',
    'content-security-policy': ""default-src 'none'"",
    'content-type': 'application/json; charset=utf-8',
    date: 'Tue, 27 Jun [20](https://github.com/apache/incubator-pegasus/actions/runs/5386751996/jobs/9777176479?pr=1552#step:2:21)23 07:00:46 GMT',
    'referrer-policy': 'origin-when-cross-origin, strict-origin-when-cross-origin',
    server: 'GitHub.com',
    'strict-transport-security': 'max-age=31536000; includeSubdomains; preload',
    'transfer-encoding': 'chunked',
    vary: 'Accept-Encoding, Accept, X-Requested-With',
    'x-content-type-options': 'nosniff',
    'x-frame-options': 'deny',
    'x-github-api-version-selected': '20[22](https://github.com/apache/incubator-pegasus/actions/runs/5386751996/jobs/9777176479?pr=1552#step:2:23)-11-28',
    'x-github-media-type': 'github.v3; format=json',
    'x-github-request-id': '5501:8C31:1F989E:20C608:649A891E',
    'x-ratelimit-limit': '15000',
    'x-ratelimit-remaining': '14960',
    'x-ratelimit-reset': '1687851297',
    'x-ratelimit-resource': 'core',
    'x-ratelimit-used': '40',
    'x-xss-protection': '0'
  },
  request: {
    method: 'PATCH',
    url: 'https://api.github.com/repos/apache/incubator-pegasus/pulls/1552',
    headers: {
      accept: 'application/vnd.github.v3+json',
      'user-agent': 'octokit.js/16.[43](https://github.com/apache/incubator-pegasus/actions/runs/5386751996/jobs/9777176479?pr=1552#step:2:44).1 Node.js/16.16.0 (Linux 5.15; x64)',
      authorization: 'token [REDACTED]',
      'content-type': 'application/json; charset=utf-8'
    },
    body: '{""title"":""chore(github): upgrade neofinancial/ticket-check-action to 2.0.0""}',
    request: {
      hook: [Function: bound bound register],
      validate: {
        base: { type: 'string' },
        body: { type: 'string' },
        maintainer_can_modify: { type: 'boolean' },
        number: { alias: 'pull_number', deprecated: true, type: 'integer' },
        owner: { required: true, type: 'string' },
        pull_number: { required: true, type: 'integer' },
        repo: { required: true, type: 'string' },
        state: { enum: [ 'open', 'closed' ], type: 'string' },
        title: { type: 'string' }
      }
    }
  },
  documentation_url: 'https://docs.github.com/rest/reference/pulls/#update-a-pull-request'
}
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1551/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1554,https://api.github.com/repos/apache/incubator-pegasus/issues/1554,incubator-pegasus,1782215240,1554,fix Compilation Pegasus job,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2023-06-30T09:34:04Z,2023-07-03T08:16:08Z,"Job `Lint and build regularly ` failed coz of uninitialized number and so on.

https://github.com/apache/incubator-pegasus/actions/runs/5415276515/jobs/9843316433","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1554/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1559,https://api.github.com/repos/apache/incubator-pegasus/issues/1559,incubator-pegasus,1790700363,1559,Dockerfile build failed ,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-07-06T02:53:35Z,2023-07-07T06:43:34Z,"[build_compilation_env_docker_images (ubuntu1804)](https://github.com/apache/incubator-pegasus/actions/runs/5456465090/jobs/9929305540#logs) failed, the logs like:
```
#9 [5/7] RUN pip3 install --no-cache-dir cmake
#9 0.943 Collecting cmake
#9 1.406   Downloading https://files.pythonhosted.org/packages/8a/a7/da765b3f525ac9ab34e1fb700a4b95307ba33bc408461714b61e0938dd88/cmake-3.26.4.tar.gz
#9 1.499     Complete output from command python setup.py egg_info:
#9 1.501     Traceback (most recent call last):
#9 1.501       File ""<string>"", line 1, in <module>
#9 1.501     ModuleNotFoundError: No module named 'setuptools'
#9 1.501     
#9 1.501     ----------------------------------------
#9 1.505 Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-hs7rd736/cmake/
#9 ERROR: process ""/bin/sh -c pip3 install --no-cache-dir cmake"" did not complete successfully: exit code: 1
------
 > [5/7] RUN pip3 install --no-cache-dir cmake:
0.943 Collecting cmake
1.406   Downloading https://files.pythonhosted.org/packages/8a/a7/da765b3f525ac9ab34e1fb700a4b95307ba33bc408461714b61e0938dd88/cmake-3.26.4.tar.gz
1.499     Complete output from command python setup.py egg_info:
1.501     Traceback (most recent call last):
1.501       File ""<string>"", line 1, in <module>
1.501     ModuleNotFoundError: No module named 'setuptools'
1.501     
1.501     ----------------------------------------
1.505 Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-hs7rd736/cmake/
------
Dockerfile:63
--------------------
  61 |         rm -rf /var/lib/apt/lists/*
  62 |     
  63 | >>> RUN pip3 install --no-cache-dir cmake
  64 |     
  65 |     RUN wget --progress=dot:giga https://github.com/apache/thrift/archive/refs/tags/0.11.0.tar.gz -P /opt/thrift && \
--------------------
ERROR: failed to solve: process ""/bin/sh -c pip3 install --no-cache-dir cmake"" did not complete successfully: exit code: 1
Error: buildx failed with: ERROR: failed to solve: process ""/bin/sh -c pip3 install --no-cache-dir cmake"" did not complete successfully: exit code: 1
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1559/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1560,https://api.github.com/repos/apache/incubator-pegasus/issues/1560,incubator-pegasus,1791154574,1560,"fix：Add a flag ""rocksdb_write_global_seqno"" and set it to false by default.",ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,CLOSED,2023-07-06T09:19:27Z,2023-07-07T02:32:42Z,"**Is your feature request related to a problem? Please describe:**

In the production environment, Pegasus performs bulkload involving rocksdb ingestion. Since write_global_seqno is true, rocksdb will modify the `rocksdb.external_sst_file.global_seqno` field in the external sstable file during ingestion. The modified is sometimes inaccurate, which will cause rocksdb fail to read the external sstable file. This will cause rocksdb coredump.

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->
After research, I verified that write_global_seqno can be set to false.

The following are conclusions：
- `write_global_seqno=false` will give up modify the external sst file. `rocksdb.external_sst_file.global_seqno` will always be zero. The global seqno information is retained by **MANIFEST** (smallest_seqno and largest_seqno field), and no additional performance overhead will be generated. In rocksdb, the order of external files and internal files is still identified through global seqno.
- `write_global_seqno=false` does not conflict with the ingest_behind function of bulkload. Read, write, and delete operations are performed normally. The sst file is no longer modified during the ingest process. Speed up the ingest speed and check the sst file through checksum.
- **Disadvantages**: Not compatible with versions prior to rocksdb 5.16.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1560/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1562,https://api.github.com/repos/apache/incubator-pegasus/issues/1562,incubator-pegasus,1792660774,1562,"fix: Add a flag ""rocksdb_write_global_seqno"" and set it to false by default.",ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,CLOSED,2023-07-07T03:14:36Z,2023-12-14T15:59:37Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
In the production environment, Pegasus performs bulkload involving rocksdb ingestion. Since write_global_seqno is true, rocksdb will modify the rocksdb.external_sst_file.global_seqno field in the external sstable file during ingestion. The modified is sometimes inaccurate. Later, when rocksdb verifies the field legality, it will cause a coredump.

2. What did you expect to see?
Hope rocksdb will not coredump.
3. What did you see instead?
When the `rocksdb.external_sst_file.global_seqno` field is inaccurate, rocksdb will coredump. The error message is:
```c++
log.1741.txt:71521:D2023-04-23 08:28:50.578 (1682209730578970297 110362) replica.compact7.040500143ab21610: pegasus_server_impl.cpp:2888:do_manual_compact(): [10.6@ip:port] finish CompactRange, status = Corruption: An external sst file with version 2 have global seqno property with value �, while largest seqno in the file is 64789, time_used = 18ms
``` 
The location of the rocksdb source code is:
```c++
  uint32_t version = DecodeFixed32(version_pos->second.c_str());
  if (version < 2) {
    if (seqno_pos != props.end() || version != 1) {
      std::array<char, 200> msg_buf;
      // This is a v1 external sst file, global_seqno is not supported.
      snprintf(msg_buf.data(), msg_buf.max_size(),
               ""An external sst file with version %u have global seqno ""
               ""property with value %s"",
               version, seqno_pos->second.c_str());
      return Status::Corruption(msg_buf.data());
    }
    return Status::OK();
  }
``` 

4. What version of Pegasus are you using?
Not related to Pegasus version.

## Solution

After research, I verified that `write_global_seqno` set to false can solve this problem. The `write_global_seqno` is used to control whether rocksdb modifies `rocksdb.external_sst_file.global_seqno` during ingest process. After I set `write_global_seqno` to false, the above problem will not occur.  Rocksdb 8.0.0 also recommend this.

The following are conclusions:
- **Disadvantages**: Not compatible with versions prior to rocksdb 5.16.
- `write_global_seqno=false` will give up modify the external sst file. rocksdb.external_sst_file.global_seqno will always be zero. The global seqno information is retained by MANIFEST (smallest_seqno and largest_seqno field), and no additional performance overhead will be generated. In rocksdb, the order of external files and internal files is still identified through global seqno.
- `write_global_seqno=false` does not conflict with the ingest_behind function of bulkload. Read, write, and delete operations are performed normally. The sst file is no longer modified during the ingest process. Speed up the ingest speed and check the sst file through checksum.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1562/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1564,https://api.github.com/repos/apache/incubator-pegasus/issues/1564,incubator-pegasus,1795843867,1564,Bug:manual compact unexpected start,ninsmiracle,110282526,,,OPEN,2023-07-10T03:11:56Z,2023-07-10T03:11:56Z,"## Bug Report

  In online usage, Pegasus users often use Spark for offline data bulk loading. In the configuration of Spark tasks, users can set the `manual_compact.periodic.trigger_time parameter` to define the desired time for manual compaction, avoiding peak traffic periods. For example, a user generates a batch of data during the day (2 PM) and imports it using Spark, expecting to perform a manual compaction to reduce disk usage at 1 AM the next day.

  However, in the existing code logic now, when a user performs the Bulkload operation and sets the `manual_compact.periodic.trigger_time`, if the specified time (e.g., 01:00) is earlier than the current time (14:00), an immediate manual compaction is triggered. This unexpected behavior leads to two problems:

1. The user expects to perform two bulk loading operations in a short time. The unexpected manual compaction may prevent the second bulk load from starting.

2. In pegasus duplication, when using full-app duplicate, all parameters of the primary cluster's original app are copied to the corresponding app in the backup cluster. If the primary cluster's table has undergone manual compaction using Spark, the parameters related to manual compaction are also copied. **This leads to the backup cluster's RocksDB instances performing manual compaction before they are fully initialized, resulting in undefined behavior and ultimately causing all nodes in the backup cluster to crash.** Although the SRE can delete all manual compaction parameters of the original app before enabling full duplicate, forgetting to do so will result in serious issues.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1564/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1568,https://api.github.com/repos/apache/incubator-pegasus/issues/1568,incubator-pegasus,1809351877,1568,pegasus-shell supports user to execute kinit outside the pegasus program,WHBANG,38547944,,,CLOSED,2023-07-18T07:46:35Z,2023-10-17T16:22:18Z,"pegasus-shell supports user to execute kinit outside the pegasus program, pegasus does not need to do kinit behavior, it directly obtains authentication information from the system unix account, like kudu, impala-shell.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1568/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1570,https://api.github.com/repos/apache/incubator-pegasus/issues/1570,incubator-pegasus,1821868714,1570,feat enhance : Ignore specified unhealthy tables during load balancing.,ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,OPEN,2023-07-26T08:37:56Z,2023-07-27T06:45:55Z,"## background
The current blacklist feature allows ignoring some apps during the load balancing process. However, a drawback is that load balancing is triggered only when all partitions are healthy.
## Goals
If all unhealthy apps are in the blacklist, load balancing can still start.
## Solution
1. Modifiy load balancing start condition.
2. Refactor the existing blacklist, and only transfer the required apps to the load balancer.
3. Add unit tests.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1570/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1572,https://api.github.com/repos/apache/incubator-pegasus/issues/1572,incubator-pegasus,1828341344,1572,All replicas are removed,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-07-31T03:40:11Z,2023-07-31T13:39:40Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
The slogs are corrupted on more than 3 replcia servers in a standard Pegasus cluster.
(If the slog is corrupted in a single node cluster, it would cause the same issue)
> The reason of slog corruption is there is a previous crash of replica server, it encountered a large amount of EAGAIN error when `io_submit()`, that's another issue we would handle (`io_submit()` has been replaced by `pwrite()` since 2.2)

2. What did you expect to see?
The replicas are kept in the normal directories, we can recover the table manually.

3. What did you see instead?
Some partitions' all replicas are moved to ""<app_id>.<partition_id>.pegasus.<timestamp>.err"", the cluster is not able to recover automatically, we have to find and move the replicas back one by one.
```
E2023-07-29 03:17:32.797 (1690571852797613997 1eb83) replica.default0.0000eb5600010001: mutation_log.cpp:2048:read_next_log_block(): invalid data header magic: 0x0
D2023-07-29 03:17:32.797 (1690571852797631915 1eb83) replica.default0.0000eb5600010001: mutation_log_replay.cpp:41:replay(): finish to replay mutation log (/data3/sa_cluster/skv_offline/replica/slog/log.139.18306729623) [err: ERR_INVALID_DATA: failed to read log block]
E2023-07-29 03:17:32.797 (1690571852797648461 1eb83) replica.default0.0000eb5600010001: mutation_log_replay.cpp:189:replay(): replay mutation log failed: ERR_INVALID_DATA
E2023-07-29 03:17:32.798 (1690571852798053578 1eb83) replica.default0.0000eb5600010001: replica_stub.cpp:505:initialize(): replay shared log failed, err = ERR_INVALID_DATA, time_used = 618 ms, clear all logs ...
E2023-07-29 03:17:32.803 (1690571852803234241 1eb83) replica.default0.0000eb5600010001: pegasus_server_impl.cpp:2777:flush_all_family_columns(): [11.203@x.x.x.x:8171] flush failed, error = Shutdown in progress:
D2023-07-29 03:17:32.804 (1690571852804425084 1eb83) replica.default0.0000eb5600010001: pegasus_server_impl.cpp:1638:stop(): 11.203@x.x.x.x:8171: close app succeed, clear_state = false
D2023-07-29 03:17:32.804 (1690571852804709002 1eb83) replica.default0.0000eb5600010001: replica.cpp:419:close(): 11.203@x.x.x.x:8171: replica closed, time_used = 6ms
W2023-07-29 03:17:32.804 (1690571852804765679 1eb83) replica.default0.0000eb5600010001: replica_stub.cpp:524:initialize(): init_replica: {replica_dir_op} succeed to move directory '/data1/sa_cluster/skv_offline/replica/reps/11.203.pegasus' to '/data1/sa_cluster/skv_offline/replica/reps/11.203.pegasus.1690571852804715.err'
```

4. What version of Pegasus are you using?
2.0
(2.4 has the same issue)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1572/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1572,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5iy63E,incubator-pegasus,1657515460,1572,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-07-31T04:11:56Z,2023-07-31T04:11:56Z,"slog will not be written since 2.4, but it will still be replayed in 2.4, if the slog is corrupted for some reason, the issue will be reproduced.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5iy63E/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1575,https://api.github.com/repos/apache/incubator-pegasus/issues/1575,incubator-pegasus,1830436998,1575,Data at rest encryption,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-08-01T04:23:46Z,2024-02-28T04:24:21Z,"# Motivation

There are some Pegasus users that store privacy data in Pegasus, it’s important to protect the data against unauthorized access by persons who gain access to the storage media used by Pegasus.

It's possible to support transparent data at rest encryption to provide a way to protect users’ data, which is transparent to users and straightforward to set up for operators.

Data at rest encryption refers to encrypting data for storage and decrypting it when reading the stored data. It uses symmetric encryption where the same key is used to encrypt and to decrypt the data. Keys need to be stored and handled securely as anyone with access to a key will be able to decrypt any data encrypted with it.

# Cloud disk encryption

If your Pegasus clusters are deployed on public cloud service storages, it’s possible to use their own encryption solutions. See:

- [Amazon EBS Encryption](https://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/EBSEncryption.html)
- [Aliyun ECS Encryption](https://help.aliyun.com/document_detail/59643.html?spm=a2c4g.59643.0.0)
- [Tencent cloud CBS Encryption](https://cloud.tencent.com/document/product/362/38946)
- [Huawei cloud EVS Encryption](https://support.huaweicloud.com/productdesc-evs/evs_01_0001.html)

It’s not needed to enable Pegasus Data at rest encryption to avoid encrypting/decrypting data twice, which may lead to poor performance.

# Goals

- Data at rest encryption of all user data (key-values) on a fresh Pegasus cluster.
- Pluggable key management to enable interfacing with existing key management systems, such as [Hadoop KMS](https://hadoop.apache.org/docs/stable/hadoop-kms/index.html#KMS_HTTP_REST_API).
- Cluster key architecture (see Key management).
- User data in logs will be redacted.

# Non-Goals

- Enabling (or disabling) data at rest encryption on an existing cluster
  TODO: It's possible to implement this, after all the data been full compacted, the data could transfer to plaintext/ciphertext.
- Multiple tenants, or table granularity encryption.
  TODO: It's possible to implement this after the cluster granularity encryption been implemented.
- Selective encryption (certain tables are encrypted, others are not)
  TODO: Same to the above.
- Encrypt data of shell-tools output.
- Transport layer encryption.
  TODO: Use TLS libs.
- Core dump encryption.
- [pegasus-spark](https://github.com/pegasus-kv/pegasus-spark)
  pegasus-spark only supports to read plaintext data from source, the generated data is in plaintext as well, it doesn't break the security. When load the generated plaintext data into Pegasus, the data will be encrypted if the encrypt_data_at_rest feature is enabled.

# Cryptography overview

[Symmetric-key algorithm](https://en.wikipedia.org/wiki/Symmetric-key_algorithm)

Symmetric-key algorithms are [algorithms](https://en.wikipedia.org/wiki/Algorithm) for [cryptography](https://en.wikipedia.org/wiki/Cryptography) that use the same [cryptographic keys](https://en.wikipedia.org/wiki/Key_(cryptography)) for both the encryption of [plaintext](https://en.wikipedia.org/wiki/Plaintext) and the decryption of [cipher-text](https://en.wikipedia.org/wiki/Ciphertext). The keys may be identical, or there may be a simple transformation to go between the two keys. The keys, in practice, represent a shared secret between two or more parties that can be used to maintain a private information link. The requirement that both parties have access to the secret key is one of the main drawbacks of symmetric-key encryption, in comparison to [public-key encryption](https://en.wikipedia.org/wiki/Public_key_encryption) (also known as asymmetric-key encryption).

[AES](https://en.wikipedia.org/wiki/Advanced_Encryption_Standard)

Advanced Encryption Standard, is a block cipher with a block size of 128 bits, but three different key lengths: 128, 192 and 256 bits. AES supersedes the Data Encryption Standard (DES), the algorithm described by AES is a symmetric-key algorithm, meaning the same key is used for both encrypting and decrypting the data.

[Block cipher](https://en.wikipedia.org/wiki/Block_cipher)

A block cipher is a deterministic algorithm that operates on fixed-length groups of bits, called blocks. Block ciphers are the elementary building blocks of many cryptographic protocols. They are ubiquitous in the storage and exchange of data, where such data is secured and authenticated via encryption.

A block cipher uses blocks as an unvarying transformation. Even a secure block cipher is suitable for the encryption of only a single block of data at a time, using a fixed key. A multitude of modes of operation have been designed to allow their repeated use in a secure way to achieve the security goals of confidentiality and authenticity. However, block ciphers may also feature as building blocks in other cryptographic protocols, such as universal hash functions and pseudorandom number generators.

[ROT13](https://en.wikipedia.org/wiki/ROT13)

ROT13 (""rotate by 13 places"") is a simple letter substitution cipher that replaces a letter with the 13th letter after it in the latin alphabet.

Because there are 26 letters (2×13) in the basic Latin alphabet, ROT13 is its own inverse; that is, to undo ROT13, the same algorithm is applied, so the same action can be used for encoding and decoding. The algorithm provides virtually no cryptographic security, and is often cited as a canonical example of weak encryption.

facebook/rocksdb uses ROT13 as an encryption sample.

[block cipher mode of operation](https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation)

In cryptography, a block cipher mode of operation is an algorithm that uses a block cipher to provide information security such as confidentiality or authenticity. A block cipher by itself is only suitable for the secure cryptographic transformation (encryption or decryption) of one fixed-length group of bits called a block. A mode of operation describes how to repeatedly apply a cipher's single-block operation to securely transform amounts of data larger than a block.

Most modes require a unique binary sequence, often called an initialization vector (IV), for each encryption operation. The IV has to be non-repeating and, for some modes, random as well. The initialization vector is used to ensure distinct ciphertexts are produced even when the same plaintext is encrypted multiple times independently with the same key. Block ciphers may be capable of operating on more than one block size, but during transformation the block size is always fixed. Block cipher modes operate on whole blocks and require that the last part of the data be padded to a full block if it is smaller than the current block size. There are, however, modes that do not require padding because they effectively use a block cipher as a stream cipher.

[IV，Initialization Vector](https://en.wikipedia.org/wiki/Initialization_vector)

In cryptography, an initialization vector (IV) or starting variable (SV) is an input to a cryptographic primitive being used to provide the initial state. The IV is typically required to be random or pseudorandom, but sometimes an IV only needs to be unpredictable or unique. Randomization is crucial for some encryption schemes to achieve semantic security, a property whereby repeated usage of the scheme under the same key does not allow an attacker to infer relationships between (potentially similar) segments of the encrypted message. For block ciphers, the use of an IV is described by the modes of operation.

[CTR, Counter mode](https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation#CTR)

Counter mode turns a block cipher into a stream cipher. It generates the next keystream block by encrypting successive values of a ""counter"". The counter can be any function which produces a sequence which is guaranteed not to repeat for a long time, although an actual increment-by-one counter is the simplest and most popular. The usage of a simple deterministic input function used to be controversial; critics argued that ""deliberately exposing a cryptosystem to a known systematic input represents an unnecessary risk"". However, today CTR mode is widely accepted, and any problems are considered a weakness of the underlying block cipher, which is expected to be secure regardless of systemic bias in its input. Along with CBC, CTR mode is one of two block cipher modes recommended by Niels Ferguson and Bruce Schneier.

[OpenSSL](https://en.wikipedia.org/wiki/OpenSSL)

OpenSSL contains an open-source implementation of the SSL and TLS protocols. The core library, written in the C programming language, implements basic cryptographic functions and provides various utility functions. Wrappers allowing the use of the OpenSSL library in a variety of computer languages are available.

OpenSSL supports a number of different cryptographic algorithms, including AES mentioned above.

# Design

## Key management

> Most of the design and implementation is inspired by Apache Kudu and TiKV, see [Kudu data at rest encryption](https://docs.google.com/document/d/1rrQtAU4LPgAUi6fUsQZTGME6Yl6ranWjfItnUhluCQE/edit#heading=h.87x03dnkjvch) and [TiKV encryption](https://github.com/tikv/rocksdb/pull/279), thanks to the two projects!

For Pegasus , overview of the design:

- Each disk file uses an independent File Key (FK) to encrypt data.
- FK is generated locally.
- FK is encrypted (as Encrypted FK, EFK) and store in the newly added file header of the file it used to encrypt/decrypt.
- Each disk file has a fixed length file header to store encryption information (including EFK).
- FK is encrypted by using the independent Server Key (SK) of each server as EFK.
- SK is encrypted by using the shared Cluster Key (CK) among the servers in a Pegasus cluster as ESK.
- Adds a new instance file on each server to store ESK.
- ESK is encrypted by using the Cluster Key (CK), and stored in the instance file.
- The plaintext SK is generated/obtained from the remote KMS by RESTful API:
  - GET `<kms_url>/v1/key/<cluster_key_name>/_eek?eek_op=generate&num_keys=1`
- When the server bootstrap, the local ESK is decrypted by using the remote KMS, then the plaintext SK is stored in memory. Decrypt the ESK by KMS RESTful API:
  - POST `<kms_url>/v1/keyversion/<key_version>/_eek?eek_op=decrypt`
    with payload:
    - cluster_key_name
    - iv
    - ESK
- SK is used in`rocksdb::EncryptedEnv` to encrypt and encrypt FK.

## New Configurations

- encrypt_data_at_rest

  bool(false), Whether sensitive files should be encrypted on the file system.

- encryption_key_length

  int(128), Encryption key length. Can be 128, 192 or 256.

- encryption_key_provider

  string(""default""), Key provider implementation to generate and decrypt server keys. Valid values are: 'default' (not for production usage), and 'hadoop-kms'.

- hadoop_kms_url

  string(""""), Comma-separated list of Hadoop KMS server URLs. Must be set when 'encryption_key_provider' is set to 'hadoop-kms'.

- encryption_cluster_key_name

  string(""kudu_cluster_key""), Name of the cluster key that is used to encrypt server encryption keys as stored in Hadoop KMS.

- redact_logs

  bool(false), Whether sensitive data (e.g. keys, values, table names) in logs should be redacted.

# Implementation overview

## RocksDB

### Encryption file header

Encrypted Env has [a fixed length of header](https://github.com/pegasus-kv/rocksdb/blob/v8.3.2-pegasus/include/rocksdb/env_encryption.h#L148C18-L148C18), we can define it as 4096 (one page size).
The first of 64 bytes are used to store encryption information, including:

```
char magic[7];         // ""encrypt""
uint8_t algorithm[1];  // Encryption algorithm, e.g. AES128/192/256CTR
char file_key[32];     // 32 bytes length of EFK
// char file_key[24];  // reserved
```

### Encryption data

facebook/rocksdb uses ROT13 to encrypt data, it’s just a sample and can not be used in a product environment, we will use AES encryption algorithms.

tikv/rocksdb and Kudu have implemented AES encryption algorithms by using OpenSSL, we will use OpenSSL library as well.

### Git repository

Because we are planning to add AES encryption on RocksDB, I guess it would a long journey to merge the modify code into the upstream facebook/rocksdb repository, so I suggest to maintenance Pegasus owned git repository (i.e. https://github.com/pegasus-kv/rocksdb), we can commit the patches to the upstream when the feature is fully tested and stable.

Now Pegasus uses official RocksDB 6.6.4, it’s a chance to upgrade the third-party library to the latest stable version (8.3.2 when write the doc).

## Pegasus

### Git repository

I'm planning to develop the functionality on the master branch of apache/incubator-pegasus after the 2.5 branch has been created.

### Modules updates

#### native_linux_aio_provider

In fact the `native_linux_aio_provider` module doesn't use AIO since Pegasus 2.2.0, instead it uses `pwrite` and `pread` .

RocksDB uses `pwrite` and `pread` too, it's possible to replace the underlying implementation of filesystem of Pegasus by `rocksdb::Env` .

 `rocksdb::Env`  has a plenty of file operation features, includes mmap, direct io, prefetch, preallocate, encryption at rest, and so on, they are public APIs of RocksDB library, and we believe in the stability of RocksDB.

So we will introduce  `rocksdb::Env`  to Pegasus as the underlying implementation of filesystem layer.

#### plog

`plog` uses `native_linux_aio_provider`, if `native_linux_aio_provider` has implemented data at rest encryption, `plog` has this feature logically.

#### nfs

The nfs module is used to transfer files (e.g. rocksdb SST files) between replica servers. The files are encrypted if data at rest encryption is enabled, and different replica servers have different SK, so the nfs server side should support to decrypt data when uploading (by using the soure SK), the nfs client side should support to encrypt data when downloading (by using the target SK).

The nfs module uses `native_linux_aio_provider` too, so it's convenient to support encryption for nfs module.

#### block service

The block server module is used to backup and restore data, it supports 3 type of targets, including local filesystem, [Xiaomi FDS](https://docs.api.xiaomi.com/fds/introduction.html) and Apache HDFS. We should also provide the encryption ability of block service to ensure the data security. However, the corresponding SK is needed to be backed up and restored along with the data, the backup SK will be used to decrypt data when downloading in restore stage, and the data will be encrypted again by using the replica server's own SK when writing in restore stage.

#### logs

User key-values printed in logs should be redacted.

#### others

Some other modules which read/write files are possible to use rocksdb::Env to refactor as well, e.g. the replica_app_info module.

# Roadmap

## Prepare the rocksdb repository

Commits are merged to https://github.com/pegasus-kv/rocksdb/tree/v8.3.2-pegasus-encrypt firstly.

- https://github.com/pegasus-kv/rocksdb/pull/3
- https://github.com/pegasus-kv/rocksdb/pull/1

## Cherry-pick encryption related commits from TiKV

Commits are cherryp-icked from branch https://github.com/tikv/rocksdb/commits/6.29.tikv

- https://github.com/pegasus-kv/rocksdb/pull/4
- https://github.com/pegasus-kv/rocksdb/pull/5
- https://github.com/pegasus-kv/rocksdb/pull/6
- https://github.com/pegasus-kv/rocksdb/pull/7
- https://github.com/pegasus-kv/rocksdb/pull/8
- https://github.com/pegasus-kv/rocksdb/pull/9
- https://github.com/pegasus-kv/rocksdb/pull/10
- https://github.com/pegasus-kv/rocksdb/pull/11
- https://github.com/pegasus-kv/rocksdb/pull/12
- https://github.com/pegasus-kv/rocksdb/pull/13

## Remove the key manager
- https://github.com/pegasus-kv/rocksdb/pull/16

## Implement the self-served file key managment
- https://github.com/pegasus-kv/rocksdb/pull/16

## Update rocksdb to 8.5.3
- https://github.com/pegasus-kv/rocksdb/pull/18
- https://github.com/apache/incubator-pegasus/pull/1601
- https://github.com/apache/incubator-pegasus/pull/1607
- https://github.com/apache/incubator-pegasus/pull/1610
- https://github.com/apache/incubator-pegasus/pull/1612

## Other fixes of pegasus-kv/rocksdb
- https://github.com/pegasus-kv/rocksdb/pull/19
- https://github.com/apache/incubator-pegasus/pull/1614

## Pegasus use rocksdb::EncryptedEnv when data at rest encryption enabled
- https://github.com/apache/incubator-pegasus/pull/1606

## Refactor Pegasus to use rocksdb::Env to access other disk files
- https://github.com/apache/incubator-pegasus/pull/1624
- https://github.com/apache/incubator-pegasus/pull/1637
- https://github.com/apache/incubator-pegasus/pull/1599
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1575/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1575,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5i7ewb,incubator-pegasus,1659759643,1575,NA,GiantKing,10461183,,giantkingww@163.com,NA,2023-08-01T07:51:23Z,2023-08-01T07:51:23Z,"1. The config unit for encrypt_data_at_rest is cluster? Why not table?
2. In bulkload process, we generate the underlying data file by spark. So we need to ensure the data parser/generator of RocksDB is running well.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5i7ewb/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1575,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5i7rjE,incubator-pegasus,1659812036,1575,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-08-01T08:20:56Z,2023-08-01T08:20:56Z,"Hi @GiantKing , thanks for your reply!

1. In the first step, I just want to implement cluster granularity encryption. It would be easy to extend to table granularity encryption.
2. Sorry I didn't get your key point, does the design break some rules?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5i7rjE/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1575,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5i77qP,incubator-pegasus,1659878031,1575,NA,kirbyzhou,3401630,,,NA,2023-08-01T08:58:15Z,2023-08-01T08:58:15Z,Missing the authentication credentials required to connect KMS.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5i77qP/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1575,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5i78uj,incubator-pegasus,1659882403,1575,NA,kirbyzhou,3401630,,,NA,2023-08-01T09:00:52Z,2023-08-01T09:00:52Z,"Good Question
It depends on how RocksDb is distributed among multiple replication server.

One possible solution is that:
Spark encrypts FK of each rocksdb with a unified BK, then import them into pegasus with BK.
Pegasus use BK to decrypt FK then re-encrypt FK with its own SK and write into the header of rocksdb.




> * In bulkload process, we generate the underlying data file by spark. So we need to ensure the data parser/generator of RocksDB is running well.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5i78uj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1575,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5i8NeP,incubator-pegasus,1659950991,1575,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-08-01T09:40:03Z,2023-08-01T09:40:03Z,"> * pegasus-spark only supports to read plaintext data from source, the generated data is in plaintext as well, it doesn't break the security. When load the generated plaintext data into Pegasus, the data will be encrypted if the encrypt_data_at_rest feature is enabled.

I added this as a non-goal.

> pegasus-spark only supports to read plaintext data from source, the generated data is in plaintext as well, it doesn't break the security. When load the generated plaintext data into Pegasus, the data will be encrypted if the encrypt_data_at_rest feature is enabled.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5i8NeP/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1575,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5mU14Q,incubator-pegasus,1716739600,1575,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-09-13T00:02:54Z,2023-09-13T00:02:54Z,"Another pull request to facebook/rocksdb, https://github.com/facebook/rocksdb/pull/7020, but it seems not updated near 3 years.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5mU14Q/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1576,https://api.github.com/repos/apache/incubator-pegasus/issues/1576,incubator-pegasus,1830921060,1576,Free more disk space for workflows on Github,empiredan,743379,Dan Wang,,CLOSED,2023-08-01T10:08:51Z,2023-12-14T15:59:21Z,"We've reduce the usage of disk space for workflows on Github, it's not enough:

- https://github.com/apache/incubator-pegasus/pull/1486
- https://github.com/apache/incubator-pegasus/pull/1498

However, as more and more workflows on Github have failed for the reason ""No space left on device"", we have to free more disk space to get them to work normally again:
![image](https://github.com/apache/incubator-pegasus/assets/743379/7fb4b3b6-44d3-4faf-9f3d-504ef3a57f96)

We could reference what we've done for branch [migrate-metrics-dev](https://github.com/apache/incubator-pegasus/tree/migrate-metrics-dev): https://github.com/apache/incubator-pegasus/pull/1537

Also, some tests failed due to SPACE_INSUFFICIENT:
![image](https://github.com/apache/incubator-pegasus/assets/743379/10685de8-b8ae-4922-91a4-53257087e232)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1576/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1579,https://api.github.com/repos/apache/incubator-pegasus/issues/1579,incubator-pegasus,1833261911,1579,Support fatal logging for logging_provider::dsn_logv and fix assertion,empiredan,743379,Dan Wang,,CLOSED,2023-08-02T14:06:34Z,2023-10-17T16:22:38Z,"Though we've made process core dump for fatal logging, there are still some problems left:

- `simple_logger::dsn_logv` would not core dump for fatal logging;
- In the code, assertion called fatal logging twice (then called core dump), where actually it would just core dump at the first call for fatal logging;
- Unit tests for logging in dsn_utils_tests exited abnormally as below, since fatal logging would lead to termination of the process.

```
I2023-08-03 05:17:04.62 (1691039824062930088 244) : sortkey = \x00%d\x00\x01%n/nm
W2023-08-03 05:17:04.62 (1691039824062951288 244) : sortkey = \x00%d\x00\x01%n/nm
E2023-08-03 05:17:04.62 (1691039824062954888 244) : sortkey = \x00%d\x00\x01%n/nm
F2023-08-03 05:17:04.62 (1691039824062967788 244) : sortkey = \x00%d\x00\x01%n/nm
Error: Process completed with exit code 1.
``` ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1579/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1582,https://api.github.com/repos/apache/incubator-pegasus/issues/1582,incubator-pegasus,1853199396,1582,Feature: support http client API for the components that need to access http services,empiredan,743379,Dan Wang,,OPEN,2023-08-16T13:01:31Z,2023-08-16T13:01:31Z,"According to https://github.com/apache/incubator-pegasus/issues/1206, metrics would be fetched by RESTful API.

In Pegasus, some components require access to http services. For example, Pegasus shell needs to access metrics. Thus, while providing RESTful service, we should also offer http client API to for the components that need to access http services.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1582/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1584,https://api.github.com/repos/apache/incubator-pegasus/issues/1584,incubator-pegasus,1854375002,1584,Release 2.5.0,empiredan,743379,Dan Wang,,CLOSED,2023-08-17T06:53:00Z,2024-03-13T01:45:43Z,"# Release Notes

https://github.com/apache/incubator-pegasus/blob/50563ea33b33c5ae1a6d9b8e4cf74a7d1d9133b0/HISTORY.md

# Change List

All of the changes that are made in version 2.5.0 are list here.

## Behavior changes

- https://github.com/apache/incubator-pegasus/pull/1048
- https://github.com/apache/incubator-pegasus/pull/1200
- https://github.com/apache/incubator-pegasus/pull/1638
- https://github.com/apache/incubator-pegasus/pull/1553
- https://github.com/apache/incubator-pegasus/pull/1557

## New Features

- https://github.com/apache/incubator-pegasus/pull/1094
- https://github.com/apache/incubator-pegasus/pull/1132
- https://github.com/apache/incubator-pegasus/pull/1133
- https://github.com/apache/incubator-pegasus/pull/1148
- https://github.com/apache/incubator-pegasus/pull/1272
- https://github.com/apache/incubator-pegasus/pull/1350
- https://github.com/apache/incubator-pegasus/pull/1471
- https://github.com/apache/incubator-pegasus/pull/1511
- https://github.com/apache/incubator-pegasus/pull/1544

Some bigger features with multiple pull requests are listed as below:

### Access control based on Apache Ranger

- https://github.com/apache/incubator-pegasus/pull/1360
- https://github.com/apache/incubator-pegasus/pull/1375
- https://github.com/apache/incubator-pegasus/pull/1378
- https://github.com/apache/incubator-pegasus/pull/1379
- https://github.com/apache/incubator-pegasus/pull/1388
- https://github.com/apache/incubator-pegasus/pull/1402
- https://github.com/apache/incubator-pegasus/pull/1433
- https://github.com/apache/incubator-pegasus/pull/1445
- https://github.com/apache/incubator-pegasus/pull/1452
- https://github.com/apache/incubator-pegasus/pull/1500
- https://github.com/apache/incubator-pegasus/pull/1507
- https://github.com/apache/incubator-pegasus/pull/1518
- https://github.com/apache/incubator-pegasus/pull/1569

### Support FQDN

- https://github.com/apache/incubator-pegasus/pull/1430
- https://github.com/apache/incubator-pegasus/pull/1436
- https://github.com/apache/incubator-pegasus/pull/1464
- https://github.com/apache/incubator-pegasus/pull/1496

### Build Feature

- https://github.com/apache/incubator-pegasus/pull/1369
- https://github.com/apache/incubator-pegasus/pull/1370
- https://github.com/apache/incubator-pegasus/pull/1401
- https://github.com/apache/incubator-pegasus/pull/1574

## Feature enhancement

### Count data

- https://github.com/apache/incubator-pegasus/pull/1091

### Jemalloc

- https://github.com/apache/incubator-pegasus/pull/1143

### Metrics

- https://github.com/apache/incubator-pegasus/pull/1196
- https://github.com/apache/incubator-pegasus/pull/1219
- https://github.com/apache/incubator-pegasus/pull/1237
- https://github.com/apache/incubator-pegasus/pull/1258
- https://github.com/apache/incubator-pegasus/pull/1285
- https://github.com/apache/incubator-pegasus/pull/1298
- https://github.com/apache/incubator-pegasus/pull/1302
- https://github.com/apache/incubator-pegasus/pull/1304
- https://github.com/apache/incubator-pegasus/pull/1458

### Building

- https://github.com/apache/incubator-pegasus/pull/1516
- https://github.com/apache/incubator-pegasus/pull/1640

### Other

- https://github.com/apache/incubator-pegasus/pull/1225
- https://github.com/apache/incubator-pegasus/pull/1229
- https://github.com/apache/incubator-pegasus/pull/1198
- https://github.com/apache/incubator-pegasus/pull/1300
- https://github.com/apache/incubator-pegasus/pull/1338


## Code Refactor

### RDSN

- https://github.com/apache/incubator-pegasus/pull/1099
- https://github.com/apache/incubator-pegasus/pull/1189
- https://github.com/apache/incubator-pegasus/pull/1190

### Java Client

- https://github.com/apache/incubator-pegasus/pull/1087
- https://github.com/apache/incubator-pegasus/pull/1104
- https://github.com/apache/incubator-pegasus/pull/1130
- https://github.com/apache/incubator-pegasus/pull/1137
- https://github.com/apache/incubator-pegasus/pull/1152
- https://github.com/apache/incubator-pegasus/pull/1256
- https://github.com/apache/incubator-pegasus/pull/1260
- https://github.com/apache/incubator-pegasus/pull/1266
- https://github.com/apache/incubator-pegasus/pull/1264
- https://github.com/apache/incubator-pegasus/pull/1271
- https://github.com/apache/incubator-pegasus/pull/1273
- https://github.com/apache/incubator-pegasus/pull/1274
- https://github.com/apache/incubator-pegasus/pull/1277
- https://github.com/apache/incubator-pegasus/pull/1278
- https://github.com/apache/incubator-pegasus/pull/1393
- https://github.com/apache/incubator-pegasus/pull/1416

### Test

- https://github.com/apache/incubator-pegasus/pull/1169
- https://github.com/apache/incubator-pegasus/pull/1171
- https://github.com/apache/incubator-pegasus/pull/1179

### Logging

- https://github.com/apache/incubator-pegasus/pull/1201
- https://github.com/apache/incubator-pegasus/pull/1202
- https://github.com/apache/incubator-pegasus/pull/1246
- https://github.com/apache/incubator-pegasus/pull/1250
- https://github.com/apache/incubator-pegasus/pull/1306
- https://github.com/apache/incubator-pegasus/pull/1307
- https://github.com/apache/incubator-pegasus/pull/1310
- https://github.com/apache/incubator-pegasus/pull/1311
- https://github.com/apache/incubator-pegasus/pull/1312
- https://github.com/apache/incubator-pegasus/pull/1315
- https://github.com/apache/incubator-pegasus/pull/1316
- https://github.com/apache/incubator-pegasus/pull/1318
- https://github.com/apache/incubator-pegasus/pull/1319
- https://github.com/apache/incubator-pegasus/pull/1320
- https://github.com/apache/incubator-pegasus/pull/1322
- https://github.com/apache/incubator-pegasus/pull/1457
- https://github.com/apache/incubator-pegasus/pull/1465

### Macro

- https://github.com/apache/incubator-pegasus/pull/1205
- https://github.com/apache/incubator-pegasus/pull/1211
- https://github.com/apache/incubator-pegasus/pull/1212
- https://github.com/apache/incubator-pegasus/pull/1214
- https://github.com/apache/incubator-pegasus/pull/1217
- https://github.com/apache/incubator-pegasus/pull/1226
- https://github.com/apache/incubator-pegasus/pull/1228
- https://github.com/apache/incubator-pegasus/pull/1230
- https://github.com/apache/incubator-pegasus/pull/1253

### IDL

- https://github.com/apache/incubator-pegasus/pull/1284
- https://github.com/apache/incubator-pegasus/pull/1290
- https://github.com/apache/incubator-pegasus/pull/1317
- https://github.com/apache/incubator-pegasus/pull/1335

### Flag/Config

- https://github.com/apache/incubator-pegasus/pull/1308
- https://github.com/apache/incubator-pegasus/pull/1324
- https://github.com/apache/incubator-pegasus/pull/1346
- https://github.com/apache/incubator-pegasus/pull/1352
- https://github.com/apache/incubator-pegasus/pull/1357
- https://github.com/apache/incubator-pegasus/pull/1359
- https://github.com/apache/incubator-pegasus/pull/1362
- https://github.com/apache/incubator-pegasus/pull/1363
- https://github.com/apache/incubator-pegasus/pull/1371

### IWYU

- https://github.com/apache/incubator-pegasus/pull/1354
- https://github.com/apache/incubator-pegasus/pull/1434
- https://github.com/apache/incubator-pegasus/pull/1519

### dir_node status

- https://github.com/apache/incubator-pegasus/pull/1487
- https://github.com/apache/incubator-pegasus/pull/1489

### Metrics

- https://github.com/apache/incubator-pegasus/pull/1192

### Other

- https://github.com/apache/incubator-pegasus/pull/1194
- https://github.com/apache/incubator-pegasus/pull/1238
- https://github.com/apache/incubator-pegasus/pull/1235
- https://github.com/apache/incubator-pegasus/pull/1240
- https://github.com/apache/incubator-pegasus/pull/1241
- https://github.com/apache/incubator-pegasus/pull/1248
- https://github.com/apache/incubator-pegasus/pull/1251
- https://github.com/apache/incubator-pegasus/pull/1287
- https://github.com/apache/incubator-pegasus/pull/1384
- https://github.com/apache/incubator-pegasus/pull/1406
- https://github.com/apache/incubator-pegasus/pull/1423
- https://github.com/apache/incubator-pegasus/pull/1429
- https://github.com/apache/incubator-pegasus/pull/1448
- https://github.com/apache/incubator-pegasus/pull/1456
- https://github.com/apache/incubator-pegasus/pull/1476
- https://github.com/apache/incubator-pegasus/pull/1480
- https://github.com/apache/incubator-pegasus/pull/1477
- https://github.com/apache/incubator-pegasus/pull/1509

## Bug Fix

- https://github.com/apache/incubator-pegasus/pull/1106
- https://github.com/apache/incubator-pegasus/pull/1108
- https://github.com/apache/incubator-pegasus/pull/1155
- https://github.com/apache/incubator-pegasus/pull/1208
- https://github.com/apache/incubator-pegasus/pull/1289
- https://github.com/apache/incubator-pegasus/pull/1340
- https://github.com/apache/incubator-pegasus/pull/1376
- https://github.com/apache/incubator-pegasus/pull/1392
- https://github.com/apache/incubator-pegasus/pull/1411
- https://github.com/apache/incubator-pegasus/pull/1417
- https://github.com/apache/incubator-pegasus/pull/1422
- https://github.com/apache/incubator-pegasus/pull/1428
- https://github.com/apache/incubator-pegasus/pull/1442
- https://github.com/apache/incubator-pegasus/pull/1444
- https://github.com/apache/incubator-pegasus/pull/1451
- https://github.com/apache/incubator-pegasus/pull/1447
- https://github.com/apache/incubator-pegasus/pull/1453
- https://github.com/apache/incubator-pegasus/pull/1468
- https://github.com/apache/incubator-pegasus/pull/1563
- https://github.com/apache/incubator-pegasus/pull/1573
- https://github.com/apache/incubator-pegasus/pull/1580
- https://github.com/apache/incubator-pegasus/pull/1588

### Replica failed again and again due to corruption

- https://github.com/apache/incubator-pegasus/pull/1512
- https://github.com/apache/incubator-pegasus/pull/1522
- https://github.com/apache/incubator-pegasus/pull/1473

### Building

- https://github.com/apache/incubator-pegasus/pull/1521
- https://github.com/apache/incubator-pegasus/pull/1555
- https://github.com/apache/incubator-pegasus/pull/1558
- https://github.com/apache/incubator-pegasus/pull/1648
- https://github.com/apache/incubator-pegasus/pull/1657

### Scripts

- https://github.com/apache/incubator-pegasus/pull/1526
- https://github.com/apache/incubator-pegasus/pull/1494

## Unit Test

- https://github.com/apache/incubator-pegasus/pull/1373

## CI/Workflows

- https://github.com/apache/incubator-pegasus/pull/1114
- https://github.com/apache/incubator-pegasus/pull/1123
- https://github.com/apache/incubator-pegasus/pull/1124
- https://github.com/apache/incubator-pegasus/pull/1127
- https://github.com/apache/incubator-pegasus/pull/1136
- https://github.com/apache/incubator-pegasus/pull/1138
- https://github.com/apache/incubator-pegasus/pull/1142
- https://github.com/apache/incubator-pegasus/pull/1149
- https://github.com/apache/incubator-pegasus/pull/1154
- https://github.com/apache/incubator-pegasus/pull/1161
- https://github.com/apache/incubator-pegasus/pull/1163
- https://github.com/apache/incubator-pegasus/pull/1167
- https://github.com/apache/incubator-pegasus/pull/1168
- https://github.com/apache/incubator-pegasus/pull/1182
- https://github.com/apache/incubator-pegasus/pull/1178
- https://github.com/apache/incubator-pegasus/pull/1187
- https://github.com/apache/incubator-pegasus/pull/1186
- https://github.com/apache/incubator-pegasus/pull/1210
- https://github.com/apache/incubator-pegasus/pull/1270
- https://github.com/apache/incubator-pegasus/pull/1292
- https://github.com/apache/incubator-pegasus/pull/1295
- https://github.com/apache/incubator-pegasus/pull/1314
- https://github.com/apache/incubator-pegasus/pull/1332
- https://github.com/apache/incubator-pegasus/pull/1356
- https://github.com/apache/incubator-pegasus/pull/1381
- https://github.com/apache/incubator-pegasus/pull/1395
- https://github.com/apache/incubator-pegasus/pull/1420
- https://github.com/apache/incubator-pegasus/pull/1641
- https://github.com/apache/incubator-pegasus/pull/1646
- https://github.com/apache/incubator-pegasus/pull/1653
- https://github.com/apache/incubator-pegasus/pull/1669

### Free disk space

- https://github.com/apache/incubator-pegasus/pull/1486
- https://github.com/apache/incubator-pegasus/pull/1498
- https://github.com/apache/incubator-pegasus/pull/1530
- https://github.com/apache/incubator-pegasus/pull/1577
- https://github.com/apache/incubator-pegasus/pull/1581
- https://github.com/apache/incubator-pegasus/pull/1586
- https://github.com/apache/incubator-pegasus/pull/1643

## Security

- https://github.com/apache/incubator-pegasus/pull/1222

## Doc

- https://github.com/apache/incubator-pegasus/pull/1180
- https://github.com/apache/incubator-pegasus/pull/1282

## License

- https://github.com/apache/incubator-pegasus/pull/1119
- https://github.com/apache/incubator-pegasus/pull/1121
- https://github.com/apache/incubator-pegasus/pull/1172
- https://github.com/apache/incubator-pegasus/pull/1175
- https://github.com/apache/incubator-pegasus/pull/1672
- https://github.com/apache/incubator-pegasus/pull/1677

# Configuration Changes

There's no configuration removed by 2.5.0. All of the following configurations are added:

```diff
[network]
+ enable_udp = true
[metrics]
+ entity_retirement_delay_ms = 600000
[security]
+ enable_ranger_acl = false
[ddl_client]
+ ddl_client_max_attempt_count = 3
+ ddl_client_retry_interval_ms = 10000
[replication]
+ ignore_broken_disk = true
[ranger]
+ legacy_table_database_mapping_policy_name = __default__
[pegasus.server]
+ rocksdb_write_global_seqno = false
[replication]
+ crash_on_slog_error = false
``` 

All of the pull requests that are related to the added configurations:

- https://github.com/apache/incubator-pegasus/pull/1132
- https://github.com/apache/incubator-pegasus/pull/1304
- https://github.com/apache/incubator-pegasus/pull/1379
- https://github.com/apache/incubator-pegasus/pull/1453
- https://github.com/apache/incubator-pegasus/pull/1477
- https://github.com/apache/incubator-pegasus/pull/1507
- https://github.com/apache/incubator-pegasus/pull/1563
- https://github.com/apache/incubator-pegasus/pull/1574

# Contributors

Thanks all of the contributors!

[acelyc111](https://github.com/acelyc111)
[AlexNodex](https://github.com/AlexNodex)
[Apache9](https://github.com/Apache9)
[empiredan](https://github.com/empiredan)
[foreverneverer](https://github.com/foreverneverer)
[GehaFearless](https://github.com/GehaFearless)
[liangyuanpeng](https://github.com/liangyuanpeng)
[littlepangdi](https://github.com/littlepangdi)
[ninsmiracle](https://github.com/ninsmiracle)
[padmejin](https://github.com/padmejin)
[Praying](https://github.com/Praying)
[ruojieranyishen](https://github.com/ruojieranyishen)
[shalk](https://github.com/shalk)
[Smityz](https://github.com/Smityz)
[totalo](https://github.com/totalo)
[WHBANG](https://github.com/WHBANG)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1584/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1585,https://api.github.com/repos/apache/incubator-pegasus/issues/1585,incubator-pegasus,1860457908,1585,"Daily building cpp for ubuntu 2004 failed frequently due to ""No space left on device""",empiredan,743379,Dan Wang,,CLOSED,2023-08-22T02:55:14Z,2023-12-14T15:59:04Z,"Daily building cpp ([Lint and build regularly](https://github.com/apache/incubator-pegasus/actions/workflows/regular-build.yml)) for ubuntu 2004 failed frequently due to ""No space left on device"" as below:

![image](https://github.com/apache/incubator-pegasus/assets/743379/a88da58c-68d7-4709-91c9-efb4ae3e9d1c)

```
Build Cpp (ubuntu2004, gcc,g++)
System.IO.IOException: No space left on device : '/home/runner/runners/2.308.0/_diag/Worker_20230821-180203-utc.log'
   at System.IO.RandomAccess.WriteAtOffset(SafeFileHandle handle, ReadOnlySpan`1 buffer, Int64 fileOffset)
   at System.IO.Strategies.BufferedFileStreamStrategy.FlushWrite()
   at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder)
   at System.Diagnostics.TextWriterTraceListener.Flush()
   at GitHub.Runner.Common.HostTraceListener.WriteHeader(String source, TraceEventType eventType, Int32 id)
   at GitHub.Runner.Common.HostTraceListener.TraceEvent(TraceEventCache eventCache, String source, TraceEventType eventType, Int32 id, String message)
   at System.Diagnostics.TraceSource.TraceEvent(TraceEventType eventType, Int32 id, String message)
   at GitHub.Runner.Worker.Worker.RunAsync(String pipeIn, String pipeOut)
   at GitHub.Runner.Worker.Program.MainAsync(IHostContext context, String[] args)
System.IO.IOException: No space left on device : '/home/runner/runners/2.308.0/_diag/Worker_20230821-180203-utc.log'
   at System.IO.RandomAccess.WriteAtOffset(SafeFileHandle handle, ReadOnlySpan`1 buffer, Int64 fileOffset)
   at System.IO.Strategies.BufferedFileStreamStrategy.FlushWrite()
   at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder)
   at System.Diagnostics.TextWriterTraceListener.Flush()
   at GitHub.Runner.Common.HostTraceListener.WriteHeader(String source, TraceEventType eventType, Int32 id)
   at GitHub.Runner.Common.HostTraceListener.TraceEvent(TraceEventCache eventCache, String source, TraceEventType eventType, Int32 id, String message)
   at System.Diagnostics.TraceSource.TraceEvent(TraceEventType eventType, Int32 id, String message)
   at GitHub.Runner.Common.Tracing.Error(Exception exception)
   at GitHub.Runner.Worker.Program.MainAsync(IHostContext context, String[] args)
Unhandled exception. System.IO.IOException: No space left on device : '/home/runner/runners/2.308.0/_diag/Worker_20230821-180203-utc.log'
   at System.IO.RandomAccess.WriteAtOffset(SafeFileHandle handle, ReadOnlySpan`1 buffer, Int64 fileOffset)
   at System.IO.Strategies.BufferedFileStreamStrategy.FlushWrite()
   at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder)
   at System.Diagnostics.TextWriterTraceListener.Flush()
   at System.Diagnostics.TraceSource.Flush()
   at GitHub.Runner.Common.TraceManager.Dispose(Boolean disposing)
   at GitHub.Runner.Common.TraceManager.Dispose()
   at GitHub.Runner.Common.HostContext.Dispose(Boolean disposing)
   at GitHub.Runner.Common.HostContext.Dispose()
   at GitHub.Runner.Worker.Program.Main(String[] args)
```

This could be fixed by referring to https://github.com/apache/incubator-pegasus/pull/1577.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1585/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1587,https://api.github.com/repos/apache/incubator-pegasus/issues/1587,incubator-pegasus,1863053629,1587,Fix errors in format string for logging and macro,empiredan,743379,Dan Wang,,CLOSED,2023-08-23T10:33:46Z,2023-10-17T16:23:23Z,"There are some problems in current format string for logging and macro, such as:

### Still use c-style format string, which would not print the correct messages:

```c++
     CHECK(false, ""unsupported filter type: %d"", filter_type);
```


### Call `to_string()` method for the classes that have implemented `operator<<`: 

```c++
    CHECK(!in_dropped,
                  ""adjust position of existing node({}) failed, this is a bug, partition({}.{})"",
                  node.to_string(),
                  config_owner->pid.get_app_id(),
                  config_owner->pid.get_partition_index());
    CHECK(false,
                  ""we can't handle this when create backup policy, err({})"",
                  err.to_string());
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1587/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1589,https://api.github.com/repos/apache/incubator-pegasus/issues/1589,incubator-pegasus,1869890760,1589,Bug:The server crash when balance with duplication,ninsmiracle,110282526,,,OPEN,2023-08-28T14:34:28Z,2023-10-16T16:29:38Z,"## Bug Report
Encountering the same situation as described in issue #693. 
However, I can reproduce this bug to occur 100%.

### Please answer these questions before submitting your issue. Thanks!

This bug appear in online server when the clueter doing duplication and try to do load balance at the same time. This bug make a lot of machine coredump.

## What did you do?
1. Created a large-scale application with a substantial replica size (greater than 10GB).
2. Shutdown two replica servers.
3. Waited for the cluster to achieve complete health.
4. Initiated data streaming (and waited for 30 minutes).
5. Enabled duplication function (and waited for 30 minutes).
6. Restarted the two replica servers.
7. Set the meta level to lively, initiating the balancing process.

## What did you see instead?
server crush with coredump info:
```
#0  dsn::zlock::lock (this=this@entry=0x98)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/runtime/zlocks.cpp:89
89      /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/runtime/zlocks.cpp: No such file or directory.
(gdb) #0  dsn::zlock::lock (this=this@entry=0x98)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/runtime/zlocks.cpp:89
#1  0x00007f7d063eb9af in zauto_lock (lock=..., this=<synthetic pointer>)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/include/dsn/tool-api/zlocks.h:121
#2  dsn::replication::mutation_log::max_commit_on_disk (this=this@entry=0x0)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/replica/mutation_log.cpp:877
#3  0x00007f7d064bcf1b in dsn::replication::load_mutation::run (1678968919
    this=0x2fe2ce380)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/replica/duplication/duplication_pipeline.cpp:45
#4  0x00007f7d06625631 in dsn::task::exec_internal (
    this=this@entry=0xcc81a0870)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/runtime/task/task.cpp:176
#5  0x00007f7d0663ace2 in dsn::task_worker::loop (this=0x2e2e580)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/runtime/task/task_worker.cpp:224
#6  0x00007f7d0663ae60 in dsn::task_worker::run_internal (this=0x2e2e580)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/runtime/task/task_worker.cpp:204
#7  0x00007f7d052b9a2f in execute_native_thread_routine ()
   from /home/work/app/pegasus/alsgsrv-monetization-master/replica/package/bin/libdsn_utils.so
#8  0x00007f7d030c4dc5 in start_thread () from /lib64/libpthread.so.0
#9  0x00007f7d015c373d in clone () from /lib64/libc.so.6
(gdb) quit
```

## What version of Pegasus are you using?
[pegasus2.4](https://github.com/apache/incubator-pegasus/tree/v2.4)

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1589/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1593,https://api.github.com/repos/apache/incubator-pegasus/issues/1593,incubator-pegasus,1874979896,1593,Feature: apply and remove all shared logs,empiredan,743379,Dan Wang,,CLOSED,2023-08-31T07:52:17Z,2023-10-19T15:59:52Z,"In https://github.com/XiaoMi/rdsn/pull/1019 we've written private logs as WAL instead of shared logs.

However, obsolete shared logs that had been applied to rocksdb were not removed since then. Since we've kept at least 1 shared log file which would never be removed, we should change policy of garbage collection to delete all shared logs. ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1593/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1595,https://api.github.com/repos/apache/incubator-pegasus/issues/1595,incubator-pegasus,1875465897,1595,Bug:(duplication) master cluster verify start decree error when restart a pause dup,ninsmiracle,110282526,,,CLOSED,2023-08-31T12:49:27Z,2024-04-03T06:14:18Z,"## Bug Report

When I restart a paused dup app on a recovered servers , the start decree of the duplication looking for will been loss,and those servers coredump agian.
It also has been occured online.

1. What did you do?
- Set meta level to lively firstly.
- Begin a duplication task firstly and wait for 5 minutes.
- Shut down two nodes of the master cluster, and then pause the duplication task immediately.
- Wait for one minutes,and restart the previously shut down nodes.
- Wait for ten minutes.(Give replica enough time to GC plog)
- Resume the duplication task 10 minutes later.
- Nodes which stopped just now core dump.

3. What did you see instead?
There are many coredump with the message ""replica_duplicator::verify_start_decree"" in master cluster.
```
#0  0x00007f968e6a01d7 in raise () from /lib64/libc.so.6
Missing separate debuginfos, use: debuginfo-install glibc-2.17-157.el7_3.1.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.14.1-27.el7_3.x86_64 libcom_err-1.42.9-9.el7.x86_64 libgcc-4.8.5-28.el7_5.1.x86_64 libselinux-2.5-6.el7.x86_64 pcre-8.32-15.el7_2.1.x86_64 zlib-1.2.7-17.el7.x86_64
(gdb) #0  0x00007f968e6a01d7 in raise () from /lib64/libc.so.6
#1  0x00007f968e6a18c8 in abort () from /lib64/libc.so.6
#2  0x00007f969378763e in dsn_coredump ()
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/runtime/service_api_c.cpp:93
#3  0x00007f969364dc54 in dsn::replication::replica_duplicator::verify_start_decree (this=<optimized out>, start_decree=64525717)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/replica/duplication/replica_duplicator.cpp:218   
#4  0x00007f969365ea05 in dsn::replication::load_from_private_log::run (
    this=0xa3016f040)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/replica/duplication/load_from_private_log.cpp:71
#5  0x00007f96937c4631 in dsn::task::exec_internal (
    this=this@entry=0x90e7ce0f0)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/runtime/task/task.cpp:176
#6  0x00007f96937d9ce2 in dsn::task_worker::loop (this=0x1c05600)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/runtime/task/task_worker.cpp:224
#7  0x00007f96937d9e60 in dsn::task_worker::run_internal (this=0x1c05600)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/runtime/task/task_worker.cpp:204
#8  0x00007f9692458a2f in execute_native_thread_routine ()
   from /home/work/app/pegasus/alsgsrv-monetization-master/replica/package/bin/libdsn_utils.so
#9  0x00007f9690263dc5 in start_thread () from /lib64/libpthread.so.0
#10 0x00007f968e76273d in clone () from /lib64/libc.so.6
(gdb) quit
```

4. What version of Pegasus are you using?
[pegasus2.4](https://github.com/apache/incubator-pegasus/tree/v2.4)

5.What happened?
FYI: the black word ""duplicating"" is a parameter of app_info. App_info file store in every replica's data dir.
![restart dup error](https://github.com/apache/incubator-pegasus/assets/110282526/6bc5bc42-d800-430a-98e0-bcea7d19085d)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1595/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1595,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM55Nuej,incubator-pegasus,2033641379,1595,NA,empiredan,743379,Dan Wang,,NA,2024-04-03T06:14:17Z,2024-04-03T06:14:17Z,This issue has been fixed by https://github.com/apache/incubator-pegasus/pull/1608.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM55Nuej/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1596,https://api.github.com/repos/apache/incubator-pegasus/issues/1596,incubator-pegasus,1877263578,1596,Bug:(duplication) master cluster down when duplication checking a private log to load,ninsmiracle,110282526,,,CLOSED,2023-09-01T11:36:54Z,2024-04-03T06:18:03Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!
Master cluster core dump after duplication task running some times.

1. What did you do?
This coredump occured in an online cluster. We begin running a duplication task and after a few days, and some nodes begin coredump.

2. What did you see instead?
We can see that some error happened when duplication checking a plog to load(a staget of duplication).
```
Program terminated with signal 6, Aborted.
#0  0x00007fbfd14e21d7 in raise () from /lib64/libc.so.6
Missing separate debuginfos, use: debuginfo-install bzip2-libs-1.0.6-13.el7.x86_64 elfutils-libelf-0.166-2.el7.x86_64 elfutils-libs-0.166-2.el7.x86_64 glibc-2.17-157.el7_3.1.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.14.1-27.el7_3.x86_64 libattr-2.4.46-12.el7.x86_64 libcap-2.22-8.el7.x86_64 libcom_err-1.42.9-9.el7.x86_64 libgcc-4.8.5-28.el7_5.1.x86_64 libselinux-2.5-6.el7.x86_64 pcre-8.32-15.el7_2.1.x86_64 systemd-libs-219-30.el7_3.8.x86_64 xz-libs-5.2.2-1.el7.x86_64 zlib-1.2.7-17.el7.x86_64
(gdb) #0  0x00007fbfd14e21d7 in raise () from /lib64/libc.so.6
#1  0x00007fbfd14e38c8 in abort () from /lib64/libc.so.6
#2  0x00007fbfd65c963e in dsn_coredump ()
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/runtime/service_api_c.cpp:93
#3  0x00007fbfd63c47c3 in dsn::replication::log_file::log_file (
    this=0x52b031600,
    path=0x672f88718 ""/home/work/ssd2/pegasus/alsgsrv-monetization-master/replica/reps/8.79.pegasus/plog/log.2889.96902391528"", handle=<optimized out>,
    index=<optimized out>, start_offset=96902391528, is_read=<optimized out>)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/replica/log_file.cpp:166
#4  0x00007fbfd63c630e in dsn::replication::log_file::open_read (
    path=0x672f88718 ""/home/work/ssd2/pegasus/alsgsrv-monetization-master/replica/reps/8.79.pegasus/plog/log.2889.96902391528"", err=...)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/replica/log_file.cpp:92
#5  0x00007fbfd63de83a in dsn::replication::log_utils::open_read (path=...,
    file=...)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/replica/mutation_log_utils.cpp:43
#6  0x00007fbfd649feea in dsn::replication::load_from_private_log::find_log_file_to_start (this=this@entry=0x25734b7c0)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/replica/duplication/load_from_private_log.cpp:121
#7  0x00007fbfd64a0a40 in dsn::replication::load_from_private_log::run (
    this=0x25734b7c0)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/replica/duplication/load_from_private_log.cpp:100   
#8  0x00007fbfd6606631 in dsn::task::exec_internal (
    this=this@entry=0x13a800d950)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/runtime/task/task.cpp:176
#9  0x00007fbfd661bce2 in dsn::task_worker::loop (this=0x3535760)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/runtime/task/task_worker.cpp:224
#10 0x00007fbfd661be60 in dsn::task_worker::run_internal (this=0x3535760)
    at /home/jiashuo1/work/incubator-pegasus/src/rdsn/src/runtime/task/task_worker.cpp:204
#11 0x00007fbfd529aa2f in execute_native_thread_routine ()
   from /home/work/app/pegasus/alsgsrv-monetization-master/replica/package/bin/libdsn_utils.so
#12 0x00007fbfd30a5dc5 in start_thread () from /lib64/libpthread.so.0
#13 0x00007fbfd15a473d in clone () from /lib64/libc.so.6
(gdb) quit
```

3. What version of Pegasus are you using?
[pegasus2.4](https://github.com/apache/incubator-pegasus/tree/v2.4)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1596/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1596,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM55Nvqp,incubator-pegasus,2033646249,1596,NA,empiredan,743379,Dan Wang,,NA,2024-04-03T06:18:03Z,2024-04-03T06:18:03Z,This issue has been fixed by https://github.com/apache/incubator-pegasus/pull/1597.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM55Nvqp/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1602,https://api.github.com/repos/apache/incubator-pegasus/issues/1602,incubator-pegasus,1889166934,1602,Make C++17 as required,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-09-10T14:29:02Z,2023-09-13T15:30:40Z,"An overview of C++17:
https://www.cppstories.com/2017/01/cpp17features/

Besides, if we upgrade some thirdparty libraries, they required C++17, for example rocksdb 7.0 (see https://github.com/facebook/rocksdb/releases/tag/v7.0.1)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1602/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1604,https://api.github.com/repos/apache/incubator-pegasus/issues/1604,incubator-pegasus,1890609099,1604,Upgrade thirdparty libraries,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-09-11T14:09:06Z,2023-11-16T03:31:50Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1604/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1631,https://api.github.com/repos/apache/incubator-pegasus/issues/1631,incubator-pegasus,1916782792,1631,Flaky unit test test_rename_path_while_writing,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-09-28T06:09:52Z,2023-10-12T03:57:39Z,"The unit test `HDFSClientTest.test_rename_path_while_writing/0` is flaky:
```
[ RUN      ] HDFSClientTest.test_rename_path_while_writing/0
/__w/incubator-pegasus/incubator-pegasus/src/block_service/test/hdfs_service_test.cpp:457: Failure
Expected: (success_count) > (0), actual: 0 vs 0
0
[  FAILED  ] HDFSClientTest.test_rename_path_while_writing/0, where GetParam() = false (1 ms)
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1631/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1632,https://api.github.com/repos/apache/incubator-pegasus/issues/1632,incubator-pegasus,1916809301,1632,"Building failed for centos 7 due to ""Could not find a package configuration file provided by Snappy"" after rocksdb was upgraded to version 8.5.3",empiredan,743379,Dan Wang,,CLOSED,2023-09-28T06:35:23Z,2023-12-14T05:52:20Z,"In my environment of centos 7, building failed due to ""Could not find a package configuration file provided by Snappy"" as below:
```
-- USE_JEMALLOC = OFF
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE
-- FIND_LIBRARY_USE_LIB64_PATHS = ON
-- Found components for RT
-- RT_INCLUDES = /usr/include
-- RT_LIBRARIES = /usr/lib64/librt.so
-- Found components for DL
-- DL_INCLUDES = /usr/include
-- DL_LIBRARIES = /usr/lib64/libdl.so
-- Found OpenSSL: /usr/lib64/libcrypto.so (found version ""1.0.2k"")
-- CMAKE_PREFIX_PATH = /data/sa_cluster/src/empiredan-pegasus-new/thirdparty/output
-- Found Boost: /data/sa_cluster/src/empiredan-pegasus-new/thirdparty/output/include (found version ""1.69.0"") found components: system filesystem regex
CMake Warning at cmake_modules/BaseFunctions.cmake:338 (find_package):
  By not providing ""Findsnappy.cmake"" in CMAKE_MODULE_PATH this project has
  asked CMake to find a package configuration file provided by ""snappy"", but
  CMake did not find one.

  Could not find a package configuration file provided by ""snappy"" with any
  of the following names:

    snappyConfig.cmake
    snappy-config.cmake

  Add the installation prefix of ""snappy"" to CMAKE_PREFIX_PATH or set
  ""snappy_DIR"" to a directory containing one of the above files.  If ""snappy""
  provides a separate development package or SDK, be sure it has been
  installed.
Call Stack (most recent call first):
  cmake_modules/BaseFunctions.cmake:396 (dsn_setup_thirdparty_libs)
  src/CMakeLists.txt:24 (dsn_common_setup)


-- Found zstd: /lib/libzstd.so
-- Found lz4: /lib64/liblz4.so
CMake Error at /usr/local/share/cmake-3.16/Modules/CMakeFindDependencyMacro.cmake:47 (find_package):
  Could not find a package configuration file provided by ""Snappy"" with any
  of the following names:

    SnappyConfig.cmake
    snappy-config.cmake

  Add the installation prefix of ""Snappy"" to CMAKE_PREFIX_PATH or set
  ""Snappy_DIR"" to a directory containing one of the above files.  If ""Snappy""
  provides a separate development package or SDK, be sure it has been
  installed.
Call Stack (most recent call first):
  thirdparty/output/lib64/cmake/rocksdb/RocksDBConfig.cmake:45 (find_dependency)
  cmake_modules/BaseFunctions.cmake:344 (find_package)
  cmake_modules/BaseFunctions.cmake:396 (dsn_setup_thirdparty_libs)
  src/CMakeLists.txt:24 (dsn_common_setup)


-- Configuring incomplete, errors occurred!
```

Also, building also failed for [Lint and build regularly](https://github.com/apache/incubator-pegasus/actions/workflows/regular-build.yml) after [upgrading rocksdb to 8.5.3](https://github.com/apache/incubator-pegasus/commit/e978e4c78b1436aa0ae06915365f85c601781ba7):
```
Selected compiler gcc 7.3.1
Selected compiler built with --prefix=/opt/rh/devtoolset-7/root/usr
-- ENABLE_GPERF = ON
-- USE_JEMALLOC = OFF
-- CCACHE: /usr/bin/ccache
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE  
-- FIND_LIBRARY_USE_LIB[64](https://github.com/apache/incubator-pegasus/actions/runs/6189196092/job/16802799615#step:6:65)_PATHS = ON
-- Found components for RT
-- RT_INCLUDES = /usr/include
-- RT_LIBRARIES = /usr/lib64/librt.so
-- Found components for DL
-- DL_INCLUDES = /usr/include
-- DL_LIBRARIES = /usr/lib64/libdl.so
-- Found OpenSSL: /usr/lib64/libcrypto.so (found version ""1.0.2k"")  
-- use ccache to speed up compilation
-- CMAKE_PREFIX_PATH = /root/incubator-pegasus/thirdparty/output
-- Found Boost: /root/incubator-pegasus/thirdparty/output/include (found version ""1.[69](https://github.com/apache/incubator-pegasus/actions/runs/6189196092/job/16802799615#step:6:70).0"") found components: system filesystem regex 
CMake Warning at cmake_modules/BaseFunctions.cmake:338 (find_package):
  By not providing ""Findsnappy.cmake"" in CMAKE_MODULE_PATH this project has
  asked CMake to find a package configuration file provided by ""snappy"", but
  CMake did not find one.

  Could not find a package configuration file provided by ""snappy"" with any
  of the following names:

    snappyConfig.cmake
    snappy-config.cmake

  Add the installation prefix of ""snappy"" to CMAKE_PREFIX_PATH or set
  ""snappy_DIR"" to a directory containing one of the above files.  If ""snappy""
  provides a separate development package or SDK, be sure it has been
  installed.
Call Stack (most recent call first):
  cmake_modules/BaseFunctions.cmake:396 (dsn_setup_thirdparty_libs)
  src/CMakeLists.txt:24 (dsn_common_setup)


-- Found zstd: /lib64/libzstd.so  
-- Found lz4: /lib64/liblz4.so  
CMake Error at /usr/local/lib64/python3.6/site-packages/cmake/data/share/cmake-3.27/Modules/CMakeFindDependencyMacro.cmake:[76](https://github.com/apache/incubator-pegasus/actions/runs/6189196092/job/16802799615#step:6:77) (find_package):
  Could not find a package configuration file provided by ""Snappy"" with any
  of the following names:

    SnappyConfig.cmake
    snappy-config.cmake

  Add the installation prefix of ""Snappy"" to CMAKE_PREFIX_PATH or set
  ""Snappy_DIR"" to a directory containing one of the above files.  If ""Snappy""
  provides a separate development package or SDK, be sure it has been
  installed.
Call Stack (most recent call first):
  thirdparty/output/lib64/cmake/rocksdb/RocksDBConfig.cmake:45 (find_dependency)
  cmake_modules/BaseFunctions.cmake:344 (find_package)
  cmake_modules/BaseFunctions.cmake:396 (dsn_setup_thirdparty_libs)
  src/CMakeLists.txt:24 (dsn_common_setup)


-- Configuring incomplete, errors occurred!
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1632/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1632,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5uk-TM,incubator-pegasus,1855186124,1632,NA,empiredan,743379,Dan Wang,,NA,2023-12-14T05:51:57Z,2023-12-14T05:51:57Z,This issue has been resolved by https://github.com/apache/incubator-pegasus/pull/1709.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5uk-TM/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1635,https://api.github.com/repos/apache/incubator-pegasus/issues/1635,incubator-pegasus,1938142187,1635,Jemalloc unit test failed,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-10-11T15:49:14Z,2023-10-12T03:57:30Z,"The failure looks like:
```
[ RUN      ] je_ctl_test.dump_brief_arena_stats
/home/laiyingchun/dev/pegasus_review/src/utils/test/je_ctl_test.cpp:57: Failure
Expected: (stats.find(""Merged arenas stats:"")) != (std::string::npos), actual: 18446744073709551615 vs 18446744073709551615
___ Begin jemalloc statistics ___
Allocated: 2693712, active: 2846720, metadata: 6733656 (n_thp 0), resident: 9637888, mapped: 15511552, retained: 1789952
Background threads: 0, num_runs: 0, run_interval: 0 ns
arenas[0]:
assigned threads: 1
uptime: 10999845
dss allocation precedence: ""secondary""
decaying:  time       npages       sweeps     madvises       purged
   dirty: 10000           20            0            0            0
   muzzy:     0            0            0            0            0
                            allocated         nmalloc (#/sec)         ndalloc (#/sec)       nrequests   (#/sec)           nfill   (#/sec)          nflush   (#/sec)
small:                        1272400           10208   10208             540     540           20241     20241             187       187              30        30
large:                        1421312               8       8               5       5               8         8               8         8               0         0
total:                        2693712           10216   10216             545     545           20249     20249             195       195              30        30

active:                       2846720
mapped:                      15511552
retained:                     1789952
base:                         6704976
internal:                       28680
metadata_thp:                       0
tcache_bytes:                  199808
resident:                     9637888
abandoned_vm:                       0
extent_avail:                       0
--- End jemalloc statistics ---

/home/laiyingchun/dev/pegasus_review/src/utils/test/je_ctl_test.cpp:89: Failure
Expected: check_arena_marks(stats) doesn't generate new fatal failures in the current thread.
  Actual: it does.
[  FAILED  ] je_ctl_test.dump_brief_arena_stats (1 ms)
[ RUN      ] je_ctl_test.dump_detailed_stats
/home/laiyingchun/dev/pegasus_review/src/utils/test/je_ctl_test.cpp:60: Failure
Expected: (stats.find(""arenas[0]:"")) != (std::string::npos), actual: 18446744073709551615 vs 18446744073709551615
___ Begin jemalloc statistics ___
Version: ""5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756""
Build-time option settings
  config.cache_oblivious: true
  config.debug: false
  config.fill: true
  config.lazy_lock: false
  config.malloc_conf: """"
  config.opt_safety_checks: false
  config.prof: true
  config.prof_libgcc: true
  config.prof_libunwind: false
  config.stats: true
  config.utrace: false
  config.xmalloc: false
Run-time option settings
  opt.abort: false
  opt.abort_conf: false
  opt.confirm_conf: false
  opt.retain: true
  opt.dss: ""secondary""
  opt.narenas: 128
  opt.percpu_arena: ""disabled""
  opt.oversize_threshold: 8388608
  opt.metadata_thp: ""disabled""
  opt.background_thread: false (background_thread: false)
  opt.dirty_decay_ms: 10000 (arenas.dirty_decay_ms: 10000)
  opt.muzzy_decay_ms: 0 (arenas.muzzy_decay_ms: 0)
  opt.lg_extent_max_active_fit: 6
  opt.junk: ""false""
  opt.zero: false
  opt.tcache: true
  opt.lg_tcache_max: 15
  opt.thp: ""default""
  opt.prof: false
  opt.prof_prefix: ""jeprof""
  opt.prof_active: true (prof.active: false)
  opt.prof_thread_active_init: true (prof.thread_active_init: false)
  opt.lg_prof_sample: 19 (prof.lg_sample: 0)
  opt.prof_accum: false
  opt.lg_prof_interval: -1
  opt.prof_gdump: false
  opt.prof_final: false
  opt.prof_leak: false
  opt.stats_print: false
  opt.stats_print_opts: """"
Profiling settings
  prof.thread_active_init: false
  prof.active: false
  prof.gdump: false
  prof.interval: 0
  prof.lg_sample: 0
Arenas: 129
Quantum size: 16
Page size: 4096
Maximum thread-cached size class: 32768
Number of bin size classes: 36
Number of thread-cache bin size classes: 41
Number of large size classes: 196
Allocated: 4006992, active: 4165632, metadata: 6733912 (n_thp 0), resident: 12263424, mapped: 18137088, retained: 2310144
Background threads: 0, num_runs: 0, run_interval: 0 ns
                           n_lock_ops (#/sec)       n_waiting (#/sec)      n_spin_acq (#/sec)  n_owner_switch (#/sec)   total_wait_ns   (#/sec)     max_wait_ns  max_n_thds
background_thread                  11      11               0       0               0       0               1       1               0         0               0           0
ctl                               404     404               0       0               0       0               1       1               0         0               0           0
prof                                0       0               0       0               0       0               0       0               0         0               0           0
Merged arenas stats:
assigned threads: 1
uptime: 11999831
dss allocation precedence: ""N/A""
decaying:  time       npages       sweeps     madvises       purged
   dirty:   N/A          339            0            0            0
   muzzy:   N/A            0            0            0            0
                            allocated         nmalloc (#/sec)         ndalloc (#/sec)       nrequests   (#/sec)           nfill   (#/sec)          nflush   (#/sec)
small:                        1274960           10236   10236             547     547           20257     20257             191       191              31        31
large:                        2732032               9       9               6       6               9         9               9         9               0         0
total:                        4006992           10245   10245             553     553           20266     20266             200       200              31        31

active:                       4165632
mapped:                      18137088
retained:                     2310144
base:                         6705232
internal:                       28680
metadata_thp:                       0
tcache_bytes:                  198112
resident:                    12263424
abandoned_vm:                       0
extent_avail:                       0
                           n_lock_ops (#/sec)       n_waiting (#/sec)      n_spin_acq (#/sec)  n_owner_switch (#/sec)   total_wait_ns   (#/sec)     max_wait_ns  max_n_thds
large                               5       5               0       0               0       0               1       1               0         0               0           0
extent_avail                      155     155               0       0               0       0               3       3               0         0               0           0
extents_dirty                     217     217               0       0               0       0               3       3               0         0               0           0
extents_muzzy                       5       5               0       0               0       0               1       1               0         0               0           0
extents_retained                  279     279               0       0               0       0               3       3               0         0               0           0
decay_dirty                         5       5               0       0               0       0               1       1               0         0               0           0
decay_muzzy                         5       5               0       0               0       0               1       1               0         0               0           0
base                              299     299               0       0               0       0               3       3               0         0               0           0
tcache_list                         6       6               0       0               0       0               1       1               0         0               0           0
--- End jemalloc statistics ---

/home/laiyingchun/dev/pegasus_review/src/utils/test/je_ctl_test.cpp:102: Failure
Expected: check_arena_marks(stats) doesn't generate new fatal failures in the current thread.
  Actual: it does.
[  FAILED  ] je_ctl_test.dump_detailed_stats (2 ms)
```

It seems the buffer size is not large enough to fill these information.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1635/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1642,https://api.github.com/repos/apache/incubator-pegasus/issues/1642,incubator-pegasus,1946666051,1642,Remove release tarball before unit tests in case there is no space left on device,empiredan,743379,Dan Wang,,CLOSED,2023-10-17T06:44:14Z,2023-10-19T09:37:50Z,"Many unit tests failed due to no space left on device:
![image](https://github.com/apache/incubator-pegasus/assets/743379/8567bc4f-49a5-49b3-8505-f1587390b7a7)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1642/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1644,https://api.github.com/repos/apache/incubator-pegasus/issues/1644,incubator-pegasus,1947795442,1644,DISCUSS: remove the FDS supporting,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-10-17T16:19:01Z,2023-12-14T15:58:40Z,"Hi, Pegasus developers and users,

As it known, the [FDS](https://docs.api.xiaomi.com/fds/introduction.html) is a Xiaomi internal storage system, the community users are not possible to use FDS as a [Backup](https://pegasus.apache.org/zh/administration/cold-backup) target, and since Pegasus [2.2.0](https://github.com/apache/incubator-pegasus/releases/tag/v2.2.0) (released in Jun, 2021), the Backup start to support using Apache HDFS as the target storage.

So I propose to terminate the supporting of FDS in the next version (maybe 2.6.0 or later). Of course, we will still fix critical bugs if there are in released versions.

Developers are also encouraged to contribute to support more public used storage systems, for example S3, OSS, COS and etc.

Looking forward to your reply!","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1644/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1644,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5podTC,incubator-pegasus,1772213442,1644,NA,GiantKing,10461183,,giantkingww@163.com,NA,2023-10-20T07:19:52Z,2023-10-20T07:19:52Z,"I recommend keeping FDS, otherwise we have to maintain a new Pegasus branch internally. And keeping FDS, there is no impact on users to use other storages for backup.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5podTC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1644,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5pxne1,incubator-pegasus,1774614453,1644,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-10-23T07:49:17Z,2023-10-23T07:49:17Z,"@GiantKing Do you have a plan to use HDFS instead of FDS in your production environments? What's the blocker if not?

When developing new features or refactoring code, the community developers have to consider whether the FDS feature has been impacted or not, but they have no such a FDS testing enviroment to check it. It become a burden on the project in future developmemnt more or less.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5pxne1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1644,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5stppE,incubator-pegasus,1823906372,1644,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-11-23T07:17:15Z,2023-11-23T07:17:15Z,"If there isn't any more strong reasons to keep FDS related code in the project, I'm planing to remove these code.
Thanks!","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5stppE/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1645,https://api.github.com/repos/apache/incubator-pegasus/issues/1645,incubator-pegasus,1948699231,1645,Avoid reporting false 'odr-violation' issues,empiredan,743379,Dan Wang,,CLOSED,2023-10-18T03:08:57Z,2023-10-19T09:14:06Z,"Now the master branch complain an ASAN issuse like:
```
==246==ERROR: AddressSanitizer: odr-violation (0x1477e70d8b40):
  [1] size=1 'FLAGS_enable_http_server' /__w/incubator-pegasus/incubator-pegasus/src/http/http_server.cpp:45:1
  [2] size=1 'FLAGS_enable_http_server' /__w/incubator-pegasus/incubator-pegasus/src/http/http_server.cpp:45:1
These globals were registered at these points:
  [1]:
    #0 0x1477eb021928 in __asan_register_globals ../../../../src/libsanitizer/asan/asan_globals.cpp:341
    #1 0x1477eb9e147d  (/lib64/ld-linux-x86-64.so.2+0x647d)

  [2]:
    #0 0x1477eb021928 in __asan_register_globals ../../../../src/libsanitizer/asan/asan_globals.cpp:341
    #1 0x1477eb9e147d  (/lib64/ld-linux-x86-64.so.2+0x647d)

==HINT: if you don't care about these errors you may set ASAN_OPTIONS=detect_odr_violation=0
SUMMARY: AddressSanitizer: odr-violation: global 'FLAGS_enable_http_server' at /__w/incubator-pegasus/incubator-pegasus/src/http/http_server.cpp:45:1
==246==ABORTING
```

This patch is aim to mute this false report. See details related to the issuse:
1. https://github.com/google/sanitizers/issues/1017
2. https://github.com/google/sanitizers/wiki/AddressSanitizerOneDefinitionRuleViolation
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1645/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1647,https://api.github.com/repos/apache/incubator-pegasus/issues/1647,incubator-pegasus,1949747980,1647,"Compilation error ""no member named 'make_unique' in namespace 'std'"" on MacOS for v2.5",empiredan,743379,Dan Wang,,CLOSED,2023-10-18T13:22:03Z,2023-10-19T08:26:30Z,"According to https://github.com/apache/incubator-pegasus/pull/1640#issuecomment-1767531204, for v2.5, MacOS compilation failed due to ""no member named 'make_unique' in namespace 'std'"" as below:
```
>>> compiling thrift file dsn.layer2.thrift ...
mkdir /Users/runner/work/incubator-pegasus/incubator-pegasus/idl/output
/Users/runner/work/incubator-pegasus/incubator-pegasus/thirdparty/output/bin/thrift -gen cpp:moveable_types -out output dsn.layer2.thrift
mv output/dsn.layer2_types.h /Users/runner/work/incubator-pegasus/incubator-pegasus/src/common/serialization_helper
mv output/dsn.layer2_types.cpp /Users/runner/work/incubator-pegasus/incubator-pegasus/src/runtime
rm -rf /Users/runner/work/incubator-pegasus/incubator-pegasus/idl/output

done
Running cmake Pegasus...
~/work/incubator-pegasus/incubator-pegasus/build/release ~/work/incubator-pegasus/incubator-pegasus ~/work/incubator-pegasus/incubator-pegasus
CMake Warning (dev) at CMakeLists.txt:18 (project):
  cmake_minimum_required() should be called prior to this top-level project()
  call.  Please see the cmake-commands(7) manual for usage documentation of
  both commands.
This warning is for project developers.  Use -Wno-dev to suppress it.

-- The C compiler identification is AppleClang 14.0.0.14000029
-- The CXX compiler identification is AppleClang 14.0.0.14000029
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/gcc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/g++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
Running CompilerInfo.cmake
Apple clang version 14.0.0 (clang-1400.0.29.202)
Target: x86_64-apple-darwin21.6.0
Thread model: posix
InstalledDir: /Applications/Xcode_14.2.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

Selected compiler clang 14.0
-- THIRDPARTY_INSTALL_DIR = /Users/runner/work/incubator-pegasus/incubator-pegasus/thirdparty/output
-- BUILD_DIR = /Users/runner/work/incubator-pegasus/incubator-pegasus/src/builder
-- BUILD_TEST = ON
-- ENABLE_GCOV = NO
-- ENABLE_GPERF = ON
-- USE_JEMALLOC = OFF
-- CCACHE: /usr/local/bin/ccache
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- FIND_LIBRARY_USE_LIB64_PATHS = ON
-- Found components for DL
-- DL_INCLUDES = /Applications/Xcode_14.2.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX13.1.sdk/usr/include
-- DL_LIBRARIES = /Applications/Xcode_14.2.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX13.1.sdk/usr/lib/libdl.tbd
-- Found OpenSSL: /usr/local/opt/openssl/lib/libcrypto.dylib (found version ""3.1.2"")  
-- use ccache to speed up compilation
-- CMAKE_PREFIX_PATH = /Users/runner/work/incubator-pegasus/incubator-pegasus/thirdparty/output
-- Found Boost: /Users/runner/work/incubator-pegasus/incubator-pegasus/thirdparty/output/include (found version ""1.[69](https://github.com/apache/incubator-pegasus/actions/runs/6546807374/job/17778038617?pr=1640#step:6:70).0"") found components: system filesystem regex 
-- Found snappy: /usr/local/lib/libsnappy.dylib  
-- Found zstd: /usr/local/lib/libzstd.dylib  
-- Found lz4: /usr/local/lib/liblz4.dylib  
-- Found JNI: /Users/runner/hostedtoolcache/Java_Temurin-Hotspot_jdk/8.0.382-5/x64/Contents/Home/include  found components: AWT JVM 
-- JAVA_JVM_LIBRARY=/Users/runner/hostedtoolcache/Java_Temurin-Hotspot_jdk/8.0.382-5/x64/Contents/Home/jre/lib/server/libjvm.dylib
-- MACOS_OPENSSL_ROOT_DIR: /usr/local/opt/openssl
-- THRIFT_GENERATED_FILE_PATH=/Users/runner/work/incubator-pegasus/incubator-pegasus/build/release/thrift-gen
-- THRIFT_GENERATE_CPP: /Users/runner/work/incubator-pegasus/incubator-pegasus/idl/metadata.thrift
-- THRIFT_GENERATE_CPP: /Users/runner/work/incubator-pegasus/incubator-pegasus/src/common/consensus.thrift
-- THRIFT_GENERATE_CPP: /Users/runner/work/incubator-pegasus/incubator-pegasus/src/common/duplication_internal.thrift
-- THRIFT_GENERATE_CPP: /Users/runner/work/incubator-pegasus/incubator-pegasus/idl/duplication.thrift
-- THRIFT_GENERATE_CPP: /Users/runner/work/incubator-pegasus/incubator-pegasus/idl/backup.thrift
-- THRIFT_GENERATE_CPP: /Users/runner/work/incubator-pegasus/incubator-pegasus/idl/meta_admin.thrift
-- THRIFT_GENERATE_CPP: /Users/runner/work/incubator-pegasus/incubator-pegasus/idl/bulk_load.thrift
-- THRIFT_GENERATE_CPP: /Users/runner/work/incubator-pegasus/incubator-pegasus/idl/partition_split.thrift
-- THRIFT_GENERATE_CPP: /Users/runner/work/incubator-pegasus/incubator-pegasus/idl/replica_admin.thrift
-- THRIFT_GENERATE_CPP: /Users/runner/work/incubator-pegasus/incubator-pegasus/src/failure_detector/fd.thrift
-- THRIFT_GENERATE_CPP: /Users/runner/work/incubator-pegasus/incubator-pegasus/src/nfs/nfs.thrift
-- THRIFT_GENERATE_CPP: /Users/runner/work/incubator-pegasus/incubator-pegasus/idl/command.thrift
-- THRIFT_GENERATE_CPP: /Users/runner/work/incubator-pegasus/incubator-pegasus/src/replica/storage/simple_kv/simple_kv.thrift
-- Found ZLIB: /Applications/Xcode_14.2.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX13.1.sdk/usr/lib/libz.tbd (found version ""1.2.11"")  
-- Found CURL: /Users/runner/work/incubator-pegasus/incubator-pegasus/thirdparty/output/lib/libcurl.a (found version ""7.47.0"")  
-- THRIFT_GENERATE_CPP: /Users/runner/work/incubator-pegasus/incubator-pegasus/src/runtime/rpc/request_meta.thrift
-- THRIFT_GENERATE_CPP: /Users/runner/work/incubator-pegasus/incubator-pegasus/idl/security.thrift
-- Configuring done (7.9s)
-- Generating done (1.0s)
-- Build files have been written to: /Users/runner/work/incubator-pegasus/incubator-pegasus/build/release
Gen git_commit.h ...
~/work/incubator-pegasus/incubator-pegasus/src ~/work/incubator-pegasus/incubator-pegasus/build/release ~/work/incubator-pegasus/incubator-pegasus ~/work/incubator-pegasus/incubator-pegasus
d2aa7978d9d141e39841f114d48b6b8[70](https://github.com/apache/incubator-pegasus/actions/runs/6546807374/job/17778038617?pr=1640#step:6:71)1a9e76f
PEGASUS_GIT_COMMIT=d2aa7978d9d141e39841f114d48b6b8701a9e76f
Generating /Users/runner/work/incubator-pegasus/incubator-pegasus/src/include/pegasus/git_commit.h...
[Tue Oct 17 12:35:48 UTC 2023] Building Pegasus ...
~/work/incubator-pegasus/incubator-pegasus/build/release ~/work/incubator-pegasus/incubator-pegasus/src ~/work/incubator-pegasus/incubator-pegasus/build/release ~/work/incubator-pegasus/incubator-pegasus ~/work/incubator-pegasus/incubator-pegasus
[  0%] Building CXX object src/perf_counter/CMakeFiles/dsn.perf_counter.dir/builtin_counters.cpp.o
[  0%] Building CXX object src/utils/CMakeFiles/dsn_utils.dir/alloc.cpp.o
[  0%] Building CXX object src/aio/CMakeFiles/dsn_aio.dir/aio_provider.cpp.o
[  0%] Building CXX object src/utils/CMakeFiles/dsn_utils.dir/binary_reader.cpp.o
[  0%] Building CXX object src/perf_counter/CMakeFiles/dsn.perf_counter.dir/perf_counter.cpp.o

......

[ 33%] Built target dsn_runtime
[ 33%] Building CXX object src/aio/test/CMakeFiles/dsn_aio_test.dir/aio.cpp.o
[ 33%] Building CXX object src/block_service/test/CMakeFiles/dsn_block_service_test.dir/block_service_manager_test.cpp.o
[ 33%] Building CXX object src/base/test/CMakeFiles/base_test.dir/main.cpp.o
[ 34%] Building CXX object src/base/test/CMakeFiles/base_test.dir/utils_test.cpp.o
/Users/runner/work/incubator-pegasus/incubator-pegasus/src/aio/test/aio.cpp:177:21: error: no member named 'make_unique' in namespace 'std'
    auto err = std::make_unique<dsn::error_code>();
               ~~~~~^
/Users/runner/work/incubator-pegasus/incubator-pegasus/src/aio/test/aio.cpp:177:48: error: expected '(' for function-style cast or type construction
    auto err = std::make_unique<dsn::error_code>();
                                ~~~~~~~~~~~~~~~^
/Users/runner/work/incubator-pegasus/incubator-pegasus/src/aio/test/aio.cpp:177:50: error: expected expression
    auto err = std::make_unique<dsn::error_code>();
                                                 ^
/Users/runner/work/incubator-pegasus/incubator-pegasus/src/aio/test/aio.cpp:178:23: error: no member named 'make_unique' in namespace 'std'
    auto count = std::make_unique<size_t>();
                 ~~~~~^
/Users/runner/work/incubator-pegasus/incubator-pegasus/src/aio/test/aio.cpp:178:35: error: unexpected type name 'size_t': expected expression
    auto count = std::make_unique<size_t>();
                                  ^
/Users/runner/work/incubator-pegasus/incubator-pegasus/src/aio/test/aio.cpp:178:43: error: expected expression
    auto count = std::make_unique<size_t>();
                                          ^
6 errors generated.
make[2]: *** [src/aio/test/CMakeFiles/dsn_aio_test.dir/aio.cpp.o] Error 1
make[1]: *** [src/aio/test/CMakeFiles/dsn_aio_test.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
[ 35%] Building CXX object src/block_service/test/CMakeFiles/dsn_block_service_test.dir/fds_service_test.cpp.o
[ 35%] Building CXX object src/base/test/CMakeFiles/base_test.dir/value_manager_test.cpp.o
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/block_service/test/block_service_manager_test.cpp:29:
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/block_service/local/local_service.h:24:
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/block_service/block_service.h:32:
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/common/serialization_helper/thrift_helper.h:30:
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/runtime/rpc/rpc_host_port.h:31:
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/utils/errors.h:36:
/Users/runner/work/incubator-pegasus/incubator-pegasus/src/utils/smart_pointers.h:110:60: error: cannot define or redeclare 'make_unique' here because namespace 'dsn' does not enclose namespace 'std'
typename memory_internal::MakeUniqueResult<T>::scalar std::make_unique(Args &&... args)
                                                      ~~~~~^
/Users/runner/work/incubator-pegasus/incubator-pegasus/src/utils/smart_pointers.h:120:59: error: cannot define or redeclare 'make_unique' here because namespace 'dsn' does not enclose namespace 'std'
typename memory_internal::MakeUniqueResult<T>::array std::make_unique(size_t n)
                                                     ~~~~~^
/Users/runner/work/incubator-pegasus/incubator-pegasus/src/utils/smart_pointers.h:129:6: error: cannot define or redeclare 'make_unique' here because namespace 'dsn' does not enclose namespace 'std'
std::make_unique(Args &&... /* args */) = delete;
~~~~~^
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/block_service/test/fds_service_test.cpp:18:
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/block_service/fds/fds_service.h:29:
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/block_service/block_service.h:32:
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/runtime/rpc/serialization.h:32:
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/common/serialization_helper/thrift_helper.h:30:
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/runtime/rpc/rpc_host_port.h:31:
/Users/runner/work/incubator-pegasus/incubator-pegasus/src/utils/errors.h:165:21: error: no matching function for call to 'make_unique'
            _info = std::make_unique<error_info>(rhs._info->code, rhs._info->msg);
                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/block_service/test/fds_service_test.cpp:18:
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/block_service/fds/fds_service.h:29:
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/block_service/block_service.h:34:
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/runtime/serverlet.h:29:
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/runtime/rpc/rpc_holder.h:28:
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/client/partition_resolver.h:40:
/Users/runner/work/incubator-pegasus/incubator-pegasus/src/runtime/task/async_calls.h:133:10: error: initialized lambda captures are a C++14 extension [-Werror,-Wc++14-extensions]
        [cb_fwd = std::move(callback)](
         ^
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/block_service/test/fds_service_test.cpp:18:
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/block_service/fds/fds_service.h:29:
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/block_service/block_service.h:34:
In file included from /Users/runner/work/incubator-pegasus/incubator-pegasus/src/runtime/serverlet.h:29:
/Users/runner/work/incubator-pegasus/incubator-pegasus/src/runtime/rpc/rpc_holder.h:171:15: error: initialized lambda captures are a C++14 extension [-Werror,-Wc++14-extensions]
            [ cb_fwd = std::forward<TCallback>(callback),
              ^
/Users/runner/work/incubator-pegasus/incubator-pegasus/src/runtime/rpc/rpc_holder.h:172:15: error: initialized lambda captures are a C++14 extension [-Werror,-Wc++14-extensions]
              rpc = *this ](error_code err, message_ex * req, message_ex * resp) mutable {
              ^
/Users/runner/work/incubator-pegasus/incubator-pegasus/src/runtime/rpc/rpc_holder.h:203:15: error: initialized lambda captures are a C++14 extension [-Werror,-Wc++14-extensions]
            [ cb_fwd = std::forward<TCallback>(callback),
              ^
/Users/runner/work/incubator-pegasus/incubator-pegasus/src/runtime/rpc/rpc_holder.h:204:15: error: initialized lambda captures are a C++14 extension [-Werror,-Wc++14-extensions]
              rpc = *this ](error_code err, message_ex * req, message_ex * resp) mutable {
              ^
9 errors generated.
make[2]: *** [src/block_service/test/CMakeFiles/dsn_block_service_test.dir/fds_service_test.cpp.o] Error 1
make[1]: *** [src/block_service/test/CMakeFiles/dsn_block_service_test.dir/all] Error 2
make: *** [all] Error 2
Error: Process completed with exit code 2.
```

The compilation error on MacOS for master branch was fixed by https://github.com/apache/incubator-pegasus/pull/1603 which mark C++17 as required.

`std::make_unique` was introduced since C++14:
- https://stackoverflow.com/questions/24609271/errormake-unique-is-not-a-member-of-std
- https://en.cppreference.com/w/cpp/memory/unique_ptr/make_unique

And actually pegasus has adopted C++14 before as `c++1y`:
```cmake
  include(CheckCXXCompilerFlag)
  CHECK_CXX_COMPILER_FLAG(""-std=c++1y"" COMPILER_SUPPORTS_CXX1Y)
  if(NOT ${COMPILER_SUPPORTS_CXX1Y})
    message(FATAL_ERROR ""You need a compiler with C++1y support."")
  endif()
```

`c++1y` has been deprecated, thus
- https://stackoverflow.com/questions/28825113/difference-between-gcc-compile-options-std-c1y-and-std-c14
- https://gcc.gnu.org/onlinedocs/gcc/C-Dialect-Options.html#index-std-1

 In https://github.com/apache/incubator-pegasus/pull/1605 and https://github.com/apache/incubator-pegasus/pull/1640 we use following statements instead, however it does not work for MacOS, the reason should be found and the compilation error should be fixed:
```cmake
  set(CMAKE_CXX_STANDARD 14)
  set(CMAKE_CXX_STANDARD_REQUIRED ON)
  set(CMAKE_CXX_EXTENSIONS OFF)
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1647/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1650,https://api.github.com/repos/apache/incubator-pegasus/issues/1650,incubator-pegasus,1951134929,1650,Bug(bulkload):rocksDB parameter allow_ingest_behind `lose` after replica migration,ninsmiracle,110282526,,,CLOSED,2023-10-19T03:38:47Z,2025-01-19T07:44:08Z,"## Bug Report

1. What did you do?
- create a new app with parameter rocksdb.allow_ingest_behind = true
- put some data in this new app
- stop one node
- change meta.lb.assign_delay_ms and let other nodes learn missing replicas of this app
- those replicas that has been migrated start with  rocksdb.allow_ingest_behind = false(which is default)


2. What did you expect to see?
In my opinion,this parameter should be consistent after replica migration.**If I doing a data migration by using bulkload function,it may change data ingestion method.**


3. What version of Pegasus are you using?
[pegasus2.4](https://github.com/apache/incubator-pegasus/tree/v2.4)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1650/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1654,https://api.github.com/repos/apache/incubator-pegasus/issues/1654,incubator-pegasus,1956826647,1654,`Lint Dockerfile` action failed,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-10-23T10:16:26Z,2023-10-23T15:21:44Z,"The [Lint Dockerfile](https://github.com/apache/incubator-pegasus/actions/runs/6604609252/job/17938922079?pr=1652#logs) action report an issue:
```
/usr/bin/docker run --name b03f8bbe8c33c7d45414ebdfbb34354fc422b_624fd8 --label 4b03f8 --workdir /github/workspace --rm -e ""INPUT_RECURSIVE"" -e ""INPUT_IGNORE"" -e ""INPUT_DOCKERFILE"" -e ""INPUT_CONFIG"" -e ""INPUT_OUTPUT-FILE"" -e ""INPUT_NO-COLOR"" -e ""INPUT_NO-FAIL"" -e ""INPUT_VERBOSE"" -e ""INPUT_FORMAT"" -e ""INPUT_FAILURE-THRESHOLD"" -e ""INPUT_OVERRIDE-ERROR"" -e ""INPUT_OVERRIDE-WARNING"" -e ""INPUT_OVERRIDE-INFO"" -e ""INPUT_OVERRIDE-STYLE"" -e ""INPUT_TRUSTED-REGISTRIES"" -e ""NO_COLOR"" -e ""HADOLINT_NOFAIL"" -e ""HADOLINT_VERBOSE"" -e ""HADOLINT_FORMAT"" -e ""HADOLINT_FAILURE_THRESHOLD"" -e ""HADOLINT_OVERRIDE_ERROR"" -e ""HADOLINT_OVERRIDE_WARNING"" -e ""HADOLINT_OVERRIDE_INFO"" -e ""HADOLINT_OVERRIDE_STYLE"" -e ""HADOLINT_IGNORE"" -e ""HADOLINT_TRUSTED_REGISTRIES"" -e ""HADOLINT_CONFIG"" -e ""HADOLINT_RECURSIVE"" -e ""HADOLINT_OUTPUT"" -e ""HOME"" -e ""GITHUB_JOB"" -e ""GITHUB_REF"" -e ""GITHUB_SHA"" -e ""GITHUB_REPOSITORY"" -e ""GITHUB_REPOSITORY_OWNER"" -e ""GITHUB_REPOSITORY_OWNER_ID"" -e ""GITHUB_RUN_ID"" -e ""GITHUB_RUN_NUMBER"" -e ""GITHUB_RETENTION_DAYS"" -e ""GITHUB_RUN_ATTEMPT"" -e ""GITHUB_REPOSITORY_ID"" -e ""GITHUB_ACTOR_ID"" -e ""GITHUB_ACTOR"" -e ""GITHUB_TRIGGERING_ACTOR"" -e ""GITHUB_WORKFLOW"" -e ""GITHUB_HEAD_REF"" -e ""GITHUB_BASE_REF"" -e ""GITHUB_EVENT_NAME"" -e ""GITHUB_SERVER_URL"" -e ""GITHUB_API_URL"" -e ""GITHUB_GRAPHQL_URL"" -e ""GITHUB_REF_NAME"" -e ""GITHUB_REF_PROTECTED"" -e ""GITHUB_REF_TYPE"" -e ""GITHUB_WORKFLOW_REF"" -e ""GITHUB_WORKFLOW_SHA"" -e ""GITHUB_WORKSPACE"" -e ""GITHUB_ACTION"" -e ""GITHUB_EVENT_PATH"" -e ""GITHUB_ACTION_REPOSITORY"" -e ""GITHUB_ACTION_REF"" -e ""GITHUB_PATH"" -e ""GITHUB_ENV"" -e ""GITHUB_STEP_SUMMARY"" -e ""GITHUB_STATE"" -e ""GITHUB_OUTPUT"" -e ""RUNNER_OS"" -e ""RUNNER_ARCH"" -e ""RUNNER_NAME"" -e ""RUNNER_ENVIRONMENT"" -e ""RUNNER_TOOL_CACHE"" -e ""RUNNER_TEMP"" -e ""RUNNER_WORKSPACE"" -e ""ACTIONS_RUNTIME_URL"" -e ""ACTIONS_RUNTIME_TOKEN"" -e ""ACTIONS_CACHE_URL"" -e ""ACTIONS_RESULTS_URL"" -e GITHUB_ACTIONS=true -e CI=true -v ""/var/run/docker.sock"":""/var/run/docker.sock"" -v ""/home/runner/work/_temp/_github_home"":""/github/home"" -v ""/home/runner/work/_temp/_github_workflow"":""/github/workflow"" -v ""/home/runner/work/_temp/_runner_file_commands"":""/github/file_commands"" -v ""/home/runner/work/incubator-pegasus/incubator-pegasus"":""/github/workspace"" 4b03f8:bbe8c33c7d45414ebdfbb34354fc422b  ""Dockerfile""
Error: docker/pegasus-build-env/ubuntu1804/Dockerfile:66 DL3042 warning: Avoid use of cache directory with pip. Use `pip install --no-cache-dir <package>`
Hadolint output saved to: /dev/stdout
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1654/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1659,https://api.github.com/repos/apache/incubator-pegasus/issues/1659,incubator-pegasus,1962649077,1659,Replace rpc_address to host_prot for function & Add host_port on thrift struct,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2023-10-26T03:50:33Z,2024-03-19T07:55:53Z,"## Feature Request

A part of [support fqdn](https://github.com/apache/incubator-pegasus/issues/1403). 

1.Replace type `rpc_address` to `host_port ` on class variables. Modify the parameter types of the function if necessary.

2.Add type 'host_port' member to thrift struct which have 'rpc_address'.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1659/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1663,https://api.github.com/repos/apache/incubator-pegasus/issues/1663,incubator-pegasus,1968372715,1663,CI Workflows on Github failed for Scala client due to missing necessary generated thrift classes,empiredan,743379,Dan Wang,,CLOSED,2023-10-30T13:33:27Z,2023-10-31T08:04:08Z,"CI Workflows on Github failed for Scala client, due to missing necessary generated thrift classes as follows:
```
[INFO] -------------------------------------------------------------
Error:  COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/operator/negotiation_operator.java:[21,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/operator/negotiation_operator.java:[22,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/operator/negotiation_operator.java:[23,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/operator/client_operator.java:[23,37] error: package org.apache.pegasus.replication does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/operator/negotiation_operator.java:[32,30] error: cannot find symbol
  symbol:   class negotiation_request
  location: class negotiation_operator
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/operator/negotiation_operator.java:[58,9] error: cannot find symbol
  symbol:   class negotiation_response
  location: class negotiation_operator
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/operator/negotiation_operator.java:[62,10] error: cannot find symbol
  symbol:   class negotiation_request
  location: class negotiation_operator
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/operator/negotiation_operator.java:[63,10] error: cannot find symbol
  symbol:   class negotiation_response
  location: class negotiation_operator
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/operator/client_operator.java:[109,9] error: cannot find symbol
  symbol:   class request_meta
  location: class client_operator
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/PegasusTableInterface.java:[26,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/PegasusTableInterface.java:[261,46] error: cannot find symbol
  symbol:   class batch_get_request
  location: interface PegasusTableInterface
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/Mutations.java:[25,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/Mutations.java:[26,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/PegasusTable.java:[38,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/PegasusTable.java:[39,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/PegasusTable.java:[40,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/PegasusTable.java:[41,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/PegasusTable.java:[42,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/PegasusTable.java:[43,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/PegasusTable.java:[44,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/PegasusTable.java:[45,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/PegasusTable.java:[46,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/PegasusTable.java:[47,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/PegasusTable.java:[48,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/PegasusTable.java:[49,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/PegasusTable.java:[50,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/PegasusTable.java:[51,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/rpc/async/TableHandler.java:[41,37] error: package org.apache.pegasus.replication does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/rpc/async/TableHandler.java:[42,37] error: package org.apache.pegasus.replication does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/rpc/async/TableHandler.java:[43,37] error: package org.apache.pegasus.replication does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/Mutations.java:[37,21] error: cannot find symbol
  symbol:   class mutate
  location: class Mutations
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/Mutations.java:[69,14] error: cannot find symbol
  symbol:   class mutate
  location: class Mutations
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/tools/WriteLimiter.java:[24,30] error: package org.apache.pegasus.apps does not exist
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/client/PegasusTable.java:[484,46] error: cannot find symbol
  symbol:   class batch_get_request
  location: class PegasusTable
Error:  /home/runner/work/incubator-pegasus/incubator-pegasus/scala-client/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/rpc/async/TableHandler.java:[155,30] error: cannot find symbol
  symbol:   class query_cfg_response
  location: class TableHandler
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1663/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1664,https://api.github.com/repos/apache/incubator-pegasus/issues/1664,incubator-pegasus,1968508264,1664,Use thrift-maven-plugin to generate thrift sources instead of script,empiredan,743379,Dan Wang,,CLOSED,2023-10-30T14:28:13Z,2024-06-27T10:16:57Z,"We could use [thrift-maven-plugin](https://mvnrepository.com/artifact/org.apache.thrift/thrift-maven-plugin) to generate thrift sources instead of scripts, just like what parquet has done:

- https://issues.apache.org/jira/browse/PARQUET-1506
- https://github.com/apache/parquet-java/pull/600
- https://github.com/apache/parquet-java/blob/master/parquet-format-structures/pom.xml","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1664/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1664,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM553FSB,incubator-pegasus,2044482689,1664,NA,shalk,2435781,shalk(xiao kun),,NA,2024-04-09T08:53:25Z,2024-04-09T08:53:25Z,@empiredan  parquet link is broken,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM553FSB/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1664,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Cyjba,incubator-pegasus,2194290394,1664,NA,empiredan,743379,Dan Wang,,NA,2024-06-27T10:04:42Z,2024-06-27T10:04:42Z,"> @empiredan parquet link is broken

Sorry for the late reply. Have updated the description.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Cyjba/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1665,https://api.github.com/repos/apache/incubator-pegasus/issues/1665,incubator-pegasus,1969553433,1665,Bug(manual_compact):replica lose manual compact finished status after replica migrate,ninsmiracle,110282526,,,CLOSED,2023-10-31T03:12:51Z,2025-01-19T07:43:48Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
-Set a suitable  time and let  a table finish manual compact
-View the progress of manual compaction,now is 100
-Stop a node
-View the progress of manual compaction again ,now is 79
-Go to the alive nodes and view their logs,some replicas begin manual compact
-View the progress of manual compaction again ,now is 91
-However some replica DO NOT manual compaction this day manual compaction progress stack in 91

So that , in our online envs,some pegasus users can not finish manual compact after replica migrate. 

2. What did you expect to see?
Replicas which finished manual compact can hold the status after replica migrate.

3. What version of Pegasus are you using?
[pegasus2.4](https://github.com/apache/incubator-pegasus/tree/v2.4)","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1665/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1665,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5sBMrz,incubator-pegasus,1812253427,1665,NA,abdraheem98,139868961,Abdul Raheem,,NA,2023-11-15T10:51:49Z,2023-11-15T10:51:49Z,Is the issue still open? I'm interested to solve this issue. Could please tell some more details. Thanks in Advance,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5sBMrz/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1665,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5sOUbD,incubator-pegasus,1815692995,1665,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-11-17T03:28:13Z,2023-11-17T03:28:13Z,"@ninsmiracle Each replica of a partition will do manual compaction independently, when one node shutdown, the other replicas on other nodes may not start or in progress manual compaction.
What tools you were using to check the progress, could you leave some details? Thanks!","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5sOUbD/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1665,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5snUlq,incubator-pegasus,1822247274,1665,NA,ninsmiracle,110282526,,,NA,2023-11-22T07:33:25Z,2023-11-22T07:33:25Z,"I used `admin-cli` to check the manaul compaction progress. And this is the result I use `manual-compaction query -a TABLE_NAME` commond after manual compact begin or node stop:
 
![image](https://github.com/apache/incubator-pegasus/assets/110282526/d819feb0-884d-4b3d-9840-047d60aa27a3)

And we can use pegasus http interface to check the replicas compact detail of target table.
`curl YOUR_REPLICA_SERVER_IP:PORT/replica/manual_compaction?app_id=YOUR_APP_ID`
![image](https://github.com/apache/incubator-pegasus/assets/110282526/85d70398-73ac-4874-a054-53d22d208cdf)


We can observe that some replica keep `idle` status for a long time until it triggered the periodic manual compact  time next day.








","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5snUlq/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1665,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5snafW,incubator-pegasus,1822271446,1665,NA,ninsmiracle,110282526,,,NA,2023-11-22T07:54:57Z,2023-11-22T07:54:57Z,"> Is the issue still open? I'm interested to solve this issue. Could please tell some more details. Thanks in Advance

This is my way to recurrent this bug:

1. Create a Pegasus table and input some data.
2. Set the parameters of this Pegasus app by using `set_app_envs`. To achieve our  target, we should set `manual_compact.disabled` to `false` and set `manual_compact.periodic.trigger_time` to a specific time, such as `10:00`.(when I do this job,it's 9:58.So that I can observe the result soon)
3. Build the admin-cli tool and use it to connect to our table.
4. Use the `manual-compaction query -a` command to check the progress. We can see that it has reached 100.
5. Stop one node of our cluster.
6. The progress will not be 100 anymore until the next day's `10:00`.


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5snafW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1673,https://api.github.com/repos/apache/incubator-pegasus/issues/1673,incubator-pegasus,1975545593,1673,"The data directories of replicas are removed immediately after they are renamed with postfixes "".err/.gar""",empiredan,743379,Dan Wang,,CLOSED,2023-11-03T06:45:51Z,2023-11-08T03:07:41Z,"On the production environment, sometimes it's found that the data directories of replicas are removed immediately after they are renamed with postfixes `.err/.gar`, though actually both of `gc_disk_error_replica_interval_seconds` and `gc_disk_garbage_replica_interval_seconds` have been set as one day.

The reason is that the base time for expiration time is **the last write time**, that is, `st_mtime` within `struct stat` returned by `stat()`. Once a long time has passed since the last write time, the data directory will be removed immediately after it is renamed with postfixes `.err/.gar`.

To fix this problem, just use the timestamp within the directory name as the base time that is generated when the data directory is renamed with postfixes `.err/.gar`.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1673/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1676,https://api.github.com/repos/apache/incubator-pegasus/issues/1676,incubator-pegasus,1982693584,1676,Ensure copyright info of all files are correctly included in .licenserc.yaml and LICENSE ,empiredan,743379,Dan Wang,,CLOSED,2023-11-08T04:09:37Z,2023-12-15T07:23:28Z,"Before new version would be released, we should check .licenserc.yaml and LICENSE to ensure that copyright info of all files are correctly included.

Moreover, to facilitate the later check for copyright, we could develop a script to check the consistency between `.licenserc.yaml` and all files of the project.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1676/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1676,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5utaSB,incubator-pegasus,1857397889,1676,NA,empiredan,743379,Dan Wang,,NA,2023-12-15T07:23:28Z,2023-12-15T07:23:28Z,This problem has been resolved by https://github.com/apache/incubator-pegasus/pull/1674.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5utaSB/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1685,https://api.github.com/repos/apache/incubator-pegasus/issues/1685,incubator-pegasus,2001592902,1685,Server crashed in tcmalloc,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-11-20T08:06:20Z,2023-11-20T08:06:20Z,"Server crashed with backtrace:
```
Program terminated with signal SIGSEGV, Segmentation fault.
#0  0x0000000000000000 in ?? ()
Installing openjdk unwinder
Traceback (most recent call last):
  File ""/usr/share/gdb/auto-load/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so-gdb.py"", line 52, in <module>
    class Types(object):
  File ""/usr/share/gdb/auto-load/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so-gdb.py"", line 66, in Types
    nmethodp_t = gdb.lookup_type('nmethod').pointer()
gdb.error: No type named nmethod.
(gdb)
(gdb) bt
#0  0x0000000000000000 in ?? ()
#1  0x00007fd1da732bf0 in (anonymous namespace)::stacktrace_generic_fp::capture<false, false> (result=result@entry=0x560793e14010, max_depth=31, skip_count=1, initial_frame=initial_frame@entry=0x7ffc7bc14de0, initial_pc=initial_pc@entry=0x0, sizes=0x0)
    at src/stacktrace_generic_fp-inl.h:175
#2  0x00007fd1da732c4a in GetStackTrace_generic_fp (result=0x560793e14010, max_depth=<optimized out>, skip_count=<optimized out>) at src/stacktrace_generic_fp-inl.h:332
#3  0x00007fd1da732fd2 in GetStackTrace (result=result@entry=0x560793e14010, max_depth=max_depth@entry=30, skip_count=skip_count@entry=0) at src/stacktrace.cc:346
#4  0x00007fd1da72390d in tcmalloc::PageHeap::HandleUnlock (this=0x7fd1da74f700 <tcmalloc::Static::pageheap_>, context=context@entry=0x7ffc7bc14e70) at src/page_heap.cc:155
#5  0x00007fd1da7253dc in tcmalloc::PageHeap::LockingContext::~LockingContext (this=0x7ffc7bc14e70, __in_chrg=<optimized out>) at src/page_heap.cc:77
#6  tcmalloc::PageHeap::NewWithSizeClass (this=this@entry=0x7fd1da74f700 <tcmalloc::Static::pageheap_>, n=n@entry=9, sizeclass=62) at src/page_heap.cc:172
#7  0x00007fd1da723010 in tcmalloc::CentralFreeList::Populate (this=this@entry=0x7fd1da8e4f00 <tcmalloc::Static::central_cache_+75392>) at src/static_vars.h:76
#8  0x00007fd1da7231e8 in tcmalloc::CentralFreeList::FetchFromOneSpansSafe (end=0x7ffc7bc14fc0, start=0x7ffc7bc14fb8, N=1, this=0x7fd1da8e4f00 <tcmalloc::Static::central_cache_+75392>) at src/central_freelist.cc:273
#9  tcmalloc::CentralFreeList::FetchFromOneSpansSafe (this=0x7fd1da8e4f00 <tcmalloc::Static::central_cache_+75392>, N=1, start=0x7ffc7bc14fb8, end=0x7ffc7bc14fc0) at src/central_freelist.cc:270
#10 0x00007fd1da723293 in tcmalloc::CentralFreeList::RemoveRange (this=0x7fd1da8e4f00 <tcmalloc::Static::central_cache_+75392>, start=start@entry=0x7ffc7bc14fb8, end=end@entry=0x7ffc7bc14fc0, N=1) at src/central_freelist.cc:253
#11 0x00007fd1da7271a7 in tcmalloc::ThreadCache::FetchFromCentralCache (this=this@entry=0x560793e34000, cl=cl@entry=62, byte_size=byte_size@entry=73728, oom_handler=oom_handler@entry=0x7fd1da713300 <(anonymous namespace)::nop_oom_handler(size_t)>)
    at src/thread_cache.cc:124
#12 0x00007fd1da73671d in tcmalloc::ThreadCache::Allocate (oom_handler=0x7fd1da713300 <(anonymous namespace)::nop_oom_handler(size_t)>, cl=62, size=73728, this=0x560793e34000) at src/thread_cache.h:381
#13 (anonymous namespace)::do_malloc (size=72704) at src/tcmalloc.cc:1414
#14 tcmalloc::do_allocate_full<tcmalloc::malloc_oom> (size=72704) at src/tcmalloc.cc:1804
#15 tcmalloc::allocate_full_malloc_oom (size=72704) at src/tcmalloc.cc:1820
#16 0x00007fd1d9e9997a in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6
#17 0x00007fd1df54947e in ?? () from /lib64/ld-linux-x86-64.so.2
#18 0x00007fd1df549568 in ?? () from /lib64/ld-linux-x86-64.so.2
#19 0x00007fd1df5632ea in ?? () from /lib64/ld-linux-x86-64.so.2
#20 0x0000000000000004 in ?? ()
#21 0x00007ffc7bc1583e in ?? ()
#22 0x00007ffc7bc15878 in ?? ()
#23 0x00007ffc7bc15883 in ?? ()
#24 0x00007ffc7bc1588d in ?? ()
#25 0x0000000000000000 in ?? ()
(gdb)
```

Not sure why it happen, but I guess there are some ralationship of bumping the gperftools to 2.13 (https://github.com/apache/incubator-pegasus/commit/3238686bf4ee062f819521817fc43f39bd70a548).","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1685/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1691,https://api.github.com/repos/apache/incubator-pegasus/issues/1691,incubator-pegasus,2008312434,1691,Public Pegasus Java client to org.apache repository,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-11-23T14:12:47Z,2023-11-23T14:13:53Z,"[Maven releases for Apache projects](https://infra.apache.org/publishing-maven-artifacts.html)
[Apache Maven repositories Basic information & FAQs about the ASF Jar repositories](https://infra.apache.org/repository-faq.html)
[Release Distribution Policy](https://infra.apache.org/release-distribution.html)

Remember to update the website: https://pegasus.apache.org/zh/clients/java-client
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1691/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1692,https://api.github.com/repos/apache/incubator-pegasus/issues/1692,incubator-pegasus,2008320925,1692,Make the official YCSB supports Pegasus,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-11-23T14:17:39Z,2023-11-23T14:17:39Z,"Make the official [YCSB](https://github.com/brianfrankcooper/YCSB) supports Pegasus.

There is already an emplementation in https://github.com/xiaomi/pegasus-ycsb.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1692/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1693,https://api.github.com/repos/apache/incubator-pegasus/issues/1693,incubator-pegasus,2008436810,1693,Remove the slog(shared log) completely,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-11-23T15:22:56Z,2023-12-14T15:57:40Z,"Since Pegasus 2.4, we have changed to way of using WALs(in Pegasus, they are shared log, a.k.a. slog, and private log, a.k.a. plog) to read&write slog, and just read legacy slog when bootstrap.

Since Pegasus 2.5, we will ensure all slog will be GCed.

So now, it's safe to remove all the slog related code. Meanwhile, It's possible that Pegasus users may upgrade Pegasus server from 2.3 or older versions to current version directly, so we have to leave a vadication to check that there is no slog when bootstrap to ensure all slogs have been consumed.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1693/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1693,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5uofph,incubator-pegasus,1856109153,1693,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-12-14T15:57:13Z,2023-12-14T15:57:13Z,Done by https://github.com/apache/incubator-pegasus/pull/1759,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5uofph/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1698,https://api.github.com/repos/apache/incubator-pegasus/issues/1698,incubator-pegasus,2016039955,1698,Admin-cli report ERR_INVALID_PARAMETERS when execute `backup` command,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-11-29T08:11:25Z,2023-11-29T09:14:58Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
```
Pegasus-AdminCli-1.2.0 » backup -h

backup a table

Usage:
  backup <TABLE_ID> <PROVIDER_TYPE> [SPECIFIC_BACKUP_PATH]

Args:
  tableID       int       the table ID
  providerType  string    the provider type of backup
  backupPath    string    the user specified backup path

Flags:
  -h, --help     display help

Pegasus-AdminCli-1.2.0 » backup 1 hdfs_service 12
error: StartBackupApp failed: ErrorCode({Errno:ERR_INVALID_PARAMETERS}) [hint: Backup failed: the default backup path has already configured in `hdfs_service`, please modify the configuration if you want to use a specific backup path.]
Pegasus-AdminCli-1.2.0 » backup 1 hdfs_service
error: StartBackupApp failed: ErrorCode({Errno:ERR_INVALID_PARAMETERS}) [hint: Backup failed: the default backup path has already configured in `hdfs_service`, please modify the configuration if you want to use a specific backup path.]

```

2. What did you expect to see?
The backup task can be executed successfully.

3. What did you see instead?
No matter if I pass `backupPath` parameter or not, the error reported in AdminCli.

4. What version of Pegasus are you using?
2.4.0","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1698/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1712,https://api.github.com/repos/apache/incubator-pegasus/issues/1712,incubator-pegasus,2027934953,1712,Improve Pegasus website,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-12-06T08:01:58Z,2025-01-19T07:49:15Z,"# Background
The Pegasus website (https://pegasus.apache.org) is not well maintenanced for a period of time, some of the documents are out of date, some new features are not mentioned or described how to use, lack a lot of English documents, some links are broken, some pages are not well rendered, etc.
Pegasus is an excellent distributed key-value store, it's a basic work to improve and perfect the website to make more users know how it works and how to use it.

NOTE: The codebase of the website is https://github.com/apache/incubator-pegasus-website, which is separated from https://github.com/apache/incubator-pegasus.

# Who can countribute?
Anybody!
Any volunteer is recruited and encouraged to improve any big or small imprvoments of the website!
Don't hesitate to assign the following task to yourself.

# How to countribute?
[The website](https://pegasus.apache.org) is a static website based on [Jekyll](https://jekyllrb.com/), and almost all of the documents are wrote in Markdown.
- Documents: It's a bit of easy to improve the documents as long as you know Markdown.
- Website: If you are familiar to static website frontend(basic HTML, CSS, JavaScript), you can help to improve the website layout, rendering, multiple version management, etc.

Now you can start with:

1. [Fork](https://docs.github.com/en/get-started/quickstart/fork-a-repo) the website Github repository

<img width=""1617"" alt=""image"" src=""https://github.com/apache/incubator-pegasus/assets/10775040/912d46de-a707-4e8d-a96f-4a01c4e73e3a"">

2. Git clone the code of the repository you just forked.

```
git clone git@github.com:<you_github_id>/incubator-pegasus-website.git
```

3. Find a task in the following list, or create a new task if you want.

4. Write the code to complete the task in your local workspace.

5. Check if it works as expect:

Install Jekyll at first https://jekyllrb.com/docs/

Then, simply, in you local workspace
```
$ bundle exec jekyll serve
...
 Auto-regeneration: enabled for '/Users/laiyingchun/dev/incubator-pegasus-website'
    Server address: http://127.0.0.1:4000
  Server running... press ctrl-c to stop.
```
You can preview the result by opening `http://127.0.0.1:4000` in you browser.

6. Submit the patch as a [pullrequest](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests)

7. The project managers will review the pullrequest soon, and will merge the PR if there is no problem.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1712/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1712,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5t0V-w,incubator-pegasus,1842438064,1712,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-12-06T08:43:43Z,2023-12-06T08:43:43Z,"# Tasks
Feel free to leave a message in the task link if you want to assign it to yourself.
## Translating
It's recommand to update the Chinese docs at first (if necessary), then translate it to English.
- [x] https://github.com/apache/incubator-pegasus/issues/1806
- [x] https://github.com/apache/incubator-pegasus/issues/1713
- [x] https://github.com/apache/incubator-pegasus/issues/1714
- [x] https://github.com/apache/incubator-pegasus/issues/1715
- [x] https://github.com/apache/incubator-pegasus/issues/1716
- [x] https://github.com/apache/incubator-pegasus/issues/1717
- [ ] https://github.com/apache/incubator-pegasus/issues/1718
- [ ] https://github.com/apache/incubator-pegasus/issues/1719
- [ ] https://github.com/apache/incubator-pegasus/issues/1720
- [ ] https://github.com/apache/incubator-pegasus/issues/1721
- [ ] https://github.com/apache/incubator-pegasus/issues/1722
- [x] https://github.com/apache/incubator-pegasus/issues/1723
- [x] https://github.com/apache/incubator-pegasus/issues/1724
- [x] https://github.com/apache/incubator-pegasus/issues/1725
- [x] https://github.com/apache/incubator-pegasus/issues/1726
- [x] https://github.com/apache/incubator-pegasus/issues/1727
- [x] https://github.com/apache/incubator-pegasus/issues/1728
- [x] https://github.com/apache/incubator-pegasus/issues/1729
- [x] https://github.com/apache/incubator-pegasus/issues/1730
- [ ] https://github.com/apache/incubator-pegasus/issues/1731
- [ ] https://github.com/apache/incubator-pegasus/issues/1732
- [x] https://github.com/apache/incubator-pegasus/issues/1733
- [x] https://github.com/apache/incubator-pegasus/issues/1734
- [x] https://github.com/apache/incubator-pegasus/issues/1735
- [ ] https://github.com/apache/incubator-pegasus/issues/1736
- [ ] https://github.com/apache/incubator-pegasus/issues/1737
- [ ] https://github.com/apache/incubator-pegasus/issues/1738
- [ ] https://github.com/apache/incubator-pegasus/issues/1739
- [ ] https://github.com/apache/incubator-pegasus/issues/1740
- [ ] https://github.com/apache/incubator-pegasus/issues/1741
- [x] https://github.com/apache/incubator-pegasus/issues/1742
- [x] https://github.com/apache/incubator-pegasus/issues/1743
- [ ] https://github.com/apache/incubator-pegasus/issues/1744
- [x] https://github.com/apache/incubator-pegasus/issues/1745
- [x] https://github.com/apache/incubator-pegasus/issues/1746
- [ ] https://github.com/apache/incubator-pegasus/issues/1747
- [x] https://github.com/apache/incubator-pegasus/issues/1748
- [ ] https://github.com/apache/incubator-pegasus/issues/1749
- [x] https://github.com/apache/incubator-pegasus/issues/1750
- [x] https://github.com/apache/incubator-pegasus/issues/1751
- [x] https://github.com/apache/incubator-pegasus/issues/1752
- [x] https://github.com/apache/incubator-pegasus/issues/1753
- [ ] https://github.com/apache/incubator-pegasus/issues/1754

## Website

- [ ] Upgrade the framework
- [ ] Improve the UI/UE
- [ ] Adapt both desktop and mobile devices
- [ ] You can add more tasks

## Others
- [ ] You can add more tasks","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5t0V-w/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1713,https://api.github.com/repos/apache/incubator-pegasus/issues/1713,incubator-pegasus,2028036109,1713,[Website][translate] https://pegasus.apache.org/overview/background/,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T08:42:34Z,2023-12-18T07:43:56Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1713/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1713,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5uFN93,incubator-pegasus,1846861687,1713,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-12-08T09:38:41Z,2023-12-08T09:38:41Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5uFN93/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1714,https://api.github.com/repos/apache/incubator-pegasus/issues/1714,incubator-pegasus,2028039581,1714,[Website][translate] https://pegasus.apache.org/overview/architecture/,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T08:44:24Z,2023-12-18T07:43:08Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1714/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1714,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5uO9Z_,incubator-pegasus,1849415295,1714,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-12-11T06:41:12Z,2023-12-11T06:41:12Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5uO9Z_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1715,https://api.github.com/repos/apache/incubator-pegasus/issues/1715,incubator-pegasus,2028040175,1715,[Website][translate] https://pegasus.apache.org/overview/data-model/,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T08:44:48Z,2023-12-18T13:06:47Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1715/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1715,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5uO-kb,incubator-pegasus,1849420059,1715,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-12-11T06:46:03Z,2023-12-11T06:46:03Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5uO-kb/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1716,https://api.github.com/repos/apache/incubator-pegasus/issues/1716,incubator-pegasus,2028044719,1716,[Website][translate] https://pegasus.apache.org/overview/benchmark/,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T08:47:48Z,2023-12-22T09:10:07Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1716/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1716,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5vOxoB,incubator-pegasus,1866144257,1716,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-12-21T12:11:22Z,2023-12-21T12:11:22Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5vOxoB/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1717,https://api.github.com/repos/apache/incubator-pegasus/issues/1717,incubator-pegasus,2028045680,1717,[Website][translate] https://pegasus.apache.org/overview/onebox/,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T08:48:26Z,2023-12-29T06:26:10Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1717/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1717,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5u3gZr,incubator-pegasus,1860044395,1717,NA,Samunroyu,36890229,,yujingweiop@gmail.com,NA,2023-12-18T10:26:09Z,2023-12-18T10:26:09Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5u3gZr/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1718,https://api.github.com/repos/apache/incubator-pegasus/issues/1718,incubator-pegasus,2028051277,1718,[Website][translate] https://pegasus.apache.org/clients/java-client,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-12-06T08:52:07Z,2023-12-18T10:26:01Z,"It would be greater to generate java client doc automatically, but at first, improve the java client code, make the comments more clear and complete.

Feel free to translate this site at first.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1718/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1718,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5u3gJR,incubator-pegasus,1860043345,1718,NA,Samunroyu,36890229,,yujingweiop@gmail.com,NA,2023-12-18T10:25:38Z,2023-12-18T10:25:38Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5u3gJR/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1719,https://api.github.com/repos/apache/incubator-pegasus/issues/1719,incubator-pegasus,2028059250,1719,[Website][translate] https://pegasus.apache.org/clients/cpp-client/,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-12-06T08:56:41Z,2023-12-06T09:07:40Z,"It would be greater to generate C++ client doc automatically (for example, by [doxygen](https://www.doxygen.nl/index.html)), but at first, improve the C++ client code, make the comments more clear and complete.

Feel free to translate this site at first.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1719/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1720,https://api.github.com/repos/apache/incubator-pegasus/issues/1720,incubator-pegasus,2028075731,1720,[Website][translate] https://pegasus.apache.org/clients/python-client/,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-12-06T09:06:11Z,2023-12-06T09:07:32Z,"It would be greater to generate Python API doc automatically (for example, by [pydoc](https://docs.python.org/3/library/pydoc.html)), but at first, improve the python client code, make the comments more clear and complete.

Feel free to translate this site at first.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1720/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1721,https://api.github.com/repos/apache/incubator-pegasus/issues/1721,incubator-pegasus,2028083275,1721,[Website][translate] https://pegasus.apache.org/clients/node-client,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-12-06T09:10:41Z,2023-12-06T09:10:41Z,"It would be greater to generate node.js API doc automatically (for example, by [apidocjs](https://apidocjs.com/)), but at first, improve the node.js client code, make the comments more clear and complete.

Feel free to translate this site at first.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1721/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1722,https://api.github.com/repos/apache/incubator-pegasus/issues/1722,incubator-pegasus,2028086684,1722,[Website][translate] https://pegasus.apache.org/clients/scala-client/,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-12-06T09:12:43Z,2023-12-06T09:12:43Z,"It would be greater to generate scala API doc automatically (for example, by [scaladoc](https://docs.scala-lang.org/style/scaladoc.html)), but at first, improve the scala client code, make the comments more clear and complete.

Feel free to translate this site at first.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1722/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1723,https://api.github.com/repos/apache/incubator-pegasus/issues/1723,incubator-pegasus,2028087996,1723,[Website][translate] https://pegasus.apache.org/docs/tools/shell/,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:13:35Z,2024-09-24T03:40:56Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1723/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1723,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5vm0qC,incubator-pegasus,1872448130,1723,NA,Samunroyu,36890229,,yujingweiop@gmail.com,NA,2023-12-30T04:21:43Z,2023-12-30T04:21:43Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5vm0qC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1724,https://api.github.com/repos/apache/incubator-pegasus/issues/1724,incubator-pegasus,2028089796,1724,[Website][translate] https://pegasus.apache.org/api/ttl,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:14:39Z,2024-01-04T03:50:05Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1724/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1724,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5vkgjj,incubator-pegasus,1871841507,1724,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2023-12-29T08:37:54Z,2023-12-29T08:37:54Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5vkgjj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1725,https://api.github.com/repos/apache/incubator-pegasus/issues/1725,incubator-pegasus,2028090502,1725,[Website][translate] https://pegasus.apache.org/api/single-atomic,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:15:02Z,2024-01-17T14:17:55Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1725/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1726,https://api.github.com/repos/apache/incubator-pegasus/issues/1726,incubator-pegasus,2028091173,1726,[Website][translate] https://pegasus.apache.org/api/redis,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:15:26Z,2024-01-12T04:04:55Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1726/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1726,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5v9gEE,incubator-pegasus,1878393092,1726,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-01-05T09:46:46Z,2024-01-05T09:46:46Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5v9gEE/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1727,https://api.github.com/repos/apache/incubator-pegasus/issues/1727,incubator-pegasus,2028091979,1727,[Website][translate] https://pegasus.apache.org/api/geo,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:15:54Z,2024-01-12T04:16:29Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1727/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1727,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5v9gGn,incubator-pegasus,1878393255,1727,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-01-05T09:46:54Z,2024-01-05T09:46:54Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5v9gGn/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1728,https://api.github.com/repos/apache/incubator-pegasus/issues/1728,incubator-pegasus,2028092866,1728,[Website][translate] https://pegasus.apache.org/api/http,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:16:27Z,2024-04-09T06:24:54Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1728/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1728,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5xAU-I,incubator-pegasus,1895911304,1728,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-01-17T14:20:00Z,2024-01-17T14:20:00Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5xAU-I/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1729,https://api.github.com/repos/apache/incubator-pegasus/issues/1729,incubator-pegasus,2028095055,1729,[Website][translate] https://pegasus.apache.org/administration/deployment,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:17:37Z,2024-01-24T08:24:08Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1729/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1729,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5xAaNi,incubator-pegasus,1895932770,1729,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-01-17T14:31:35Z,2024-01-17T14:31:35Z,Assign this task to me,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5xAaNi/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1730,https://api.github.com/repos/apache/incubator-pegasus/issues/1730,incubator-pegasus,2028095708,1730,[Website][translate] https://pegasus.apache.org/administration/config,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:17:57Z,2024-03-19T12:53:31Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1730/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1730,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5xrXaY,incubator-pegasus,1907193496,1730,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-01-24T01:23:24Z,2024-01-24T01:23:24Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5xrXaY/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1731,https://api.github.com/repos/apache/incubator-pegasus/issues/1731,incubator-pegasus,2028097371,1731,[Website][translate] https://pegasus.apache.org/administration/rebalance,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-12-06T09:18:38Z,2024-01-17T03:09:44Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1731/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1731,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5w8V1b,incubator-pegasus,1894866267,1731,NA,empiredan,743379,Dan Wang,,NA,2024-01-17T03:09:43Z,2024-01-17T03:09:43Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5w8V1b/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1732,https://api.github.com/repos/apache/incubator-pegasus/issues/1732,incubator-pegasus,2028098218,1732,[Website][translate] https://pegasus.apache.org/administration/monitoring,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-12-06T09:19:08Z,2024-01-17T03:19:30Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1732/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1732,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5w8XY_,incubator-pegasus,1894872639,1732,NA,limowang,64081036,Guangshuo Wang,,NA,2024-01-17T03:19:30Z,2024-01-17T03:19:30Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5w8XY_/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1733,https://api.github.com/repos/apache/incubator-pegasus/issues/1733,incubator-pegasus,2028099150,1733,[Website][translate] https://pegasus.apache.org/administration/rolling-update,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:19:42Z,2024-02-01T06:17:52Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1733/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1733,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5xwMqP,incubator-pegasus,1908460175,1733,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-01-24T16:16:47Z,2024-01-24T16:16:47Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5xwMqP/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1734,https://api.github.com/repos/apache/incubator-pegasus/issues/1734,incubator-pegasus,2028099747,1734,[Website][translate] https://pegasus.apache.org/administration/scale-in-out,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:20:05Z,2024-01-31T09:11:50Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1734/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1734,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5xwMXI,incubator-pegasus,1908458952,1734,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-01-24T16:16:08Z,2024-01-24T16:16:08Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5xwMXI/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1735,https://api.github.com/repos/apache/incubator-pegasus/issues/1735,incubator-pegasus,2028100301,1735,[Website][translate] https://pegasus.apache.org/administration/resource-management,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:20:26Z,2024-02-01T06:17:18Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1735/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1735,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yD60W,incubator-pegasus,1913629974,1735,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-01-28T15:15:53Z,2024-01-28T15:15:53Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yD60W/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1736,https://api.github.com/repos/apache/incubator-pegasus/issues/1736,incubator-pegasus,2028101157,1736,[Website][translate] https://pegasus.apache.org/administration/cold-backup,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-12-06T09:20:58Z,2023-12-06T09:20:58Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1736/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1737,https://api.github.com/repos/apache/incubator-pegasus/issues/1737,incubator-pegasus,2028101824,1737,[Website][translate] https://pegasus.apache.org/administration/meta-recovery,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-12-06T09:21:22Z,2024-01-31T03:21:41Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1737/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1737,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yVxvp,incubator-pegasus,1918311401,1737,NA,Samunroyu,36890229,,yujingweiop@gmail.com,NA,2024-01-31T03:21:40Z,2024-01-31T03:21:40Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yVxvp/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1738,https://api.github.com/repos/apache/incubator-pegasus/issues/1738,incubator-pegasus,2028102983,1738,[Website][translate] https://pegasus.apache.org/administration/replica-recovery,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-12-06T09:21:49Z,2024-01-31T03:21:37Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1738/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1738,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yVxvC,incubator-pegasus,1918311362,1738,NA,Samunroyu,36890229,,yujingweiop@gmail.com,NA,2024-01-31T03:21:36Z,2024-01-31T03:21:36Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yVxvC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1739,https://api.github.com/repos/apache/incubator-pegasus/issues/1739,incubator-pegasus,2028103759,1739,[Website][translate] https://pegasus.apache.org/administration/zk-migration,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-12-06T09:22:08Z,2024-02-06T16:36:32Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1739/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1739,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5zDc2J,incubator-pegasus,1930284425,1739,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-02-06T16:36:31Z,2024-02-06T16:36:31Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5zDc2J/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1740,https://api.github.com/repos/apache/incubator-pegasus/issues/1740,incubator-pegasus,2028105246,1740,[Website][translate] https://pegasus.apache.org/administration/table-migration,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-12-06T09:23:01Z,2023-12-06T09:23:01Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1740/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1741,https://api.github.com/repos/apache/incubator-pegasus/issues/1741,incubator-pegasus,2028105988,1741,[Website][translate] https://pegasus.apache.org/administration/table-soft-delete,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-12-06T09:23:29Z,2023-12-06T09:23:29Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1741/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1742,https://api.github.com/repos/apache/incubator-pegasus/issues/1742,incubator-pegasus,2028106834,1742,[Website][translate] https://pegasus.apache.org/administration/table-env,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:23:57Z,2024-04-22T06:37:03Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1742/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1742,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM54-e5T,incubator-pegasus,2029645395,1742,NA,ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,NA,2024-04-01T11:55:40Z,2024-04-01T11:55:40Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM54-e5T/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1743,https://api.github.com/repos/apache/incubator-pegasus/issues/1743,incubator-pegasus,2028107364,1743,[Website][translate] https://pegasus.apache.org/administration/remote-commands,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:24:17Z,2024-04-22T13:02:36Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1743/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1743,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yIwDU,incubator-pegasus,1914896596,1743,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-01-29T15:12:10Z,2024-01-29T15:12:10Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yIwDU/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1744,https://api.github.com/repos/apache/incubator-pegasus/issues/1744,incubator-pegasus,2028107870,1744,[Website][translate] https://pegasus.apache.org/administration/partition-split,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-12-06T09:24:34Z,2024-01-31T03:21:33Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1744/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1744,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yVxtr,incubator-pegasus,1918311275,1744,NA,Samunroyu,36890229,,yujingweiop@gmail.com,NA,2024-01-31T03:21:31Z,2024-01-31T03:21:31Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yVxtr/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1745,https://api.github.com/repos/apache/incubator-pegasus/issues/1745,incubator-pegasus,2028108488,1745,[Website][translate] https://pegasus.apache.org/administration/duplication,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:24:56Z,2024-01-29T15:10:45Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1745/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1745,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5uVxZc,incubator-pegasus,1851201116,1745,NA,ninsmiracle,110282526,,,NA,2023-12-12T02:25:04Z,2023-12-12T02:25:04Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5uVxZc/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1746,https://api.github.com/repos/apache/incubator-pegasus/issues/1746,incubator-pegasus,2028108998,1746,[Website][translate] https://pegasus.apache.org/administration/compression,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:25:13Z,2024-02-03T11:19:31Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1746/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1746,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5ypU7Q,incubator-pegasus,1923436240,1746,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-02-02T09:42:30Z,2024-02-02T09:42:30Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5ypU7Q/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1747,https://api.github.com/repos/apache/incubator-pegasus/issues/1747,incubator-pegasus,2028109518,1747,[Website][translate] https://pegasus.apache.org/administration/throttling,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-12-06T09:25:31Z,2023-12-06T09:25:31Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1747/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1748,https://api.github.com/repos/apache/incubator-pegasus/issues/1748,incubator-pegasus,2028109922,1748,[Website][translate] https://pegasus.apache.org/administration/experiences,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:25:46Z,2024-02-04T13:22:46Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1748/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1748,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5ywYOn,incubator-pegasus,1925284775,1748,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-02-03T11:20:05Z,2024-02-03T11:20:05Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5ywYOn/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1749,https://api.github.com/repos/apache/incubator-pegasus/issues/1749,incubator-pegasus,2028110391,1749,[Website][translate] https://pegasus.apache.org/administration/manual-compact,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-12-06T09:26:02Z,2024-01-18T02:43:41Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1749/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1749,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5xHE96,incubator-pegasus,1897680762,1749,NA,ninsmiracle,110282526,,,NA,2024-01-18T02:43:40Z,2024-01-18T02:43:40Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5xHE96/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1750,https://api.github.com/repos/apache/incubator-pegasus/issues/1750,incubator-pegasus,2028111996,1750,[Website][translate] https://pegasus.apache.org/administration/usage-scenario,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:26:58Z,2024-02-06T14:35:54Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1750/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1750,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yxjxw,incubator-pegasus,1925594224,1750,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-02-04T06:07:36Z,2024-02-04T06:07:36Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yxjxw/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1751,https://api.github.com/repos/apache/incubator-pegasus/issues/1751,incubator-pegasus,2028112519,1751,[Website][translate] https://pegasus.apache.org/administration/bad-disk,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:27:15Z,2024-02-06T14:35:44Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1751/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1751,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yyWi5,incubator-pegasus,1925802169,1751,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-02-04T15:52:10Z,2024-02-04T15:52:10Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yyWi5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1752,https://api.github.com/repos/apache/incubator-pegasus/issues/1752,incubator-pegasus,2028113047,1752,[Website][translate] https://pegasus.apache.org/administration/whitelist,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:27:32Z,2024-02-06T14:35:19Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1752/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1752,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5y0VGO,incubator-pegasus,1926320526,1752,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-02-05T06:39:05Z,2024-02-05T06:39:05Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5y0VGO/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1753,https://api.github.com/repos/apache/incubator-pegasus/issues/1753,incubator-pegasus,2028113544,1753,[Website][translate] https://pegasus.apache.org/administration/backup-request,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-06T09:27:49Z,2025-01-19T07:39:24Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1753/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1753,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6bBC0w,incubator-pegasus,2600742192,1753,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2025-01-19T07:39:20Z,2025-01-19T07:39:20Z,https://github.com/apache/incubator-pegasus-website/pull/88,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6bBC0w/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1754,https://api.github.com/repos/apache/incubator-pegasus/issues/1754,incubator-pegasus,2028113990,1754,[Website][translate] https://pegasus.apache.org/administration/hotspot-detection,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2023-12-06T09:28:05Z,2024-01-17T03:18:57Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1754/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1754,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5w8XTW,incubator-pegasus,1894872278,1754,NA,limowang,64081036,Guangshuo Wang,,NA,2024-01-17T03:18:57Z,2024-01-17T03:18:57Z,Assign this task to me.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5w8XTW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1761,https://api.github.com/repos/apache/incubator-pegasus/issues/1761,incubator-pegasus,2032441910,1761,"Compilation failed for go-collector in branch migrate-metrics-dev due to ""time.Now().UnixMilli undefined (type time.Time has no field or method UnixMilli)"" in avail/detector.go",empiredan,743379,Dan Wang,,CLOSED,2023-12-08T11:01:34Z,2023-12-11T12:33:58Z,"Compilation failed for go-collector as following messages:

```
go: downloading github.com/onsi/ginkgo v1.10.1
go: downloading github.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415
go: downloading github.com/xeipuuv/gojsonpointer v0.0.0-20180127040702-4e3ac2762d5f
go: downloading github.com/onsi/gomega v1.7.0
go: downloading github.com/mattn/go-isatty v0.0.12
go: downloading github.com/k0kubun/colorstring v0.0.0-20150214042306-9440f1994b88
go: downloading github.com/hpcloud/tail v1.0.0
go: downloading gopkg.in/tomb.v1 v1.0.0-2014[102](https://github.com/apache/incubator-pegasus/actions/runs/7137165426/job/19436733684?pr=1758#step:4:103)4135613-dd632973f1e7
go: downloading gopkg.in/fsnotify.v1 v1.4.7
go: downloading github.com/andybalholm/brotli v1.0.0
go: downloading github.com/valyala/bytebufferpool v1.0.0
go mod verify
all modules verified
go build -o collector
# github.com/pegasus-kv/collector/avail
Error: avail/detector.go:128:20: time.Now().UnixMilli undefined (type time.Time has no field or method UnixMilli)
Error: avail/detector.go:133:37: time.Now().UnixMilli undefined (type time.Time has no field or method UnixMilli)
Error: avail/detector.go:134:19: time.Now().UnixMilli undefined (type time.Time has no field or method UnixMilli)
Error: avail/detector.go:139:38: time.Now().UnixMilli undefined (type time.Time has no field or method UnixMilli)
make: *** [Makefile:21: build] Error 2
Error: Process completed with exit code 2.
```

and:
```
run golangci-lint
  Running [/home/runner/golangci-lint-1.29.0-linux-amd64/golangci-lint run --out-format=github-actions --path-prefix=./collector] in [/home/runner/work/incubator-pegasus/incubator-pegasus/collector] ...
  Error: level=warning msg=""[runner] Can't run linter goanalysis_metalinter: bodyclose: failed prerequisites: [buildssa@github.com/pegasus-kv/collector/avail: analysis skipped: errors in package: [/home/runner/work/incubator-pegasus/incubator-pegasus/collector/avail/detector.go:1[28](https://github.com/apache/incubator-pegasus/actions/runs/7137165426/job/19436733856?pr=1758#step:4:30):21: time.Now().UnixMilli undefined (type time.Time has no field or method UnixMilli) /home/runner/work/incubator-pegasus/incubator-pegasus/collector/avail/detector.go:133:38: time.Now().UnixMilli undefined (type time.Time has no field or method UnixMilli) /home/runner/work/incubator-pegasus/incubator-pegasus/collector/avail/detector.go:134:20: time.Now().UnixMilli undefined (type time.Time has no field or method UnixMilli) /home/runner/work/incubator-pegasus/incubator-pegasus/collector/avail/detector.go:139:39: time.Now().UnixMilli undefined (type time.Time has no field or method UnixMilli)]]""
  Error: level=warning msg=""[runner] Can't run linter unused: buildir: analysis skipped: errors in package: [/home/runner/work/incubator-pegasus/incubator-pegasus/collector/avail/detector.go:128:21: time.Now().UnixMilli undefined (type time.Time has no field or method UnixMilli) /home/runner/work/incubator-pegasus/incubator-pegasus/collector/avail/detector.go:133:38: time.Now().UnixMilli undefined (type time.Time has no field or method UnixMilli) /home/runner/work/incubator-pegasus/incubator-pegasus/collector/avail/detector.go:134:20: time.Now().UnixMilli undefined (type time.Time has no field or method UnixMilli) /home/runner/work/incubator-pegasus/incubator-pegasus/collector/avail/detector.go:139:39: time.Now().UnixMilli undefined (type time.Time has no field or method UnixMilli)]""
  Error: level=error msg=""Running error: buildir: analysis skipped: errors in package: [/home/runner/work/incubator-pegasus/incubator-pegasus/collector/avail/detector.go:128:21: time.Now().UnixMilli undefined (type time.Time has no field or method UnixMilli) /home/runner/work/incubator-pegasus/incubator-pegasus/collector/avail/detector.go:133:38: time.Now().UnixMilli undefined (type time.Time has no field or method UnixMilli) /home/runner/work/incubator-pegasus/incubator-pegasus/collector/avail/detector.go:134:20: time.Now().UnixMilli undefined (type time.Time has no field or method UnixMilli) /home/runner/work/incubator-pegasus/incubator-pegasus/collector/avail/detector.go:139:39: time.Now().UnixMilli undefined (type time.Time has no field or method UnixMilli)]""
  
  Error: golangci-lint exit with code 3
  Ran golangci-lint in 962ms
```

The reason is that `UnixMilli` was introduced in go version 1.17. We could upgrade the go version to 1.17 for the workflows on Github. Also, official document should be updated to tell the users what is the minimum go version that is supported.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1761/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1761,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5uQMM-,incubator-pegasus,1849738046,1761,NA,empiredan,743379,Dan Wang,,NA,2023-12-11T10:13:53Z,2023-12-11T10:13:53Z,"Following errors were also founded for the workflows:
![image](https://github.com/apache/incubator-pegasus/assets/743379/7579623d-dc26-4b55-9957-6e62feede631)

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5uQMM-/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1772,https://api.github.com/repos/apache/incubator-pegasus/issues/1772,incubator-pegasus,2041603790,1772,"Building jobs failed for lower C++ compiler versions due to ""fatal error: filesystem: No such file or directory  #include <filesystem>""",empiredan,743379,Dan Wang,,CLOSED,2023-12-14T12:30:35Z,2023-12-15T07:09:47Z,"Building jobs for Ubuntu 1804, CentOS 7 and clang-9 failed while the Action `Lint and build regularly` was run on Github:
```
[ 90%] Building CXX object src/shell/CMakeFiles/pegasus_shell.dir/commands/debugger.cpp.o
/root/incubator-pegasus/src/shell/commands/debugger.cpp:38:10: fatal error: filesystem: No such file or directory
 #include <filesystem>
          ^~~~~~~~~~~~
compilation terminated.
make[2]: *** [src/shell/CMakeFiles/pegasus_shell.dir/commands/debugger.cpp.o] Error 1
make[1]: *** [src/shell/CMakeFiles/pegasus_shell.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
```

The compiler versions of Ubuntu 1804, CentOS 7 and clang-9 are gcc 7.5.0, gcc 7.3.1 and clang 9. All of these versions do not support including `<filesystem>` header file and using `std::filesystem`. The minimum compiler versions that support `std::filesystem` are gcc 8 and clang 10.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1772/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1772,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5utWub,incubator-pegasus,1857383323,1772,NA,empiredan,743379,Dan Wang,,NA,2023-12-15T07:09:47Z,2023-12-15T07:09:47Z,"This problem has been fixed in https://github.com/apache/incubator-pegasus/pull/1771.

All jobs of Action [Lint and build regularly](https://github.com/apache/incubator-pegasus/actions/workflows/regular-build.yml) are now back to normal(https://github.com/apache/incubator-pegasus/actions/runs/7218700571):
![image](https://github.com/apache/incubator-pegasus/assets/743379/de705e64-2d34-4e2b-aed1-c748c368710b)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5utWub/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1779,https://api.github.com/repos/apache/incubator-pegasus/issues/1779,incubator-pegasus,2046988624,1779,Improve the client libraries CI,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-18T16:07:49Z,2024-01-05T09:45:43Z,"Now the various client libraries CI procedure will download old versions (e.g. 2.0, 2.1) of Pegasus server binaries (even from external repository).
In the case of adding new interface for both client and server side, the current implementation dosen't meet the requirement.
We should imptove this situation.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1779/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1788,https://api.github.com/repos/apache/incubator-pegasus/issues/1788,incubator-pegasus,2047957435,1788,Error occurred while building rocksdb on the environment where a lower-version zstd is pre-installed,empiredan,743379,Dan Wang,,CLOSED,2023-12-19T05:37:29Z,2023-12-19T09:20:54Z,"Error occurred while building rocksdb -- undefined reference to `ZSTD_compressStream2', as below:
```
[ 81%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/lock/range/range_tree/range_tree_lock_manager.cc.o
[ 81%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/lock/range/range_tree/range_tree_lock_tracker.cc.o
[ 81%] Building CXX object CMakeFiles/rocksdb.dir/utilities/blob_db/blob_db.cc.o
[ 81%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/optimistic_transaction_db_impl.cc.o
[ 81%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/optimistic_transaction.cc.o
[ 82%] Building CXX object CMakeFiles/rocksdb.dir/utilities/blob_db/blob_db_impl.cc.o
[ 82%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/pessimistic_transaction.cc.o
[ 82%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/pessimistic_transaction_db.cc.o
[ 82%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/snapshot_checker.cc.o
[ 83%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/transaction_base.cc.o
[ 83%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/transaction_db_mutex_impl.cc.o
[ 83%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/transaction_util.cc.o
[ 83%] Building CXX object CMakeFiles/rocksdb.dir/utilities/blob_db/blob_db_impl_filesnapshot.cc.o
[ 83%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/write_prepared_txn.cc.o
[ 83%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/write_prepared_txn_db.cc.o
[ 83%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/write_unprepared_txn.cc.o
[ 83%] Building CXX object CMakeFiles/rocksdb.dir/utilities/blob_db/blob_dump_tool.cc.o
[ 84%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/write_unprepared_txn_db.cc.o
[ 84%] Building CXX object CMakeFiles/rocksdb.dir/utilities/blob_db/blob_file.cc.o
[ 84%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/ttl/db_ttl_impl.cc.o
[ 84%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/wal_filter.cc.o
[ 84%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/write_batch_with_index/write_batch_with_index.cc.o
[ 84%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/write_batch_with_index/write_batch_with_index_internal.cc.o
[ 84%] Building CXX object CMakeFiles/rocksdb.dir/utilities/cache_dump_load.cc.o
[ 84%] Building CXX object CMakeFiles/rocksdb.dir/utilities/cache_dump_load_impl.cc.o
[ 84%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/lock/range/range_tree/lib/locktree/concurrent_tree.cc.o
[ 84%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/lock/range/range_tree/lib/locktree/keyrange.cc.o
[ 85%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/lock/range/range_tree/lib/locktree/lock_request.cc.o
[ 85%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/lock/range/range_tree/lib/locktree/locktree.cc.o
[ 85%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/lock/range/range_tree/lib/locktree/manager.cc.o
[ 85%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/lock/range/range_tree/lib/locktree/range_buffer.cc.o
[ 85%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/lock/range/range_tree/lib/locktree/treenode.cc.o
[ 85%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/lock/range/range_tree/lib/locktree/txnid_set.cc.o
[ 85%] Building CXX object CMakeFiles/rocksdb.dir/utilities/cassandra/cassandra_compaction_filter.cc.o
[ 85%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/lock/range/range_tree/lib/locktree/wfg.cc.o
[ 86%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/lock/range/range_tree/lib/standalone_port.cc.o
[ 86%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/lock/range/range_tree/lib/util/dbt.cc.o
[ 86%] Building CXX object CMakeFiles/rocksdb-shared.dir/utilities/transactions/lock/range/range_tree/lib/util/memarena.cc.o
[ 86%] Building CXX object CMakeFiles/rocksdb-shared.dir/plugin/encfs/encfs.cc.o
[ 86%] Building CXX object CMakeFiles/rocksdb-shared.dir/port/port_posix.cc.o
[ 86%] Building CXX object CMakeFiles/rocksdb-shared.dir/env/env_posix.cc.o
[ 86%] Building CXX object CMakeFiles/rocksdb-shared.dir/env/fs_posix.cc.o
[ 87%] Building CXX object CMakeFiles/rocksdb.dir/utilities/cassandra/format.cc.o
[ 88%] Building CXX object CMakeFiles/rocksdb-shared.dir/env/io_posix.cc.o
[ 88%] Building CXX object CMakeFiles/rocksdb.dir/utilities/cassandra/merge_operator.cc.o
[ 88%] Building CXX object CMakeFiles/rocksdb-shared.dir/build_version.cc.o
[ 88%] Building CXX object CMakeFiles/rocksdb.dir/utilities/checkpoint/checkpoint_impl.cc.o
[ 88%] Building CXX object CMakeFiles/rocksdb.dir/utilities/compaction_filters.cc.o
[ 88%] Building CXX object CMakeFiles/rocksdb.dir/utilities/compaction_filters/remove_emptyvalue_compactionfilter.cc.o
[ 88%] Building CXX object CMakeFiles/rocksdb.dir/utilities/counted_fs.cc.o
[ 88%] Linking CXX shared library librocksdb.so
[ 88%] Building CXX object CMakeFiles/rocksdb.dir/utilities/debug.cc.o
[ 88%] Built target rocksdb-shared
[ 88%] Building CXX object CMakeFiles/block_cache_trace_analyzer.dir/tools/block_cache_analyzer/block_cache_trace_analyzer_tool.cc.o
[ 88%] Linking CXX executable block_cache_trace_analyzer
librocksdb.so.8.5.3: undefined reference to `ZSTD_compressStream2'
collect2: error: ld returned 1 exit status
make[5]: *** [block_cache_trace_analyzer] Error 1
make[4]: *** [CMakeFiles/block_cache_trace_analyzer.dir/all] Error 2
make[4]: *** Waiting for unfinished jobs....

......

[ 99%] Linking CXX static library librocksdb.a
[ 99%] Built target rocksdb
make[3]: *** [all] Error 2
make[2]: *** [Stamp/rocksdb/rocksdb-build] Error 2
make[1]: *** [CMakeFiles/rocksdb.dir/all] Error 2
make: *** [all] Error 2

```

It was found that instead of zstd that was installed by the third-party package, pre-installed /lib/libzstd.so with lower version was used to build rocksdb, leading to undefined reference to `ZSTD_compressStream2':
```
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /opt/rh/devtoolset-7/root/usr/bin/g++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /opt/rh/devtoolset-7/root/usr/bin/gcc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Found lz4: /lib64/liblz4.so
-- Found zstd: /lib/libzstd.so
-- Performing Test HAVE_OMIT_LEAF_FRAME_POINTER
-- Performing Test HAVE_OMIT_LEAF_FRAME_POINTER - Success
-- Performing Test BUILTIN_ATOMIC
-- Performing Test BUILTIN_ATOMIC - Success
-- Could NOT find uring (missing: uring_LIBRARIES uring_INCLUDE_DIR)
-- Enabling RTTI in all builds
-- Performing Test HAVE_FALLOCATE
-- Performing Test HAVE_FALLOCATE - Success
-- Performing Test HAVE_SYNC_FILE_RANGE_WRITE
-- Performing Test HAVE_SYNC_FILE_RANGE_WRITE - Success
-- Performing Test HAVE_PTHREAD_MUTEX_ADAPTIVE_NP
-- Performing Test HAVE_PTHREAD_MUTEX_ADAPTIVE_NP - Success
-- Looking for malloc_usable_size
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1788/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1788,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5vAe9z,incubator-pegasus,1862397811,1788,NA,empiredan,743379,Dan Wang,,NA,2023-12-19T09:20:53Z,2023-12-19T09:20:53Z,This issue has been resolved by https://github.com/apache/incubator-pegasus/pull/1777.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5vAe9z/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1792,https://api.github.com/repos/apache/incubator-pegasus/issues/1792,incubator-pegasus,2048639295,1792,Error occurred that snappy/zstd/lz4 is not required dependencies while running packing server,empiredan,743379,Dan Wang,,CLOSED,2023-12-19T13:15:36Z,2023-12-20T02:55:41Z,"All of snappy/zstd/lz4 are reported error ""is not a required dependency"" while running `./run.sh pack_server`:
```
$ ./run.sh pack_server
Packaging pegasus server 2.5.0-SNAPSHOT (b5dfde84f06075c7928c6db55cdba3ace5718e3f) glibc-2.17 release ...
‘/data/src/pegasus/build/latest/output/bin/pegasus_server/pegasus_server’ -> ‘pegasus-server-2.5.0-SNAPSHOT-b5dfde8-glibc2.17-release/bin/pegasus_server’
‘/data/src/pegasus/build/latest/output/lib/libdsn_meta_server.so’ -> ‘pegasus-server-2.5.0-SNAPSHOT-b5dfde8-glibc2.17-release/bin/libdsn_meta_server.so’
‘/data/src/pegasus/build/latest/output/lib/libdsn_replica_server.so’ -> ‘pegasus-server-2.5.0-SNAPSHOT-b5dfde8-glibc2.17-release/bin/libdsn_replica_server.so’
‘/data/src/pegasus/build/latest/output/lib/libdsn_utils.so’ -> ‘pegasus-server-2.5.0-SNAPSHOT-b5dfde8-glibc2.17-release/bin/libdsn_utils.so’
‘./thirdparty/output/lib/libtcmalloc_and_profiler.so’ -> ‘pegasus-server-2.5.0-SNAPSHOT-b5dfde8-glibc2.17-release/bin/libtcmalloc_and_profiler.so’
‘./thirdparty/output/lib/libboost_filesystem.so.1.69.0’ -> ‘pegasus-server-2.5.0-SNAPSHOT-b5dfde8-glibc2.17-release/bin/libboost_filesystem.so.1.69.0’
‘./thirdparty/output/lib/libboost_system.so.1.69.0’ -> ‘pegasus-server-2.5.0-SNAPSHOT-b5dfde8-glibc2.17-release/bin/libboost_system.so.1.69.0’
‘./thirdparty/output/lib/libhdfs.a’ -> ‘pegasus-server-2.5.0-SNAPSHOT-b5dfde8-glibc2.17-release/bin/libhdfs.a’
‘./thirdparty/output/lib/libhdfs.so’ -> ‘pegasus-server-2.5.0-SNAPSHOT-b5dfde8-glibc2.17-release/bin/libhdfs.so’
‘./thirdparty/output/lib/libhdfs.so.0.0.0’ -> ‘pegasus-server-2.5.0-SNAPSHOT-b5dfde8-glibc2.17-release/bin/libhdfs.so.0.0.0’
‘./scripts/sendmail.sh’ -> ‘pegasus-server-2.5.0-SNAPSHOT-b5dfde8-glibc2.17-release/bin/sendmail.sh’
‘./src/server/config.ini’ -> ‘pegasus-server-2.5.0-SNAPSHOT-b5dfde8-glibc2.17-release/bin/config.ini’
‘./src/server/config.min.ini’ -> ‘pegasus-server-2.5.0-SNAPSHOT-b5dfde8-glibc2.17-release/bin/config.min.ini’
‘./scripts/config_hdfs.sh’ -> ‘pegasus-server-2.5.0-SNAPSHOT-b5dfde8-glibc2.17-release/bin/config_hdfs.sh’
‘/lib64/libstdc++.so.6’ -> ‘pegasus-server-2.5.0-SNAPSHOT-b5dfde8-glibc2.17-release/bin/libstdc++.so.6’
ERROR: snappy is not a required dependency, skip packaging this lib
‘/lib64/libcrypto.so.10’ -> ‘pegasus-server-2.5.0-SNAPSHOT-b5dfde8-glibc2.17-release/bin/libcrypto.so.10’
‘/lib64/libssl.so.10’ -> ‘pegasus-server-2.5.0-SNAPSHOT-b5dfde8-glibc2.17-release/bin/libssl.so.10’
ERROR: zstd is not a required dependency, skip packaging this lib
ERROR: lz4 is not a required dependency, skip packaging this lib
Couldn't find env  or no valid keytab file was specified,
          hadoop-related files were not packed.
Done
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1792/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1792,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5vFusv,incubator-pegasus,1863772975,1792,NA,empiredan,743379,Dan Wang,,NA,2023-12-20T02:55:41Z,2023-12-20T02:55:41Z,This issue is fixed by https://github.com/apache/incubator-pegasus/pull/1794.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5vFusv/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1795,https://api.github.com/repos/apache/incubator-pegasus/issues/1795,incubator-pegasus,2048657796,1795,"The executable binary pegasus_server generated by pack_server.sh failed to be run due to ""librocksdb.so.8: cannot open shared object file"" ",empiredan,743379,Dan Wang,,CLOSED,2023-12-19T13:26:24Z,2023-12-20T02:55:57Z,"The executable binary `pegasus_server` generated by `./run.sh pack_server` failed to be run due to ""librocksdb.so.8: cannot open shared object file"":
```
/data/pegasus/bin/pegasus_server: error while loading shared libraries: librocksdb.so.8: cannot open shared object file: No such file or directory
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1795/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1795,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5vFuvS,incubator-pegasus,1863773138,1795,NA,empiredan,743379,Dan Wang,,NA,2023-12-20T02:55:56Z,2023-12-20T02:55:56Z,This issue is fixed by https://github.com/apache/incubator-pegasus/pull/1796.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5vFuvS/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1805,https://api.github.com/repos/apache/incubator-pegasus/issues/1805,incubator-pegasus,2050241499,1805,"Workflow failed while packing server due to ""'./thirdparty/output/lib64/librocksdb.so.8': No such file or directory""",empiredan,743379,Dan Wang,,CLOSED,2023-12-20T10:10:45Z,2023-12-22T03:57:48Z,"Workflow [lint_and_test_cpp](https://github.com/apache/incubator-pegasus/actions/workflows/lint_and_test_cpp.yaml) failed while packing server:
```
Run ./run.sh pack_server
  ./run.sh pack_server
  rm -rf pegasus-server-*
  shell: sh -e {0}
  env:
    ONEBOX_OPTS: disk_min_available_space_ratio=5
    TEST_OPTS: disk_min_available_space_ratio=5,throttle_test_medium_value_kb=10,throttle_test_large_value_kb=[2](https://github.com/apache/incubator-pegasus/actions/runs/7271954614/job/19813303678?pr=1803#step:11:2)5
Packaging pegasus server 2.5.0-SNAPSHOT (non-git-repo) glibc-2.[3](https://github.com/apache/incubator-pegasus/actions/runs/7271954614/job/19813303678?pr=1803#step:11:3)5 release ...
'/__w/incubator-pegasus/incubator-pegasus/build/latest/output/bin/pegasus_server/pegasus_server' -> 'pegasus-server-2.5.0-SNAPSHOT-non-git-glibc2.35-release/bin/pegasus_server'
'/__w/incubator-pegasus/incubator-pegasus/build/latest/output/lib/libdsn_meta_server.so' -> 'pegasus-server-2.5.0-SNAPSHOT-non-git-glibc2.35-release/bin/libdsn_meta_server.so'
'/__w/incubator-pegasus/incubator-pegasus/build/latest/output/lib/libdsn_replica_server.so' -> 'pegasus-server-2.5.0-SNAPSHOT-non-git-glibc2.35-release/bin/libdsn_replica_server.so'
'/__w/incubator-pegasus/incubator-pegasus/build/latest/output/lib/libdsn_utils.so' -> 'pegasus-server-2.5.0-SNAPSHOT-non-git-glibc2.35-release/bin/libdsn_utils.so'
'./thirdparty/output/lib/libtcmalloc_and_profiler.so' -> 'pegasus-server-2.5.0-SNAPSHOT-non-git-glibc2.35-release/bin/libtcmalloc_and_profiler.so'
'./thirdparty/output/lib/libboost_filesystem.so.1.69.0' -> 'pegasus-server-2.5.0-SNAPSHOT-non-git-glibc2.35-release/bin/libboost_filesystem.so.1.69.0'
'./thirdparty/output/lib/libboost_system.so.1.69.0' -> 'pegasus-server-2.5.0-SNAPSHOT-non-git-glibc2.35-release/bin/libboost_system.so.1.69.0'
'./thirdparty/output/lib/libhdfs.a' -> 'pegasus-server-2.5.0-SNAPSHOT-non-git-glibc2.35-release/bin/libhdfs.a'
'./thirdparty/output/lib/libhdfs.so' -> 'pegasus-server-2.5.0-SNAPSHOT-non-git-glibc2.35-release/bin/libhdfs.so'
'./thirdparty/output/lib/libhdfs.so.0.0.0' -> 'pegasus-server-2.5.0-SNAPSHOT-non-git-glibc2.35-release/bin/libhdfs.so.0.0.0'
cp: cannot stat './thirdparty/output/lib64/librocksdb.so.8': No such file or directory
Error: Process completed with exit code 1.
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1805/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1805,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5vS0Ks,incubator-pegasus,1867203244,1805,NA,empiredan,743379,Dan Wang,,NA,2023-12-22T03:57:48Z,2023-12-22T03:57:48Z,This issue is fixed by https://github.com/apache/incubator-pegasus/pull/1804.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5vS0Ks/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1806,https://api.github.com/repos/apache/incubator-pegasus/issues/1806,incubator-pegasus,2050484556,1806,[Website][translate] https://pegasus.apache.org/overview/,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2023-12-20T12:46:27Z,2023-12-20T13:21:48Z,,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1806/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1807,https://api.github.com/repos/apache/incubator-pegasus/issues/1807,incubator-pegasus,2051026952,1807,The process is not terminated while invalid options are being loaded from configuration file,empiredan,743379,Dan Wang,,CLOSED,2023-12-20T17:57:31Z,2023-12-22T03:58:33Z,"While some options are loaded from configuration file with invalid values, all of them should be found by flags' individual and grouped validators and the process should be terminated by `abort()`.

However, after the options were set with invalid values in configuration file for test, the process was not terminated, and there were also not any error log.

By checking the code, it is found that flags' validation, for both individuals and groups, are actually run normally and correctly. The problem is that the loading for options happens before the logging is initialized. At that time, `simple_logger` has not been created. Default logger, i.e. `screen_logger`, would be used. Therefore, once invalid values are found, `screen_logger::dsn_log` would be called. However, this function is empty, thus there would not be any logging message being printed; and the process would never be terminated due to fatal error.

According to this analysis, once any assertion, such as `CHECK` or `CHECK_EQ`, would fail to be executed for the invalid values: never print any logging message and terminate the process.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1807/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1807,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5vS0a6,incubator-pegasus,1867204282,1807,NA,empiredan,743379,Dan Wang,,NA,2023-12-22T03:58:33Z,2023-12-22T03:58:33Z,This bug is fixed by https://github.com/apache/incubator-pegasus/pull/1803.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5vS0a6/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1810,https://api.github.com/repos/apache/incubator-pegasus/issues/1810,incubator-pegasus,2052598869,1810,json_decode & json_encode for rpc_host_port,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2023-12-21T15:08:03Z,2023-12-28T09:47:20Z,"## Feature Request

A part of https://github.com/apache/incubator-pegasus/issues/1403.

Implement json_decode & json_encode for class `rpc_host_port`.
From this, we can transfer struct `rpc_host_port` by rpc.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1810/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1817,https://api.github.com/repos/apache/incubator-pegasus/issues/1817,incubator-pegasus,2053953478,1817,"Standby meta server exits abnormally with core dump after receiving http request ""/meta/cluster"" ",empiredan,743379,Dan Wang,,CLOSED,2023-12-22T13:52:25Z,2023-12-25T06:32:28Z,"It is found that after sending http request ""/meta/cluster"" to any standby meta server (for example, to sending request to a local meta server with port 34601, the command would be `curl -v 127.0.0.1:34601/meta/cluster`), it would exits abnormally with core dump. Running gdb over the core dump file would output following messages:
```
root@d04a6b37202f:/data/code/incubator-pegasus/onebox/meta1# gdb /data/code/incubator-pegasus/onebox/meta1/pegasus_server 'core.1703160155. meta.default.4.149'
Core was generated by `/data/code/incubator-pegasus/onebox/meta1/pegasus_server config.ini -app_list m'.
Program terminated with signal SIGSEGV, Segmentation fault.
#0  0x00007f686b5fb653 in dsn::replication::meta_http_service::get_cluster_info_handler (this=0x55da218de340, req=..., resp=...) at /data/code/incubator-pegasus/src/meta/meta_http_service.cpp:487
487	        _service->_balancer->get_balance_operation_count(balance_operation_type));
[Current thread is 1 (Thread 0x7f685f73a640 (LWP 423))]
Installing openjdk unwinder
Traceback (most recent call last):
  File ""/usr/share/gdb/auto-load/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so-gdb.py"", line 52, in <module>
    class Types(object):
  File ""/usr/share/gdb/auto-load/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so-gdb.py"", line 66, in Types
    nmethodp_t = gdb.lookup_type('nmethod').pointer()
gdb.error: No type named nmethod.
(gdb) bt
#0  0x00007f686b5fb653 in dsn::replication::meta_http_service::get_cluster_info_handler (this=0x55da218de340, req=..., resp=...) at /data/code/incubator-pegasus/src/meta/meta_http_service.cpp:487
#1  0x00007f686c1a89c7 in std::function<void (dsn::http_request const&, dsn::http_response&)>::operator()(dsn::http_request const&, dsn::http_response&) const (__args#1=..., __args#0=..., this=0x55da21f73660) at /usr/include/c++/11/bits/std_function.h:590
#2  dsn::http_server::serve (this=<optimized out>, msg=0x55da220648f0) at /data/code/incubator-pegasus/src/http/http_server.cpp:152
#3  0x00007f686c205d6b in std::function<void (dsn::message_ex*)>::operator()(dsn::message_ex*) const (__args#0=<optimized out>, this=0x55da219387d0) at /usr/include/c++/11/bits/std_function.h:586
#4  dsn::rpc_request_task::exec (this=0x55da21938700) at /data/code/incubator-pegasus/src/runtime/task/task.h:435
#5  0x00007f686c2083cf in dsn::task::exec_internal (this=0x55da21938700) at /data/code/incubator-pegasus/src/runtime/task/task.cpp:173
#6  0x00007f686c22285a in dsn::task_worker::loop (this=0x55da21f44f00) at /data/code/incubator-pegasus/src/runtime/task/task_worker.cpp:245
#7  0x00007f686a031253 in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6
#8  0x00007f6869cbaac3 in ?? () from /lib/x86_64-linux-gnu/libc.so.6
#9  0x00007f6869d4bbf4 in clone () from /lib/x86_64-linux-gnu/libc.so.6
(gdb) f 1
#1  0x00007f686c1a89c7 in std::function<void (dsn::http_request const&, dsn::http_response&)>::operator()(dsn::http_request const&, dsn::http_response&) const (__args#1=..., __args#0=..., this=0x55da21f73660) at /usr/include/c++/11/bits/std_function.h:590
590		return _M_invoker(_M_functor, std::forward<_ArgTypes>(__args)...);
(gdb) f 0
#0  0x00007f686b5fb653 in dsn::replication::meta_http_service::get_cluster_info_handler (this=0x55da218de340, req=..., resp=...) at /data/code/incubator-pegasus/src/meta/meta_http_service.cpp:487
487	        _service->_balancer->get_balance_operation_count(balance_operation_type));
(gdb) list
482	        _meta_function_level_VALUES_TO_NAMES.find(_service->get_function_level())->second + 3);
483	    std::vector<std::string> balance_operation_type;
484	    balance_operation_type.emplace_back(""detail"");
485	    tp.add_row_name_and_data(
486	        ""balance_operation_count"",
487	        _service->_balancer->get_balance_operation_count(balance_operation_type));
488	    double primary_stddev, total_stddev;
489	    _service->_state->get_cluster_balance_score(primary_stddev, total_stddev);
490	    tp.add_row_name_and_data(""primary_replica_count_stddev"", primary_stddev);
491	    tp.add_row_name_and_data(""total_replica_count_stddev"", total_stddev);
(gdb) p _service
$1 = (dsn::replication::meta_service *) 0x55da21aaa000
(gdb) p _service->_balancer
$2 = std::shared_ptr<dsn::replication::server_load_balancer> (empty) = {get() = 0x0}
```

It is obvious that the direct reason for core dump `is _service->_balancer` is null. The root cause for this problem is that since meta server is built with `DSN_MOCK_TEST`, besides primary meta server, `redirect_if_not_primary` would also return true for any standby meta server,  whose `_service->_balancer` is certainly null. 

```c++
bool meta_http_service::redirect_if_not_primary(const http_request &req, http_response &resp)
{
#ifdef DSN_MOCK_TEST
    return true;
#endif
    rpc_address leader;
    if (_service->_failure_detector->get_leader(&leader))
        return true;
    // set redirect response
    resp.location = ""http://"" + leader.to_std_string() + '/' + req.path;
    if (!req.query_args.empty()) {
        resp.location += '?';
        for (const auto &i : req.query_args) {
            resp.location += i.first + '=' + i.second + '&';
        }
        resp.location.pop_back(); // remove final '&'
    }
    resp.location.erase(std::remove(resp.location.begin(), resp.location.end(), '\0'),
                        resp.location.end()); // remove final '\0'
    resp.status_code = http_status_code::temporary_redirect;
    return false;
}
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1817/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1817,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5vY1hY,incubator-pegasus,1868781656,1817,NA,empiredan,743379,Dan Wang,,NA,2023-12-25T06:32:28Z,2023-12-25T06:32:28Z,This issue has been fixed by https://github.com/apache/incubator-pegasus/pull/1816.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5vY1hY/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1819,https://api.github.com/repos/apache/incubator-pegasus/issues/1819,incubator-pegasus,2058127812,1819,"go client dosen't contain some packages, eg. replication、rrdb in the lastest verion",lengyuexuexuan,46274877,,,CLOSED,2023-12-28T08:08:28Z,2023-12-29T02:08:41Z,"After updating the go client, the test program report errors. As follows.
![1](https://github.com/apache/incubator-pegasus/assets/46274877/d9fc469a-5d28-493c-b87e-213d4205dd46)

I don't find these packages in go client code.
![2](https://github.com/apache/incubator-pegasus/assets/46274877/465e72b7-8624-4bf7-ae6d-4a1b8259c96e)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1819/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1820,https://api.github.com/repos/apache/incubator-pegasus/issues/1820,incubator-pegasus,2059159435,1820,Feature(new_metrics): go-collector supports to collect and expose the metrics to Prometheus,empiredan,743379,Dan Wang,,OPEN,2023-12-29T07:36:04Z,2024-01-03T09:44:56Z,"# 1. Prometheus metrics of old framework

In the old framework, all the metrics have the identical metric labels. The only difference is that for those metrics which have no attributes of `app` and `partition`, their values will be left empty as following example:

```
zion_profiler_RPC_RRDB_RRDB_GET_qps{cluster=""pegasus_cluster"",host_name=""abc.xyz"",pegasus_job=""replica"",port=""34601"",service=""pegasus"",app="""",partition=""""} 0.000000
```

For those metrics which have attributes of `app` and `partition` their values will certainly left non-empty:

```
replica_app_pegasus_get_qps{cluster=""pegasus_cluster"",host_name=""abc.xyz"",pegasus_job=""replica"",port=""34801"",service=""pegasus"",app=""4"",partition=""2""} 0.000000
```

# 2. The Design of Prometheus metrics of new framework

Referring to the old framework and the features of new metrics system, the new Prometheus labels could be designed as below:

| Label name | Description | Metric types | Entities |
| :-----: | :----: | :----: |:----: |
| cluster | cluster name | - | - |
| role | role name, i.e. meta/replica | - | - |
| host | hostname of the server | - | - |
| port | the port of the role instance | - | - |
| entity | such as server/table/replica ... | - | - |
| table | table id | - | for table/partition/replica entity |
| partition | partition id | - | for partition/replica entity |
| p | the percentile, for example the value may be ""90"", ""95"", ""999"" | only for Percentile | - |
| task | the task name | - | only for profiler entity |
| dir | the disk directory | - | only for disk entity |
| queue | the queue name | - | only for queue entity |
| policy | the policy name | - | only for backup_policy entity |
| tracer | the latency_tracer description | - | only for latency_tracer entity |
| start | the starting point of  latency_trace | - | only for latency_tracer entity |
| end | the end point of latency_tracer | - | only for latency_tracer entity |
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1820/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1824,https://api.github.com/repos/apache/incubator-pegasus/issues/1824,incubator-pegasus,2063484108,1824,Complement the implementation of class host_port,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2024-01-03T08:24:32Z,2024-01-11T06:27:26Z,"A part of https://github.com/apache/incubator-pegasus/issues/1403.

Complement the implementation of class `host_port` for immediate use in the future.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1824/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1832,https://api.github.com/repos/apache/incubator-pegasus/issues/1832,incubator-pegasus,2065484638,1832,"After upgrading from 2.4.0 to 2.5.0, some replicas frequently encounter 'client read accounted for an unhandled error'",chendongjiu,12393042,,,CLOSED,2024-01-04T11:29:06Z,2024-01-05T09:45:03Z,"## Bug Report

![Vc47SSlm8O](https://github.com/apache/incubator-pegasus/assets/12393042/f71bdb32-96ad-4381-a38b-a4b861cea952)

Besides having a large number of error logs like this, the replica can work properly","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1832/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1832,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5v9fc-,incubator-pegasus,1878390590,1832,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-01-05T09:44:47Z,2024-01-05T09:44:47Z,"@chendongjiu Thanks for the reply!

This is resolved by https://github.com/apache/incubator-pegasus/commit/82d981f51cf5e7b22971f9ac3f26f02a063600fe on master branch and cherry-picked to branch v2.5 as https://github.com/apache/incubator-pegasus/commit/d59b7579df83784a53f006cb3d5cd51c64dc0ff4","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5v9fc-/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1838,https://api.github.com/repos/apache/incubator-pegasus/issues/1838,incubator-pegasus,2070281677,1838,specify ambiguous meta_server_list on recovery_test ,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2024-01-08T11:44:06Z,2024-01-11T03:11:54Z,"## Bug Report

For easier configuration modifications, `recovery_test` just start one meta_server of onebox on `run.sh`.
`
if [ ""${module}"" == ""recovery_test"" ]; then master_count=1; fi
`
But the meta_server_list of class `recovery_test` is still three meta_servers.

Let fix it.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1838/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1840,https://api.github.com/repos/apache/incubator-pegasus/issues/1840,incubator-pegasus,2071583682,1840,Bug(duplication):some nodes can not send duplication data in master when big difference in data size,ninsmiracle,110282526,,,OPEN,2024-01-09T03:58:40Z,2024-05-09T07:06:14Z,"## Bug Report

1. What did you do?
In online production situation. Some nodes in matser cluster can not send dupcalition data to backup cluster.
![image](https://github.com/apache/incubator-pegasus/assets/110282526/6102f4aa-ccd2-4ff5-98e8-85f5089019b4)

2. What version of Pegasus are you using?
[pegasus2.4](https://github.com/apache/incubator-pegasus/tree/v2.4)

3.Why?
The root cause is that the master cluster sent a write RPC to the backup cluster, and the request body size exceeded the `max_allowed_write_size` set by the backup cluster. The reason why the master cluster sends this illegal data is due to the mechanism in the process of packaging and sending mutations: 
- the master cluster traverses the writes received in a time period
- checks whether all writes have been traversed or whether the current batch bytes are already greater than the `duplicate_log_batch_bytes` set in the cluster hot standby configuration (config.ini). 
- If it is not greater than the batch size, then two mutations are combined into one batch.

This can happen if the data length distribution of the table is too large.

For example:
The length of the first mutation A is 200byte. This naturally is to be combined with the next piece.
But the length of the next mutation B is 1048376(1048576-200) .
At this point, **A and B are already in a batch**, RPC will be sent out smoothly, but the standby cluster can not accept such a large write, and the standby cluster throws an ERR error. The master cluster is delayed recovery, throwing an ERR error.


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1840/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1846,https://api.github.com/repos/apache/incubator-pegasus/issues/1846,incubator-pegasus,2075972411,1846,Failed to build Pegasus due to undefined references to all thrift-generated objects while linking libdsn_client.a,empiredan,743379,Dan Wang,,CLOSED,2024-01-11T07:45:01Z,2024-01-12T09:57:57Z,"Failed to build Pegasus due to undefined references to all thrift-generated objects while linking libdsn_client.a:
  
```
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::create_app(std::string const&, std::string const&, int, int, std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > const&, bool, bool)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:194: undefined reference to `dsn::replication::configuration_create_app_response::~configuration_create_app_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:194: undefined reference to `dsn::replication::configuration_create_app_response::~configuration_create_app_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::drop_app(std::string const&, int)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:230: undefined reference to `dsn::replication::drop_app_options::__set_reserve_seconds(long)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:232: undefined reference to `dsn::replication::configuration_drop_app_response::~configuration_drop_app_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:232: undefined reference to `dsn::replication::configuration_drop_app_response::~configuration_drop_app_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::recall_app(int, std::string const&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:257: undefined reference to `dsn::replication::configuration_recall_app_response::~configuration_recall_app_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:257: undefined reference to `dsn::replication::configuration_recall_app_response::~configuration_recall_app_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::list_apps(dsn::app_status::type, std::vector<dsn::app_info, std::allocator<dsn::app_info> >&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:287: undefined reference to `dsn::replication::configuration_list_apps_response::~configuration_list_apps_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:287: undefined reference to `dsn::replication::configuration_list_apps_response::~configuration_list_apps_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::list_nodes(dsn::replication::node_status::type, std::map<dsn::rpc_address, dsn::replication::node_status::type, std::less<dsn::rpc_address>, std::allocator<std::pair<dsn::rpc_address const, dsn::replication::node_status::type> > >&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:487: undefined reference to `dsn::replication::configuration_list_nodes_response::~configuration_list_nodes_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:487: undefined reference to `dsn::replication::configuration_list_nodes_response::~configuration_list_nodes_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::cluster_name(long, std::string&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:632: undefined reference to `dsn::replication::configuration_cluster_info_response::~configuration_cluster_info_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:632: undefined reference to `dsn::replication::configuration_cluster_info_response::~configuration_cluster_info_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::cluster_info(std::string const&, bool, bool)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:659: undefined reference to `dsn::replication::configuration_cluster_info_response::~configuration_cluster_info_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:659: undefined reference to `dsn::replication::configuration_cluster_info_response::~configuration_cluster_info_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::control_meta_function_level(dsn::replication::meta_function_level::type)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:875: undefined reference to `dsn::replication::configuration_meta_control_response::~configuration_meta_control_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::send_balancer_proposal(dsn::replication::configuration_balancer_request const&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:887: undefined reference to `dsn::replication::configuration_balancer_response::~configuration_balancer_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:887: undefined reference to `dsn::replication::configuration_balancer_response::~configuration_balancer_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::do_recovery(std::vector<dsn::rpc_address, std::allocator<dsn::rpc_address> > const&, int, bool, bool, std::string const&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:951: undefined reference to `dsn::replication::configuration_recovery_response::~configuration_recovery_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:951: undefined reference to `dsn::replication::configuration_recovery_response::~configuration_recovery_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::do_restore(std::string const&, std::string const&, std::string const&, long, std::string const&, int, std::string const&, bool, std::string const&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1000: undefined reference to `dsn::replication::configuration_restore_request::__set_restore_path(std::string const&)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1014: undefined reference to `dsn::replication::configuration_create_app_response::~configuration_create_app_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1014: undefined reference to `dsn::replication::configuration_create_app_response::~configuration_create_app_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::add_backup_policy(std::string const&, std::string const&, std::vector<int, std::allocator<int> > const&, long, int, std::string const&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1047: undefined reference to `dsn::replication::configuration_add_backup_policy_response::~configuration_add_backup_policy_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1047: undefined reference to `dsn::replication::configuration_add_backup_policy_response::~configuration_add_backup_policy_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::backup_app(int, std::string const&, std::string const&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1066: undefined reference to `dsn::replication::start_backup_app_request::__set_backup_path(std::string const&)'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::query_backup(int, long)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1078: undefined reference to `dsn::replication::query_backup_status_request::__set_backup_id(long)'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::disable_backup_policy(std::string const&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1087: undefined reference to `dsn::replication::configuration_modify_backup_policy_request::__set_is_disable(bool)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1096: undefined reference to `dsn::replication::configuration_modify_backup_policy_response::~configuration_modify_backup_policy_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1096: undefined reference to `dsn::replication::configuration_modify_backup_policy_response::~configuration_modify_backup_policy_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::enable_backup_policy(std::string const&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1116: undefined reference to `dsn::replication::configuration_modify_backup_policy_request::__set_is_disable(bool)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1125: undefined reference to `dsn::replication::configuration_modify_backup_policy_response::~configuration_modify_backup_policy_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1125: undefined reference to `dsn::replication::configuration_modify_backup_policy_response::~configuration_modify_backup_policy_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::ls_backup_policy()':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1189: undefined reference to `dsn::replication::configuration_query_backup_policy_response::~configuration_query_backup_policy_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1189: undefined reference to `dsn::replication::configuration_query_backup_policy_response::~configuration_query_backup_policy_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::query_backup_policy(std::vector<std::string, std::allocator<std::string> > const&, int)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1219: undefined reference to `dsn::replication::configuration_query_backup_policy_response::~configuration_query_backup_policy_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1219: undefined reference to `dsn::replication::configuration_query_backup_policy_response::~configuration_query_backup_policy_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::update_backup_policy(std::string const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, long, int, std::string const&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1254: undefined reference to `dsn::replication::configuration_modify_backup_policy_request::__set_add_appids(std::vector<int, std::allocator<int> > const&)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1257: undefined reference to `dsn::replication::configuration_modify_backup_policy_request::__set_removal_appids(std::vector<int, std::allocator<int> > const&)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1260: undefined reference to `dsn::replication::configuration_modify_backup_policy_request::__set_new_backup_interval_sec(long)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1264: undefined reference to `dsn::replication::configuration_modify_backup_policy_request::__set_backup_history_count_to_keep(int)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1268: undefined reference to `dsn::replication::configuration_modify_backup_policy_request::__set_start_time(std::string const&)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1277: undefined reference to `dsn::replication::configuration_modify_backup_policy_response::~configuration_modify_backup_policy_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1277: undefined reference to `dsn::replication::configuration_modify_backup_policy_response::~configuration_modify_backup_policy_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::query_restore(int, bool)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1309: undefined reference to `dsn::replication::configuration_query_restore_response::~configuration_query_restore_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1309: undefined reference to `dsn::replication::configuration_query_restore_response::~configuration_query_restore_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::change_dup_status(std::string, int, dsn::replication::duplication_status::type)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1379: undefined reference to `dsn::replication::duplication_modify_request::__set_status(dsn::replication::duplication_status::type)'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::update_dup_fail_mode(std::string, int, dsn::replication::duplication_fail_mode::type)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1387: undefined reference to `dsn::replication::_duplication_fail_mode_VALUES_TO_NAMES'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1386: undefined reference to `dsn::replication::_duplication_fail_mode_VALUES_TO_NAMES'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1393: undefined reference to `dsn::replication::duplication_modify_request::__set_fail_mode(dsn::replication::duplication_fail_mode::type)'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::set_app_envs(std::string const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1482: undefined reference to `dsn::replication::configuration_update_app_env_request::__set_app_name(std::string const&)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1483: undefined reference to `dsn::replication::configuration_update_app_env_request::__set_keys(std::vector<std::string, std::allocator<std::string> > const&)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1484: undefined reference to `dsn::replication::configuration_update_app_env_request::__set_values(std::vector<std::string, std::allocator<std::string> > const&)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1485: undefined reference to `dsn::replication::configuration_update_app_env_request::__set_op(dsn::replication::app_env_operation::type)'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::del_app_envs(std::string const&, std::vector<std::string, std::allocator<std::string> > const&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1493: undefined reference to `dsn::replication::configuration_update_app_env_request::__set_app_name(std::string const&)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1494: undefined reference to `dsn::replication::configuration_update_app_env_request::__set_op(dsn::replication::app_env_operation::type)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1495: undefined reference to `dsn::replication::configuration_update_app_env_request::__set_keys(std::vector<std::string, std::allocator<std::string> > const&)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1503: undefined reference to `dsn::replication::configuration_update_app_env_response::~configuration_update_app_env_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1503: undefined reference to `dsn::replication::configuration_update_app_env_response::~configuration_update_app_env_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::clear_app_envs(std::string const&, bool, std::string const&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1523: undefined reference to `dsn::replication::configuration_update_app_env_request::__set_app_name(std::string const&)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1524: undefined reference to `dsn::replication::configuration_update_app_env_request::__set_op(dsn::replication::app_env_operation::type)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1526: undefined reference to `dsn::replication::configuration_update_app_env_request::__set_clear_prefix(std::string const&)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1529: undefined reference to `dsn::replication::configuration_update_app_env_request::__set_clear_prefix(std::string const&)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1538: undefined reference to `dsn::replication::configuration_update_app_env_response::~configuration_update_app_env_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1538: undefined reference to `dsn::replication::configuration_update_app_env_response::~configuration_update_app_env_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::ddd_diagnose(dsn::gpid, std::vector<dsn::replication::ddd_partition_info, std::allocator<dsn::replication::ddd_partition_info> >&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1566: undefined reference to `dsn::replication::ddd_diagnose_response::~ddd_diagnose_response()'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1566: undefined reference to `dsn::replication::ddd_diagnose_response::~ddd_diagnose_response()'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::detect_hotkey(dsn::rpc_address const&, dsn::replication::detect_hotkey_request&, dsn::replication::detect_hotkey_response&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1645: undefined reference to `dsn::replication::detect_hotkey_response::operator=(dsn::replication::detect_hotkey_response const&)'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::start_partition_split(std::string const&, int)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1653: undefined reference to `dsn::replication::start_partition_split_request::__set_app_name(std::string const&)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1654: undefined reference to `dsn::replication::start_partition_split_request::__set_new_partition_count(int)'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::control_partition_split(std::string const&, dsn::replication::split_control_type::type, int, int)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1686: undefined reference to `dsn::replication::control_split_request::__set_app_name(std::string const&)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1687: undefined reference to `dsn::replication::control_split_request::__set_control_type(dsn::replication::split_control_type::type)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1688: undefined reference to `dsn::replication::control_split_request::__set_parent_pidx(int)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1689: undefined reference to `dsn::replication::control_split_request::__set_old_partition_count(int)'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::query_partition_split(std::string const&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1697: undefined reference to `dsn::replication::query_split_request::__set_app_name(std::string const&)'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::start_app_manual_compact(std::string const&, bool, int, int)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1720: undefined reference to `dsn::replication::start_app_manual_compact_request::__set_trigger_time(long)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1721: undefined reference to `dsn::replication::start_app_manual_compact_request::__set_target_level(int)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1722: undefined reference to `dsn::replication::start_app_manual_compact_request::__set_bottommost(bool)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1724: undefined reference to `dsn::replication::start_app_manual_compact_request::__set_max_running_count(int)'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::get_max_replica_count(std::string const&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1742: undefined reference to `dsn::replication::configuration_get_max_replica_count_request::__set_app_name(std::string const&)'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::set_max_replica_count(std::string const&, int)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1752: undefined reference to `dsn::replication::configuration_set_max_replica_count_request::__set_app_name(std::string const&)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1753: undefined reference to `dsn::replication::configuration_set_max_replica_count_request::__set_max_replica_count(int)'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::replication_ddl_client::rename_app(std::string const&, std::string const&)':
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1765: undefined reference to `dsn::replication::configuration_rename_app_request::__set_old_app_name(std::string const&)'
/data/sa_cluster/src/empiredan-pegasus-new/src/client/replication_ddl_client.cpp:1766: undefined reference to `dsn::replication::configuration_rename_app_request::__set_new_app_name(std::string const&)'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::start_bulk_load_request::start_bulk_load_request()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/bulk_load_types.h:161: undefined reference to `vtable for dsn::replication::start_bulk_load_request'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::start_bulk_load_response::start_bulk_load_response()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/bulk_load_types.h:228: undefined reference to `vtable for dsn::replication::start_bulk_load_response'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::control_bulk_load_request::control_bulk_load_request()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/bulk_load_types.h:828: undefined reference to `vtable for dsn::replication::control_bulk_load_request'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::control_bulk_load_response::control_bulk_load_response()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/bulk_load_types.h:880: undefined reference to `vtable for dsn::replication::control_bulk_load_response'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::query_bulk_load_request::query_bulk_load_request()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/bulk_load_types.h:933: undefined reference to `vtable for dsn::replication::query_bulk_load_request'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::query_bulk_load_response::query_bulk_load_response()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/bulk_load_types.h:986: undefined reference to `vtable for dsn::replication::query_bulk_load_response'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::clear_bulk_load_state_request::clear_bulk_load_state_request()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/bulk_load_types.h:1071: undefined reference to `vtable for dsn::replication::clear_bulk_load_state_request'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::clear_bulk_load_state_response::clear_bulk_load_state_response()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/bulk_load_types.h:1118: undefined reference to `vtable for dsn::replication::clear_bulk_load_state_response'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::duplication_add_request::duplication_add_request()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/duplication_types.h:79: undefined reference to `vtable for dsn::replication::duplication_add_request'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::duplication_add_response::duplication_add_response()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/duplication_types.h:140: undefined reference to `vtable for dsn::replication::duplication_add_response'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::duplication_modify_request::duplication_modify_request()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/duplication_types.h:206: undefined reference to `vtable for dsn::replication::duplication_modify_request'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::duplication_modify_response::duplication_modify_response()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/duplication_types.h:272: undefined reference to `vtable for dsn::replication::duplication_modify_response'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::duplication_query_request::duplication_query_request()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/duplication_types.h:403: undefined reference to `vtable for dsn::replication::duplication_query_request'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::duplication_query_response::duplication_query_response()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/duplication_types.h:451: undefined reference to `vtable for dsn::replication::duplication_query_response'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::configuration_restore_request::configuration_restore_request()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/backup_types.h:138: undefined reference to `vtable for dsn::replication::configuration_restore_request'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::configuration_modify_backup_policy_request::configuration_modify_backup_policy_request()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/backup_types.h:432: undefined reference to `vtable for dsn::replication::configuration_modify_backup_policy_request'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::configuration_modify_backup_policy_response::configuration_modify_backup_policy_response()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/backup_types.h:521: undefined reference to `vtable for dsn::replication::configuration_modify_backup_policy_response'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::configuration_add_backup_policy_request::configuration_add_backup_policy_request()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/backup_types.h:577: undefined reference to `vtable for dsn::replication::configuration_add_backup_policy_request'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::configuration_add_backup_policy_response::configuration_add_backup_policy_response()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/backup_types.h:649: undefined reference to `vtable for dsn::replication::configuration_add_backup_policy_response'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::configuration_query_backup_policy_request::configuration_query_backup_policy_request()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/backup_types.h:847: undefined reference to `vtable for dsn::replication::configuration_query_backup_policy_request'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::configuration_query_backup_policy_response::configuration_query_backup_policy_response()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/backup_types.h:901: undefined reference to `vtable for dsn::replication::configuration_query_backup_policy_response'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::configuration_query_restore_request::configuration_query_restore_request()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/backup_types.h:1076: undefined reference to `vtable for dsn::replication::configuration_query_restore_request'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::configuration_query_restore_response::configuration_query_restore_response()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/backup_types.h:1124: undefined reference to `vtable for dsn::replication::configuration_query_restore_response'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::start_backup_app_request::start_backup_app_request()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/backup_types.h:1182: undefined reference to `vtable for dsn::replication::start_backup_app_request'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::start_backup_app_response::start_backup_app_response()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/backup_types.h:1242: undefined reference to `vtable for dsn::replication::start_backup_app_response'
../../client/libdsn_client.a(replication_ddl_client.cpp.o): In function `dsn::replication::query_backup_status_request::query_backup_status_request()':
/data/sa_cluster/src/empiredan-pegasus-new/build/debug/thrift-gen/backup_types.h:1383: undefined reference to `vtable for dsn::replication::query_backup_status_request'

......

collect2: error: ld returned 1 exit status
make[2]: *** [src/redis_protocol/proxy_ut/pegasus_rproxy_test] Error 1
make[2]: Leaving directory `/data/sa_cluster/src/empiredan-pegasus-new/build/debug'
make[1]: *** [src/redis_protocol/proxy_ut/CMakeFiles/pegasus_rproxy_test.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....

......

make[2]: Leaving directory `/data/sa_cluster/src/empiredan-pegasus-new/build/debug'
[ 43%] Built target dsn_meta_server
make[1]: Leaving directory `/data/sa_cluster/src/empiredan-pegasus-new/build/debug'
make: *** [all] Error 2
```

Running command：
```
./run.sh build --test -t debug -v -j $(nproc) -c --clear_thirdparty
```

GCC version：
```
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/opt/rh/devtoolset-7/root/usr/libexec/gcc/x86_64-redhat-linux/7/lto-wrapper
Target: x86_64-redhat-linux
Configured with: ../configure --enable-bootstrap --enable-languages=c,c++,fortran,lto --prefix=/opt/rh/devtoolset-7/root/usr --mandir=/opt/rh/devtoolset-7/root/usr/share/man --infodir=/opt/rh/devtoolset-7/root/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-shared --enable-threads=posix --enable-checking=release --enable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-gcc-major-version-only --enable-plugin --with-linker-hash-style=gnu --enable-initfini-array --with-default-libstdcxx-abi=gcc4-compatible --with-isl=/builddir/build/BUILD/gcc-7.3.1-20180303/obj-x86_64-redhat-linux/isl-install --enable-libmpx --enable-gnu-indirect-function --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux
Thread model: posix
gcc version 7.3.1 20180303 (Red Hat 7.3.1-5) (GCC)
```

CMake version: 3.27.6

OS Version:
```
Linux version 4.19.0-6.el7.ucloud.x86_64 (root@2d6b517b2939) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-16) (GCC)) #1 SMP Wed Feb 12 07:32:16 UTC 2020

CentOS Linux release 7.6.1810 (Core)
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1846/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1846,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5wlIvh,incubator-pegasus,1888783329,1846,NA,empiredan,743379,Dan Wang,,NA,2024-01-12T09:57:57Z,2024-01-12T09:57:57Z,This issue has been fixed by https://github.com/apache/incubator-pegasus/pull/1847.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5wlIvh/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1848,https://api.github.com/repos/apache/incubator-pegasus/issues/1848,incubator-pegasus,2075992521,1848,Replica server exits abnormally with coredump ,empiredan,743379,Dan Wang,,CLOSED,2024-01-11T07:58:18Z,2024-01-19T03:46:09Z,"Coredump information are as below:
```
#0  0x00007f6733ec02c7 in raise () from /lib64/libc.so.6
#1  0x00007f6733ec19b8 in abort () from /lib64/libc.so.6
#2  0x00007f6735778b66 in tcmalloc::Log(tcmalloc::LogMode, char const*, int, tcmalloc::LogItem, tcmalloc::LogItem, tcmalloc::LogItem, tcmalloc::LogItem) () from /lib64/libtcmalloc_and_profiler.so.4
#3  0x00007f673576b444 in (anonymous namespace)::InvalidFree(void*) () from /lib64/libtcmalloc_and_profiler.so.4
#4  0x00007f6736689c9e in rocksdb::ColumnFamilyData::~ColumnFamilyData() () from /home/sa_cluster/sp/pegasus/lib/librocksdb.so.8
#5  0x00007f6736689f35 in rocksdb::ColumnFamilyData::UnrefAndTryDelete() () from /home/sa_cluster/sp/pegasus/lib/librocksdb.so.8
#6  0x00007f6736689fa8 in rocksdb::SuperVersion::Cleanup() () from /home/sa_cluster/sp/pegasus/lib/librocksdb.so.8
#7  0x00007f6736689f12 in rocksdb::ColumnFamilyData::UnrefAndTryDelete() () from /home/sa_cluster/sp/pegasus/lib/librocksdb.so.8
#8  0x00007f673668b8c5 in rocksdb::ColumnFamilySet::~ColumnFamilySet() () from /home/sa_cluster/sp/pegasus/lib/librocksdb.so.8
#9  0x00007f6736866e3b in rocksdb::VersionSet::~VersionSet() () from /home/sa_cluster/sp/pegasus/lib/librocksdb.so.8
#10 0x00007f6736867341 in rocksdb::VersionSet::~VersionSet() () from /home/sa_cluster/sp/pegasus/lib/librocksdb.so.8
#11 0x00007f673670315e in rocksdb::DBImpl::CloseHelper() () from /home/sa_cluster/sp/pegasus/lib/librocksdb.so.8
#12 0x00007f67367038d1 in rocksdb::DBImpl::CloseImpl() () from /home/sa_cluster/sp/pegasus/lib/librocksdb.so.8
#13 0x00007f673672174b in rocksdb::DBImpl::~DBImpl() () from /home/sa_cluster/sp/pegasus/lib/librocksdb.so.8
#14 0x00007f6736788731 in rocksdb::DBImplReadOnly::~DBImplReadOnly() [clone .localalias.348] () from /home/sa_cluster/sp/pegasus/lib/librocksdb.so.8
#15 0x000000000058a9bd in pegasus::server::pegasus_server_impl::<lambda(bool)>::operator()(bool) const (__closure=__closure@entry=0x7f66e08b4cc0, remove_checkpoint=remove_checkpoint@entry=false) at /data/sa_cluster/src/empiredan-pegasus-new/src/server/pegasus_server_impl.cpp:2147
#16 0x000000000058d3b9 in pegasus::server::pegasus_server_impl::copy_checkpoint_to_dir_unsafe (this=this@entry=0x2e71600, checkpoint_dir=<optimized out>, checkpoint_decree=checkpoint_decree@entry=0x7f66e08b5a90, flush_memtable=flush_memtable@entry=false)
    at /data/sa_cluster/src/empiredan-pegasus-new/src/server/pegasus_server_impl.cpp:2172
#17 0x00000000005a7555 in pegasus::server::pegasus_server_impl::async_checkpoint (this=0x2e71600, flush_memtable=<optimized out>) at /data/sa_cluster/src/empiredan-pegasus-new/src/server/pegasus_server_impl.cpp:2039
#18 0x00007f6737fdeae3 in dsn::replication::replica::background_async_checkpoint (this=0x2f47200, is_emergency=<optimized out>) at /data/sa_cluster/src/empiredan-pegasus-new/src/replica/replica_chkpt.cpp:273
#19 0x00007f673820e401 in dsn::task::exec_internal (this=0x56ca690) at /data/sa_cluster/src/empiredan-pegasus-new/src/runtime/task/task.cpp:173
#20 0x00007f6738226f07 in dsn::task_worker::loop (this=0x26f3810) at /data/sa_cluster/src/empiredan-pegasus-new/src/runtime/task/task_worker.cpp:245
#21 0x00007f6738227a70 in dsn::task_worker::run_internal (this=0x26f3810) at /data/sa_cluster/src/empiredan-pegasus-new/src/runtime/task/task_worker.cpp:225
#22 0x00007f6736dab23f in execute_native_thread_routine () from /home/sa_cluster/sp/pegasus/lib/librocksdb.so.8
#23 0x00007f673553ddd5 in start_thread () from /lib64/libpthread.so.0
#24 0x00007f6733f8802d in clone () from /lib64/libc.so.6
```

Replica server exits abnormally with coredump immediately after this logging: 
```
I2024-01-08 18:41:18.943 (1704710478943709712 43700) replica.rep_long5.0404001700000097: pegasus_server_impl.cpp:2131:copy_checkpoint_to_dir_unsafe(): [1.0@10.120.69.237:8171] copy checkpoint to dir(/data/sa_cluster/pegasus_cluster/replica/reps/1.0.pegasus/data/checkpoint.tmp.1704710478941804) succeed
```

Running command:
```
./run.sh build --test -t release -v -j $(nproc)
```

GCC version:
```
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/opt/rh/devtoolset-7/root/usr/libexec/gcc/x86_64-redhat-linux/7/lto-wrapper
Target: x86_64-redhat-linux
Configured with: ../configure --enable-bootstrap --enable-languages=c,c++,fortran,lto --prefix=/opt/rh/devtoolset-7/root/usr --mandir=/opt/rh/devtoolset-7/root/usr/share/man --infodir=/opt/rh/devtoolset-7/root/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-shared --enable-threads=posix --enable-checking=release --enable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-gcc-major-version-only --enable-plugin --with-linker-hash-style=gnu --enable-initfini-array --with-default-libstdcxx-abi=gcc4-compatible --with-isl=/builddir/build/BUILD/gcc-7.3.1-20180303/obj-x86_64-redhat-linux/isl-install --enable-libmpx --enable-gnu-indirect-function --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux
Thread model: posix
gcc version 7.3.1 20180303 (Red Hat 7.3.1-5) (GCC)
```

OS Version:
```
Linux version 4.19.0-6.el7.ucloud.x86_64 (root@2d6b517b2939) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-16) (GCC)) #1 SMP Wed Feb 12 07:32:16 UTC 2020

CentOS Linux release 7.6.1810 (Core)
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1848/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1848,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5wdnn6,incubator-pegasus,1886812666,1848,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-01-11T10:30:18Z,2024-01-11T10:30:18Z,"We upgraded the gperftools to 2.13 on the master branch recently, but I saw the stack it was using the lib in `/lib64/libtcmalloc_and_profiler.so.4`. Please check if there is any issue of the run.sh script.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5wdnn6/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1848,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5w_BBX,incubator-pegasus,1895567447,1848,NA,empiredan,743379,Dan Wang,,NA,2024-01-17T10:55:01Z,2024-01-17T10:55:01Z,"> We upgraded the gperftools to 2.13 on the master branch recently, but I saw the stack it was using the lib in `/lib64/libtcmalloc_and_profiler.so.4`. Please check if there is any issue of the run.sh script.

Yes, this problem relates to the linked shared object of tcmalloc, namely `libtcmalloc_and_profiler.so*`.

In my environment, while replica server exited abnormally with coredump, I found that by `ldd bin/pegasus_server` it was always linked to `/lib64/libtcmalloc_and_profiler.so.4`, even if the libraries packed by [pack_server.sh](https://github.com/apache/incubator-pegasus/blob/master/scripts/pack_server.sh) were added in `LD_LIBRARY_PATH`:
```
libtcmalloc_and_profiler.so.4 => /lib64/libtcmalloc_and_profiler.so.4 (0x00007fa415ee3000)
```

However, while I checked the libraries packed by [pack_server.sh](https://github.com/apache/incubator-pegasus/blob/master/scripts/pack_server.sh), I found `libtcmalloc_and_profiler.so`, rather than `libtcmalloc_and_profiler.so.4`. After I fixed [pack_server.sh](https://github.com/apache/incubator-pegasus/blob/master/scripts/pack_server.sh) to copy `libtcmalloc_and_profiler.so.4` into the package, the replica server did  not exit any more.

Checking the commit history of [pack_server.sh](https://github.com/apache/incubator-pegasus/blob/master/scripts/pack_server.sh), I found that `libtcmalloc_and_profiler.so.4` had been changed to `libtcmalloc_and_profiler.so` in https://github.com/apache/incubator-pegasus/pull/1682/files while bumping `gperftools` to 2.13.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5w_BBX/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1848,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5w_GqV,incubator-pegasus,1895590549,1848,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-01-17T11:09:56Z,2024-01-17T11:09:56Z,"Good finding, maybe it's related to https://github.com/apache/incubator-pegasus/issues/1685 as well. I'll check it later.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5w_GqV/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1848,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5xOkw5,incubator-pegasus,1899646009,1848,NA,empiredan,743379,Dan Wang,,NA,2024-01-19T03:46:09Z,2024-01-19T03:46:09Z,This issue has been fixed by https://github.com/apache/incubator-pegasus/pull/1858.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5xOkw5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1853,https://api.github.com/repos/apache/incubator-pegasus/issues/1853,incubator-pegasus,2081968615,1853,Feature: support sampling record users to read specific information into detail log file,ninsmiracle,110282526,,,OPEN,2024-01-15T12:47:41Z,2024-01-15T12:47:41Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
  When we operat and maintain pegasus,there are many user give us feedback that they need to know the specific hash_key and sort_key and which client read it.
There is no doubt that ,this feature incurs a significant computational cost. As we need to add relevant logic in the main flow of the read operation, we have to handle it with caution. We should be able to dynamically configure the sampling rate, such as 1/10000. This means that on average, only one out of every 10000 reads will be recorded in Detail LOG.
  This feature will help some users better understand which of their data has been read and which data is redundant.They may no need to write unecessary data any more. This will help them reduce the amount of online write traffic and the storage capacity of Pegasus.
  In addition, we can also configure a threshold. When the size of a key or value is greater than this threshold, the key-value pair will be recorded. This will help us notify users to improve their data in order to achieve better read performance.

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->
In my opinion,there are 5 parameters that we could config:

- Log path
The  path of detail log file can be configured by the user. These logs are independent of the main path currently used by Pegasus.

- Sampling function  switch
When we don't need to use this feature, it should be possible to dynamically turn it off.

- Sampling Rate
Set a certain sampling rate. Each time a get or multi_get operation is performed, there is a certain probability of being recorded, instead of logging every time.

- Filter size
 A built-in bloom filter is included, mainly to reduce the size of the generated special logs. The size of the filter can be configured to prevent excessive memory usage.

- sampling status check time
Periodically check if there are any changes in the sampling status.


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1853/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1856,https://api.github.com/repos/apache/incubator-pegasus/issues/1856,incubator-pegasus,2083854941,1856,"BUG(go client):when go client is writing to one partition and the replica node core dump, go client will finish after timeout without updating the configuration.",lengyuexuexuan,46274877,,,CLOSED,2024-01-16T12:30:27Z,2025-01-19T07:42:52Z,"In the code, when replcia core dump, the function loopForResponse() will return ""nil"".
![1](https://github.com/apache/incubator-pegasus/assets/46274877/0f140e05-5572-4c55-9425-fcc909228aac)
Then, the process will be blocked in function CallWithGpid() until the time exceeds the timeout.
![2](https://github.com/apache/incubator-pegasus/assets/46274877/e34dcff9-538c-4d94-9c12-f72a2997442f)
![3](https://github.com/apache/incubator-pegasus/assets/46274877/d2c90da3-c116-4772-b8e1-b8f7bd9c807f)


why not update the configuration of table and retry previous operation when the above situation occurs?


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1856/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1856,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yIusx,incubator-pegasus,1914891057,1856,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-01-29T15:09:23Z,2024-01-29T15:09:23Z,"@lengyuexuexuan Thanks for the feedback, could you please submit a patch to fix it?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yIusx/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1856,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yNxOd,incubator-pegasus,1916212125,1856,NA,lengyuexuexuan,46274877,,,NA,2024-01-30T07:15:58Z,2024-01-30T07:15:58Z,"> @lengyuexuexuan Thanks for the feedback, could you please submit a patch to fix it?

OK. No problem.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yNxOd/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1863,https://api.github.com/repos/apache/incubator-pegasus/issues/1863,incubator-pegasus,2090603899,1863,"Meta_server unable to connect kerberos zookeeper where KDC configuration ""rdns = false""",GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2024-01-19T13:43:51Z,2024-01-23T08:19:14Z,"## Feature Request

/etc/krb5.conf
![image](https://github.com/apache/incubator-pegasus/assets/48315319/cef98f5c-7691-475e-87ad-9d767e92de06)

I need to deploy Pegasus on machine configured with KDC as above.
At the same time, ZooKeeper enables SASL restrictions on some znodes.

I got error as follow.

zookeeper client log
`2024-01-19 21:37:26,792:13938(0x7fe9d9852700):ZOO_INFO@check_events@2988: initiated connection to server x.x.x.x:2181
2024-01-19 21:37:26,795:13938(0x7fe9d9852700):ZOO_ERROR@zoo_sasl_client_start@287: Starting SASL negotiation: generic failure SASL(-1): generic failure: GSSAPI Error: Unspecified GSS failure.  Minor code may provide more information (Server zookeeper/x.x.x.x@REALMS.COM not found in Kerberos database)
2024-01-19 21:37:26,795:13938(0x7fe9d9852700):ZOO_ERROR@_zsasl_fail@148: SASL authentication failed. rc=-1`

kdc log
`Jan 19 21:37:26 cdh01.newcluster.com krb5kdc[18354](info): TGS_REQ (8 etypes {18 17 20 19 16 23 25 26}) x.x.x.x: LOOKING_UP_SERVER: authtime 0,  pegasus/FQDN1@REALMS.COM  for zookeeper/x.x.x.x@REALMS.COM, Server not found in Kerberos database
`

FQDN `FQDN1`correspond to IP `x.x.x.x`

zookeeper service principal are `zookeeper/x.x.x.x@REALMS.COM`



","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1863/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1863,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5xee8A,incubator-pegasus,1903816448,1863,NA,empiredan,743379,Dan Wang,,NA,2024-01-22T11:34:27Z,2024-01-22T11:34:27Z,"Have we tried to set `host` in `zoo_sasl_params_t` with `NULL` ? 

```
typedef struct zoo_sasl_params {
  const char *service;          /*!< The service name, usually ""zookeeper"" */
  const char *host;             /*!< The server name, e.g. ""zk-sasl-md5"" */
  const char *mechlist;         /*!< Mechanisms to try, e.g. ""DIGEST-MD5"" */
  const sasl_callback_t *callbacks;  /*!< List of callbacks */
} zoo_sasl_params_t;

ZOOAPI zhandle_t *zookeeper_init_sasl(const char *host, watcher_fn fn,
  int recv_timeout, const clientid_t *clientid, void *context, int flags,
  log_callback_fn log_callback, zoo_sasl_params_t *sasl_params);
```

I found that `host` would be set automatically if it was `NULL`:

```c
int zoo_sasl_connect(zhandle_t *zh)
{
......
    rc = _zsasl_getipport(zh, (const struct sockaddr *)&local_ip, salen,
        iplocalport, NULL);
    if (rc < 0) {
        return _zsasl_fail(zh, rc);
    }

    salen = sizeof(remote_ip);
    if (getpeername(zh->fd->sock, (struct sockaddr *)&remote_ip, &salen) < 0) {
        LOG_ERROR(LOGCALLBACK(zh), ""getpeername"");
        return _zsasl_fail(zh, ZSYSTEMERROR);
    }

    rc = _zsasl_getipport(zh, (const struct sockaddr *)&remote_ip, salen,
        ipremoteport, host);
    if (rc < 0) {
        return _zsasl_fail(zh, rc);
    }
......
    /* client new connection */
    sr = sasl_client_new(
        sc->params.service,
        sc->params.host ? sc->params.host : host,
        iplocalport,
        ipremoteport,
        sc->params.callbacks,
        /*secflags*/0,
        &sc->conn);

    if (sr != SASL_OK) {
        LOG_ERROR(LOGCALLBACK(zh),
            ""allocating SASL connection state: %s"",
            sasl_errstring(sr, NULL, NULL));
        return _zsasl_fail(zh, ZSYSTEMERROR);
    }

    return ZOK;
}
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5xee8A/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1868,https://api.github.com/repos/apache/incubator-pegasus/issues/1868,incubator-pegasus,2095910534,1868,use getaddrinfo instead of gethostbyname,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2024-01-23T11:50:11Z,2024-05-09T06:32:48Z,"## Feature Request

`getaddrinfo` is a new system call to replace the old `gethostbyname`, which handles both IPv4 and IPv6 addresses.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1868/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1868,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5xmWEo,incubator-pegasus,1905877288,1868,NA,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,NA,2024-01-23T11:51:40Z,2024-01-23T11:51:40Z,https://github.com/apache/incubator-pegasus/pull/1869,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5xmWEo/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1880,https://api.github.com/repos/apache/incubator-pegasus/issues/1880,incubator-pegasus,2107085004,1880,"Bug(go client):The cluster added a new meta node, but the meta server configuration of the go client was not updated. As a result, the client cannot find the new meta address and can only access the meta listed in the meta list",lengyuexuexuan,46274877,,,OPEN,2024-01-30T06:54:08Z,2024-01-30T08:32:00Z," Assuming the Pegasus client is configured with a meta server list of ""127.0.0.1:34602"" and ""127.0.0.1:34603,"" but the actual primary meta server for the Pegasus server is ""127.0.0.1:34601,"" the Pegasus client will not be able to connect to the Pegasus server until a timeout occurs.

   The reason is that when the go client searches for the primary, it iterates through the meta server list, sending an RPC RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX to each meta server and making a determination based on the response.

   Unlike the Java client, the go client cannot directly use indirection to add meta servers not specified in the configuration to the client.
   Below is the logic code for this part of the go client.
```
// go-client/session/meta_call.go  
func (c *metaCall) issueBackupMetas(ctx context.Context) {
    for i := range c.metas {
       if i == c.lead {
          continue
       }
       // concurrently issue RPC to the rest of meta servers.
       go func(idx int) {
          c.issueSingleMeta(ctx, idx)
       }(i)
    }
}

// issueSingleMeta returns false if we should try another meta
func (c *metaCall) issueSingleMeta(ctx context.Context, i int) bool {
    meta := c.metas[i]
    resp, err := c.callFunc(ctx, meta)
    if err != nil || resp.GetErr().Errno == base.ERR_FORWARD_TO_OTHERS.String() {
       return false
    }
    // the RPC succeeds, this meta becomes the new leader now.
    atomic.StoreUint32(&c.newLead, uint32(i))
    select {
    case <-ctx.Done():
    case c.respCh <- resp:
       // notify the caller
    }
    return true
}
```
Here is the relevant part of the Java client code for this:
```
// com/xiaomi/infra/pegasus/rpc/async/MetaSession.java  onFinishQueryMeta()

synchronized (this) {
  if (needSwitchLeader) {
    if (forwardAddress != null && !forwardAddress.isInvalid()) {
      boolean found = false;
      for (int i = 0; i < metaList.size(); i++) {
        if (metaList.get(i).getAddress().equals(forwardAddress)) {
          curLeader = i;
          found = true;
          break;
        }
      }
      if (!found) {
        logger.info(""add forward address {} as meta server"", forwardAddress);
        metaList.add(clusterManager.getReplicaSession(forwardAddress));
        curLeader = metaList.size() - 1;
      }
    } else if (metaList.get(curLeader) == round.lastSession) {
      curLeader = (curLeader + 1) % metaList.size();
      if (curLeader == 0 && hostPort != null && round.maxResolveCount != 0) {
        resolveHost(hostPort);
        round.maxResolveCount--;
        round.maxExecuteCount = metaList.size();
      }
    }
  }
  round.lastSession = metaList.get(curLeader);
}
```
### In summary
The primary impact of this issue is that, in the online cluster, a new meta server was added, and at some point thereafter, this meta server became the primary. Users, without changing their configurations, are unable to connect to the server.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1880/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1880,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yNyhe,incubator-pegasus,1916217438,1880,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-01-30T07:20:30Z,2024-01-30T07:20:30Z,"Good point, could you please submit a patch to fix it?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yNyhe/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1880,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yOKpE,incubator-pegasus,1916316228,1880,NA,lengyuexuexuan,46274877,,,NA,2024-01-30T08:31:59Z,2024-01-30T08:31:59Z,"> Good point, could you please submit a patch to fix it?

OK, get it.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yOKpE/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1881,https://api.github.com/repos/apache/incubator-pegasus/issues/1881,incubator-pegasus,2107099566,1881,"Question:The Go client is not compiled on GitHub, and users need to download it and build it themselves.",lengyuexuexuan,46274877,,,CLOSED,2024-01-30T07:05:08Z,2024-07-26T08:31:55Z,"
This change was made in December 2023.

Currently, if users fetch the source code and build it themselves, there are a few issues:

The Go client requires go version 1.18+, and thrift version 0.13.0. If users decide to upgrade, they may also need to update these two packages, which could be a bit troublesome.

When compiling the Go client, the process relies on files under the ""idl"" directory within the ""pegasus"" directory. The relevant commands in the makefile for this part are as follows:
```
build:    thrift -I ../idl -out idl --gen go:thrift_import='github.com/apache/thrift/lib/go/thrift',package_prefix='github.com/apache/incubator-pegasus/go-client/idl/' ../idl/backup.thrift
    thrift -I ../idl -out idl --gen go:thrift_import='github.com/apache/thrift/lib/go/thrift',package_prefix='github.com/apache/incubator-pegasus/go-client/idl/' ../idl/bulk_load.thrift
    thrift -I ../idl -out idl --gen go:thrift_import='github.com/apache/thrift/lib/go/thrift',package_prefix='github.com/apache/incubator-pegasus/go-client/idl/' ../idl/dsn.layer2.thrift
    thrift -I ../idl -out idl --gen go:thrift_import='github.com/apache/thrift/lib/go/thrift',package_prefix='github.com/apache/incubator-pegasus/go-client/idl/' ../idl/duplication.thrift
    thrift -I ../idl -out idl --gen go:thrift_import='github.com/apache/thrift/lib/go/thrift',package_prefix='github.com/apache/incubator-pegasus/go-client/idl/' ../idl/meta_admin.thrift
    thrift -I ../idl -out idl --gen go:thrift_import='github.com/apache/thrift/lib/go/thrift',package_prefix='github.com/apache/incubator-pegasus/go-client/idl/' ../idl/metadata.thrift
    thrift -I ../idl -out idl --gen go:thrift_import='github.com/apache/thrift/lib/go/thrift',package_prefix='github.com/apache/incubator-pegasus/go-client/idl/' ../idl/partition_split.thrift
    thrift -I ../idl -out idl --gen go:thrift_import='github.com/apache/thrift/lib/go/thrift',package_prefix='github.com/apache/incubator-pegasus/go-client/idl/' ../idl/replica_admin.thrift
    thrift -I ../idl -out idl --gen go:thrift_import='github.com/apache/thrift/lib/go/thrift',package_prefix='github.com/apache/incubator-pegasus/go-client/idl/' ../idl/rrdb.thrift
    thrift -I ../idl -out idl --gen go:thrift_import='github.com/apache/thrift/lib/go/thrift',package_prefix='github.com/apache/incubator-pegasus/go-client/idl/' ../idl/command.thrift
```
Therefore, if users compile it themselves, they also need to clone all of the Pegasus code. Is this too complex for users?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1881/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1886,https://api.github.com/repos/apache/incubator-pegasus/issues/1886,incubator-pegasus,2112184138,1886,Errors occurred while launching Pegasus shell,empiredan,743379,Dan Wang,,CLOSED,2024-02-01T10:48:01Z,2024-02-04T07:25:48Z,"While launching Pegasus shell, error occurred that config file was not found:
```
$ pwd
/home/data/pegasus

$ /home/data/pegasus/tools/run.sh shell --cluster 127.0.0.1:34601
sed: can't read /home/data/pegasus/src/shell/config.ini: No such file or directory
```

The reason was that the command was not executed under `/home/data/pegasus/tools`. Solving this problem, error occurred that `libdsn_replica_server.so` was not found:
```
$ /home/data/pegasus/tools/run.sh shell --cluster 127.0.0.1:34601
./pegasus_shell: error while loading shared libraries: libdsn_replica_server.so: cannot open shared object file: No such file or directory
```

The reason was that the dir of this library was not put in `LD_LIBRARY_PATH`. Solving this problem, error occurred that `librocksdb.so.8` was not found:
```
$ /home/data/pegasus/tools/run.sh shell --cluster 127.0.0.1:34601
./pegasus_shell: error while loading shared libraries: librocksdb.so.8: cannot open shared object file: No such file or directory
```

The reason was that `librocksdb.so.8` was not been put into the package while packing tools. Solving this problem, Pegasus shell was launched successfully.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1886/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1886,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yxnnm,incubator-pegasus,1925609958,1886,NA,empiredan,743379,Dan Wang,,NA,2024-02-04T07:25:47Z,2024-02-04T07:25:47Z,This issue is fixed by https://github.com/apache/incubator-pegasus/pull/1887.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5yxnnm/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1893,https://api.github.com/repos/apache/incubator-pegasus/issues/1893,incubator-pegasus,2116985443,1893,Implement an expiration mechanism to limit the cache size,GehaFearless,48315319,Guohao Li,gehafearless@apache.org,CLOSED,2024-02-04T09:38:26Z,2024-02-04T09:39:46Z,"## Feature Request

Now the cache of dns_resolve is unlimited, the cache size may be huge.
Implement an expiration mechanism to limit the cache size
and make it possible to update the resolve result.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1893/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1895,https://api.github.com/repos/apache/incubator-pegasus/issues/1895,incubator-pegasus,2117625945,1895,Errors occurred while launching Pegasus bench,empiredan,743379,Dan Wang,,CLOSED,2024-02-05T03:58:57Z,2024-02-05T06:35:56Z,"While launching Pegasus bench, error occurred that config file was not found:

```
$ /home/data/pegasus/tools/run.sh bench --type fillrandom_pegasus --num 100000 --cluster 127.0.0.1:34601--app_name test --thread_num 2
cp: cannot stat ‘/home/data/pegasus/tools/build/latest/output/bin/pegasus_bench/config.ini’: No such file or directory
```

The reason is that the tools is packaged by `pack_tools` for production environment, where `build/latest/output` could not be found since it is only used for local test environment.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1895/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1895,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5y0UZ5,incubator-pegasus,1926317689,1895,NA,empiredan,743379,Dan Wang,,NA,2024-02-05T06:35:56Z,2024-02-05T06:35:56Z,This issue is fixed by https://github.com/apache/incubator-pegasus/pull/1896.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5y0UZ5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1900,https://api.github.com/repos/apache/incubator-pegasus/issues/1900,incubator-pegasus,2120179578,1900,Error occurred that snappy/zstd/lz4 is not required dependencies while running packing tools,empiredan,743379,Dan Wang,,CLOSED,2024-02-06T08:00:28Z,2024-02-07T02:38:52Z,"Like https://github.com/apache/incubator-pegasus/issues/1792, all of snappy/zstd/lz4 are reported error ""is not a required dependency"" while running ./run.sh pack_tools:

```
$ ./run.sh pack_tools
......
ERROR: snappy is not a required dependency, skip packaging this lib
......
ERROR: zstd is not a required dependency, skip packaging this lib
ERROR: lz4 is not a required dependency, skip packaging this lib
......
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1900/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1900,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5zGye1,incubator-pegasus,1931159477,1900,NA,empiredan,743379,Dan Wang,,NA,2024-02-07T02:38:52Z,2024-02-07T02:38:52Z,This issue is fixed by https://github.com/apache/incubator-pegasus/pull/1901.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5zGye1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1904,https://api.github.com/repos/apache/incubator-pegasus/issues/1904,incubator-pegasus,2129099131,1904,Error occurred that namespace in group validator of flags was missed while building Pegasus daily,empiredan,743379,Dan Wang,,CLOSED,2024-02-11T17:59:53Z,2024-02-19T07:58:50Z,"Error occurred that namespace in group validator of flags was missed while building Pegasus daily:

```
[ 80%] Building CXX object src/replica/CMakeFiles/dsn_replica_server.dir/replica_2pc.cpp.o
[ 80%] Building CXX object src/replica/CMakeFiles/dsn_replica_server.dir/replica_backup.cpp.o
[ 80%] Building CXX object src/replica/CMakeFiles/dsn_replica_server.dir/replica_base.cpp.o
[ 81%] Building CXX object src/replica/CMakeFiles/dsn_replica_server.dir/replica_check.cpp.o
[ 81%] Building CXX object src/replica/CMakeFiles/dsn_replica_server.dir/replica_chkpt.cpp.o
[ 81%] Building CXX object src/replica/CMakeFiles/dsn_replica_server.dir/replica_config.cpp.o
[ 82%] Building CXX object src/replica/CMakeFiles/dsn_replica_server.dir/replica_context.cpp.o
[ 82%] Building CXX object src/replica/CMakeFiles/dsn_replica_server.dir/replica_disk_migrator.cpp.o
[ 82%] Building CXX object src/replica/CMakeFiles/dsn_replica_server.dir/replica_failover.cpp.o
[ 82%] Building CXX object src/replica/CMakeFiles/dsn_replica_server.dir/replica_http_service.cpp.o
[ 83%] Building CXX object src/replica/CMakeFiles/dsn_replica_server.dir/replica_init.cpp.o
[ 83%] Building CXX object src/replica/CMakeFiles/dsn_replica_server.dir/replica_learn.cpp.o
[ 83%] Building CXX object src/replica/CMakeFiles/dsn_replica_server.dir/replica_restore.cpp.o
[ 84%] Building CXX object src/replica/CMakeFiles/dsn_replica_server.dir/replica_stub.cpp.o
[ 84%] Building CXX object src/replica/CMakeFiles/dsn_replica_server.dir/replica_throttle.cpp.o
In file included from /__w/incubator-pegasus/incubator-pegasus/src/http/http_server.h:33,
                 from /__w/incubator-pegasus/incubator-pegasus/src/utils/metrics.h:43,
                 from /__w/incubator-pegasus/incubator-pegasus/src/replica/replica_base.h:34,
                 from /__w/incubator-pegasus/incubator-pegasus/src/replica/mutation_log.h:44,
                 from /__w/incubator-pegasus/incubator-pegasus/src/replica/replica.h:43,
                 from /__w/incubator-pegasus/incubator-pegasus/src/replica/bulk_load/replica_bulk_loader.h:29,
                 from /__w/incubator-pegasus/incubator-pegasus/src/replica/replica_stub.cpp:47:
/__w/incubator-pegasus/incubator-pegasus/src/replica/replica_stub.cpp: In lambda function:
/__w/incubator-pegasus/incubator-pegasus/src/replica/replica_stub.cpp:332:39: error: 'utils' has not been declared
  332 |     if (FLAGS_encrypt_data_at_rest && utils::is_empty(FLAGS_hadoop_kms_url)) {
      |                                       ^~~~~
/__w/incubator-pegasus/incubator-pegasus/src/utils/flags.h:122:80: note: in definition of macro 'DSN_DEFINE_group_validator'
  122 |     static const dsn::group_flag_validator FLAGS_GROUP_VALIDATOR_##name(#name, validator)
      |                                                                                ^~~~~~~~~
At global scope:
cc1plus: note: unrecognized command-line option '-Wno-inconsistent-missing-override' may have been intended to silence earlier diagnostics
make[2]: *** [src/replica/CMakeFiles/dsn_replica_server.dir/build.make:412: src/replica/CMakeFiles/dsn_replica_server.dir/replica_stub.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
make[1]: *** [CMakeFiles/Makefile2:1986: src/replica/CMakeFiles/dsn_replica_server.dir/all] Error 2
make: *** [Makefile:136: all] Error 2
Error: Process completed with exit code 2.
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1904/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1904,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM50V3to,incubator-pegasus,1951890280,1904,NA,empiredan,743379,Dan Wang,,NA,2024-02-19T07:58:49Z,2024-02-19T07:58:49Z,"This issue has been fixed by https://github.com/apache/incubator-pegasus/pull/1905.

![image](https://github.com/apache/incubator-pegasus/assets/743379/4cf32a62-3eb6-48e7-bc0a-4c039e3e8e10)

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM50V3to/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1907,https://api.github.com/repos/apache/incubator-pegasus/issues/1907,incubator-pegasus,2140959635,1907,Feature: support force no idempotent wirte when doing duplication,ninsmiracle,110282526,,,OPEN,2024-02-18T12:12:59Z,2024-02-18T12:12:59Z,"## Feature Request

  In our online situations , there are some Pegasus user need to copy some mutation from master cluster to backup cluster via Pegasus duplication function . However , user have used some No-idempotent write to master cluster. In current Pegasus version , master cluster will block those No-idempotent mutations.
 So we need a force dup option for those mutations.

**Describe the feature you'd like:**
An force dup option , which can config by admin-cli. And then , master cluster can receive and dup those No-idempotent  to backup cluster via duplication.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1907/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1913,https://api.github.com/repos/apache/incubator-pegasus/issues/1913,incubator-pegasus,2145752644,1913,Meta server and Replica server process could not exit normally after backing up or restoring on HDFS,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2024-02-21T03:43:17Z,2024-02-21T07:25:41Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
Using HDFS as the remote storage to backup or restore data.

2. What did you expect to see?
The server processes could not exit normally.
```
Thread 24 (Thread 0x7f3af56aa700 (LWP 726)):
#0  0x00007f3b57507017 in pthread_join () from /lib64/libpthread.so.0
#1  0x00007f3b562880f7 in std::thread::join() () from /lib64/libstdc++.so.6
#2  0x00007f3b5ab2eb1e in dsn::task_worker::stop (this=<optimized out>) at /home/laiyingchun/dev/skv_240/src/rdsn/src/runtime/task/task_worker.cpp:89
#3  0x00007f3b5ab1f82e in dsn::task_worker_pool::stop (this=0x1cb2240) at /home/laiyingchun/dev/skv_240/src/rdsn/src/runtime/task/task_engine.cpp:116
#4  0x00007f3b5ab1f975 in dsn::task_engine::stop (this=0x23b4cf0) at /home/laiyingchun/dev/skv_240/src/rdsn/src/runtime/task/task_engine.cpp:254
#5  0x00007f3b5aae0075 in dsn::service_node::~service_node (this=0x1b3a370, __in_chrg=<optimized out>) at /home/laiyingchun/dev/skv_240/src/rdsn/src/runtime/service_engine.cpp:173
#6  0x00007f3b5aae08c2 in _M_release (this=0x1b3a360) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/shared_ptr_base.h:154
#7  ~__shared_count (this=<optimized out>, __in_chrg=<optimized out>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/shared_ptr_base.h:684
#8  ~__shared_ptr (this=<optimized out>, __in_chrg=<optimized out>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/shared_ptr_base.h:1123
#9  ~shared_ptr (this=<optimized out>, __in_chrg=<optimized out>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/shared_ptr.h:93
#10 ~pair (this=<optimized out>, __in_chrg=<optimized out>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/stl_pair.h:198
#11 destroy<std::pair<int const, std::shared_ptr<dsn::service_node> > > (this=<optimized out>, __p=<optimized out>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/ext/new_allocator.h:140
#12 destroy<std::pair<int const, std::shared_ptr<dsn::service_node> > > (__a=..., __p=<optimized out>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/alloc_traits.h:487
#13 _M_destroy_node (this=0x7f3b5aef8d38 <dsn::utils::singleton<dsn::service_engine>::instance()::_instance+280>, __p=0x2438ec0) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/stl_tree.h:650
#14 _M_drop_node (this=0x7f3b5aef8d38 <dsn::utils::singleton<dsn::service_engine>::instance()::_instance+280>, __p=0x2438ec0) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/stl_tree.h:658
#15 std::_Rb_tree<int, std::pair<int const, std::shared_ptr<dsn::service_node> >, std::_Select1st<std::pair<int const, std::shared_ptr<dsn::service_node> > >, std::less<int>, std::allocator<std::pair<int const, std::shared_ptr<dsn::service_node> > > >::_M_erase (this=this
#16 0x00007f3b5aadeca0 in clear (this=0x7f3b5aef8d38 <dsn::utils::singleton<dsn::service_engine>::instance()::_instance+280>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/stl_tree.h:1171
#17 clear (this=0x7f3b5aef8d38 <dsn::utils::singleton<dsn::service_engine>::instance()::_instance+280>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/stl_map.h:1128
#18 dsn::service_engine::~service_engine (this=0x7f3b5aef8c20 <dsn::utils::singleton<dsn::service_engine>::instance()::_instance>, __in_chrg=<optimized out>) at /home/laiyingchun/dev/skv_240/src/rdsn/src/runtime/service_engine.cpp:197
#19 0x00007f3b55926ce9 in __run_exit_handlers () from /lib64/libc.so.6
#20 0x00007f3b55926d37 in exit () from /lib64/libc.so.6
#21 0x00007f3b5b5b155c in vm_direct_exit(int) () from /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.382.b05-1.el7_9.x86_64/jre/lib/amd64/server/libjvm.so
#22 0x00007f3b5ba82c05 in VM_Operation::evaluate() () from /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.382.b05-1.el7_9.x86_64/jre/lib/amd64/server/libjvm.so
#23 0x00007f3b5ba80c2a in VMThread::evaluate_operation(VM_Operation*) () from /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.382.b05-1.el7_9.x86_64/jre/lib/amd64/server/libjvm.so
#24 0x00007f3b5ba81099 in VMThread::loop() () from /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.382.b05-1.el7_9.x86_64/jre/lib/amd64/server/libjvm.so
#25 0x00007f3b5ba81549 in VMThread::run() () from /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.382.b05-1.el7_9.x86_64/jre/lib/amd64/server/libjvm.so
#26 0x00007f3b5b853cd2 in java_start(Thread*) () from /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.382.b05-1.el7_9.x86_64/jre/lib/amd64/server/libjvm.so
#27 0x00007f3b57505ea5 in start_thread () from /lib64/libpthread.so.0
#28 0x00007f3b559ebb0d in clone () from /lib64/libc.so.6
```

```
Thread 22 (Thread 0x7f30a9132700 (LWP 9032)):
#0  0x00007f315f0fb54d in __lll_lock_wait () from /lib64/libpthread.so.0
#1  0x00007f315f0f914d in pthread_cond_signal@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#2  0x00007f315de73b09 in std::condition_variable::notify_one() () from /lib64/libstdc++.so.6
#3  0x000000000087a46d in rocksdb::ThreadPoolImpl::Schedule(void (*)(void*), void*, void*, void (*)(void*)) ()
#4  0x00000000006a7c18 in rocksdb::DBImpl::MaybeScheduleFlushOrCompaction() ()
#5  0x00000000006a9aa1 in rocksdb::DBImpl::AtomicFlushMemTables(rocksdb::autovector<rocksdb::ColumnFamilyData*, 8ul> const&, rocksdb::FlushOptions const&, rocksdb::FlushReason, bool) ()
#6  0x00000000006aa21d in rocksdb::DBImpl::Flush(rocksdb::FlushOptions const&, std::vector<rocksdb::ColumnFamilyHandle*, std::allocator<rocksdb::ColumnFamilyHandle*> > const&) ()
#7  0x00000000005bb2a7 in pegasus::server::pegasus_server_impl::flush_all_family_columns (this=0x4b20400, wait=wait@entry=true) at /home/laiyingchun/dev/skv_240/src/server/pegasus_server_impl.cpp:3235
#8  0x00000000005bc05c in pegasus::server::pegasus_server_impl::stop (this=0x4b20400, clear_state=<optimized out>) at /home/laiyingchun/dev/skv_240/src/server/pegasus_server_impl.cpp:1790
#9  0x00007f316257c69c in dsn::replication::replication_app_base::close (this=this@entry=0x4b20400, clear_state=clear_state@entry=false) at /home/laiyingchun/dev/skv_240/src/rdsn/src/replica/replication_app_base.cpp:345
#10 0x00007f31624dafff in dsn::replication::replica::close (this=0x4f20800) at /home/laiyingchun/dev/skv_240/src/rdsn/src/replica/replica.cpp:502
#11 0x00007f316253d585 in dsn::replication::replica_stub::close (this=0x2cd2e00) at /home/laiyingchun/dev/skv_240/src/rdsn/src/replica/replica_stub.cpp:2834
#12 0x00007f316258433d in dsn::replication::replication_service_app::stop (this=this@entry=0x3571b30, cleanup=cleanup@entry=false) at /home/laiyingchun/dev/skv_240/src/rdsn/src/replica/replication_service_app.cpp:73
#13 0x0000000000599d61 in pegasus::server::pegasus_replication_service_app::stop (this=0x3571b30, cleanup=<optimized out>) at /home/laiyingchun/dev/skv_240/src/server/pegasus_service_app.h:57
#14 0x00007f31626cc6ee in dsn::service_node::stop_app (this=this@entry=0x2cb8370, cleanup=cleanup@entry=false) at /home/laiyingchun/dev/skv_240/src/rdsn/src/runtime/service_engine.cpp:84
#15 0x00007f31626cf069 in dsn::service_node::~service_node (this=0x2cb8370, __in_chrg=<optimized out>) at /home/laiyingchun/dev/skv_240/src/rdsn/src/runtime/service_engine.cpp:172
#16 0x00007f31626cf8c2 in _M_release (this=0x2cb8360) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/shared_ptr_base.h:154
#17 ~__shared_count (this=<optimized out>, __in_chrg=<optimized out>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/shared_ptr_base.h:684
#18 ~__shared_ptr (this=<optimized out>, __in_chrg=<optimized out>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/shared_ptr_base.h:1123
#19 ~shared_ptr (this=<optimized out>, __in_chrg=<optimized out>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/shared_ptr.h:93
#20 ~pair (this=<optimized out>, __in_chrg=<optimized out>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/stl_pair.h:198
#21 destroy<std::pair<int const, std::shared_ptr<dsn::service_node> > > (this=<optimized out>, __p=<optimized out>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/ext/new_allocator.h:140
#22 destroy<std::pair<int const, std::shared_ptr<dsn::service_node> > > (__a=..., __p=<optimized out>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/alloc_traits.h:487
#23 _M_destroy_node (this=0x7f3162ae7d38 <dsn::utils::singleton<dsn::service_engine>::instance()::_instance+280>, __p=0x359fd80) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/stl_tree.h:650
#24 _M_drop_node (this=0x7f3162ae7d38 <dsn::utils::singleton<dsn::service_engine>::instance()::_instance+280>, __p=0x359fd80) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/stl_tree.h:658
#25 std::_Rb_tree<int, std::pair<int const, std::shared_ptr<dsn::service_node> >, std::_Select1st<std::pair<int const, std::shared_ptr<dsn::service_node> > >, std::less<int>, std::allocator<std::pair<int const, std::shared_ptr<dsn::service_node> > > >::_M_erase (this=this
#26 0x00007f31626cdca0 in clear (this=0x7f3162ae7d38 <dsn::utils::singleton<dsn::service_engine>::instance()::_instance+280>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/stl_tree.h:1171
#27 clear (this=0x7f3162ae7d38 <dsn::utils::singleton<dsn::service_engine>::instance()::_instance+280>) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/stl_map.h:1128
#28 dsn::service_engine::~service_engine (this=0x7f3162ae7c20 <dsn::utils::singleton<dsn::service_engine>::instance()::_instance>, __in_chrg=<optimized out>) at /home/laiyingchun/dev/skv_240/src/rdsn/src/runtime/service_engine.cpp:197
#29 0x00007f315d515ce9 in __run_exit_handlers () from /lib64/libc.so.6
#30 0x00007f315d515d37 in exit () from /lib64/libc.so.6
#31 0x00007f31631a055c in vm_direct_exit(int) () from /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.382.b05-1.el7_9.x86_64/jre/lib/amd64/server/libjvm.so
#32 0x00007f3163671c05 in VM_Operation::evaluate() () from /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.382.b05-1.el7_9.x86_64/jre/lib/amd64/server/libjvm.so
#33 0x00007f316366fc2a in VMThread::evaluate_operation(VM_Operation*) () from /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.382.b05-1.el7_9.x86_64/jre/lib/amd64/server/libjvm.so
#34 0x00007f3163670099 in VMThread::loop() () from /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.382.b05-1.el7_9.x86_64/jre/lib/amd64/server/libjvm.so
#35 0x00007f3163670549 in VMThread::run() () from /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.382.b05-1.el7_9.x86_64/jre/lib/amd64/server/libjvm.so
#36 0x00007f3163442cd2 in java_start(Thread*) () from /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.382.b05-1.el7_9.x86_64/jre/lib/amd64/server/libjvm.so
#37 0x00007f315f0f4ea5 in start_thread () from /lib64/libpthread.so.0
#38 0x00007f315d5dab0d in clone () from /lib64/libc.so.6
```

3. What did you see instead?
Servers could exit normally.

4. What version of Pegasus are you using?
2.4","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1913/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1919,https://api.github.com/repos/apache/incubator-pegasus/issues/1919,incubator-pegasus,2151114622,1919,pegasus uses a non OSS friendly version of org.json:json jar,pjfanning,11783444,PJ Fanning,,CLOSED,2024-02-23T13:38:16Z,2024-02-28T10:19:46Z,"https://github.com/apache/incubator-pegasus/blob/af256941f4062c77377d05d379ebd7f9b4a327ee/java-client/pom.xml#L68

See https://www.apache.org/legal/resolved.html (JSON license section)

Could you upgrade to a newer version? And these versions have security fixes too.

Since late 2022, this jar is now properly in the public domain.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1919/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1919,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM51V3eB,incubator-pegasus,1968666497,1919,NA,empiredan,743379,Dan Wang,,NA,2024-02-28T10:19:45Z,2024-02-28T10:19:45Z,"> https://github.com/apache/incubator-pegasus/blob/af256941f4062c77377d05d379ebd7f9b4a327ee/java-client/pom.xml#L68
> 
> See https://www.apache.org/legal/resolved.html (JSON license section)
> 
> Could you upgrade to a newer version? And these versions have security fixes too.
> 
> Since late 2022, this jar is now properly in the public domain.

Thanks @pjfanning ! Upgraded by https://github.com/apache/incubator-pegasus/pull/1781.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM51V3eB/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1927,https://api.github.com/repos/apache/incubator-pegasus/issues/1927,incubator-pegasus,2162902844,1927,"Error occurred that ""get_property could not find TARGET DOWNLOAD_EXTRACT_TIMESTAMP"" while building third-parties",empiredan,743379,Dan Wang,,CLOSED,2024-03-01T08:44:56Z,2024-03-08T10:04:45Z,"Error occurred while building third-parties:

```
-- Setting up third-parties...
CMake Error at /usr/share/cmake/Modules/ExternalProject.cmake:3111 (get_property):
  get_property could not find TARGET DOWNLOAD_EXTRACT_TIMESTAMP.  Perhaps it
  has not yet been created.
Call Stack (most recent call first):
  /usr/share/cmake/Modules/ExternalProject.cmake:3236 (_ep_get_file_deps)
  /usr/share/cmake/Modules/ExternalProject.cmake:3684 (_ep_add_configure_command)
  CMakeLists.txt:151 (ExternalProject_Add)
```

CMake version: `3.20.2`

GCC version: `8.5.0 20210514 (Red Hat 8.5.0-10) (GCC)`

Building command:

```
./run.sh build --test -t release -v -j $(nproc) -c --clear_thirdparty
```

As is described in https://cmake.org/cmake/help/latest/module/ExternalProject.html, `DOWNLOAD_EXTRACT_TIMESTAMP` was introduced in version 3.24.

```
DOWNLOAD_EXTRACT_TIMESTAMP <bool>
New in version 3.24.
```

Thus the minimum required version of CMake should be increased from 3.11.0 to 3.24.0.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1927/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1927,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM52Vt7K,incubator-pegasus,1985404618,1927,NA,empiredan,743379,Dan Wang,,NA,2024-03-08T10:04:45Z,2024-03-08T10:04:45Z,This issue is fixed by https://github.com/apache/incubator-pegasus/pull/1928 and https://github.com/apache/incubator-pegasus-website/pull/80.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM52Vt7K/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1937,https://api.github.com/repos/apache/incubator-pegasus/issues/1937,incubator-pegasus,2175436985,1937,"Bug(message_ex::copy): In some case, the message_ex::copy function will trigger an assertion",Sunflower876,162662221,,,CLOSED,2024-03-08T07:19:49Z,2024-05-23T01:52:32Z,"## Bug Report

There are two formats for message_ex, one is that both the header and body are saved in the buffers field, and the header and buffers [0] are the same. the other format is that the header is not saved in the buffers field and exists separately. In the message_ex::copy function, total_length represents the sum of the lengths of the header and body. However, in the second case, i only counts the length of the body, so checking that i and total_length are equal will fail. 

![20240308-151800](https://github.com/apache/incubator-pegasus/assets/162662221/89ff3782-2ccb-46ac-84d2-96e74e7e220e)




","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1937/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1937,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM52U4lD,incubator-pegasus,1985186115,1937,NA,Sunflower876,162662221,,,NA,2024-03-08T07:29:58Z,2024-03-08T07:29:58Z,@acelyc111 ,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM52U4lD/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1937,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM52e7q8,incubator-pegasus,1987820220,1937,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-03-11T08:00:28Z,2024-03-11T08:00:28Z,"@Sunflower876 Thanks!

Have you encountered such a check failure? Could you please leave some logs here, a backtrace would be better.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM52e7q8/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1937,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM52hJeo,incubator-pegasus,1988401064,1937,NA,Sunflower876,162662221,,,NA,2024-03-11T13:07:01Z,2024-03-11T13:07:01Z,"> @Sunflower876 Thanks!
> 
> Have you encountered such a check failure? Could you please leave some logs here, a backtrace would be better.
```
#0  0x00007fe05b4a3694 in vfprintf () from /lib64/libc.so.6
Missing separate debuginfos, use: debuginfo-install glibc-2.17-157.el7_3.1.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 libcom_err-1.42.9-9.el7.x86_64 libgcc-4.8.5-28.el7_5.1.x86_64 zlib-1.2.7-17.el7.x86_64
(gdb) #0  0x00007fe05b4a3694 in vfprintf () from /lib64/libc.so.6
#1  0x00007fe05f16ae00 in dsn::tools::simple_logger::dsn_logv (
    this=0x2b25030, file=<optimized out>, function=<optimized out>,
    line=<optimized out>, log_level=LOG_LEVEL_FATAL,
    fmt=0x7fe06058ab7d ""%d VS %d, rpc_name = %s"", args=<optimized out>)
    at /home/work/pegasus/rdsn/src/utils/simple_logger.cpp:206
#2  0x00007fe05f15e04c in dsn_logv (file=0x7fe0604f25b7 ""compiler_depend.ts"",
    function=0x7fe06058ac56 <dsn::message_ex::copy(bool, bool)::__FUNCTION__> ""copy"", line=248, log_level=LOG_LEVEL_FATAL,
    fmt=0x7fe06058ab7d ""%d VS %d, rpc_name = %s"",
    args=args@entry=0x7fe01b3ce598)
    at /home/work/pegasus/rdsn/src/utils/logging.cpp:129
#3  0x00007fe05f15e0cf in dsn_logf (
    file=file@entry=0x7fe0604f25b7 ""compiler_depend.ts"",
    function=function@entry=0x7fe06058ac56 <dsn::message_ex::copy(bool, bool)::__FUNCTION__> ""copy"", line=line@entry=248,
    log_level=log_level@entry=LOG_LEVEL_FATAL,
    fmt=fmt@entry=0x7fe06058ab7d ""%d VS %d, rpc_name = %s"")
    at /home/work/pegasus/rdsn/src/utils/logging.cpp:141
#4  0x00007fe060452348 in dsn::message_ex::copy (this=<optimized out>,
    clone_content=clone_content@entry=true,
    copy_for_receive=copy_for_receive@entry=true)
    at /home/work/pegasus/rdsn/src/runtime/rpc/rpc_message.cpp:247
#5  0x000000000058d6da in pegasus::server::pegasus_server_impl::on_refresh_ttl
    (this=0x41a4100, rpc=...)
    at /home/work/pegasus/src/server/pegasus_server_impl.cpp:972
#6  0x0000000000560a8b in pegasus::server::pegasus_read_service::on_refresh_ttl (svc=<optimized out>, rpc=...)
    at /home/work/pegasus/src/server/pegasus_read_service.h:100
#7  0x0000000000568513 in operator() (request=<optimized out>, p=0x41a4100,
    __closure=<optimized out>)
    at /home/work/pegasus/DSN_ROOT/include/dsn/dist/replication/storage_serverlet.h:41
#8  std::_Function_handler<void (pegasus::server::pegasus_read_service*, dsn::message_ex*), bool dsn::replication::storage_serverlet<pegasus::server::pegasus_read_service>::register_rpc_handler_with_rpc_holder<dsn::rpc_holder<dsn::apps::refresh_ttl_request, dsn::apps::refresh_ttl_response> >(dsn::task_code, char const*, void (*)(pegasus::server::pegasus_read_service*, dsn::rpc_holder<dsn::apps::refresh_ttl_request, dsn::apps::refresh_ttl_response>))::{lambda(pegasus::server::pegasus_read_service*, dsn::message_ex*)#1}>::_M_invoke(std::_Any_data const&, pegasus::server::pegasus_read_service*&&, dsn::message_ex*&&) (
    __functor=..., __args#0=<optimized out>, __args#1=<optimized out>)
    at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/std_function.h:316
#9  0x000000000059f71b in operator() (__args#1=<optimized out>,
    __args#0=<optimized out>, this=<optimized out>)
    at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/std_function.h:706
#10 handle_request (request=0x218218c0, this=0x41a4100)
    at /home/work/pegasus/DSN_ROOT/include/dsn/dist/replication/storage_serverlet.h:92
#11 pegasus::server::pegasus_read_service::on_request (this=0x41a4100,
    request=0x218218c0)
    at /home/work/pegasus/src/server/pegasus_read_service.h:48
#12 0x00007fe06024a2b7 in dsn::replication::replica::on_client_read (
    this=0x3472800, request=request@entry=0x218218c0,
    ignore_throttling=ignore_throttling@entry=false)
    at /home/work/pegasus/rdsn/src/replica/replica.cpp:252
#13 0x00007fe0602adb01 in dsn::replication::replica_stub::on_client_read (
    this=0x2b48300, id=..., request=0x218218c0)
    at /home/work/pegasus/rdsn/src/replica/replica_stub.cpp:956
#14 0x00007fe060453c22 in operator() (__args#0=<optimized out>,
    this=0x2d4bd0d0)
    at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/std_function.h:706
#15 dsn::rpc_request_task::exec (this=0x2d4bd000)
    at /home/work/pegasus/rdsn/include/dsn/tool-api/task.h:435
#16 0x00007fe0604549d1 in dsn::task::exec_internal (
    this=this@entry=0x2d4bd000)
    at /home/work/pegasus/rdsn/src/runtime/task/task.cpp:176
#17 0x00007fe060469d9a in dsn::task_worker::loop (this=0x337f550)
    at /home/work/pegasus/rdsn/src/runtime/task/task_worker.cpp:211
#18 0x00007fe060469fc0 in dsn::task_worker::run_internal (this=0x337f550)
    at /home/work/pegasus/rdsn/src/runtime/task/task_worker.cpp:191
#19 0x00007fe0604f225f in execute_native_thread_routine ()
   from /home/work/app/pegasus/c3srv-algosample-other/replica/package/bin/[libdsn_replica_server.so](http://libdsn_replica_server.so/)
#20 0x00007fe05d03fdc5 in start_thread () from /lib64/libpthread.so.0
#21 0x00007fe05b55273d in clone () from /lib64/libc.so.6
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM52hJeo/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1937,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM52hMM7,incubator-pegasus,1988412219,1937,NA,Sunflower876,162662221,,,NA,2024-03-11T13:12:52Z,2024-03-11T13:12:52Z,"> > @Sunflower876 Thanks!
> > Have you encountered such a check failure? Could you please leave some logs here, a backtrace would be better.
> 
> #0 0x00007fe05b4a3694 in vfprintf () from /lib64/libc.so.6 Missing separate debuginfos, use: debuginfo-install glibc-2.17-157.el7_3.1.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 libcom_err-1.42.9-9.el7.x86_64 libgcc-4.8.5-28.el7_5.1.x86_64 zlib-1.2.7-17.el7.x86_64 (gdb) #0 0x00007fe05b4a3694 in vfprintf () from /lib64/libc.so.6 #1 0x00007fe05f16ae00 in dsn::tools::simple_logger::dsn_logv ( this=0x2b25030, file=, function=, line=, log_level=LOG_LEVEL_FATAL, fmt=0x7fe06058ab7d ""%d VS %d, rpc_name = %s"", args=) at /home/work/pegasus/rdsn/src/utils/simple_logger.cpp:206 #2 0x00007fe05f15e04c in dsn_logv (file=0x7fe0604f25b7 ""compiler_depend.ts"", function=0x7fe06058ac56 <dsn::message_ex::copy(bool, bool)::**FUNCTION**> ""copy"", line=248, log_level=LOG_LEVEL_FATAL, fmt=0x7fe06058ab7d ""%d VS %d, rpc_name = %s"", args=args@entry=0x7fe01b3ce598) at /home/work/pegasus/rdsn/src/utils/logging.cpp:129 #3 0x00007fe05f15e0cf in dsn_logf ( file=file@entry=0x7fe0604f25b7 ""compiler_depend.ts"", function=function@entry=0x7fe06058ac56 <dsn::message_ex::copy(bool, bool)::**FUNCTION**> ""copy"", line=line@entry=248, log_level=log_level@entry=LOG_LEVEL_FATAL, fmt=fmt@entry=0x7fe06058ab7d ""%d VS %d, rpc_name = %s"") at /home/work/pegasus/rdsn/src/utils/logging.cpp:141 #4 0x00007fe060452348 in dsn::message_ex::copy (this=, clone_content=clone_content@entry=true, copy_for_receive=copy_for_receive@entry=true) at /home/work/pegasus/rdsn/src/runtime/rpc/rpc_message.cpp:247 #5 0x000000000058d6da in pegasus::server::pegasus_server_impl::on_refresh_ttl (this=0x41a4100, rpc=...) at /home/work/pegasus/src/server/pegasus_server_impl.cpp:972 #6 0x0000000000560a8b in pegasus::server::pegasus_read_service::on_refresh_ttl (svc=, rpc=...) at /home/work/pegasus/src/server/pegasus_read_service.h:100 #7 0x0000000000568513 in operator() (request=, p=0x41a4100, __closure=) at /home/work/pegasus/DSN_ROOT/include/dsn/dist/replication/storage_serverlet.h:41 #8 std::_Function_handler<void (pegasus::server::pegasus_read_service*, dsn::message_ex*), bool dsn::replication::storage_serverletpegasus::server::pegasus_read_service::register_rpc_handler_with_rpc_holder<dsn::rpc_holder<dsn::apps::refresh_ttl_request, dsn::apps::refresh_ttl_response> >(dsn::task_code, char const*, void (_)(pegasus::server::pegasus_read_service_, dsn::rpc_holder<dsn::apps::refresh_ttl_request, dsn::apps::refresh_ttl_response>))::{lambda(pegasus::server::pegasus_read_service*, dsn::message_ex*)#1}>::_M_invoke(std::_Any_data const&, pegasus::server::pegasus_read_service*&&, dsn::message_ex*&&) ( __functor=..., __args#0=, __args#1=) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/std_function.h:316 #9 0x000000000059f71b in operator() (__args#1=, __args#0=, this=) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/std_function.h:706 #10 handle_request (request=0x218218c0, this=0x41a4100) at /home/work/pegasus/DSN_ROOT/include/dsn/dist/replication/storage_serverlet.h:92 #11 pegasus::server::pegasus_read_service::on_request (this=0x41a4100, request=0x218218c0) at /home/work/pegasus/src/server/pegasus_read_service.h:48 #12 0x00007fe06024a2b7 in dsn::replication::replica::on_client_read ( this=0x3472800, request=request@entry=0x218218c0, ignore_throttling=ignore_throttling@entry=false) at /home/work/pegasus/rdsn/src/replica/replica.cpp:252 #13 0x00007fe0602adb01 in dsn::replication::replica_stub::on_client_read ( this=0x2b48300, id=..., request=0x218218c0) at /home/work/pegasus/rdsn/src/replica/replica_stub.cpp:956 #14 0x00007fe060453c22 in operator() (__args#0=, this=0x2d4bd0d0) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/std_function.h:706 #15 dsn::rpc_request_task::exec (this=0x2d4bd000) at /home/work/pegasus/rdsn/include/dsn/tool-api/task.h:435 #16 0x00007fe0604549d1 in dsn::task::exec_internal ( this=this@entry=0x2d4bd000) at /home/work/pegasus/rdsn/src/runtime/task/task.cpp:176 #17 0x00007fe060469d9a in dsn::task_worker::loop (this=0x337f550) at /home/work/pegasus/rdsn/src/runtime/task/task_worker.cpp:211 #18 0x00007fe060469fc0 in dsn::task_worker::run_internal (this=0x337f550) at /home/work/pegasus/rdsn/src/runtime/task/task_worker.cpp:191 #19 0x00007fe0604f225f in execute_native_thread_routine () from /home/work/app/pegasus/c3srv-algosample-other/replica/package/bin/[libdsn_replica_server.so](http://libdsn_replica_server.so/) #20 0x00007fe05d03fdc5 in start_thread () from /lib64/libpthread.so.0 #21 0x00007fe05b55273d in clone () from /lib64/libc.so.6

![image](https://github.com/apache/incubator-pegasus/assets/162662221/588ed24d-6d15-403a-a931-17a6b7572ffb)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM52hMM7/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1937,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM55E5vC,incubator-pegasus,2031328194,1937,NA,Sunflower876,162662221,,,NA,2024-04-02T07:59:38Z,2024-04-02T07:59:38Z,"> @Sunflower876 Thanks!
> 
> Have you encountered such a check failure? Could you please leave some logs here, a backtrace would be better.

The first format is the Thrift message format, used for communication between client and server. The second format is the RDSN message format, used for communication between servers. The second format calling message_ex:: copy will definitely crash.

![7d85bfd6-62d8-4e40-b57d-c43a757c03b1](https://github.com/apache/incubator-pegasus/assets/162662221/eb2d7200-868e-45f1-9123-335341d5d9a4)



![image](https://github.com/apache/incubator-pegasus/assets/162662221/c3d9aa08-c110-4c92-9e4c-a883e802d7e0)


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM55E5vC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1937,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM55vuRc,incubator-pegasus,2042553436,1937,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-04-08T11:55:31Z,2024-04-08T11:55:31Z,"I saw the crash is caused by request `on_refresh_ttl`, is it a internal API in your environment?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM55vuRc/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1937,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM552g22,incubator-pegasus,2044333494,1937,NA,Sunflower876,162662221,,,NA,2024-04-09T07:34:59Z,2024-04-09T07:34:59Z,"> I saw the crash is caused by request `on_refresh_ttl`, is it a internal API in your environment?

Yes.  But it is because there is a bug in the implementation of message_ex:: copy that any message call between servers will lead to crash. You can try the case below. This will definitely crash.

![image](https://github.com/apache/incubator-pegasus/assets/162662221/10e5159a-ddde-4a21-bd59-b5801e84efaa)
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM552g22/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1940,https://api.github.com/repos/apache/incubator-pegasus/issues/1940,incubator-pegasus,2175665480,1940,"Docker image for compilation environment failed to be built due to ""cannot verify archive.apache.org's certificate""",empiredan,743379,Dan Wang,,CLOSED,2024-03-08T09:47:17Z,2024-03-14T03:06:21Z,"Docker image for compilation environment failed to be built due to error as below:

```
#8 [4/6] RUN wget --progress=dot:giga https://archive.apache.org/dist/maven/maven-3/3.8.3/binaries/apache-maven-3.8.3-bin.tar.gz -P /opt/maven     && cd /opt/maven     && tar -zxf apache-maven-3.8.3-bin.tar.gz     && rm apache-maven-3.8.3-bin.tar.gz
#8 0.086 --2024-03-08 08:48:50--  https://archive.apache.org/dist/maven/maven-3/3.8.3/binaries/apache-maven-3.8.3-bin.tar.gz
#8 0.092 Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2
#8 0.124 Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.
#8 0.479 ERROR: cannot verify archive.apache.org's certificate, issued by '/C=US/O=Let\'s Encrypt/CN=R3':
#8 0.479   Issued certificate has expired.
#8 0.479 To connect to archive.apache.org insecurely, use `--no-check-certificate'.
#8 ERROR: process ""/bin/sh -c wget --progress=dot:giga https://archive.apache.org/dist/maven/maven-3/3.8.3/binaries/apache-maven-3.8.3-bin.tar.gz -P /opt/maven     && cd /opt/maven     && tar -zxf apache-maven-3.8.3-bin.tar.gz     && rm apache-maven-3.8.3-bin.tar.gz"" did not complete successfully: exit code: 5
------
 > [4/6] RUN wget --progress=dot:giga https://archive.apache.org/dist/maven/maven-3/3.8.3/binaries/apache-maven-3.8.3-bin.tar.gz -P /opt/maven     && cd /opt/maven     && tar -zxf apache-maven-3.8.3-bin.tar.gz     && rm apache-maven-3.8.3-bin.tar.gz:
connected.
0.479 ERROR: cannot verify archive.apache.org's certificate, issued by '/C=US/O=Let\'s Encrypt/CN=R3':
0.479   Issued certificate has expired.
0.479 To connect to archive.apache.org insecurely, use `--no-check-certificate'.
------
Dockerfile:59
--------------------
  58 |     
  59 | >>> RUN wget --progress=dot:giga https://archive.apache.org/dist/maven/maven-3/3.8.3/binaries/apache-maven-3.8.3-bin.tar.gz -P /opt/maven \
  60 | >>>     && cd /opt/maven \
  61 | >>>     && tar -zxf apache-maven-3.8.3-bin.tar.gz \
  62 | >>>     && rm apache-maven-3.8.3-bin.tar.gz
  63 |     
--------------------
ERROR: failed to solve: process ""/bin/sh -c wget --progress=dot:giga https://archive.apache.org/dist/maven/maven-3/3.8.3/binaries/apache-maven-3.8.3-bin.tar.gz -P /opt/maven     && cd /opt/maven     && tar -zxf apache-maven-3.8.3-bin.tar.gz     && rm apache-maven-3.8.3-bin.tar.gz"" did not complete successfully: exit code: 5
Error: buildx failed with: ERROR: failed to solve: process ""/bin/sh -c wget --progress=dot:giga https://archive.apache.org/dist/maven/maven-3/3.8.3/binaries/apache-maven-3.8.3-bin.tar.gz -P /opt/maven     && cd /opt/maven     && tar -zxf apache-maven-3.8.3-bin.tar.gz     && rm apache-maven-3.8.3-bin.tar.gz"" did not complete successfully: exit code: 5
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1940/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1945,https://api.github.com/repos/apache/incubator-pegasus/issues/1945,incubator-pegasus,2183460686,1945,Feature: refactor cold backup and restore function,ninsmiracle,110282526,,,OPEN,2024-03-13T09:04:23Z,2024-03-13T09:04:23Z,"## Feature Request
Need to continue merge new backup code to current master. And do not do priodic backup in pegasus server side. 

For more detail design:
#1081 ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1945/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1947,https://api.github.com/repos/apache/incubator-pegasus/issues/1947,incubator-pegasus,2184039220,1947,Bug(go client):Go client fails to compile.,lengyuexuexuan,46274877,,,CLOSED,2024-03-13T13:35:46Z,2024-04-10T07:39:14Z,"## Bug Report

1. What did you do?
the go client  failed to compile.
![1](https://github.com/apache/incubator-pegasus/assets/46274877/69c2ccdc-958e-479d-bafb-e0c34ac26263)

2. What did you expect to see?
the go client can compile.

3. What version of Pegasus are you using?
the lastest version.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1947/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1947,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM52_Toy,incubator-pegasus,1996306994,1947,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-03-14T03:05:54Z,2024-03-14T03:05:54Z,"Thanks! Fixed in https://github.com/apache/incubator-pegasus/pull/1946.

Tips: It would be better to paste text rather than picture, the text content can be easily searched.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM52_Toy/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1963,https://api.github.com/repos/apache/incubator-pegasus/issues/1963,incubator-pegasus,2208216311,1963,Partition split hung,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2024-03-26T13:14:53Z,2024-03-28T02:40:31Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
Create a table with 8 partitions
Write some data to the table
Start partition split

2. What did you expect to see?
The partition split can be completed successfully.

3. What did you see instead?
The partitions are in `SPLITTING` state, the process hung and could not response requests
![企业微信截图_59984773-3820-4bd4-8fbf-c0fb02312e36](https://github.com/apache/incubator-pegasus/assets/10775040/a89f7390-15ce-496c-944d-8352231a0e35)

4. What version of Pegasus are you using?
2.4","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1963/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1963,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM54bUR3,incubator-pegasus,2020426871,1963,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-03-26T13:26:03Z,2024-03-26T13:26:03Z,"Piece of the pstack:
```
Thread 273 (Thread 0x7f3df4dbc700 (LWP 25825)):
#0  0x00007f3e0f2bbb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0
#1  0x00007f3e0f2bbbcf in __new_sem_wait_slow.constprop.0 () from /lib64/libpthread.so.0
#2  0x00007f3e0f2bbc6b in sem_wait@@GLIBC_2.2.5 () from /lib64/libpthread.so.0
#3  0x00007f3e1192a6e2 in dsn::task::wait(int) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#4  0x00007f3e1193e44f in dsn::task_tracker::wait_outstanding_tasks() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#5  0x00007f3e1179eef6 in ?? () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#6  0x00007f3e117a0e1a in dsn::replication::replica_app_info::store(std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#7  0x00007f3e116fbdd4 in dsn::replication::replica::store_app_info(dsn::app_info&, std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#8  0x00007f3e1173b18d in dsn::replication::replica::initialize_on_new() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#9  0x00007f3e11764fe8 in dsn::replication::replica_stub::new_replica(dsn::gpid, dsn::app_info const&, bool, bool, std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#10 0x00007f3e1176564f in dsn::replication::replica_stub::create_child_replica_if_not_found(dsn::gpid, dsn::app_info*, std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#11 0x00007f3e11765917 in dsn::replication::replica_stub::create_child_replica(dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#12 0x00007f3e117ee1a6 in std::_Function_handler<void (), std::_Bind<void (dsn::replication::replica_stub::*(dsn::replication::replica_stub*, dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string))(dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string const&)> >::_M_invoke(std::_Any_data const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#13 0x00007f3e11929f21 in dsn::task::exec_internal() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#14 0x00007f3e1193f422 in dsn::task_worker::loop() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#15 0x00007f3e1193f5a0 in dsn::task_worker::run_internal() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#16 0x00007f3e11125e9f in ?? () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_utils.so
#17 0x00007f3e0f2b5ea5 in start_thread () from /lib64/libpthread.so.0
#18 0x00007f3e0d785b0d in clone () from /lib64/libc.so.6
Thread 272 (Thread 0x7f3df45bb700 (LWP 25826)):
#0  0x00007f3e0f2bbb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0
#1  0x00007f3e0f2bbbcf in __new_sem_wait_slow.constprop.0 () from /lib64/libpthread.so.0
#2  0x00007f3e0f2bbc6b in sem_wait@@GLIBC_2.2.5 () from /lib64/libpthread.so.0
#3  0x00007f3e1196a132 in dsn::tools::std_rwlock_nr_provider::lock_write() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#4  0x00007f3e118f8e0d in dsn::zrwlock_nr::lock_write() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#5  0x00007f3e117653a3 in dsn::replication::replica_stub::create_child_replica_if_not_found(dsn::gpid, dsn::app_info*, std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#6  0x00007f3e11765917 in dsn::replication::replica_stub::create_child_replica(dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#7  0x00007f3e117ee1a6 in std::_Function_handler<void (), std::_Bind<void (dsn::replication::replica_stub::*(dsn::replication::replica_stub*, dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string))(dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string const&)> >::_M_invoke(std::_Any_data const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#8  0x00007f3e11929f21 in dsn::task::exec_internal() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#9  0x00007f3e1193f422 in dsn::task_worker::loop() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#10 0x00007f3e1193f5a0 in dsn::task_worker::run_internal() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#11 0x00007f3e11125e9f in ?? () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_utils.so
#12 0x00007f3e0f2b5ea5 in start_thread () from /lib64/libpthread.so.0
#13 0x00007f3e0d785b0d in clone () from /lib64/libc.so.6
Thread 271 (Thread 0x7f3df3dba700 (LWP 25827)):
#0  0x00007f3e0f2bbb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0
#1  0x00007f3e0f2bbbcf in __new_sem_wait_slow.constprop.0 () from /lib64/libpthread.so.0
#2  0x00007f3e0f2bbc6b in sem_wait@@GLIBC_2.2.5 () from /lib64/libpthread.so.0
#3  0x00007f3e1196a132 in dsn::tools::std_rwlock_nr_provider::lock_write() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#4  0x00007f3e118f8e0d in dsn::zrwlock_nr::lock_write() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#5  0x00007f3e117653a3 in dsn::replication::replica_stub::create_child_replica_if_not_found(dsn::gpid, dsn::app_info*, std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#6  0x00007f3e11765917 in dsn::replication::replica_stub::create_child_replica(dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#7  0x00007f3e117ee1a6 in std::_Function_handler<void (), std::_Bind<void (dsn::replication::replica_stub::*(dsn::replication::replica_stub*, dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string))(dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string const&)> >::_M_invoke(std::_Any_data const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#8  0x00007f3e11929f21 in dsn::task::exec_internal() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#9  0x00007f3e1193f422 in dsn::task_worker::loop() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#10 0x00007f3e1193f5a0 in dsn::task_worker::run_internal() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#11 0x00007f3e11125e9f in ?? () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_utils.so
#12 0x00007f3e0f2b5ea5 in start_thread () from /lib64/libpthread.so.0
#13 0x00007f3e0d785b0d in clone () from /lib64/libc.so.6
Thread 270 (Thread 0x7f3df35b9700 (LWP 25828)):
#0  0x00007f3e0f2bbb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0
#1  0x00007f3e0f2bbbcf in __new_sem_wait_slow.constprop.0 () from /lib64/libpthread.so.0
#2  0x00007f3e0f2bbc6b in sem_wait@@GLIBC_2.2.5 () from /lib64/libpthread.so.0
#3  0x00007f3e1196a132 in dsn::tools::std_rwlock_nr_provider::lock_write() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#4  0x00007f3e118f8e0d in dsn::zrwlock_nr::lock_write() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#5  0x00007f3e117653a3 in dsn::replication::replica_stub::create_child_replica_if_not_found(dsn::gpid, dsn::app_info*, std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#6  0x00007f3e11765917 in dsn::replication::replica_stub::create_child_replica(dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#7  0x00007f3e117ee1a6 in std::_Function_handler<void (), std::_Bind<void (dsn::replication::replica_stub::*(dsn::replication::replica_stub*, dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string))(dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string const&)> >::_M_invoke(std::_Any_data const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#8  0x00007f3e11929f21 in dsn::task::exec_internal() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#9  0x00007f3e1193f422 in dsn::task_worker::loop() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#10 0x00007f3e1193f5a0 in dsn::task_worker::run_internal() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#11 0x00007f3e11125e9f in ?? () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_utils.so
#12 0x00007f3e0f2b5ea5 in start_thread () from /lib64/libpthread.so.0
#13 0x00007f3e0d785b0d in clone () from /lib64/libc.so.6
Thread 269 (Thread 0x7f3df2db8700 (LWP 25829)):
#0  0x00007f3e0f2bbb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0
#1  0x00007f3e0f2bbbcf in __new_sem_wait_slow.constprop.0 () from /lib64/libpthread.so.0
#2  0x00007f3e0f2bbc6b in sem_wait@@GLIBC_2.2.5 () from /lib64/libpthread.so.0
#3  0x00007f3e1196a132 in dsn::tools::std_rwlock_nr_provider::lock_write() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#4  0x00007f3e118f8e0d in dsn::zrwlock_nr::lock_write() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#5  0x00007f3e117653a3 in dsn::replication::replica_stub::create_child_replica_if_not_found(dsn::gpid, dsn::app_info*, std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#6  0x00007f3e11765917 in dsn::replication::replica_stub::create_child_replica(dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#7  0x00007f3e117ee1a6 in std::_Function_handler<void (), std::_Bind<void (dsn::replication::replica_stub::*(dsn::replication::replica_stub*, dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string))(dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string const&)> >::_M_invoke(std::_Any_data const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#8  0x00007f3e11929f21 in dsn::task::exec_internal() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#9  0x00007f3e1193f422 in dsn::task_worker::loop() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#10 0x00007f3e1193f5a0 in dsn::task_worker::run_internal() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#11 0x00007f3e11125e9f in ?? () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_utils.so
#12 0x00007f3e0f2b5ea5 in start_thread () from /lib64/libpthread.so.0
#13 0x00007f3e0d785b0d in clone () from /lib64/libc.so.6
Thread 268 (Thread 0x7f3df25b7700 (LWP 25830)):
#0  0x00007f3e0f2bbb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0
#1  0x00007f3e0f2bbbcf in __new_sem_wait_slow.constprop.0 () from /lib64/libpthread.so.0
#2  0x00007f3e0f2bbc6b in sem_wait@@GLIBC_2.2.5 () from /lib64/libpthread.so.0
#3  0x00007f3e1196a132 in dsn::tools::std_rwlock_nr_provider::lock_write() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#4  0x00007f3e118f8e0d in dsn::zrwlock_nr::lock_write() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#5  0x00007f3e117653a3 in dsn::replication::replica_stub::create_child_replica_if_not_found(dsn::gpid, dsn::app_info*, std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#6  0x00007f3e11765917 in dsn::replication::replica_stub::create_child_replica(dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#7  0x00007f3e117ee1a6 in std::_Function_handler<void (), std::_Bind<void (dsn::replication::replica_stub::*(dsn::replication::replica_stub*, dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string))(dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string const&)> >::_M_invoke(std::_Any_data const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#8  0x00007f3e11929f21 in dsn::task::exec_internal() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#9  0x00007f3e1193f422 in dsn::task_worker::loop() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#10 0x00007f3e1193f5a0 in dsn::task_worker::run_internal() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#11 0x00007f3e11125e9f in ?? () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_utils.so
#12 0x00007f3e0f2b5ea5 in start_thread () from /lib64/libpthread.so.0
#13 0x00007f3e0d785b0d in clone () from /lib64/libc.so.6
Thread 267 (Thread 0x7f3df1db6700 (LWP 25831)):
#0  0x00007f3e0f2bbb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0
#1  0x00007f3e0f2bbbcf in __new_sem_wait_slow.constprop.0 () from /lib64/libpthread.so.0
#2  0x00007f3e0f2bbc6b in sem_wait@@GLIBC_2.2.5 () from /lib64/libpthread.so.0
#3  0x00007f3e1196a132 in dsn::tools::std_rwlock_nr_provider::lock_write() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#4  0x00007f3e118f8e0d in dsn::zrwlock_nr::lock_write() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#5  0x00007f3e117653a3 in dsn::replication::replica_stub::create_child_replica_if_not_found(dsn::gpid, dsn::app_info*, std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#6  0x00007f3e11765917 in dsn::replication::replica_stub::create_child_replica(dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#7  0x00007f3e117ee1a6 in std::_Function_handler<void (), std::_Bind<void (dsn::replication::replica_stub::*(dsn::replication::replica_stub*, dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string))(dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string const&)> >::_M_invoke(std::_Any_data const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#8  0x00007f3e11929f21 in dsn::task::exec_internal() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#9  0x00007f3e1193f422 in dsn::task_worker::loop() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#10 0x00007f3e1193f5a0 in dsn::task_worker::run_internal() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#11 0x00007f3e11125e9f in ?? () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_utils.so
#12 0x00007f3e0f2b5ea5 in start_thread () from /lib64/libpthread.so.0
#13 0x00007f3e0d785b0d in clone () from /lib64/libc.so.6
Thread 266 (Thread 0x7f3df15b5700 (LWP 25832)):
#0  0x00007f3e0f2bbb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0
#1  0x00007f3e0f2bbbcf in __new_sem_wait_slow.constprop.0 () from /lib64/libpthread.so.0
#2  0x00007f3e0f2bbc6b in sem_wait@@GLIBC_2.2.5 () from /lib64/libpthread.so.0
#3  0x00007f3e1196a132 in dsn::tools::std_rwlock_nr_provider::lock_write() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#4  0x00007f3e118f8e0d in dsn::zrwlock_nr::lock_write() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#5  0x00007f3e117653a3 in dsn::replication::replica_stub::create_child_replica_if_not_found(dsn::gpid, dsn::app_info*, std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#6  0x00007f3e11765917 in dsn::replication::replica_stub::create_child_replica(dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#7  0x00007f3e117ee1a6 in std::_Function_handler<void (), std::_Bind<void (dsn::replication::replica_stub::*(dsn::replication::replica_stub*, dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string))(dsn::rpc_address, dsn::app_info, long, dsn::gpid, dsn::gpid, std::string const&)> >::_M_invoke(std::_Any_data const&) () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#8  0x00007f3e11929f21 in dsn::task::exec_internal() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#9  0x00007f3e1193f422 in dsn::task_worker::loop() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#10 0x00007f3e1193f5a0 in dsn::task_worker::run_internal() () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_replica_server.so
#11 0x00007f3e11125e9f in ?? () from /sensorsdata/main/program/skv/skv_offline/replica_server/lib/libdsn_utils.so
#12 0x00007f3e0f2b5ea5 in start_thread () from /lib64/libpthread.so.0
#13 0x00007f3e0d785b0d in clone () from /lib64/libc.so.6
```
Piece of the config:
```
[threadpool.THREAD_POOL_DEFAULT]
name = default
partitioned = false
worker_priority = THREAD_xPRIORITY_NORMAL
worker_count = 8
```

As shown above, the `THREAD_POOL_DEFAULT` thread-pool has 8 threads, the table to be split has 8 partitions too, and the cluster has only 1 replica server.

When received the partition split request, all the 8 partitions start to add child replicas:
```
    tasking::enqueue(LPC_CREATE_CHILD,
                     tracker(),
                     std::bind(&replica_stub::create_child_replica,
                               _stub,
                               _replica->_config.hp_primary,
                               _replica->_app_info,
                               _child_init_ballot,
                               _child_gpid,
                               get_gpid(),
                               _replica->_dir),
                     get_gpid().thread_hash());
```

1. The tasks are enqueued as task code `LPC_CREATE_CHILD` which is dealt by `THREAD_POOL_DEFAULT` thread-pool.
2. Now all the 8 threads are exhausted.
3. When one of them start to create the replica info by `replica_app_info::store()`, it will be enqued as `LPC_AIO_INFO_WRITE` which is also use the thread-pool `THREAD_POOL_DEFAULT`.
4. In the step 3, the task hold the lock `_replicas_lock` of replica_stub.
5. The other tasks are waiting the lock to be released, but the owner of the lock is requiring a thread to write file in step 3, but there is no more threads avaiable.
6. Forming the deadlock.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM54bUR3/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1963,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM54qE_a,incubator-pegasus,2024296410,1963,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-03-28T02:39:24Z,2024-03-28T02:39:24Z,"The lastest version has update the logic to use rocksdb::WriteStringToFile to write file in current thread.

This issue is possible to occur on version 2.4 and 2.5, if you encounter this, you can enlarge the value of `worker_count` in `[threadpool.THREAD_POOL_DEFAULT]` section.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM54qE_a/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1963,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM54qFK4,incubator-pegasus,2024297144,1963,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-03-28T02:40:30Z,2024-03-28T02:40:30Z,Don't close the issue for convenient search.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM54qFK4/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1964,https://api.github.com/repos/apache/incubator-pegasus/issues/1964,incubator-pegasus,2208652447,1964,The amount of data increased after backing up and restoring a partition-split table,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2024-03-26T15:57:07Z,2024-03-27T02:02:33Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
- create a table A
- write some data to the table A
- count the total data on table A as `number_a1`
- execute a partition-split on the table A as `number_a2`
- count the total data on table A
- back up the table A to HDFS
- restore the data to a new table B from the backed up data from HDFS
- count the total data on table B as `number_b`

2. What did you expect to see?
`number_a1` == `number_a2` == `number_b`

3. What did you see instead?
`number_a1` == `number_a2` != `number_b`

4. What version of Pegasus are you using?
2.4
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1964/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1964,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM54c18l,incubator-pegasus,2020826917,1964,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-03-26T15:58:37Z,2024-03-26T15:58:37Z,It's because the `replica.split.validate_partition_hash` environment is lost on table B.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM54c18l/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1965,https://api.github.com/repos/apache/incubator-pegasus/issues/1965,incubator-pegasus,2209690239,1965,Meta server crashed in prometheus-cpp library,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2024-03-27T02:32:40Z,2024-04-08T03:42:25Z,"Meta server crashed if creating a backup policy with a name `test_1-1` (contains `-` character), the assert information is as below:
```
meta_server: /data/jenkins/workspace/skv/prebuild_binary-multi-arch/sub-jobs/build-develop-x86_64/thirdparty/build/Source/prometheus-cpp/core/include/prometheus/family.h:142: prometheus::Family<T>::Family(const string&, const string&, const std::map<std::basic_string<char>, std::basic_string<char> >&) [with T = prometheus::Gauge; std::string = std::basic_string<char>]: Assertion `CheckMetricName(name_)' failed.
```

The prometheus-cpp code:
```
bool CheckMetricName(const std::string& name) {
  // see https://prometheus.io/docs/concepts/data_model/
  auto reserved_for_internal_purposes = name.compare(0, 2, ""__"") == 0;
  if (reserved_for_internal_purposes) return false;
#ifdef STD_REGEX_IS_BROKEN
  return !name.empty();
#else
  static const std::regex metric_name_regex(""[a-zA-Z_:][a-zA-Z0-9_:]*"");
  return std::regex_match(name, metric_name_regex);
#endif
}

bool CheckLabelName(const std::string& name) {
  // see https://prometheus.io/docs/concepts/data_model/
  auto reserved_for_internal_purposes = name.compare(0, 2, ""__"") == 0;
  if (reserved_for_internal_purposes) return false;
#ifdef STD_REGEX_IS_BROKEN
  return !name.empty();
#else
  static const std::regex label_name_regex(""[a-zA-Z_][a-zA-Z0-9_]*"");
  return std::regex_match(name, label_name_regex);
#endif
}
```

So the metric name should:
- not prefixed by `__`
- not empty
- match `[a-zA-Z_][a-zA-Z0-9_]*`


See https://prometheus.io/docs/concepts/data_model/

> Metric names:
>
> - Metric names may contain ASCII letters, digits, underscores, and colons. It must match the regex [a-zA-Z_:][a-zA-Z0-9_:]*.
>
> Metric labels:
> - Labels may contain ASCII letters, numbers, as well as underscores. They must match the regex [a-zA-Z_][a-zA-Z0-9_]*.
> - Label names beginning with __ (two ""_"") are reserved for internal use.
> - Label values may contain any Unicode characters.
> - Labels with an empty label value are considered equivalent to labels that do not exist.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1965/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1965,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM55s37l,incubator-pegasus,2041806565,1965,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-04-08T03:42:23Z,2024-04-08T03:42:23Z,"Because the servers don't expose prometheus metrics directly now, this commit https://github.com/apache/incubator-pegasus/commit/bcb92f0fd30374d691d1cc5535ca0bf5ae671988 seems useless, and we can remove the prometheus-cpp library directly.

You can do this when you remove perf_counter* related code. @empiredan ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM55s37l/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1969,https://api.github.com/repos/apache/incubator-pegasus/issues/1969,incubator-pegasus,2218030170,1969,"Bug(Learn)：bakcup or dup a table with per disk throttling on a backup-duplication cluster , some nodes coredump",ninsmiracle,110282526,,,OPEN,2024-04-01T10:51:46Z,2024-05-09T07:18:49Z,"## Bug Report

1. What did you do?
  Firstly , I do all my work in a bakcup-duplication cluster (relatively,mater-duplication cluster send message to it.)

-   When I config `max_copy_rate_megabytes_per_disk` , Whether I do backup or duplication for an app , some nodes on bakcup-cluster will coredump.And I discovered a phenomenon that they all coredump **when secondary replica try to learn from primary replica**.

-   Without `max_copy_rate_megabytes_per_disk` , backup action or duplication action will not casue any coredump.

2.coredump file:
```
#0  0x00007fa027148c30 in dsn::replication::replica_duplicator_manager::update_duplication_map (this=0x0, new_dup_map=...)
    at /home/work/pegasus/src/rdsn/src/replica/duplication/replica_duplicator_manager.h:57
57      /home/work/pegasus/src/rdsn/src/replica/duplication/replica_duplicator_manager.h: No such file or directory.
Missing separate debuginfos, use: debuginfo-install bzip2-libs-1.0.6-13.el7.x86_64 elfutils-libelf-0.166-2.el7.x86_64 elfutils-libs-0.166-2.el7.x86_64 glibc-2.17-157.el7_3.1.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.14.1-27.el7_3.x86_64 libattr-2.4.46-12.el7.x86_64 libcap-2.22-8.el7.x86_64 libcom_err-1.42.9-9.el7.x86_64 libgcc-4.8.5-28.el7_5.1.x86_64 libselinux-2.5-15.el7.x86_64 pcre-8.32-15.el7_2.1.x86_64 systemd-libs-219-67.el7_7.1.x86_64 xz-libs-5.2.2-1.el7.x86_64 zlib-1.2.7-17.el7.x86_64
(gdb) #0  0x00007fa027148c30 in dsn::replication::replica_duplicator_manager::update_duplication_map (this=0x0, new_dup_map=...)
    at /home/work/pegasus/src/rdsn/src/replica/duplication/replica_duplicator_manager.h:57
#1  0x00007fa027146635 in dsn::replication::duplication_sync_timer::update_duplication_map (this=this@entry=0x407c5f00, dup_map=...)
    at /home/work/pegasus/src/rdsn/src/replica/duplication/duplication_sync_timer.cpp:111
#2  0x00007fa02714673b in dsn::replication::duplication_sync_timer::on_duplication_sync_reply (this=this@entry=0x407c5f00, err=..., resp=...)
    at /home/work/pegasus/src/rdsn/src/replica/duplication/duplication_sync_timer.cpp:90
#3  0x00007fa027146825 in operator() (err=..., __closure=0x8f0239e0)
    at /home/work/pegasus/src/rdsn/src/replica/duplication/duplication_sync_timer.cpp:77
#4  operator() (req=<optimized out>, resp=<optimized out>, err=...,
    __closure=0x8f0239e0)
    at /home/work/pegasus/src/rdsn/include/dsn/cpp/rpc_holder.h:161
#5  std::_Function_handler<void(dsn::error_code, dsn::message_ex*, dsn::message_ex*), dsn::rpc_holder<TRequest, TResponse>::call(const dsn::rpc_address&, dsn::task_tracker*, TCallback&&, int) [with TCallback = dsn::replication::duplication_sync_timer::run()::<lambda(dsn::error_code)>; TRequest = dsn::replication::duplication_sync_request; TResponse = dsn::replication::duplication_sync_response; dsn::task_ptr = dsn::ref_ptr<dsn::task>]::<lambda(dsn::error_code, dsn::message_ex*, dsn::message_ex*)> >::_M_invoke(const std::_Any_data &, dsn::error_code &, dsn::message_ex *&, dsn::message_ex *&) (__functor=..., __args#0=...,
    __args#1=<optimized out>, __args#2=<optimized out>)
    at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/std_function.h:316
#6  0x00007fa0272c06d7 in operator() (__args#2=<optimized out>,
    __args#1=<optimized out>, __args#0=..., this=<optimized out>)
    at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/std_function.h:706
#7  dsn::rpc_response_task::exec (this=<optimized out>)
    at /home/work/pegasus/src/rdsn/include/dsn/tool-api/task.h:478
#8  0x00007fa0272c1431 in dsn::task::exec_internal (
    this=this@entry=0x8d6b2b00)
    at /home/work/pegasus/src/rdsn/src/runtime/task/task.cpp:176
#9  0x00007fa0272d6ae2 in dsn::task_worker::loop (this=0x2e66e70)
    at /home/work/pegasus/src/rdsn/src/runtime/task/task_worker.cpp:224
#10 0x00007fa0272d6c60 in dsn::task_worker::run_internal (this=0x2e66e70)
    at /home/work/pegasus/src/rdsn/src/runtime/task/task_worker.cpp:204
#11 0x00007fa025f54a1f in execute_native_thread_routine ()
   from /home/work/app/pegasus/azpnsrv-storecom/replica/package/bin/libdsn_utils.so
#12 0x00007fa023d5fdc5 in start_thread () from /lib64/libpthread.so.0
#13 0x00007fa02225e73d in clone () from /lib64/libc.so.6
(gdb) quit
```

3. What version of Pegasus are you using?
v2.4
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1969/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1969,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM57otDy,incubator-pegasus,2074267890,1969,NA,ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,NA,2024-04-24T07:30:52Z,2024-04-24T07:30:52Z,"I found that `max_copy_rate_megabytes_per_disk` causes group check timeouts, leading to primary replica shard ballot growth. Some nodes are generating core dumps, and in severe cases, unalive nodes will appear. 
This issue can be reliably reproduced.

## What did you do?

a. Execute command `restore -c c4tst-function3 -a lpf_test -i 27 -t 1713285672087 -b hdfs_zjy -r /user/s_pegasus/lpfbackup`
b. Execute multiple times `server-config replica  max_copy_rate_megabytes_per_disk set  NUM` , NUM = [10,50,10,50,10].

## Phenomenon

Many groupcheck timeouts and node core dump. The coredump content is **the same as** the above coredump file content. 

```c++
log.2.txt:E2024-04-23 19:08:33.428 (1713870513428314325 9853) replica.replica11.0404000b000003d1: replica_failover.cpp:68:handle_remote_failure(): [31.42@x.x.x.x:45101](mailto:31.42@x.x.x.x:45101): handle remote failure caused by group check, error = ERR_TIMEOUT, status = replication::partition_status::PS_SECONDARY, node = [x.x.x.x:45101](http://x.x.x.x:45101/)
log.2.txt:E2024-04-23 19:09:06.711 (1713870546711168146 9853) replica.replica11.0404000b000003f2: replica_failover.cpp:68:handle_remote_failure(): [31.42@x.x.x.x:45101](mailto:31.42@x.x.x.x:45101): handle remote failure caused by group check, error = ERR_TIMEOUT, status = replication::partition_status::PS_SECONDARY, node = [x.x.x.x:45101](http://x.x.x.x:45101/)
log.2.txt:E2024-04-23 19:10:10.238 (1713870610238377803 9853) replica.replica11.0404000b00000411: replica_failover.cpp:68:handle_remote_failure(): [31.42@x.x.x.x:45101](mailto:31.42@x.x.x.x:45101): handle remote failure caused by group check, error = ERR_TIMEOUT, status = replication::partition_status::PS_SECONDARY, node = [x.x.x.x:45101](http://x.x.x.x:45101/)
log.2.txt:E2024-04-23 19:10:24.219 (1713870624219417888 9845) replica.replica3.04040003000002da: replica_failover.cpp:68:handle_remote_failure(): [31.82@x.x.x.x:45101](mailto:31.82@x.x.x.x:45101): handle remote failure caused by group check, error = ERR_TIMEOUT, status = replication::partition_status::PS_SECONDARY, node = [x.x.x.x:45101](http://x.x.x.x:45101/)
log.2.txt:E2024-04-23 19:10:47.670 (1713870647670827213 9845) replica.replica3.04040003000002e5: replica_failover.cpp:68:handle_remote_failure(): [31.82@x.x.x.x:45101](mailto:31.82@x.x.x.x:45101): handle remote failure caused by group check, error = ERR_TIMEOUT, status = replication::partition_status::PS_SECONDARY, node = [x.x.x.x:45101](http://x.x.x.x:45101/)
log.2.txt:E2024-04-23 19:10:59.988 (1713870659988397675 9847) replica.replica5.0404000500000214: replica_failover.cpp:68:handle_remote_failure(): [31.12@x.x.x.x:45101](mailto:31.12@x.x.x.x:45101): handle remote failure caused by group check, error = ERR_TIMEOUT, status = replication::partition_status::PS_SECONDARY, node = [x.x.x.x:45101](http://x.x.x.x:45101/)
log.2.txt:E2024-04-23 19:10:59.988 (1713870659988935233 9861) replica.replica19.04040013000007cd: replica_failover.cpp:68:handle_remote_failure(): [29.0@x.x.x.x:45101](mailto:29.0@x.x.x.x:45101): handle remote failure caused by group check, error = ERR_TIMEOUT, status = replication::partition_status::PS_SECONDARY, node = [x.x.x.x:45101](http://x.x.x.x:45101/)
```

## Case1

RPC_GROUP_CHECK_ACK is sent with a delay of 9 seconds

**primary replica send**

D2024-04-23 19:10:02.688 (1713870602688110786 9853) replica.replica11.0404000b00000410: replica_check.cpp:191:call(): rpc_name = RPC_GROUP_CHECK, remote_addr = x.x.x.x:45101, header_format = RPC_GROUP_CHECK, task_id = 289356323303195665, seq_id = 14982, trace_id = 0eb2a9e82c2277de

log.2.txt:D2024-04-23 19:10:02.688 (1713870602688121529 9853) replica.replica11.0404000b00000410: replica_check.cpp:140:broadcast_group_check(): 31.42@x.x.x.x:45101: 
send group check to x.x.x.x:45101 with state replication::partition_status::PS_SECONDARY , task_id 289356323303195665 ,ballot 7

log.2.txt:E2024-04-23 19:10:10.238 (1713870610238377803 9853) replica.replica11.0404000b00000411: replica_failover.cpp:68:handle_remote_failure(): 31.42@x.x.x.x:45101: handle remote failure caused by group check, error = ERR_TIMEOUT, status = replication::partition_status::PS_SECONDARY, node = x.x.x.x:45101

**secondary replica reply**

log.3.txt:I2024-04-23 **19:10:02.688** (1713870602688201671 84230) replica.io-thrd.84230: network.cpp:415:on_recv_message(): LPF rpc_name = RPC_GROUP_CHECK, this_recv_addr = x.x.x.x:64140, header_format = NET_HDR_DSN, seq_id = 14982, trace_id = 0eb2a9e82c2277de

D2024-04-23 19:10:02.688 (1713870602688281016 84284) replica.replica11.04004906000108f0: replica_stub.cpp:1215:on_group_check(): 31.42@x.x.x.x:45101: received group check, primary = x.x.x.x:45101, ballot = 7, status = replication::partition_status::PS_SECONDARY, last_committed_decree = 200457

D2024-04-23 19:10:02.688 (1713870602688323625 84284) replica.replica11.04004906000108f0: replica_check.cpp:164:on_group_check(): [31.42@x.x.x.x:45101] process group check, primary = x.x.x.x:45101, ballot = 7, status = replication::partition_status::PS_SECONDARY, last_committed_decree = 200457, confirmed_decree = -1

log.3.txt:I2024-04-23 **19:10:11.198** (1713870611198691041 84284) replica.replica11.04004906000108f0: rpc_engine.cpp:742:reply(): LPF response rpc_name = RPC_GROUP_CHECK_ACK, from_addr = x.x.x.x:45101, to_addr = x.x.x.x:45101, header_format = NET_HDR_DSN, seq_id = 14982, trace_id = 0eb2a9e82c2277de

## Reason

I suspect that the `sleep_for` function is causing the `replica.default` thread pool to sleep excessively, affecting the sending of `RPC_GROUP_CHECK_ACK`. **However, it is still unclear how this is happening.**

```c++
    bool consumeWithBorrowAndWait(double toConsume,
                                  double rate,
                                  double burstSize,
                                  double nowInSeconds = defaultClockNow())
    {
        auto res = consumeWithBorrowNonBlocking(toConsume, rate, burstSize, nowInSeconds);
        if (res.get_value_or(0) > 0) {
            int64_t napUSec = res.get() * 1000000;
            std::this_thread::sleep_for(std::chrono::microseconds(napUSec));
        }
        return res.is_initialized();
    }
```

When max_copy_rate_megabytes_per_disk is set 10, **Seconds-level sleep is causing the `replica.default` thread pool to sleep excessively.**

```c++
2024-04-23 19:10:02.341 84238) replica.default1.040100070000298f: sleep,microseconds is 3199715,
D2024-04-23 19:10:02.343 84239) replica.default2.0401000700002993: sleep,microseconds is 3598415,
D2024-04-23 19:10:02.440 84241) replica.default4.0401000700002999: sleep,microseconds is 3901018,
D2024-04-23 19:10:02.712 84243) replica.default6.040100080000296b: sleep,microseconds is 4029154,
D2024-04-23 19:10:02.714 84245) replica.default8.0401000700002995: sleep,microseconds is 4427141,
D2024-04-23 19:10:02.744 84242) replica.default5.0401000700002997: sleep,microseconds is 4797563,
D2024-04-23 19:10:03.144 84237) replica.default0.0401000800002969: sleep,microseconds is 4797490,
D2024-04-23 19:10:03.544 84240) replica.default3.0401000200001e72: sleep,microseconds is 4797061,
D2024-04-23 19:10:03.943 84247) replica.default10.0401000a00002990: sleep,microseconds is 4798536,
D2024-04-23 19:10:04.343 84244) replica.default7.040100070000299b: sleep,microseconds is 4798140,
D2024-04-23 19:10:04.746 84246) replica.default9.04010001000024e5: sleep,microseconds is 4795031,
D2024-04-23 19:10:05.142 84248) replica.default11.0401000600001f65: sleep,microseconds is 372727,
D2024-04-23 19:10:05.515 84248) replica.default11.0401000600001f65: sleep,microseconds is 377530,
D2024-04-23 19:10:05.542 84238) replica.default1.0401000200001e74: sleep,microseconds is 4399154,
D2024-04-23 19:10:05.904 84248) replica.default11.0401000b00002296: sleep,microseconds is 4093229,
D2024-04-23 19:10:06.75 84239) replica.default2.040100040000260e: sleep,microseconds is 217558,
D2024-04-23 19:10:06.294 84239) replica.default2.0401000900002654: sleep,microseconds is 4103047,
D2024-04-23 19:10:06.358 84241) replica.default4.0401000600001f73: sleep,microseconds is 334393,
D2024-04-23 19:10:06.693 84241) replica.default4.040100080000296f: sleep,microseconds is 399535,
D2024-04-23 19:10:06.745 84243) replica.default6.040100080000296d: sleep,microseconds is 747141,
D2024-04-23 19:10:07.93 84241) replica.default4.040100050000229c: sleep,microseconds is 799505,
D2024-04-23 19:10:07.142 84245) replica.default8.040100050000229a: sleep,microseconds is 3655019,
D2024-04-23 19:10:07.493 84243) replica.default6.04010000000026db: sleep,microseconds is 799331,
D2024-04-23 19:10:07.542 84242) replica.default5.04010000000026d9: sleep,microseconds is 3255109,
D2024-04-23 19:10:07.895 84241) replica.default4.0401000300002787: sleep,microseconds is 2901920,
D2024-04-23 19:10:07.944 84237) replica.default0.0401000300002789: sleep,microseconds is 748869,
D2024-04-23 19:10:08.294 84243) replica.default6.040100030000278b: sleep,microseconds is 798618,
D2024-04-23 19:10:08.344 84240) replica.default3.0401000a00002992: sleep,microseconds is 2853312,
D2024-04-23 19:10:08.695 84237) replica.default0.0401000a00002994: sleep,microseconds is 797321,
D2024-04-23 19:10:08.743 84247) replica.default10.040100070000299d: sleep,microseconds is 3254699,
D2024-04-23 19:10:09.94 84243) replica.default6.0401000900002660: sleep,microseconds is 3702864,
D2024-04-23 19:10:09.142 84244) replica.default7.0401000900002662: sleep,microseconds is 1150250,
D2024-04-23 19:10:09.493 84237) replica.default0.0401000b0000229a: sleep,microseconds is 799877,
D2024-04-23 19:10:09.541 84246) replica.default9.0401000b0000229c: sleep,microseconds is 751294,
D2024-04-23 19:10:09.944 84238) replica.default1.0401000b0000229e: sleep,microseconds is 348727,

```

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM57otDy/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1969,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM59Szhj,incubator-pegasus,2102081635,1969,NA,ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,NA,2024-05-09T07:07:23Z,2024-05-09T07:07:23Z,"`max_copy_rate_megabytes_per_disk` causes a large number of threads in the `replica.default` thread pool to sleep.  The `nfs_service_impl::on_copy` working on `replica.default` can also cause this issue.

In this thread pool:

1. affecting the timely processing of remote_command. For example:

   ```
   >>> server_info -r
   COMMAND: server-info
   
   CALL [meta-server] [xxxx:45001] succeed: Pegasus Server 2.4.3-without-slog (84632317ee48b2c6c013f41d0e6f73ad4955b6bb) Release, Started at 2024-03-22 14:10:53
   CALL [meta-server] [xxxx:45001] succeed: Pegasus Server 2.4.3-without-slog (84632317ee48b2c6c013f41d0e6f73ad4955b6bb) Release, Started at 2024-03-22 14:11:01
   CALL [replica-server] [xxxx:45101] succeed: Pegasus Server 2.4.3-without-slog (84632317ee48b2c6c013f41d0e6f73ad4955b6bb) Release, Started at 2024-03-22 14:11:15
   CALL [replica-server] [xxxx:45101] failed: ERR_TIMEOUT
   CALL [replica-server] [xxxx:45101] succeed: Pegasus Server 2.4.3-without-slog (84632317ee48b2c6c013f41d0e6f73ad4955b6bb) Release, Started at 2024-04-15 17:50:28
   CALL [replica-server] [xxxx:45101] failed: ERR_TIMEOUT
   CALL [replica-server] [xxxx:45101] succeed: Pegasus Server 2.4.3-without-slog (84632317ee48b2c6c013f41d0e6f73ad4955b6bb) Release, Started at 2024-03-22 14:11:22
   
   Succeed count: 5
   Failed count: 2
   ```

2. Some RPCs are being delayed for transmission. For example:

   ```
   RPC_CM_CONFIG_SYNC
   RPC_CM_DUPLICATION_SYNC
   RPC_NFS_COPY
   ```

   May be related： https://github.com/apache/incubator-pegasus/issues/1840 

3. Indirectly makes the Group Check ERR_TIMEOUT in `replica.replica` and causes ballot increase.

```
[general]
app_name           : lpf_test
app_id             : 46      
partition_count    : 100     
max_replica_count  : 3       
[replicas]
pidx  ballot  replica_count  primary              secondaries                             
0     6       3/3            x.x.x.x:55101  [x.x.x.x:55101,x.x.x.x:55101]   
1     7       3/3            x.x.x.x:55101  [x.x.x.x:55101,x.x.x.x:55101] 
2     5       3/3            x.x.x.x:55101  [x.x.x.x:55101,x.x.x.x:55101] 
3     9       3/3            x.x.x.x:55101  [x.x.x.x:55101,x.x.x.x:55101] 
4     5       3/3            x.x.x.x:55101   [x.x.x.x:55101,x.x.x.x:55101] 
5     6       3/3            x.x.x.x:55101   [x.x.x.x:55101,x.x.x.x:55101] 
6     7       3/3            x.x.x.x:55101  [x.x.x.x:55101,x.x.x.x:55101] 
```

# Solution

1. If the `max_copy_rate_megabytes_per_disk` is not set too low and not frequently adjusted, it is unlikely to trigger the problem.

2. Or replace the default RPC thread pool for NFS-related operations in `nfs_server_impl.cpp`  can resolves this issue. For example:

```diff
-DEFINE_TASK_CODE_RPC(RPC_NFS_COPY, TASK_PRIORITY_COMMON, ::dsn::THREAD_POOL_DEFAULT)
+DEFINE_TASK_CODE_RPC(RPC_NFS_COPY, TASK_PRIORITY_COMMON, ::dsn::THREAD_POOL_LEARN)
 DEFINE_TASK_CODE_RPC(RPC_NFS_GET_FILE_SIZE, TASK_PRIORITY_COMMON, ::dsn::THREAD_POOL_DEFAULT)
 // test timer task code
 DEFINE_TASK_CODE(LPC_NFS_REQUEST_TIMER, TASK_PRIORITY_COMMON, ::dsn::THREAD_POOL_DEFAULT)
 
-DEFINE_TASK_CODE_AIO(LPC_NFS_READ, TASK_PRIORITY_COMMON, THREAD_POOL_DEFAULT)
+DEFINE_TASK_CODE_AIO(LPC_NFS_READ, TASK_PRIORITY_COMMON, ::dsn::THREAD_POOL_LEARN)
 
-DEFINE_TASK_CODE_AIO(LPC_NFS_WRITE, TASK_PRIORITY_COMMON, THREAD_POOL_DEFAULT)
+DEFINE_TASK_CODE_AIO(LPC_NFS_WRITE, TASK_PRIORITY_COMMON, ::dsn::THREAD_POOL_LEARN)
 
-DEFINE_TASK_CODE_AIO(LPC_NFS_COPY_FILE, TASK_PRIORITY_COMMON, THREAD_POOL_DEFAULT)
+DEFINE_TASK_CODE_AIO(LPC_NFS_COPY_FILE, TASK_PRIORITY_COMMON, ::dsn::THREAD_POOL_LEARN)

```

**Changing the thread pool solve the above problem, but it is unclear whether there are other risks.**","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM59Szhj/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1976,https://api.github.com/repos/apache/incubator-pegasus/issues/1976,incubator-pegasus,2245432911,1976,Feature: Supports node black list for load balancing.,lengyuexuexuan,46274877,,,OPEN,2024-04-16T08:19:34Z,2024-04-16T08:19:35Z,"## Feature Request

**Describe the feature you'd like:**

<!-- A clear and concise description of what you want to happen. -->
When load balancing, we can set up a blacklist of nodes such that there will be no sharding in and out on those nodes.

**Teachability, Documentation, Adoption, Migration Strategy:**
<!-- If you can, explain some scenarios how users might use this, situations it would be helpful in. Any API designs, mockups, or diagrams are also helpful. -->
During load balancing, we use replica counts as an evaluation criterion for slice migration. However, the size of each slice is uncertain. A node may have fewer slices but larger disk load. If we perform load balancing at this time, it will cause more load pressure on this type of nodes.
In addition, the machines in a cluster may be of different models and have different disk sizes.
So this feature can help us to do more flexible load balancing.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1976/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/1998,https://api.github.com/repos/apache/incubator-pegasus/issues/1998,incubator-pegasus,2270608828,1998,avoid to modify idl generate code in client,shalk,2435781,shalk(xiao kun),,OPEN,2024-04-30T06:27:42Z,2024-05-23T02:19:59Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

I found java client use 10 idl files, c++ use 12 idl files, go use 11 idl files.

```
➜  grep 'namespace java' idl/* | wc -l 
      10
➜  grep 'namespace go' idl/* | wc -l
      11
➜  grep 'namespace cpp' idl/* | wc -l
      12
```

Though java use 10 idl files, but java client only  generate 9 idl files code.
This is strange, and hard to maintain.

For java client,
I think either 
just  remove `namespace java` from dns.thrift , not use the dns.thrift to generate java code.
or 
use the generate code of dns.thrift, modify the java-thrift  lib.


1. What did you do?
If possible, provide a recipe for reproducing the error.

2. What did you expect to see?

3. What did you see instead?

4. What version of Pegasus are you using?
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/1998/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1998,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5-rX0A,incubator-pegasus,2125298944,1998,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-05-22T16:51:07Z,2024-05-22T16:51:07Z,"Maybe it's a mistake, I'll check it later!
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5-rX0A/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/1998,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5-uUHP,incubator-pegasus,2126070223,1998,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-05-23T02:19:58Z,2024-05-23T02:19:58Z,"@shalk 
1. java client use 10 idl files, c++ use 12 idl files, go use 11 idl files

It's because each language client has different function set. For example, the `admin-cli` is using go-client to support some CLI commands, but the Java client doesn't have such function. So you can see the go-client use an extra `idl/replica_admin.thrift`.

2. Though java use 10 idl files, but java client only generate 9 idl files code

Yes, the `dsn.thrift` is used for place holder, I'll try to clarify it.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5-uUHP/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2006,https://api.github.com/repos/apache/incubator-pegasus/issues/2006,incubator-pegasus,2287588490,2006,Bug(tcmalloc):ALL nodes coredump when doing Bulkload,ninsmiracle,110282526,,,OPEN,2024-05-09T12:27:00Z,2025-01-03T06:10:31Z,"## Bug Report

1. What did you do?
Doing bulkload (download sst file stage) with any action which need to restart ONE node,may cause ALL nodes coredump.
![image](https://github.com/apache/incubator-pegasus/assets/110282526/75792367-fe15-4407-9ab1-c2d4571aa1c0)

2. What did you see ?
There are three kind of coredump in different nodes
**Type one:**
```
(gdb) #0  0x00007fc9093669ef in signalHandler(int, siginfo*, void*) ()
   from /opt/soft/openjdk1.8.0/jre/lib/amd64/server/libjvm.so
#1  <signal handler called>
#2  0x00007fc9072ea793 in dsn::utils::filesystem::get_normalized_path (
    path=..., npath=...)
    at /home/work/temp/pegasus/src/rdsn/src/utils/filesystem.cpp:116
#3  0x00007fc9072ebb4d in dsn::utils::filesystem::path_combine (path1=...,
    path2=...)
    at /home/work/temp/pegasus/src/rdsn/src/utils/filesystem.cpp:618
#4  0x00007fc9085696e7 in dsn::replication::replica_bulk_loader::download_sst_file (this=0x5a947a40, remote_dir=..., local_dir=...,
    file_index=<optimized out>, fs=0x275f74d0)
    at /home/work/temp/pegasus/src/rdsn/src/replica/bulk_load/replica_bulk_loader.cpp:460
#5  0x00007fc9086b7811 in dsn::task::exec_internal (
    this=this@entry=0xa95362d0)
    at /home/work/temp/pegasus/src/rdsn/src/runtime/task/task.cpp:176
#6  0x00007fc9086ccec2 in dsn::task_worker::loop (this=0x24e31e0)
    at /home/work/temp/pegasus/src/rdsn/src/runtime/task/task_worker.cpp:224
#7  0x00007fc9086cd040 in dsn::task_worker::run_internal (this=0x24e31e0)
    at /home/work/temp/pegasus/src/rdsn/src/runtime/task/task_worker.cpp:204
#8  0x00007fc90734ba1f in execute_native_thread_routine ()
   from /home/work/app/pegasus/alsgsrv-monetization-master/replica/package/bin/libdsn_utils.so
#9  0x00007fc905156dc5 in start_thread () from /lib64/libpthread.so.0
#10 0x00007fc90365573d in clone () from /lib64/libc.so.6
(gdb) quit
```

**Type two:**
```
#0  0x00007f8d106371d7 in raise () from /lib64/libc.so.6
Missing separate debuginfos, use: debuginfo-install bzip2-libs-1.0.6-13.el7.x86_64 elfutils-libelf-0.166-2.el7.x86_64 elfutils-libs-0.166-2.el7.x86_64 glibc-2.17-157.el7_3.1.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.14.1-27.el7_3.x86_64 libattr-2.4.46-12.el7.x86_64 libcap-2.22-8.el7.x86_64 libcom_err-1.42.9-9.el7.x86_64 libgcc-4.8.5-28.el7_5.1.x86_64 libselinux-2.5-6.el7.x86_64 pcre-8.32-15.el7_2.1.x86_64 systemd-libs-219-30.el7_3.8.x86_64 xz-libs-5.2.2-1.el7.x86_64 zlib-1.2.7-17.el7.x86_64
(gdb) #0  0x00007f8d106371d7 in raise () from /lib64/libc.so.6
#1  0x00007f8d106388c8 in abort () from /lib64/libc.so.6
#2  0x00007f8d12435dbb in tcmalloc::Log (mode=mode@entry=tcmalloc::kCrash,
    filename=filename@entry=0x7f8d1244c14e ""src/tcmalloc.cc"",
    line=line@entry=332, a=..., b=..., c=..., d=...)
    at src/internal_logging.cc:118
#3  0x00007f8d124281d9 in (anonymous namespace)::InvalidFree (
    ptr=<optimized out>) at src/tcmalloc.cc:332
#4  0x00007f8d16adaef3 in _dl_update_slotinfo ()
   from /lib64/ld-linux-x86-64.so.2
#5  0x00007f8d16aca136 in update_get_addr () from /lib64/ld-linux-x86-64.so.2
#6  0x00007f8d1438e71c in dsn::utils::filesystem::get_normalized_path (
    path=..., npath=...)
    at /home/work/temp/pegasus/src/rdsn/src/utils/filesystem.cpp:116
#7  0x00007f8d1438fb4d in dsn::utils::filesystem::path_combine (path1=...,
    path2=...)
    at /home/work/temp/pegasus/src/rdsn/src/utils/filesystem.cpp:618
#8  0x00007f8d1560d6e7 in dsn::replication::replica_bulk_loader::download_sst_file (this=0x35ba0c0, remote_dir=..., local_dir=...,
    file_index=<optimized out>, fs=0x8090b1500)
    at /home/work/temp/pegasus/src/rdsn/src/replica/bulk_load/replica_bulk_loader.cpp:460
#9  0x00007f8d1575b811 in dsn::task::exec_internal (
    this=this@entry=0x8d3964f00)
    at /home/work/temp/pegasus/src/rdsn/src/runtime/task/task.cpp:176
#10 0x00007f8d15770ec2 in dsn::task_worker::loop (this=0x1f32fd0)
    at /home/work/temp/pegasus/src/rdsn/src/runtime/task/task_worker.cpp:224
#11 0x00007f8d15771040 in dsn::task_worker::run_internal (this=0x1f32fd0)
    at /home/work/temp/pegasus/src/rdsn/src/runtime/task/task_worker.cpp:204
#12 0x00007f8d143efa1f in execute_native_thread_routine ()
   from /home/work/app/pegasus/alsgsrv-monetization-master/replica/package/bin/libdsn_utils.so
#13 0x00007f8d121fadc5 in start_thread () from /lib64/libpthread.so.0
#14 0x00007f8d106f973d in clone () from /lib64/libc.so.6
(gdb) quit
```


**Type three:**
```
Program terminated with signal 6, Aborted.
#0  0x00007fad388fa1d7 in raise () from /lib64/libc.so.6
Missing separate debuginfos, use: debuginfo-install bzip2-libs-1.0.6-13.el7.x86_64 elfutils-libelf-0.166-2.el7.x86_64 elfutils-libs-0.166-2.el7.x86_64 glibc-2.17-157.el7_3.1.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.14.1-27.el7_3.x86_64 libattr-2.4.46-12.el7.x86_64 libcap-2.22-8.el7.x86_64 libcom_err-1.42.9-9.el7.x86_64 libgcc-4.8.5-28.el7_5.1.x86_64 libselinux-2.5-6.el7.x86_64 pcre-8.32-15.el7_2.1.x86_64 systemd-libs-219-30.el7_3.8.x86_64 xz-libs-5.2.2-1.el7.x86_64 zlib-1.2.7-17.el7.x86_64
(gdb) #0  0x00007fad388fa1d7 in raise () from /lib64/libc.so.6
#1  0x00007fad388fb8c8 in abort () from /lib64/libc.so.6
#2  0x00007fad3925ca8d in __gnu_cxx::__verbose_terminate_handler() ()
   from /home/work/app/pegasus/alsgsrv-monetization-master/replica/package/bin/libstdc++.so.6
#3  0x00007fad3925abe6 in ?? ()
   from /home/work/app/pegasus/alsgsrv-monetization-master/replica/package/bin/libstdc++.so.6
#4  0x00007fad3925ac13 in std::terminate() ()
   from /home/work/app/pegasus/alsgsrv-monetization-master/replica/package/bin/libstdc++.so.6
#5  0x00007fad3c6b2a4e in execute_native_thread_routine ()
   from /home/work/app/pegasus/alsgsrv-monetization-master/replica/package/bin/libdsn_utils.so
#6  0x00007fad3a4bddc5 in start_thread () from /lib64/libpthread.so.0
#7  0x00007fad389bc73d in clone () from /lib64/libc.so.6
(gdb) quit
```

3. What version of Pegasus are you using?
v2.4
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2006/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2006,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5-xYZY,incubator-pegasus,2126874200,2006,NA,ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,NA,2024-05-23T11:27:30Z,2024-05-23T11:27:30Z,"The aforementioned phenomenon is one of issues triggered by bulkload download.

# Phenomenon

1. Doing bulkload (download sst file stage) with any action which need to **restart ONE node**,may cause ALL nodes coredump.
2. **bulkload file missing** also cause many nodes coredump
3. Phenomenon 1 and 2 both cause tcmalloc report `large alloc 2560917504`

# Reason

After execute the `clear_bulk_load_states` function, `download_sst_file` tasks still remain, which causes above phenomenon.

## Case 1 With Phenomenon 1

**operation**：we restart one node.

ballot increase，function `clear_bulk_load_states_if_needed()` clear  88.5 replica  **_metadata.files** at 15:17:56.753

```
D2024-05-20 15:17:56.753 (1716189476753079718 146668) replica.replica13.0404000d0000005d: replica_config.cpp:819:update_local_configuration(): 88.5@10.142.98.52:27101: update ballot to init file from 3 to 4 OK 
D2024-05-20 15:17:56.753 (1716189476753147052 146668) replica.replica13.0404000d0000005d:clear_bulk_load_states_if_needed(): [88.5@10.142.98.52:27101] prepare to clear bulk load states, current status = replication::bulk_load_status::BLS_DOWNLOADING
D2024-05-20 15:17:56.753 (1716189476753464144 146668) replica.replica13.0404000d0000005d: replica_config.cpp:1045:update_local_configuration(): 88.5@10.142.98.52:27101: status change replication::partition_status::PS_INACTIVE @ 3 => replication::partition_status::PS_PRIMARY @ 4, pre(1, 0), app(0, 0), duration = 3 ms, replica_configuration(pid=88.5, ballot=4, primary=10.142.98.52:27101, status=3, learner_signature=0, pop_all=0, split_sync_to_child=0)
```

**But at 15:17:56.873, the 88.5 replicais still downloading sst file, cause core. **

D2024-05-20 15:17:56.873 (1716189476873362400 146626) replica.default7.04010007000000ca: block_service_manager.cpp:181:download_file(): download file(/home/work/ssd2/pegasus/c3tst-performance2/replica/reps/88.5.pegasus/bulk_load/33.sst) succeed, file_size = 65930882, md5 = 7a4d3da9250f52b4e31095c1d7042c2f D2024-05-20 15:17:58.348 (1716189478348326864 146626) replica.default7.04010007000000ca: replica_bulk_loader.cpp:479:**download_sst_file**(): [88.5@10.142.98.52:27101] download_sst_file remote_dir /user/s_pegasus/lpfsplit/c3tst-performance2/ingest_p32_10G/5 ,local_dir /home/work/ssd2/pegasus/c3tst-performance2/replica/reps/88.5.pegasus/bulk_load,f_meta.name 33.sst

## Case 2 With Phenomenon 2

**operation**：app ingest_p4_10G partition 1，bulkload file missing 88.sst,89.sst,90.sst,93.sst

```
[general]
app_name           : ingest_p4_10G
app_id             : 95           
partition_count    : 4            
max_replica_count  : 3            

[replicas]
pidx  ballot  replica_count  primary                              secondaries                                                              
0     8       3/3            c3-hadoop-pegasus-tst-st01.bj:27101  [c3-hadoop-pegasus-tst-st03.bj:27101,c3-hadoop-pegasus-tst-st05.bj:27101]  
1     7       3/3            c3-hadoop-pegasus-tst-st03.bj:27101  [c3-hadoop-pegasus-tst-st01.bj:27101,c3-hadoop-pegasus-tst-st02.bj:27101]  
2     8       3/3            c3-hadoop-pegasus-tst-st04.bj:27101  [c3-hadoop-pegasus-tst-st03.bj:27101,c3-hadoop-pegasus-tst-st02.bj:27101]  
3     3       3/3            c3-hadoop-pegasus-tst-st01.bj:27101  [c3-hadoop-pegasus-tst-st03.bj:27101,c3-hadoop-pegasus-tst-st04.bj:27101]  
```

### primary replica

primary replica failed to download file(88.sst) ，and stop downloading all sst file.

```
log.1.txt:E2024-05-22 14:28:11.231 (1716359291231595252 102084) replica.default1.040100090000072b: replica_bulk_loader.cpp:520:download_sst_file(): [95.1@10.142.102.47:27101] failed to download file(88.sst), error = ERR_CORRUPTION
```

But meta says continue downloading.

```
D2024-05-22 14:28:18.983 (1716359298983653491 102121) replica.replica2.04008ebc00010f3f: replica_bulk_loader.cpp:71:on_bulk_load(): [95.1@10.142.102.47:27101] receive bulk load request, remote provider = hdfs_zjy, remote_root_path = /user/s_pegasus/lpfsplit, cluster_name = c3tst-performance2, app_name = ingest_p4_10G, meta_bulk_load_status = replication::bulk_load_status::BLS_DOWNLOADING, local bulk_load_status = replication::bulk_load_status::BLS_DOWNLOADING
```

primary replica reports download progress to meta.

```
D2024-05-22 14:28:18.983 (1716359298983689828 102121) replica.replica2.04008ebc00010f3f: replica_bulk_loader.cpp:879:report_group_download_progress(): [95.1@10.142.102.47:27101] primary = 10.142.102.47:27101, download progress = 89%, status = ERR_CORRUPTION
D2024-05-22 14:28:18.983 (1716359298983703147 102121) replica.replica2.04008ebc00010f3f: replica_bulk_loader.cpp:892:report_group_download_progress(): [95.1@10.142.102.47:27101] secondary = 10.142.98.52:27101, download progress = 88%, status=ERR_OK
D2024-05-22 14:28:18.983 (1716359298983714700 102121) replica.replica2.04008ebc00010f3f: replica_bulk_loader.cpp:892:report_group_download_progress(): [95.1@10.142.102.47:27101] secondary = 10.142.97.9:27101, download progress = 88%, status=ERR_OK
```

meta says stop downloading，and **clear _metadata.files**. However, all download tasks were not terminated successfully.

```
D2024-05-22 14:28:28.988 (1716359308988487559 102121) replica.replica2.04008ebc00010f46: replica_bulk_loader.cpp:71:on_bulk_load(): [95.1@10.142.102.47:27101] receive bulk load request, remote provider = hdfs_zjy, remote_root_path = /user/s_pegasus/lpfsplit, cluster_name = c3tst-performance2, app_name = ingest_p4_10G, meta_bulk_load_status = replication::bulk_load_status::BLS_FAILED, local bulk_load_status = replication::bulk_load_status::BLS_DOWNLOADING
```

At 14:28:29, download_sst_file task still exists, access _metadata.files, causing core.

```
D2024-05-22 14:28:29.529 (1716359309529341231 102089) replica.default6.04010000000007b5: replica_bulk_loader.cpp:479:download_sst_file(): [95.0@10.142.102.47:27101] download_sst_file remote_dir /user/s_pegasus/lpfsplit/c3tst-performance2/ingest_p4_10G/0 ,local_dir /home/work/ssd1/pegasus/c3tst-performance2/replica/reps/95.0.pegasus/bulk_load,f_meta.name 92.sst
F2024-05-22 14:28:29.536 (1716359309536349665 102089) replica.default6.04010000000007b5: filesystem.cpp:111:get_normalized_path(): assertion expression: len <= 4086
```

### secondary replica

Secondary receives primary replica message to cancel the bulkload task and clear _metadata.files, but does not terminate all download tasks, cause core dump.

Other replicas generate core dumps due to this reason, cause many replica server core dump.

```
D2024-05-22 14:28:28.992 (1716359308992917139 159129) replica.replica2.04006d6800010139: replica_bulk_loader.cpp:183:on_group_bulk_load(): [95.1@10.142.98.52:27101] receive group_bulk_load request, primary address = 10.142.102.47:27101, ballot = 7, **meta bulk_load_status = replication::bulk_load_status::BLS_FAILED, local bulk_load_status = replication::bulk_load_status::BLS_DOWNLOADING**
```

```
D2024-05-22 14:28:30.384 (1716359310384983585 159094) replica.default5.040100080000056a: replica_bulk_loader.cpp:479:download_sst_file(): [95.0@10.142.98.52:27101] download_sst_file remote_dir /user/s_pegasus/lpfsplit/c3tst-performance2/ingest_p4_10G/0 ,local_dir /home/work/ssd2/pegasus/c3tst-performance2/replica/reps/95.0.pegasus/bulk_load,f_meta.name 20
F2024-05-22 14:28:30.452 (1716359310452229248 159094) replica.default5.040100080000056a: filesystem.cpp:111:get_normalized_path(): assertion expression: len <= 4086
```



## tcmalloc report large alloc

_metadata.files is cleared, cause f_meta.name length in download_sst_file function is very long. 

```
const file_meta &f_meta = _metadata.files[file_index];
const std::string &file_name = utils::filesystem::path_combine(local_dir, f_meta.name);
```

```
log.1.txt:F2024-05-20 17:22:49.621 (1716196969621641630 170503) replica.default11.0401000b000000de: filesystem.cpp:111:get_normalized_path(): lpf path chao chu 4096, get_normalized_path LEN 410828079
log.2.txt:F2024-05-20 17:23:38.595 (1716197018595730772 192879) replica.default10.040100040000002e: filesystem.cpp:111:get_normalized_path(): lpf path chao chu 4096, get_normalized_path LEN 532715376
log.1.txt:F2024-05-20 17:22:50.77 (1716196970077285996 164438) replica.default11.0401000b000000c6: filesystem.cpp:111:get_normalized_path(): lpf path chao chu 4096, get_normalized_path LEN 383022703
```

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5-xYZY/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2007,https://api.github.com/repos/apache/incubator-pegasus/issues/2007,incubator-pegasus,2288800278,2007,Question(FQDN): how to connect to Pegasus cluster with FQDN pegasus version,ninsmiracle,110282526,,,CLOSED,2024-05-10T02:34:42Z,2024-07-29T12:08:12Z,"## General Question

When I deploy master branch of pegasus to real cluster, I could not connect to peagsus via peagsus_shell.

1. Firstly , I change all the IP to hostname in pegasus config
2. Then I deloy it to machines
3. I connected to peagsus cluster via admlin-cli,such as use this command `./admin-cli -n aaa:25101,bbb:25101`,but return `fatal: failed to list nodes [context deadline exceeded]`
4. I connected to pegasus cluster via pegasus-shell. It works. However,when I type `nodes -d` ,cluster crash.

stdout(error log) in meta server:
```
I2024-05-08 14:13:57.603 (1715148837603905326 81668) : pegasus server starting, pid(81668), version($Version: Pegasus Server 2.6.0-SNAPSHOT (aea1cfe632d455fcddfe4c92ebbd9d4e89037abb) Release, built by gcc 7.3.1, built on 12180ab51819, built at May  7 2024 12:14:31 $)
F2024-05-08 14:15:26.215 (1715148926215608204 81749)   meta.THREAD_POOL_META_SERVER3.02003f3d00010001: 

rpc_host_port.cpp:62:from_address(): assertion expression: [utils::hostname_from_ip(__bswap_32 (addr.ip()), &hp._host)] invalid host_port 172.17.0.1
```
**172.17.0.1 is my pegasus-shell IP , which is in a docker.**  It looks like peagsus can not resolve this IP correctly, it's a bug?

I also fonud these coredump in replica servers.
```
Program terminated with signal SIGABRT, Aborted.
#0  0x00007ffaedff01d7 in raise () from /lib64/libc.so.6
(gdb) bt
#0  0x00007ffaedff01d7 in raise () from /lib64/libc.so.6
#1  0x00007ffaedff18c8 in abort () from /lib64/libc.so.6
#2  0x00007ffaf240ca1e in dsn_coredump () at /home/guoningshen/code/incubator-pegasus/src/runtime/service_api_c.cpp:130
#3  0x00007ffaef3e8134 in process_fatal_log (log_level=<optimized out>) at /home/guoningshen/code/incubator-pegasus/src/utils/simple_logger.cpp:117
#4  dsn::tools::simple_logger::log (this=0x1a38200, file=<optimized out>, function=<optimized out>, line=<optimized out>, log_level=<optimized out>, str=<optimized out>)
    at /home/guoningshen/code/incubator-pegasus/src/utils/simple_logger.cpp:284
#5  0x00007ffaf21ec19b in dsn::replication::replica_stub::open_replica (this=0x1851800, app=..., id=..., group_check=..., configuration_update=...)
    at /home/guoningshen/code/incubator-pegasus/src/replica/replica_stub.cpp:1817
#6  0x00007ffaf2447be1 in dsn::task::exec_internal (this=0x1f50b40) at /home/guoningshen/code/incubator-pegasus/src/runtime/task/task.cpp:173
#7  0x00007ffaf245f257 in dsn::task_worker::loop (this=0x1717290) at /home/guoningshen/code/incubator-pegasus/src/runtime/task/task_worker.cpp:245
#8  0x00007ffaf245fdc0 in dsn::task_worker::run_internal (this=0x1717290) at /home/guoningshen/code/incubator-pegasus/src/runtime/task/task_worker.cpp:225
#9  0x00007ffaf0ed9a3f in execute_native_thread_routine () from /home/work/app/pegasus/c3tst-performance1/replica/package/bin/librocksdb.so.8
#10 0x00007ffaef66edc5 in start_thread () from /lib64/libpthread.so.0
#11 0x00007ffaee0b273d in clone () from /lib64/libc.so.6
(gdb)
```

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2007/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2007,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM59ZL2p,incubator-pegasus,2103754153,2007,NA,ninsmiracle,110282526,,,NA,2024-05-10T02:36:41Z,2024-05-10T02:36:41Z,"So I want to know what should I do , to deloy a peagsus cluster with FQDN now , and how to use tools control this cluster. Thanks a lot. @acelyc111 ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM59ZL2p/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2007,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM59f7so,incubator-pegasus,2105522984,2007,NA,ninsmiracle,110282526,,,NA,2024-05-11T03:48:54Z,2024-05-11T03:48:54Z,"Let me add more details：
1. deploy clusters,it works. Every nodes running...
2. useing peagsus-shell to connected to cluster
![image](https://github.com/apache/incubator-pegasus/assets/110282526/f72de404-5e30-4e33-88b8-c6b173c4b5aa)

3. send any RPC  command , like `nodes -dr` or `ls -d`. TIME_OUT
![image](https://github.com/apache/incubator-pegasus/assets/110282526/0bd9373d-1541-43b3-ac68-cb6975dba1e9)

4.A lot of core in meta-server
![image](https://github.com/apache/incubator-pegasus/assets/110282526/07c1c1f8-7d11-4b13-a8c9-21b2a882a733)
 
Core like `core.meta.THREAD_PO...`
```
Using host libthread_db library ""/lib64/libthread_db.so.1"".
Core was generated by `/home/work/app/pegasus/c3tst-performance1/meta/package/bin/pegasus_server confi'.
Program terminated with signal SIGABRT, Aborted.
#0  0x00007f3c0c8bc1d7 in raise () from /lib64/libc.so.6
(gdb) bt
#0  0x00007f3c0c8bc1d7 in raise () from /lib64/libc.so.6
#1  0x00007f3c0c8bd8c8 in abort () from /lib64/libc.so.6
#2  0x00007f3c10cd8a1e in dsn_coredump () at /home/guoningshen/code/incubator-pegasus/src/runtime/service_api_c.cpp:130
#3  0x00007f3c0dcb4134 in process_fatal_log (log_level=<optimized out>) at /home/guoningshen/code/incubator-pegasus/src/utils/simple_logger.cpp:117
#4  dsn::tools::simple_logger::log (this=0x2e3a200, file=<optimized out>, function=<optimized out>, line=<optimized out>, log_level=<optimized out>, str=<optimized out>)
    at /home/guoningshen/code/incubator-pegasus/src/utils/simple_logger.cpp:284
#5  0x00007f3c10d09ff3 in dsn::host_port::from_address (addr=...) at /home/guoningshen/code/incubator-pegasus/src/runtime/rpc/rpc_host_port.cpp:60
#6  0x00007f3c10d0f0c5 in dsn::message_ex::create_response (this=this@entry=0x327be00) at /home/guoningshen/code/incubator-pegasus/src/runtime/rpc/rpc_message.cpp:358
#7  0x00007f3c10d0638d in dsn::rpc_engine::forward (this=this@entry=0x2c4f180, request=request@entry=0x327be00, address=...) at /home/guoningshen/code/incubator-pegasus/src/runtime/rpc/rpc_engine.cpp:853
#8  0x00007f3c10cd90a3 in dsn_rpc_forward (request=0x327be00, addr=...) at /home/guoningshen/code/incubator-pegasus/src/runtime/service_api_c.cpp:207
#9  0x00007f3c0ffc6196 in forward (addr=..., this=0x7f3bee4e5f20) at /home/guoningshen/code/incubator-pegasus/src/runtime/rpc/rpc_holder.h:224
#10 dsn::replication::meta_service::check_leader<dsn::rpc_holder<dsn::replication::configuration_list_apps_request, dsn::replication::configuration_list_apps_response> > (this=this@entry=0x32ee000, 
    rpc=..., forward_address=<optimized out>) at /home/guoningshen/code/incubator-pegasus/src/meta/meta_service.h:406
#11 0x00007f3c0ffc629a in dsn::replication::meta_service::check_leader_status<dsn::rpc_holder<dsn::replication::configuration_list_apps_request, dsn::replication::configuration_list_apps_response> > (
    this=this@entry=0x32ee000, rpc=..., forward_address=forward_address@entry=0x0) at /home/guoningshen/code/incubator-pegasus/src/meta/meta_service.h:420
#12 0x00007f3c0ff9ef6a in dsn::replication::meta_service::on_list_apps (this=0x32ee000, rpc=...) at /home/guoningshen/code/incubator-pegasus/src/meta/meta_service.cpp:671
#13 0x00007f3c0fff8653 in operator() (request=<optimized out>, __closure=<optimized out>) at /home/guoningshen/code/incubator-pegasus/src/runtime/serverlet.h:201
#14 std::_Function_handler<void (dsn::message_ex*), bool dsn::serverlet<dsn::replication::meta_service>::register_rpc_handler_with_rpc_holder<dsn::rpc_holder<dsn::replication::configuration_list_apps_request, dsn::replication::configuration_list_apps_response> >(dsn::task_code, char const*, void (dsn::replication::meta_service::*)(dsn::rpc_holder<dsn::replication::configuration_list_apps_request, dsn::replication::configuration_list_apps_response>))::{lambda(dsn::message_ex*)#1}>::_M_invoke(std::_Any_data const&, dsn::message_ex*&&) (__functor=..., __args#0=<optimized out>)
    at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/std_function.h:316
#15 0x00007f3c10d123b2 in operator() (__args#0=<optimized out>, this=0x2b310d0) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/std_function.h:706
#16 dsn::rpc_request_task::exec (this=0x2b31000) at /home/guoningshen/code/incubator-pegasus/src/runtime/task/task.h:436
#17 0x00007f3c10d13be1 in dsn::task::exec_internal (this=0x2b31000) at /home/guoningshen/code/incubator-pegasus/src/runtime/task/task.cpp:173
#18 0x00007f3c10d2b257 in dsn::task_worker::loop (this=0x2b19290) at /home/guoningshen/code/incubator-pegasus/src/runtime/task/task_worker.cpp:245
#19 0x00007f3c10d2bdc0 in dsn::task_worker::run_internal (this=0x2b19290) at /home/guoningshen/code/incubator-pegasus/src/runtime/task/task_worker.cpp:225
#20 0x00007f3c0f7a5a3f in execute_native_thread_routine () from /home/work/app/pegasus/c3tst-performance1/meta/package/bin/librocksdb.so.8
#21 0x00007f3c0df3adc5 in start_thread () from /lib64/libpthread.so.0
#22 0x00007f3c0c97e73d in clone () from /lib64/libc.so.6
(gdb) 
```

Core like `core.pegasus_server....`
```
#0  0x0000000000000000 in ?? ()
#1  0x00007f693f83b6c0 in (anonymous namespace)::stacktrace_generic_fp::capture<false, false> (result=result@entry=0xaee010, max_depth=31, skip_count=1, initial_frame=initial_frame@entry=0x7ffd328eae80, 
    initial_pc=initial_pc@entry=0x0, sizes=0x0) at src/stacktrace_generic_fp-inl.h:175
#2  0x00007f693f83b74a in GetStackTrace_generic_fp (result=0xaee010, max_depth=<optimized out>, skip_count=<optimized out>) at src/stacktrace_generic_fp-inl.h:332
#3  0x00007f693f83ba52 in GetStackTrace (result=result@entry=0xaee010, max_depth=max_depth@entry=30, skip_count=skip_count@entry=0) at src/stacktrace.cc:346
#4  0x00007f693f82c37e in tcmalloc::PageHeap::HandleUnlock (this=0x7f693fa56720 <tcmalloc::Static::pageheap_>, context=0x7ffd328eaf10) at src/page_heap.cc:155
#5  0x00007f693f82e07a in ~LockingContext (this=0x7ffd328eaf10, __in_chrg=<optimized out>) at src/page_heap.cc:77
#6  tcmalloc::PageHeap::NewWithSizeClass (this=this@entry=0x7f693fa56720 <tcmalloc::Static::pageheap_>, n=n@entry=1, sizeclass=26) at src/page_heap.cc:161
#7  0x00007f693f82beb7 in tcmalloc::CentralFreeList::Populate (this=this@entry=0x7f693fbe1420 <tcmalloc::Static::central_cache_+31616>) at src/central_freelist.cc:314
#8  0x00007f693f82c088 in tcmalloc::CentralFreeList::FetchFromOneSpansSafe (this=0x7f693fbe1420 <tcmalloc::Static::central_cache_+31616>, N=1, start=0x7ffd328eb020, end=0x7ffd328eb028)
    at src/central_freelist.cc:273
#9  0x00007f693f82c120 in tcmalloc::CentralFreeList::RemoveRange (this=0x7f693fbe1420 <tcmalloc::Static::central_cache_+31616>, start=start@entry=0x7ffd328eb020, end=end@entry=0x7ffd328eb028, N=1)
    at src/central_freelist.cc:253
#10 0x00007f693f82fca3 in tcmalloc::ThreadCache::FetchFromCentralCache (this=this@entry=0xb0e000, cl=cl@entry=26, byte_size=byte_size@entry=576, 
    oom_handler=oom_handler@entry=0x7f693f81d240 <(anonymous namespace)::nop_oom_handler(size_t)>) at src/thread_cache.cc:125
#11 0x00007f693f83f15d in Allocate (oom_handler=0x7f693f81d240 <(anonymous namespace)::nop_oom_handler(size_t)>, cl=26, size=576, this=<optimized out>) at src/thread_cache.h:381
#12 do_malloc (size=568) at src/tcmalloc.cc:1414
#13 do_allocate_full<tcmalloc::malloc_oom> (size=568) at src/tcmalloc.cc:1804
#14 tcmalloc::allocate_full_malloc_oom (size=568) at src/tcmalloc.cc:1820
#15 0x00007f693dfa754d in __fopen_internal () from /lib64/libc.so.6
#16 0x00007f693ca60a16 in selinuxfs_exists () from /lib64/libselinux.so.1
#17 0x00007f693ca58ce8 in init_lib () from /lib64/libselinux.so.1
#18 0x00007f6943dfd1e3 in _dl_init_internal () from /lib64/ld-linux-x86-64.so.2
#19 0x00007f6943def21a in _dl_start_user () from /lib64/ld-linux-x86-64.so.2
#20 0x0000000000000004 in ?? ()
#21 0x00007ffd328ed220 in ?? ()
#22 0x00007ffd328ed26a in ?? ()
#23 0x00007ffd328ed275 in ?? ()
#24 0x00007ffd328ed27f in ?? ()
#25 0x0000000000000000 in ?? ()
(gdb) 
```


6. stdout(error log) in meta-server
```
W2024-05-11 10:33:36.503 (1715394816503732375 36348) : overwrite default thread pool for task RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX from THREAD_POOL_META_SERVER to THREAD_POOL_DEFAULT
W2024-05-11 10:33:36.503 (1715394816503775340 36348) : overwrite default thread pool for task RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX_ACK from THREAD_POOL_META_SERVER to THREAD_POOL_DEFAULT
I2024-05-11 10:33:36.503 (1715394816503863057 36348) : pegasus server starting, pid(36348), version($Version: Pegasus Server 2.6.0-SNAPSHOT (aea1cfe632d455fcddfe4c92ebbd9d4e89037abb) Release, built by gcc 7.3.1, built on 12180ab51819, built at May  7 2024 12:14:31 $)
F2024-05-11 10:36:03.558 (1715394963558260142 36428)   meta.THREAD_POOL_META_SERVER2.02008e370001000c: rpc_host_port.cpp:62:from_address(): assertion expression: [utils::hostname_from_ip(__bswap_32 (addr.ip()), &hp._host)] invalid host_port 172.17.0.1
```

7.By the way , all the replica-server running during that time
![image](https://github.com/apache/incubator-pegasus/assets/110282526/34f7a93b-8302-4137-a0a3-ef9c7b56e63e)


8.And I can not connect to cluster via `admin-cli`
![image](https://github.com/apache/incubator-pegasus/assets/110282526/b5ac7374-b5e5-41c5-9a92-3532b2a4a74a)



","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM59f7so/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2007,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM59gZLN,incubator-pegasus,2105643725,2007,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-05-11T08:58:21Z,2024-05-11T08:58:21Z,"Hi, @ninsmiracle !

Is the Pegasus cluster deployed as a onebox in the docker container? Do the Pegasus shell tool and admin-cli run in the same docker container?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM59gZLN/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2007,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM59k49e,incubator-pegasus,2106822494,2007,NA,ninsmiracle,110282526,,,NA,2024-05-13T07:15:14Z,2024-05-13T07:15:14Z,"> Hi, @ninsmiracle !
> 
> Is the Pegasus cluster deployed as a onebox in the docker container? Do the Pegasus shell tool and admin-cli run in the same docker container?

When I deloyed as a onebox in my Docker container , cluster run as normal. However, if I **deploy it on real node**, cluster running but can not accept any RPC.  
I think the key point is `meta.THREAD_POOL_META_SERVER2.02008e370001000c: rpc_host_port.cpp:62:from_address(): assertion expression: [utils::hostname_from_ip(__bswap_32 (addr.ip()), &hp._host)] invalid host_port 172.17.0.1`.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM59k49e/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2007,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5-rYue,incubator-pegasus,2125302686,2007,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-05-22T16:52:52Z,2024-05-22T16:52:52Z,"> I connected to peagsus cluster via admlin-cli,such as use this command ./admin-cli -n aaa:25101,bbb:25101,but return fatal: failed to list nodes [context deadline exceeded]

It's because after the main FQDN patch has been merged, a new Thrift structure (i.e. host_port)  has been introduced, but the admin-cli side dosen't know this type. You can check it in the admin-cli's shell.log, the error looks like:
```
time=""2024-05-23T00:30:55+08:00"" level=info msg=""failed to read response from [127.0.0.1:34601(meta)]: *admin.ListNodesResponse error reading struct: *admin.NodeInfo error reading struct: Unknown data type 57""
```

The resolution is to update the admin-cli dependent go-client. However, we have to resolve https://github.com/apache/incubator-pegasus/pull/1917 at first.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5-rYue/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2007,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6BCTGv,incubator-pegasus,2164863407,2007,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-06-13T07:41:12Z,2024-06-13T07:41:12Z,"> > Hi, @ninsmiracle !
> > Is the Pegasus cluster deployed as a onebox in the docker container? Do the Pegasus shell tool and admin-cli run in the same docker container?
> 
> When I deloyed as a onebox in my Docker container , cluster run as normal. However, if I **deploy it on real node**, cluster running but can not accept any RPC. I think the key point is `meta.THREAD_POOL_META_SERVER2.02008e370001000c: rpc_host_port.cpp:62:from_address(): assertion expression: [utils::hostname_from_ip(__bswap_32 (addr.ip()), &hp._host)] invalid host_port 172.17.0.1`.

@ninsmiracle You can check if this patch could solve the issue: https://github.com/apache/incubator-pegasus/pull/2044","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6BCTGv/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2007,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6GPne-,incubator-pegasus,2252240830,2007,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-07-26T08:31:27Z,2024-07-26T08:31:27Z,"@ninsmiracle If it has been resolved, I'll close the issue.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6GPne-/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2014,https://api.github.com/repos/apache/incubator-pegasus/issues/2014,incubator-pegasus,2309436408,2014,Bug(duplication):some nodes coredump after start duplication for a long time,ninsmiracle,110282526,,,OPEN,2024-05-22T02:22:25Z,2024-05-22T02:22:25Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
-Deploy duplication matser and back-up cluster.
-Begin duplicate.
-Run about 2~3 days.
-Some nodes coredump

2. What did you expect to see?
Node run as normal.

3. What did you see instead?
memory monitoring table.
![image](https://github.com/apache/incubator-pegasus/assets/110282526/16c9406a-a96a-4d01-b980-a01a3ba0a166)

coredump detail:
```
Using host libthread_db library ""/lib64/libthread_db.so.1"".
Core was generated by `/home/work/app/pegasus/c3srv-browser/replica/package/bin/pegasus_server config.'.
Program terminated with signal SIGABRT, Aborted.
#0  0x00007f01575401d7 in raise () from /lib64/libc.so.6
(gdb) bt
#0  0x00007f01575401d7 in raise () from /lib64/libc.so.6
#1  0x00007f01575418c8 in abort () from /lib64/libc.so.6
#2  0x00007f015c628f9e in dsn_coredump () at /home/work/temp/format_pegasus/pegasus/src/rdsn/src/runtime/service_api_c.cpp:93

#3  0x00007f015c422c83 in dsn::replication::log_file::log_file (this=0x73aa4a630, path=0x740561c98 ""/home/work/ssd2/pegasus/c3srv-browser/replica/reps/72.173.pegasus/plog/log.92534.3105061495163"", 
    handle=<optimized out>, index=<optimized out>, start_offset=3105061495163, is_read=<optimized out>) at /home/work/temp/format_pegasus/pegasus/src/rdsn/src/replica/log_file.cpp:166

#4  0x00007f015c4247ce in dsn::replication::log_file::open_read (path=0x740561c98 ""/home/work/ssd2/pegasus/c3srv-browser/replica/reps/72.173.pegasus/plog/log.92534.3105061495163"", err=...)
    at /home/work/temp/format_pegasus/pegasus/src/rdsn/src/replica/log_file.cpp:92
#5  0x00007f015c43ccfa in dsn::replication::log_utils::open_read (path=..., file=...) at /home/work/temp/format_pegasus/pegasus/src/rdsn/src/replica/mutation_log_utils.cpp:43
#6  0x00007f015c4ff7fa in dsn::replication::load_from_private_log::find_log_file_to_start (this=this@entry=0x384c74640)
    at /home/work/temp/format_pegasus/pegasus/src/rdsn/src/replica/duplication/load_from_private_log.cpp:123
#7  0x00007f015c500360 in dsn::replication::load_from_private_log::run (this=0x384c74640) at /home/work/temp/format_pegasus/pegasus/src/rdsn/src/replica/duplication/load_from_private_log.cpp:100
#8  0x00007f015c665f91 in dsn::task::exec_internal (this=this@entry=0x2b9bce1e0) at /home/work/temp/format_pegasus/pegasus/src/rdsn/src/runtime/task/task.cpp:176
#9  0x00007f015c67b642 in dsn::task_worker::loop (this=0x2a67c30) at /home/work/temp/format_pegasus/pegasus/src/rdsn/src/runtime/task/task_worker.cpp:224
#10 0x00007f015c67b7c0 in dsn::task_worker::run_internal (this=0x2a67c30) at /home/work/temp/format_pegasus/pegasus/src/rdsn/src/runtime/task/task_worker.cpp:204
#11 0x00007f015b2f8a3f in execute_native_thread_routine () from /home/work/app/pegasus/c3srv-browser/replica/package/bin/libdsn_utils.so
#12 0x00007f0159103dc5 in start_thread () from /lib64/libpthread.so.0
#13 0x00007f015760273d in clone () from /lib64/libc.so.6
(gdb)
```

stdout file (error log):
```
E2024-05-15 05:48:17.721 (1715723297721553663 62544) replica.rep_long9.040400031452989e: native_linux_aio_provider.cpp:49:open(): create file failed, err = No such file or directory

E2024-05-15 05:48:17.721 (1715723297721596680 62544) replica.rep_long9.040400031452989e: load_from_private_log.cpp:125:find_log_file_to_start(): [72.171@10.142.162.23:34801] ERR_FILE_OPERATION_FAILED: failed to open the log file (/home/work/ssd7/pegasus/c3srv-xxxxxx/replica/reps/72.171.pegasus/plog/log.91190.3060048707709)

F2024-05-15 06:03:20.656 (1715724200656901498 62545) replica.rep_long10.04040005181bcdaf: log_file.cpp:166:log_file(): assertion expression: false
F2024-05-15 06:03:20.656 (1715724200656954168 62545) replica.rep_long10.04040005181bcdaf: log_file.cpp:166:log_file(): fail to get file size of /home/work/ssd2/pegasus/c3srv-xxxxx/replica/reps/72.173.pegasus/plog/log.92534.3105061495163
```

4. What version of Pegasus are you using?
peagsus v2.4



","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2014/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2015,https://api.github.com/repos/apache/incubator-pegasus/issues/2015,incubator-pegasus,2309451857,2015,Bug(duplication):some nodes never start GC plog after computer room failure,ninsmiracle,110282526,,,OPEN,2024-05-22T02:39:49Z,2024-07-24T08:28:29Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
The computer room which service for our duplication master cluster meet an accidents. And most of the node in this room shutdown in a short time.
When all the nodes alive , we found some partition of the duplication table never GC private log (plog) again.

2. What did you expect to see?
All the partition can GC it's plog correctly.

4. What did you see instead?
stdout (error log):
```
// stdout
90146:E2024-05-14 15:59:52.512 (1715673592512665104 67086) replica.default8.040005fe0319646c: nfs_server_impl.cpp:221:on_get_file_size(): {nfs_service} get stat of file /home/work/ssd2/pegasus/alsgsrv-monetization-master/replica/reps/8.53.pegasus/plog/log.18129.608864535790 failed, err = No such file or directory
```
We can see this replica request a old plog. 
![image](https://github.com/apache/incubator-pegasus/assets/110282526/bc7ac83b-c6ea-4a3a-9fd2-d79a95d7b8f2)

Because the partition can not clear plog as nomarl,so the disk always fully. We need to clear the plog sometimes.


5. What version of Pegasus are you using?
Pegasus v2.4","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2015/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2015,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6F7zMU,incubator-pegasus,2247045908,2015,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-07-24T06:54:34Z,2024-07-24T06:54:34Z,Is the file /home/work/ssd2/pegasus/alsgsrv-monetization-master/replica/reps/8.53.pegasus/plog/log.18129.608864535790 actually exists or not?,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6F7zMU/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2015,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6F8cYN,incubator-pegasus,2247214605,2015,NA,ninsmiracle,110282526,,,NA,2024-07-24T08:28:27Z,2024-07-24T08:28:27Z,"> Is the file /home/work/ssd2/pegasus/alsgsrv-monetization-master/replica/reps/8.53.pegasus/plog/log.18129.608864535790 actually exists or not?

When coredump happened, the file actually not exists.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6F8cYN/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2025,https://api.github.com/repos/apache/incubator-pegasus/issues/2025,incubator-pegasus,2315125180,2025,Duplication get stuck at the status DS_APP while duplicating with checkpoints,empiredan,743379,Dan Wang,,CLOSED,2024-05-24T11:07:59Z,2024-05-28T03:31:21Z,"Firstly create a table named `test_dup1` and write 2 records into it at the source cluster of duplication:
```
>>> use test_dup1
OK
>>> full_scan
partition: all
hash_key_filter_type: no_filter
sort_key_filter_type: no_filter
value_filter_type: no_filter
max_count: -1
timout_ms: 5000
detailed: false
no_value: false

""abc"" : ""def"" => ""ghi""
""1"" : ""2"" => ""3""

2 key-value pairs got.
```

Then, still at the source cluster of duplication, add a new duplication with checkpoints and specified remote table name and replica count:
```
>>> add_dup test_dup1 target_cluster -s -a new1_test -r 3 
trying to add duplication [app_name: test_dup1, remote_cluster_name: target_cluster, is_duplicating_checkpoint: true, remote_app_name: new1_test, remote_replica_count: 3]
adding duplication succeed [app_name: test_dup1, remote_cluster_name: target_cluster, appid: 5, dupid: 1716542517, checkpoint: true, remote_app_name: new1_test, remote_replica_count: 3]
```

After a long time, this duplication is still found at the status `DS_APP` (see https://pegasus.apache.org/administration/duplication for details):
```
>>> query_dup test_dup1 -d
duplications of app [test_dup1] in detail:
{""1"":{""create_ts"":""2024-05-24 17:21:57"",""dupid"":1716542517,""fail_mode"":""FAIL_SLOW"",""remote"":""target_cluster"",""remote_app_name"":""new1_test"",""remote_replica_count"":3,""status"":""DS_APP""},""appid"":5}
```

The meta server of the source cluster has the following error log:
```
E2024-05-24 17:38:41.263 (1716543521263163609 1dc5)   meta.meta_state0.0103000000000122: meta_duplication_service.cpp:579:operator()(): query follower app[target_cluster.test_dup1] replica configuration completed, result: duplication_status = DS_APP, query_err = ERR_OBJECT_NOT_FOUND, update_err = ERR_NO_NEED_OPERATE
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2025/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2025,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5_NqLv,incubator-pegasus,2134287087,2025,NA,empiredan,743379,Dan Wang,,NA,2024-05-28T03:31:20Z,2024-05-28T03:31:20Z,This issue has been fixed by https://github.com/apache/incubator-pegasus/pull/2026.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5_NqLv/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2028,https://api.github.com/repos/apache/incubator-pegasus/issues/2028,incubator-pegasus,2315521123,2028,Parameters failed to be extracted while parsing add_dup command,empiredan,743379,Dan Wang,,CLOSED,2024-05-24T14:15:32Z,2024-06-03T08:20:23Z,"While executing add_dup with remote_app_name and remote_replica_count specified, it seems that both failed to be recognized:
```
>>> add_dup test_dup1 target_cluster -s -a test_new1 -r 3 
adding duplication succeed [app: test_dup1, remote: target_cluster, appid: 5, dupid: 1716556955], checkpoint: true, remote_app_name: test_dup1]
```

The logging of meta server is as follows:
```
meta.log.20240523_190656_153:D2024-05-24 15:09:38.402 (1716534578402782584 25ad)   meta.meta_state0.01020003000004bc: meta_duplication_service.cpp:179:add_duplication(): add duplication for app(test_dup1), remote cluster name is target_cluster, remote app name is test_dup1, remote replica count is 0
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2028/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2028,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5_NqMq,incubator-pegasus,2134287146,2028,NA,empiredan,743379,Dan Wang,,NA,2024-05-28T03:31:26Z,2024-05-28T03:31:26Z,This issue has been fixed by https://github.com/apache/incubator-pegasus/pull/2027.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM5_NqMq/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2039,https://api.github.com/repos/apache/incubator-pegasus/issues/2039,incubator-pegasus,2330336852,2039,There's no warning message while trying to add a duplication that has been existing for the same table with the same remote cluster ,empiredan,743379,Dan Wang,,CLOSED,2024-06-03T07:01:53Z,2024-06-25T08:35:32Z,"Firstly, add a new duplication to table `test2` with remote cluster `target_cluster` and remote table `test_dup2`:

```
>>> add_dup test2 target_cluster -a test_dup2
trying to add duplication [app_name: test2, remote_cluster_name: target_cluster, is_duplicating_checkpoint: false, remote_app_name: test_dup2, remote_replica_count: 0]
adding duplication succeed [app_name: test2, remote_cluster_name: target_cluster, appid: 6, dupid: 1717138357, is_duplicating_checkpoint: false, remote_app_name: test_dup2, remote_replica_count: 1]

>>> query_dup test2 -d
duplications of app [test2] in detail:
{""1"":{""create_ts"":""2024-05-31 14:52:37"",""dupid"":1717138357,""fail_mode"":""FAIL_SLOW"",""remote"":""target_cluster"",""remote_app_name"":""test_dup2"",""remote_replica_count"":1,""status"":""DS_LOG""},""appid"":6}
```

Then, try to add another duplication to table `test2` with remote cluster `target_cluster` and remote table `test_dup3`:

```
>>> add_dup test2 target_cluster -a test_dup3
trying to add duplication [app_name: test2, remote_cluster_name: target_cluster, is_duplicating_checkpoint: false, remote_app_name: test_dup3, remote_replica_count: 0]
adding duplication succeed [app_name: test2, remote_cluster_name: target_cluster, appid: 6, dupid: 1717138357, is_duplicating_checkpoint: false, remote_app_name: test_dup2, remote_replica_count: 1]
>>> query_dup test2 -d
duplications of app [test2] in detail:
{""1"":{""create_ts"":""2024-05-31 14:52:37"",""dupid"":1717138357,""fail_mode"":""FAIL_SLOW"",""remote"":""target_cluster"",""remote_app_name"":""test_dup2"",""remote_replica_count"":1,""status"":""DS_LOG""},""appid"":6}
```

Actually there's no duplication added, since the response message has given the remote table name is `test_dup2` rather than `test_dup3`:

```
adding duplication succeed [app_name: test2, remote_cluster_name: target_cluster, appid: 6, dupid: 1717138357, is_duplicating_checkpoint: false, remote_app_name: test_dup2, remote_replica_count: 1]
```

However, it just showed `adding duplication succeed` and there's not any warning message.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2039/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2039,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Cbvej,incubator-pegasus,2188310435,2039,NA,empiredan,743379,Dan Wang,,NA,2024-06-25T08:35:31Z,2024-06-25T08:35:31Z,This issue has been fixed by https://github.com/apache/incubator-pegasus/pull/2038.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Cbvej/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2041,https://api.github.com/repos/apache/incubator-pegasus/issues/2041,incubator-pegasus,2334844579,2041,Question(New Feature): Does Pegasus need `sort key` sampling function?,ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,CLOSED,2024-06-05T03:58:01Z,2024-07-02T09:29:32Z,"# background
1. Users use multiple clients to access the cluster. It is not easy to collect hot content. （Advertising business scene）
2. Obtain hot content can help users optimize reading and writing.

# idea
Implement a function to sample the read content over a period of time. [In get mult-get operation]

example：
```c++
// ""sample_key"",
        // ""start or stop sample sort keys on a replica of a replica server"",
        // ""<-a|--app_id num> ""
        // ""<-p|--partition_index_list num> ""
        // ""<-c|--sample_action start|stop|query> ""
        // ""<-t|--duration_time_minute num>""
        // ""<-s|--sampling_times num>""
        // ""<-l--node_list>""
        // ""<-r|--reslove_ip>""
``` ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2041/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2042,https://api.github.com/repos/apache/incubator-pegasus/issues/2042,incubator-pegasus,2335058355,2042,Feature(encryption): shell command local_get and sst_dump should support encryption data.,Samunroyu,36890229,,yujingweiop@gmail.com,OPEN,2024-06-05T06:55:22Z,2024-06-05T06:58:16Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
When pegasus enable encryption, the local_get and sst_dump commands should still work.And dont just simply return CORRPUT FILE like below picture.

local_get command
![截图](https://github.com/apache/incubator-pegasus/assets/36890229/9e043c9f-ab04-4e3f-bc57-6010463764a0)

sst_dump command
![截图__](https://github.com/apache/incubator-pegasus/assets/36890229/0780ffb0-87a6-451e-b939-f612d33685f1)

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2042/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2043,https://api.github.com/repos/apache/incubator-pegasus/issues/2043,incubator-pegasus,2335765442,2043,Bug(task_tracker): cancel_outstanding_tasks can't cancel all tracked tasks.,Sunflower876,162662221,,,OPEN,2024-06-05T12:24:21Z,2024-06-12T06:04:40Z,"## Bug Report
The following is the implement of task_tracker::cancel_outstanding_tasks. 

In the for loop, if i iterates to 2, take out the task of the second bucket queue head, which corresponds to task2 of the _outstanding_tasks[2]. next, and find that the status of task2 is running, and then execute task2->cancel (true). We will synchronize and wait for task2 to finish executing. The subsequent execution of task2 will generate a new task1, which corresponds to the same tracker as task2, but the bucket corresponding to task1 is 1. Because the for loop has already been executed to i=2, we will not go back to check for i=1, resulting in task1 tracked by the same tracker still being executed even after canel_standing_tasks have been executed.


![image](https://github.com/apache/incubator-pegasus/assets/162662221/59daf20f-cbac-48d8-bf7f-94d724e41e18)


","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2043/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2047,https://api.github.com/repos/apache/incubator-pegasus/issues/2047,incubator-pegasus,2365854333,2047,"Failed while checking markdown links due to dead link ""open-falcon.org""",empiredan,743379,Dan Wang,,CLOSED,2024-06-21T06:54:47Z,2024-06-21T07:26:14Z,"Errors occurred while doing ""Check Markdown links"":

```
ERROR: 1 dead links found!
[✖] http://open-falcon.org/ → Status: 0
(node:107) [DEP0040] DeprecationWarning: The `punycode` module is deprecated. Please use a userland alternative instead.
(Use `node --trace-deprecation ...` to show where the warning was created)

......

ERROR: 1 dead links found!
[✖] https://open-falcon.org/ → Status: 0
(node:195) [DEP0040] DeprecationWarning: The `punycode` module is deprecated. Please use a userland alternative instead.
(Use `node --trace-deprecation ...` to show where the warning was created)
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2047/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2047,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6CETbX,incubator-pegasus,2182166231,2047,NA,empiredan,743379,Dan Wang,,NA,2024-06-21T07:26:14Z,2024-06-21T07:26:14Z,This issue has been fixed by https://github.com/apache/incubator-pegasus/pull/2046.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6CETbX/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2050,https://api.github.com/repos/apache/incubator-pegasus/issues/2050,incubator-pegasus,2372174683,2050,Last mutation is always delayed several minutes to be shipped to the remote cluster during duplication,empiredan,743379,Dan Wang,,CLOSED,2024-06-25T09:29:20Z,2024-07-10T10:13:08Z,"While testing duplication, there's always the `last` mutation. The `last` mutation means that, after this mutation, there's not any mutation written into the source cluster.

The problem is that the `last` mutation would not be duplicated to the remote cluster; instead, it always takes 2 ~ 3 minutes to duplicate the `last` mutation to the remote cluster. To be exact, the process of duplicating itself is fast; we have to waits 2 ~ 3 minutes (until some empty write gets in) before it is duplicated to the remote cluster. ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2050/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2050,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6EVABQ,incubator-pegasus,2220097616,2050,NA,empiredan,743379,Dan Wang,,NA,2024-07-10T10:13:08Z,2024-07-10T10:13:08Z,This issue has been fixed by https://github.com/apache/incubator-pegasus/pull/2048.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6EVABQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2061,https://api.github.com/repos/apache/incubator-pegasus/issues/2061,incubator-pegasus,2390490259,2061,The memory usage of block cache is incorrect,empiredan,743379,Dan Wang,,CLOSED,2024-07-04T10:12:05Z,2024-07-09T11:36:06Z,"The block cache has always been set to 24GB, and the metric value observed in monitoring system is also 24GB. However, after Pegasus is upgraded from 2.0 to 2.4, the metric value becomes incorrect.

As is shown in the following graph, the left side is the usage of the block cache before the upgrade, which is correct value; the right side is the usage after the upgrade, which is always 0 and incorrect:
![image](https://github.com/apache/incubator-pegasus/assets/743379/5ad8caa3-086b-435f-b3e3-c94b42e6f744)

The following graph is the usage of physical memory with the time corresponding to the above graph, which is correct:
![image](https://github.com/apache/incubator-pegasus/assets/743379/1309cf2d-17af-4aa6-bd1a-2e6c52840aa1)

It's obvious that the replica server has been started and the block cache usage should has become 24GB, which is still
actually 0.

Then, restart the replica servers and their usage of block cache become non-zero values; however, After a short while
, the growth stopped and  it stayed at a fixed value forever which is incorrect:
![image](https://github.com/apache/incubator-pegasus/assets/743379/ec7badfe-858c-4d75-9325-24de95265565)

Similarly, the following graph is the usage of physical memory with the time corresponding to the above graph, which is also correct:
![image](https://github.com/apache/incubator-pegasus/assets/743379/b63e30c8-5dd7-4049-86db-df04120bae03)

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2061/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,incubator-pegasus,2397583446,2067,"Getting build error , Host dl.bintray.com:443 was resolved.",SHUFIL,22607542,Shufil Khan,,OPEN,2024-07-09T08:58:34Z,2024-07-17T02:33:00Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
Installing Apache-pegasus in ubuntu 22, using   Compile method , [doc](https://pegasus.apache.org/docs/build/compile-from-source/). Also I tried to install using docker method as well  , but got error ERROR: `build rdsn failed`
```
error: downloading 'https://dl.bintray.com/boostorg/release/1.69.0/source/boost_1_69_0.tar.bz2' failed
          status_code: 22
          status_string: ""HTTP response code said error""
```

I have installed below dependence's 
`gcc version 11.4.0 (Ubuntu 11.4.0-1ubuntu1~22.04)` , `cmake version 3.30.0`

also installed all packages  in mentioned  Dockerfile  [Dockerfile](https://github.com/apache/incubator-pegasus/blob/master/docker/pegasus-build-env/ubuntu2204/Dockerfile)

2. What did you expect to see?
Both method I am getting build issue so need to resolve Build issue 
3. What did you see instead?
Getting below error while doing build Compile method 

```
CMake Error at /home/ubuntu/pegasus/rdsn/thirdparty/build/Stamp/boost/download-boost.cmake:163 (message):
  Each download failed!

    error: downloading 'http://pegasus-thirdparties.oss-cn-beijing.aliyuncs.com/boost_1_69_0.tar.bz2' failed
          status_code: 22
          status_string: ""HTTP response code said error""
          log:
          --- LOG BEGIN ---
          timeout on name lookup is not supported

  Host pegasus-thirdparties.oss-cn-beijing.aliyuncs.com:80 was resolved.

  IPv6: (none)

  IPv4: 59.110.190.37

    Trying 59.110.190.37:80...

  Connected to pegasus-thirdparties.oss-cn-beijing.aliyuncs.com
  (59.110.190.37) port 80

  GET /boost_1_69_0.tar.bz2 HTTP/1.1

  Host: pegasus-thirdparties.oss-cn-beijing.aliyuncs.com

  User-Agent: curl/8.8.0

  Accept: */*



  Request completely sent off

  HTTP/1.1 403 Forbidden

  Server: AliyunOSS

  Date: Tue, 09 Jul 2024 07:48:13 GMT

  Content-Type: application/xml

  Content-Length: 344

  Connection: keep-alive

  x-oss-request-id: 668CEB3D35EB2634329C18EE

  x-oss-ec: 0003-00000905



  The requested URL returned error: 403

  Closing connection



          --- LOG END ---
          error: downloading 'https://dl.bintray.com/boostorg/release/1.69.0/source/boost_1_69_0.tar.bz2' failed
          status_code: 22
          status_string: ""HTTP response code said error""
          log:
          --- LOG BEGIN ---
          timeout on name lookup is not supported

  Host dl.bintray.com:443 was resolved.

  IPv6: (none)

  IPv4: 18.214.194.113, 18.232.172.199, 3.95.117.170

    Trying 18.214.194.113:443...

  Connected to dl.bintray.com (18.214.194.113) port 443

  ALPN: curl offers h2,http/1.1

  [5 bytes data]

  TLSv1.3 (OUT), TLS handshake, Client hello (1):

  [512 bytes data]

  [5 bytes data]

  TLSv1.3 (IN), TLS handshake, Server hello (2):

  [112 bytes data]

  [5 bytes data]

  TLSv1.2 (IN), TLS handshake, Certificate (11):

  [3676 bytes data]

  [5 bytes data]

  TLSv1.2 (IN), TLS handshake, Server key exchange (12):

  [300 bytes data]

  [5 bytes data]

  TLSv1.2 (IN), TLS handshake, Server finished (14):

  [4 bytes data]

  [5 bytes data]

  TLSv1.2 (OUT), TLS handshake, Client key exchange (16):

  [37 bytes data]

  [5 bytes data]

  TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):

  [1 bytes data]

  [5 bytes data]

  TLSv1.2 (OUT), TLS handshake, Finished (20):

  [16 bytes data]

  [5 bytes data]

  [5 bytes data]

  TLSv1.2 (IN), TLS handshake, Finished (20):

  [16 bytes data]

  SSL connection using TLSv1.2 / ECDHE-RSA-AES256-GCM-SHA384 / X25519 /
  RSASSA-PSS

  ALPN: server accepted http/1.1

  Server certificate:

   subject: CN=*.bintray.com
   start date: Oct 26 00:00:00 2023 GMT
   expire date: Nov 25 23:59:59 2024 GMT
   subjectAltName: host ""dl.bintray.com"" matched cert's ""*.bintray.com""
   issuer: C=US; O=DigiCert Inc; OU=www.digicert.com; CN=GeoTrust TLS RSA CA G1
   SSL certificate verify result: self-signed certificate in certificate chain (19), continuing anyway.
    Certificate level 0: Public key type RSA (2048/112 Bits/secBits), signed using sha256WithRSAEncryption
    Certificate level 1: Public key type RSA (2048/112 Bits/secBits), signed using sha256WithRSAEncryption
    Certificate level 2: Public key type RSA (2048/112 Bits/secBits), signed using sha256WithRSAEncryption

  using HTTP/1.x

  [5 bytes data]

  GET /boostorg/release/1.69.0/source/boost_1_69_0.tar.bz2 HTTP/1.1

  Host: dl.bintray.com

  User-Agent: curl/8.8.0

  Accept: */*



  Request completely sent off

  [5 bytes data]

  HTTP/1.1 404 Not Found

  Date: Tue, 09 Jul 2024 07:48:13 GMT

  Content-Type: text/html

  Content-Length: 146

  Connection: keep-alive

  Strict-Transport-Security: max-age=31536000; includeSubDomains



  The requested URL returned error: 404

  Closing connection

  [5 bytes data]

  TLSv1.2 (OUT), TLS alert, close notify (256):

  [2 bytes data]



          --- LOG END ---




make[2]: *** [CMakeFiles/boost.dir/build.make:99: Stamp/boost/boost-download] Error 1
make[1]: *** [CMakeFiles/Makefile2:119: CMakeFiles/boost.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
-- extracting...
     src='/home/ubuntu/pegasus/rdsn/thirdparty/build/Download/concurrentqueue/concurrentqueue-1.0.1.tar.gz'
     dst='/home/ubuntu/pegasus/rdsn/thirdparty/build/Source/concurrentqueue'
-- extracting... [tar xfz]
-- extracting... [analysis]
-- extracting... [rename]
-- extracting... [clean up]
-- extracting... done
[  3%] No update step for 'concurrentqueue'
[  4%] No patch step for 'concurrentqueue'
[  5%] No configure step for 'concurrentqueue'
[  5%] No build step for 'concurrentqueue'
[  5%] Performing install step for 'concurrentqueue'
[  6%] Completed 'concurrentqueue'
[  6%] Built target concurrentqueue
make: *** [Makefile:91: all] Error 2
ERROR: build rdsn failed
```

4. What version of Pegasus are you using?
apache-pegasus-2.1.0","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2067/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6EKxHY,incubator-pegasus,2217415128,2067,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-07-09T11:35:18Z,2024-07-09T11:35:18Z,"Hi, @SHUFIL 

Thanks for the reply!

```
error: downloading 'https://dl.bintray.com/boostorg/release/1.69.0/source/boost_1_69_0.tar.bz2' failed
          status_code: 22
          status_string: ""HTTP response code said error""
```
or
```
error: downloading 'http://pegasus-thirdparties.oss-cn-beijing.aliyuncs.com/boost_1_69_0.tar.bz2' failed
          status_code: 22
          status_string: ""HTTP response code said error""
```
Some thirdparty links are broken in old versions. You can
Update `OSS_URL_PREFIX` value in `rdsn/thirdparty/CMakeLists.txt` from `http://pegasus-thirdparties.oss-cn-beijing.aliyuncs.com` to `http://pegasus-thirdparty-package.oss-cn-beijing.aliyuncs.com` if you insist on Pegasus 2.1.

Or 

Try to use new Pegasus versions (say https://github.com/apache/incubator-pegasus/releases/tag/v2.5.0) instead.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6EKxHY/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6ELITP,incubator-pegasus,2217510095,2067,NA,SHUFIL,22607542,Shufil Khan,,NA,2024-07-09T12:21:34Z,2024-07-09T12:21:34Z,"Hey @acelyc111  , Thanks for reply.

I tried 2.5.0 both way compile and docker , unfortunately I got below error , This is for compiled way.

This time I did not get any reason , just though out a error , that is 
```
-- Installing: /home/ubuntu/apache-pegasus-2.5.0/thirdparty/output/include/thrift/config.h
[ 29%] Completed 'thrift'
[ 29%] Built target thrift
make: *** [Makefile:91: all] Error 2
```

```
-- Installing: /root/pegasus/thirdparty/output/bin/arc
-- Set non-toolchain portion of runtime path of ""/root/pegasus/thirdparty/output/bin/arc"" to """"
[ 94%] Completed 'poco'
[ 94%] Built target poco
make: *** [Makefile:91: all] Error 2
```

This is docker method ,  can you please check that error . Thanks 

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6ELITP/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6ETMZE,incubator-pegasus,2219624004,2067,NA,SHUFIL,22607542,Shufil Khan,,NA,2024-07-10T05:57:58Z,2024-07-10T05:57:58Z,@acelyc111 @ke4qqq @gmcdonald ,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6ETMZE/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6EUhAr,incubator-pegasus,2219970603,2067,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-07-10T09:10:04Z,2024-07-10T09:10:04Z,"> Hey @acelyc111 , Thanks for reply.
> 
> I tried 2.5.0 both way compile and docker , unfortunately I got below error , This is for compiled way.
> 
> This time I did not get any reason , just though out a error , that is
> 
> ```
> -- Installing: /home/ubuntu/apache-pegasus-2.5.0/thirdparty/output/include/thrift/config.h
> [ 29%] Completed 'thrift'
> [ 29%] Built target thrift
> make: *** [Makefile:91: all] Error 2
> ```
> 
> ```
> -- Installing: /root/pegasus/thirdparty/output/bin/arc
> -- Set non-toolchain portion of runtime path of ""/root/pegasus/thirdparty/output/bin/arc"" to """"
> [ 94%] Completed 'poco'
> [ 94%] Built target poco
> make: *** [Makefile:91: all] Error 2
> ```
> 
> This is docker method , can you please check that error . Thanks

@SHUFIL  Did you see any other error messages?

I try to reproduce it according to the doc https://github.com/apache/incubator-pegasus-website/blob/master/_docs/en/2.5.0/compile-by-docker.md, but everything goes well. My steps are:
1. git clone git@github.com:apache/incubator-pegasus.git pegasus_2.5
2. cd pegasus_2.5 && git checkout v2.5
3. pwd
4. docker run -v /home/laiyingchun/dev/pegasus_2.5:/root/pegasus apache/pegasus:build-env-ubuntu2004-v2.5 /bin/bash -c ""cd /root/pegasus; ./run.sh build --test -c --clear_thirdparty -j 32""
5. Built OK as:
```
...
-- Installing: /root/pegasus/build/release/output/lib/libdsn.replication.tool.a
-- Installing: /root/pegasus/build/release/output/lib/libdsn_utils.so
-- Set runtime path of ""/root/pegasus/build/release/output/lib/libdsn_utils.so"" to """"
-- Installing: /root/pegasus/build/release/output/bin/long_adder_bench/long_adder_bench
-- Set runtime path of ""/root/pegasus/build/release/output/bin/long_adder_bench/long_adder_bench"" to """"
-- Installing: /root/pegasus/build/release/output/bin/nth_element_bench/nth_element_bench
-- Set runtime path of ""/root/pegasus/build/release/output/bin/nth_element_bench/nth_element_bench"" to """"
-- Installing: /root/pegasus/build/release/output/lib/libdsn.replication.zookeeper_provider.a
Build finish time: Wed Jul 10 16:28:26 CST 2024
Build elapsed time: 16m 58s

# laiyingchun @ localhost in ~/dev/pegasus_2.5 on git:v2.5 o [16:28:38]
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6EUhAr/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6EU79B,incubator-pegasus,2220080961,2067,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-07-10T10:04:48Z,2024-07-10T10:04:48Z,"I also checked the `apache/pegasus:build-env-ubuntu2204-v2.5` image, it's OK too.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6EU79B/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6EWMdC,incubator-pegasus,2220410690,2067,NA,SHUFIL,22607542,Shufil Khan,,NA,2024-07-10T12:40:29Z,2024-07-10T12:40:29Z,"@acelyc111 
I dont know why it is getting me , is it is any dependency issue ? I have tried the same version docker image but getting below error .
I have used below commands for run docker , 

`sudo docker run -v /home/ubuntu/apache-pegasus-2.5.0:/root/pegasus            apache/pegasus:build-env-ubuntu2004-master            /bin/bash -c ""cd /root/pegasus; ./run.sh build -c --clear_thirdparty -j $(nproc)""`

Error 
```
[ 85%] Building CXX object src/replica/CMakeFiles/dsn_replica_server.dir/duplication/replica_duplicator_manager.cpp.o
make[2]: *** [src/replica/CMakeFiles/dsn_replica_server.dir/build.make:384: src/replica/CMakeFiles/dsn_replica_server.dir/replica_stub.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
make[1]: *** [CMakeFiles/Makefile2:1967: src/replica/CMakeFiles/dsn_replica_server.dir/all] Error 2
make: *** [Makefile:136: all] Error 2
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6EWMdC/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6EYEfV,incubator-pegasus,2220902357,2067,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-07-10T15:56:06Z,2024-07-10T15:56:06Z,"@SHUFIL I guess some important error message is missed, could you please try:
1. Use the corresponding docker image, that is to say, use `apache/pegasus:build-env-ubuntu2204-v2.5` not `apache/pegasus:build-env-ubuntu2004-master`, because some software packages are different in different version.
2. Set the build parallelism to `1` instaed of `$(nproc)`.

So run as:
```
sudo docker run -v /home/ubuntu/apache-pegasus-2.5.0:/root/pegasus apache/pegasus:build-env-ubuntu2004-v2.5 /bin/bash -c ""cd /root/pegasus; ./run.sh build -c --clear_thirdparty -j 1""
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6EYEfV/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Edy6p,incubator-pegasus,2222403241,2067,NA,SHUFIL,22607542,Shufil Khan,,NA,2024-07-11T09:02:08Z,2024-07-11T09:02:08Z,"This is worked for me , and build finished without error. Thanks","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Edy6p/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Ed6Jp,incubator-pegasus,2222432873,2067,NA,SHUFIL,22607542,Shufil Khan,,NA,2024-07-11T09:16:57Z,2024-07-11T09:16:57Z,"Next I need to `run test` command, for this I  run below command , 
```
docker run -v /home/ubuntu/pegasus:/root/pegasus \
           apache/pegasus:build-env-ubuntu2004-v2.5 \
           /bin/bash -c ""cd /root/pegasus; ./run.sh test""
```

But I got  below error ,

```
Cluster becomes healthy.
sed: can't read /root/pegasus/build/latest/src/server/test/config.ini: No such file or directory
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Ed6Jp/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Ek_Ql,incubator-pegasus,2224288805,2067,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-07-12T02:01:39Z,2024-07-12T02:01:39Z,"> Next I need to `run test` command, for this I run below command ,
> 
> ```
> docker run -v /home/ubuntu/pegasus:/root/pegasus \
>            apache/pegasus:build-env-ubuntu2004-v2.5 \
>            /bin/bash -c ""cd /root/pegasus; ./run.sh test""
> ```
> 
> But I got below error ,
> 
> ```
> Cluster becomes healthy.
> sed: can't read /root/pegasus/build/latest/src/server/test/config.ini: No such file or directory
> ```

I see, it's needed to add `--test` when build the project by `./run.sh build ...` if you want to run tests.

Please try this at first:
```
sudo docker run -v /home/ubuntu/apache-pegasus-2.5.0:/root/pegasus apache/pegasus:build-env-ubuntu2004-v2.5 /bin/bash -c ""cd /root/pegasus; ./run.sh build -c --clear_thirdparty -j 1 --test""
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Ek_Ql/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6ElAFn,incubator-pegasus,2224292199,2067,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-07-12T02:04:36Z,2024-07-12T02:04:36Z,"Please keep in mind that some tests maybe flaky, don't be worry about that 😄 .","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6ElAFn/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6EqkXA,incubator-pegasus,2225751488,2067,NA,SHUFIL,22607542,Shufil Khan,,NA,2024-07-12T14:46:48Z,2024-07-12T14:46:48Z,"Ok, that is fine, but I did not get any error this time, I have completed pack_server , pack_client (C/C++) ,and  pack_tools,

As per this  [doc ](https://pegasus.apache.org/docs/build/compile-by-docker/)  it is explain for  pack_client C/C++ development  , I need to run for Golang client , for this I need to run client separately or here I can do that ? how it is  ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6EqkXA/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6EuUUP,incubator-pegasus,2226734351,2067,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-07-13T03:00:44Z,2024-07-13T03:00:44Z,"Congrats!

For Golang client, please see docs here: https://github.com/apache/incubator-pegasus/tree/master/go-client","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6EuUUP/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6E6KED,incubator-pegasus,2229838083,2067,NA,SHUFIL,22607542,Shufil Khan,,NA,2024-07-16T01:51:32Z,2024-07-16T01:51:32Z,"I have clone again for go-client, but it is already there in main folder , anyway I tried in both folder with `make build` command , but I getting always below error .
For resolving this issue I have done below steps 
added this values 

```
ulimit -c unlimited
export GOBACTRACE=crash
thrift --version
Thrift version 0.16.0
 go version
go version go1.22.0 linux/386
```
Error 
```
go mod tidy
go mod verify
all modules verified
go build -o ./bin/echo ./rpc/main/echo.go
# runtime/cgo
In file included from _cgo_export.c:3:
/usr/include/stdlib.h:26:10: fatal error: bits/libc-header-start.h: No such file or directory
   26 | #include <bits/libc-header-start.h>
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
make: *** [Makefile:34: build] Error 1
```

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6E6KED/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6E6jj7,incubator-pegasus,2229942523,2067,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-07-16T03:32:48Z,2024-07-16T03:32:48Z,"@SHUFIL According to the [go-client CI job](https://github.com/apache/incubator-pegasus/blob/v2.5/.github/workflows/lint_and_test_go-client.yml#L40), it works well on certain thrift and golang versions.
Could you try thrift 0.13.0 and golang-1.14?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6E6jj7/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6E7vD1,incubator-pegasus,2230251765,2067,NA,SHUFIL,22607542,Shufil Khan,,NA,2024-07-16T07:51:59Z,2024-07-16T07:51:59Z,"ok, I think this will check later , because I getting thrift-0.13.0 build error 
```
./configure: line 19838: syntax error near unexpected token `QT5,'
./configure: line 19838: `    PKG_CHECK_MODULES(QT5, Qt5Core >= 5.0, Qt5Network >= 5.0,'
```

I am trying to start pegasus service after build, but below error blocking me, Can you please.

./run.sh start_onebox
ERROR: file /home/ubuntu/pegasus/build/latest/output/bin/pegasus_server/pegasus_server not exist
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6E7vD1/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2067,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6FDSNf,incubator-pegasus,2232230751,2067,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-07-17T02:32:59Z,2024-07-17T02:32:59Z,"> 
> ./run.sh start_onebox
> ERROR: file /home/ubuntu/pegasus/build/latest/output/bin/pegasus_server/pegasus_server not exist

@SHUFIL Has the Pegasus been successfully built before starting the onebox? Are you starting the onebox in the docker?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6FDSNf/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2069,https://api.github.com/repos/apache/incubator-pegasus/issues/2069,incubator-pegasus,2400171054,2069,It takes a very long time to create checkpoint for a replica with 0 or 1 record while adding new duplication,empiredan,743379,Dan Wang,,CLOSED,2024-07-10T09:01:42Z,2024-07-11T06:56:50Z,"Firstly, we create a new table in the source cluster:
```
>>> create test1 -p 8 -r 1
create app test1 succeed, waiting for app ready
test1 not ready yet, still waiting... (0/8)
test1 is ready now: (8/8)
test1 is ready now!
create app ""test1"" succeed

>>> ls -d
[general_info]
app_id  status     app_name                  app_type  partition_count  replica_count  is_stateful  create_time          drop_time  drop_expire  envs_count
1       AVAILABLE  __detect                  pegasus   8                1              true         2024-07-10_11:55:48  -          -            0
2       AVAILABLE  __stat                    pegasus   8                1              true         2024-07-10_11:55:48  -          -            0
3       AVAILABLE  temp                      pegasus   8                1              true         2024-07-10_11:55:48  -          -            0
4       AVAILABLE  xyz                       pegasus   8                1              true         2024-07-10_11:56:36  -          -            0
5       AVAILABLE  test1                     pegasus   8                1              true         2024-07-10_15:32:13  -          -            0
```

Then, put 2 different records into the table:
```
>>> use test1
OK
>>> set a b c
OK

app_id          : 5
partition_index : 4
decree          : 1
server          : 10.1.128.223:8171
>>> set 1 2 3
OK

app_id          : 5
partition_index : 2
decree          : 1
server          : 10.1.128.223:8171
>>> full_scan
partition: all
hash_key_filter_type: no_filter
sort_key_filter_type: no_filter
value_filter_type: no_filter
max_count: -1
timout_ms: 5000
detailed: false
no_value: false

""a"" : ""b"" => ""c""
""1"" : ""2"" => ""3""

2 key-value pairs got.
```

Add a new duplication for the table `test1`:
```
>>> add_dup test1 target_cluster -s -a test_dup_1 -r 3
trying to add duplication [app_name: test1, remote_cluster_name: target_cluster, is_duplicating_checkpoint: true, remote_app_name: test_dup_1, remote_replica_count: 3]
adding duplication succeed [app_name: test1, remote_cluster_name: target_cluster, appid: 5, dupid: 1720596785, is_duplicating_checkpoint: true, remote_app_name: test_dup_1, remote_replica_count: 3]
```

Check the status of the table in remote cluster, during several tens of minutes it would keep the dead state (all of the 3 replicas are unavailable):
```
>>> ls -d
[general_info]
app_id  status     app_name                  app_type  partition_count  replica_count  is_stateful  create_time          drop_time  drop_expire  envs_count
1       AVAILABLE  __detect                  pegasus   8                3              true         2024-07-10_12:00:37  -          -            0
2       AVAILABLE  __stat                    pegasus   8                3              true         2024-07-10_12:00:37  -          -            0
3       AVAILABLE  temp                      pegasus   8                3              true         2024-07-10_12:00:37  -          -            0
4       AVAILABLE  xyz                       pegasus   8                3              true         2024-07-10_12:01:51  -          -            0
5       AVAILABLE  test_dup_1                pegasus   8                3              true         2024-07-10_15:33:38  -          -            3

[healthy_info]
app_id  app_name                  partition_count  fully_healthy  unhealthy  write_unhealthy  read_unhealthy
1       __detect                  8                8              0          0                0
2       __stat                    8                8              0          0                0
3       temp                      8                8              0          0                0
4       xyz                       8                8              0          0                0
5       test_dup_1                8                0              8          8                8

[summary]
total_app_count            : 5
fully_healthy_app_count    : 4
unhealthy_app_count        : 1
write_unhealthy_app_count  : 1
read_unhealthy_app_count   : 1

>>> app test_dup_1 -d
[parameters]
app_name  : test_dup_1
detailed  : true

[general]
app_name           : test_dup_1
app_id             : 5
partition_count    : 8
max_replica_count  : 3

[replicas]
pidx  ballot  replica_count  primary  secondaries
0     0       0/3            -        []
1     0       0/3            -        []
2     0       0/3            -        []
3     0       0/3            -        []
4     0       0/3            -        []
5     0       0/3            -        []
6     0       0/3            -        []
7     0       0/3            -        []

[nodes]
node  primary  secondary  total
      0        0          0

[healthy]
fully_healthy_partition_count    : 0
unhealthy_partition_count        : 8
write_unhealthy_partition_count  : 8
read_unhealthy_partition_count   : 8
```

Check the checkpoints of table `test1` in source cluster, it is found that there is not any checkpoint created for any replica.
```
$ ll 5.*.pegasus/data
5.0.pegasus/data:
total 0
drwxr-xr-x 2 data data 163 Jul 10 15:32 rdb

5.1.pegasus/data:
total 0
drwxr-xr-x 2 data data 163 Jul 10 15:32 rdb

5.2.pegasus/data:
total 0
drwxr-xr-x 2 data data 163 Jul 10 15:32 rdb

5.3.pegasus/data:
total 0
drwxr-xr-x 2 data data 163 Jul 10 15:32 rdb

5.4.pegasus/data:
total 0
drwxr-xr-x 2 sa_cluster sa_group 163 Jul 10 15:32 rdb

5.5.pegasus/data:
total 0
drwxr-xr-x 2 data data 163 Jul 10 15:32 rdb

5.6.pegasus/data:
total 0
drwxr-xr-x 2 data data 163 Jul 10 15:32 rdb

5.7.pegasus/data:
total 0
drwxr-xr-x 2 data data 163 Jul 10 15:32 rdb
```

After nearly one hour(from `15:32` to `16:27`), all of the checkpoints were created for each replica:
```
[dev (v.v) sa_cluster@hybrid01 reps]$ ll 5.*.pegasus/data/
5.0.pegasus/data/:
total 0
drwxr-xr-x 2 data data 147 Jul 10 16:12 checkpoint.24
drwxr-xr-x 2 data data 199 Jul 10 16:12 rdb

5.1.pegasus/data/:
total 0
drwxr-xr-x 2 data data 147 Jul 10 16:07 checkpoint.21
drwxr-xr-x 2 data data 199 Jul 10 16:07 rdb

5.2.pegasus/data/:
total 0
drwxr-xr-x 2 data data 147 Jul 10 16:07 checkpoint.21
drwxr-xr-x 2 data data 199 Jul 10 16:07 rdb

5.3.pegasus/data/:
total 0
drwxr-xr-x 2 data data 147 Jul 10 16:12 checkpoint.24
drwxr-xr-x 2 data data 199 Jul 10 16:12 rdb

5.4.pegasus/data/:
total 0
drwxr-xr-x 2 data data 147 Jul 10 16:22 checkpoint.30
drwxr-xr-x 2 data data 199 Jul 10 16:22 rdb

5.5.pegasus/data/:
total 0
drwxr-xr-x 2 data data 147 Jul 10 16:12 checkpoint.24
drwxr-xr-x 2 data data 199 Jul 10 16:12 rdb

5.6.pegasus/data/:
total 0
drwxr-xr-x 2 data data 147 Jul 10 16:07 checkpoint.21
drwxr-xr-x 2 data data 199 Jul 10 16:07 rdb

5.7.pegasus/data/:
total 0
drwxr-xr-x 2 data data 147 Jul 10 16:27 checkpoint.33
drwxr-xr-x 2 data data 199 Jul 10 16:27 rdb
```

Create another table, and put 3 records (2 records has the same hash/sort key) into the table:
```
>>> create test2 -p 8 -r 1
create app test2 succeed, waiting for app ready
test2 not ready yet, still waiting... (0/8)
test2 is ready now: (8/8)
test2 is ready now!
create app ""test2"" succeed

>>> ls -d
[general_info]
app_id  status     app_name                  app_type  partition_count  replica_count  is_stateful  create_time          drop_time  drop_expire  envs_count
1       AVAILABLE  __detect                  pegasus   8                1              true         2024-07-10_11:55:48  -          -            0
2       AVAILABLE  __stat                    pegasus   8                1              true         2024-07-10_11:55:48  -          -            0
3       AVAILABLE  temp                      pegasus   8                1              true         2024-07-10_11:55:48  -          -            0
4       AVAILABLE  xyz                       pegasus   8                1              true         2024-07-10_11:56:36  -          -            0
5       AVAILABLE  test1                     pegasus   8                1              true         2024-07-10_15:32:13  -          -            0
6       AVAILABLE  test2                     pegasus   8                1              true         2024-07-10_16:20:02  -          -            0

>>> use test2
OK
>>> set a b c
OK

app_id          : 6
partition_index : 4
decree          : 1
server          : 10.1.128.223:8171
>>> set 1 2 3
OK

app_id          : 6
partition_index : 2
decree          : 1
server          : 10.1.128.223:8171
>>> set 1 2 4
OK

app_id          : 6
partition_index : 2
decree          : 2
server          : 10.1.128.223:8171
```

Add a new duplication for table `test2`:
```
>>> add_dup test2 target_cluster -s -a test_dup_2 -r 3
trying to add duplication [app_name: test2, remote_cluster_name: target_cluster, is_duplicating_checkpoint: true, remote_app_name: test_dup_2, remote_replica_count: 3]
adding duplication succeed [app_name: test2, remote_cluster_name: target_cluster, appid: 6, dupid: 1720599648, is_duplicating_checkpoint: true, remote_app_name: test_dup_2, remote_replica_count: 3]
```

The checkpoint of partition 2 with 2 mutations of the source cluster were created immediately:
```
6.2.pegasus/data/:
total 0
drwxr-xr-x 2 data data 147 Jul 10 16:20 checkpoint.2
drwxr-xr-x 2 data data 235 Jul 10 17:05 rdb
```

The partition 2 of the remote cluster also became healthy very quickly:
```
>>> ls -d
[general_info]
app_id  status     app_name                  app_type  partition_count  replica_count  is_stateful  create_time          drop_time  drop_expire  envs_count
6       AVAILABLE  test_dup_2                pegasus   8                3              true         2024-07-10_16:21:28  -          -            3

[healthy_info]
app_id  app_name                  partition_count  fully_healthy  unhealthy  write_unhealthy  read_unhealthy
6       test_dup_2                8                1              7          7                7

>>> app test_dup_2 -d
[parameters]
app_name  : test_dup_2
detailed  : true

[general]
app_name           : test_dup_2
app_id             : 6
partition_count    : 8
max_replica_count  : 3

[replicas]
pidx  ballot  replica_count  primary            secondaries
0     0       0/3            -                  []
1     0       0/3            -                  []
2     3       3/3            10.1.128.226:8171  [10.1.128.224:8171,10.1.128.225:8171]
3     0       0/3            -                  []
4     0       0/3            -                  []
5     0       0/3            -                  []
6     0       0/3            -                  []
7     0       0/3            -                  []

[nodes]
node               primary  secondary  total
10.1.128.224:8171  0        1          1
10.1.128.225:8171  0        1          1
10.1.128.226:8171  1        0          1
                   1        2          3

[healthy]
fully_healthy_partition_count    : 1
unhealthy_partition_count        : 7
write_unhealthy_partition_count  : 7
read_unhealthy_partition_count   : 7
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2069/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2069,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Ec70v,incubator-pegasus,2222177583,2069,NA,empiredan,743379,Dan Wang,,NA,2024-07-11T06:56:50Z,2024-07-11T06:56:50Z,This issue has been fixed by https://github.com/apache/incubator-pegasus/pull/2054.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Ec70v/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2071,https://api.github.com/repos/apache/incubator-pegasus/issues/2071,incubator-pegasus,2407491192,2071,Build CentOS 7 based development image failed,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2024-07-14T15:52:52Z,2024-07-15T11:50:11Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!

1. What did you do?
Build the image by [Dockerfile](https://github.com/apache/incubator-pegasus/blob/7a5593fa4a00565a48d4b72a8c7a399e2e0033a4/docker/pegasus-build-env/centos7/Dockerfile) failed.

2. What did you expect to see?
Should success.

3. What did you see instead?
Build failed, error message is:
```
[root@de100dc297d8 ~]# yum -y install centos-release-scl \
>                    scl-utils \
>                    epel-release; \
>
Loaded plugins: fastestmirror, ovl
Determining fastest mirrors
Could not retrieve mirrorlist http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=os&infra=container error was
14: curl#6 - ""Could not resolve host: mirrorlist.centos.org; Unknown error""


 One of the configured repositories failed (Unknown),
 and yum doesn't have enough cached data to continue. At this point the only
 safe thing yum can do is fail. There are a few ways to work ""fix"" this:

     1. Contact the upstream for the repository and get them to fix the problem.

     2. Reconfigure the baseurl/etc. for the repository, to point to a working
        upstream. This is most often useful if you are using a newer
        distribution release than is supported by the repository (and the
        packages for the previous distribution release still work).

     3. Run the command with the repository temporarily disabled
            yum --disablerepo=<repoid> ...

     4. Disable the repository permanently, so yum won't use it by default. Yum
        will then just ignore the repository until you permanently enable it
        again or use --enablerepo for temporary usage:

            yum-config-manager --disable <repoid>
        or
            subscription-manager repos --disable=<repoid>

     5. Configure the failing repository to be skipped, if it is unavailable.
        Note that yum will try to contact the repo. when it runs most commands,
        so will have to try and fail each time (and thus. yum will be be much
        slower). If it is a very temporary problem though, this is often a nice
        compromise:

            yum-config-manager --save --setopt=<repoid>.skip_if_unavailable=true

Cannot find a valid baseurl for repo: base/7/x86_64
```

4. What version of Pegasus are you using?

https://github.com/apache/incubator-pegasus/blob/7a5593fa4a00565a48d4b72a8c7a399e2e0033a4/docker/pegasus-build-env/centos7/Dockerfile
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2071/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2073,https://api.github.com/repos/apache/incubator-pegasus/issues/2073,incubator-pegasus,2407785966,2073,"""Build Java"" in ""Lint and build regularly"" job failed",acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,CLOSED,2024-07-15T03:24:51Z,2024-07-15T04:00:31Z,"The error looks like:
```
Downloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/mojo/maven-metadata.xml (21 kB at 529 kB/s)
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  1.291 s
[INFO] Finished at: 2024-07-12T13:34:34Z
[INFO] ------------------------------------------------------------------------
Error:  No plugin found for prefix 'spotless' in the current project and in the plugin groups [org.apache.maven.plugins, org.codehaus.mojo] available from the repositories [local (/home/runner/.m2/repository), central (https://repo.maven.apache.org/maven2)] -> [Help 1]
Error:  
Error:  To see the full stack trace of the errors, re-run Maven with the -e switch.
Error:  Re-run Maven using the -X switch to enable full debug logging.
Error:  
Error:  For more information about the errors and possible solutions, please read the following articles:
Error:  [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/NoPluginFoundForPrefixException
Error: Process completed with exit code 1.
```
See details https://github.com/apache/incubator-pegasus/actions/runs/9908766114/job/27375255558.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2073/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2080,https://api.github.com/repos/apache/incubator-pegasus/issues/2080,incubator-pegasus,2429837202,2080,build-env docker support  mac arm,shalk,2435781,shalk(xiao kun),,CLOSED,2024-07-25T12:26:50Z,2024-07-26T03:24:29Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
some dev env is mac  with arm chip, so docker can support arm64.

ref https://docs.docker.com/build/ci/github-actions/multi-platform/,  arm64 docker can be supported.

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->

**Describe alternatives you've considered:**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->

**Teachability, Documentation, Adoption, Migration Strategy:**
<!-- If you can, explain some scenarios how users might use this, situations it would be helpful in. Any API designs, mockups, or diagrams are also helpful. -->
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2080/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2080,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6GH2qJ,incubator-pegasus,2250205833,2080,NA,shalk,2435781,shalk(xiao kun),,NA,2024-07-25T12:28:55Z,2024-07-25T12:28:55Z,"I build a docker for arm64, but some compile error happen

```
docker run -v `pwd`:/root/pegasus \
           xshalk/pegasus:build-env-ubuntu2204-20240722  \
           /bin/bash -c ""cd /root/pegasus; ./run.sh build -c --clear_thirdparty -j $(nproc)""
```

log 
```
gcc.compile.c++ bin.v2/libs/system/build/gcc-11.4.0/release/cxxstd-11-iso/link-static/threading-multi/visibility-hidden/error_code.o
gcc.archive bin.v2/libs/system/build/gcc-11.4.0/release/cxxstd-11-iso/link-static/threading-multi/visibility-hidden/libboost_system.a
common.copy lib/libboost_system.a
gcc.compile.c++ bin.v2/libs/filesystem/build/gcc-11.4.0/release/cxxstd-11-iso/link-static/threading-multi/visibility-hidden/codecvt_error_category.o
gcc.compile.c++ bin.v2/libs/filesystem/build/gcc-11.4.0/release/cxxstd-11-iso/link-static/threading-multi/visibility-hidden/operations.o
gcc.compile.c++ bin.v2/libs/filesystem/build/gcc-11.4.0/release/cxxstd-11-iso/link-static/threading-multi/visibility-hidden/path.o
gcc.compile.c++ bin.v2/libs/filesystem/build/gcc-11.4.0/release/cxxstd-11-iso/link-static/threading-multi/visibility-hidden/path_traits.o
gcc.compile.c++ bin.v2/libs/filesystem/build/gcc-11.4.0/release/cxxstd-11-iso/link-static/threading-multi/visibility-hidden/portability.o
gcc.compile.c++ bin.v2/libs/filesystem/build/gcc-11.4.0/release/cxxstd-11-iso/link-static/threading-multi/visibility-hidden/unique_path.o
gcc.compile.c++ bin.v2/libs/filesystem/build/gcc-11.4.0/release/cxxstd-11-iso/link-static/threading-multi/visibility-hidden/utf8_codecvt_facet.o
gcc.compile.c++ bin.v2/libs/filesystem/build/gcc-11.4.0/release/cxxstd-11-iso/link-static/threading-multi/visibility-hidden/windows_file_codecvt.o
gcc.archive bin.v2/libs/filesystem/build/gcc-11.4.0/release/cxxstd-11-iso/link-static/threading-multi/visibility-hidden/libboost_filesystem.a
common.copy lib/libboost_filesystem.a
...updated 14775 targets...
[ 65%] Performing install step for 'boost'
[ 66%] Completed 'boost'
[ 66%] Built target boost
make: *** [Makefile:91: all] Error 2
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6GH2qJ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2080,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6GH3Ak,incubator-pegasus,2250207268,2080,NA,shalk,2435781,shalk(xiao kun),,NA,2024-07-25T12:29:42Z,2024-07-25T12:29:42Z,@acelyc111 PTAL,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6GH3Ak/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2089,https://api.github.com/repos/apache/incubator-pegasus/issues/2089,incubator-pegasus,2446365836,2089,dsn_replica_dup_test failed for ASAN due to heap-use-after-free error,empiredan,743379,Dan Wang,,CLOSED,2024-08-03T14:02:52Z,2024-08-15T03:09:52Z,"Firstly build Pegasus for ASAN by following command:
```
./run.sh build --test -t debug -v -j $(nproc) --sanitizer address --disable_gperf
```

Then, execute `dsn_replica_dup_test` by following command:
```
./run.sh test -m dsn_replica_dup_test
```

Error occurred as the following logs:
```
[ RUN      ] mutation_batch_test.add_mutation_if_valid/0
=================================================================
==39111==ERROR: AddressSanitizer: heap-use-after-free on address 0x6030010b2300 at pc 0x7faecab78397 bp 0x7faeaa68a410 sp 0x7faeaa689bb8
READ of size 5 at 0x6030010b2300 thread T37 (replica.default)
    #0 0x7faecab78396 in __interceptor_memcpy ../../../../src/libsanitizer/sanitizer_common/sanitizer_common_interceptors.inc:827
    #1 0x7faec7bdad29 in dsn::blob::create_from_bytes(char const*, unsigned long) /root/apache/pegasus/src/utils/blob.h:75
    #2 0x7faec7bc638e in dsn::replication::mutation_batch::add_mutation_if_valid(dsn::ref_ptr<dsn::replication::mutation>&, long) /root/apache/pegasus/src/replica/duplication/mutation_batch.cpp:202
    #3 0x557ccb1be340 in dsn::replication::mutation_batch_test_add_mutation_if_valid_Test::TestBody() /root/apache/pegasus/src/replica/duplication/test/mutation_batch_test.cpp:136
    #4 0x557ccb2d7dd6 in void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x486dd6)
    #5 0x557ccb2d07a0 in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x47f7a0)
    #6 0x557ccb2aae31 in testing::Test::Run() (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x459e31)
    #7 0x557ccb2ab94c in testing::TestInfo::Run() (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x45a94c)
    #8 0x557ccb2ac352 in testing::TestSuite::Run() (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x45b352)
    #9 0x557ccb2bc879 in testing::internal::UnitTestImpl::RunAllTests() (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x46b879)
    #10 0x557ccb2d8f19 in bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x487f19)
    #11 0x557ccb2d1ac6 in bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x480ac6)
    #12 0x557ccb2bae7a in testing::UnitTest::Run() (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x469e7a)
    #13 0x557ccb1b411b in RUN_ALL_TESTS() /root/apache/pegasus/thirdparty/output/include/gtest/gtest.h:2317
    #14 0x557ccb1b4c90 in gtest_app::start(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) /root/apache/pegasus/src/replica/duplication/test/main.cpp:39
    #15 0x7faec8372559 in dsn::service_node::start_app() /root/apache/pegasus/src/runtime/service_engine.cpp:87
    #16 0x7faec83ab598 in dsn::service_control_task::exec() /root/apache/pegasus/src/runtime/tool_api.cpp:66
    #17 0x7faec84ddcba in dsn::task::exec_internal() /root/apache/pegasus/src/runtime/task/task.cpp:173
    #18 0x7faec85837ab in dsn::task_worker::loop() /root/apache/pegasus/src/runtime/task/task_worker.cpp:245
    #19 0x7faec8583309 in dsn::task_worker::run_internal() /root/apache/pegasus/src/runtime/task/task_worker.cpp:225
    #20 0x7faec858e76f in void std::__invoke_impl<void, void (dsn::task_worker::*&)(), dsn::task_worker*&>(std::__invoke_memfun_deref, void (dsn::task_worker::*&)(), dsn::task_worker*&) /usr/include/c++/11/bits/invoke.h:74
    #21 0x7faec858e5ea in std::__invoke_result<void (dsn::task_worker::*&)(), dsn::task_worker*&>::type std::__invoke<void (dsn::task_worker::*&)(), dsn::task_worker*&>(void (dsn::task_worker::*&)(), dsn::task_worker*&) /usr/include/c++/11/bits/invoke.h:96
    #22 0x7faec858e516 in void std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>::__call<void, , 0ul>(std::tuple<>&&, std::_Index_tuple<0ul>) /usr/include/c++/11/functional:420
    #23 0x7faec858e3d5 in void std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>::operator()<, void>() /usr/include/c++/11/functional:503
    #24 0x7faec858e307 in void std::__invoke_impl<void, std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>>(std::__invoke_other, std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>&&) /usr/include/c++/11/bits/invoke.h:61
    #25 0x7faec858e2c2 in std::__invoke_result<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>>::type std::__invoke<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>>(std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>&&) /usr/include/c++/11/bits/invoke.h:96
    #26 0x7faec858e263 in void std::thread::_Invoker<std::tuple<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()> > >::_M_invoke<0ul>(std::_Index_tuple<0ul>) /usr/include/c++/11/bits/std_thread.h:259
    #27 0x7faec858e233 in std::thread::_Invoker<std::tuple<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()> > >::operator()() /usr/include/c++/11/bits/std_thread.h:266
    #28 0x7faec858e213 in std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()> > > >::_M_run() /usr/include/c++/11/bits/std_thread.h:211
    #29 0x7faec0cd7252  (/lib/x86_64-linux-gnu/libstdc++.so.6+0xdc252)
    #30 0x7faec095fac2  (/lib/x86_64-linux-gnu/libc.so.6+0x94ac2)
    #31 0x7faec09f0a03 in __clone (/lib/x86_64-linux-gnu/libc.so.6+0x125a03)

0x6030010b2300 is located 16 bytes inside of 32-byte region [0x6030010b22f0,0x6030010b2310)
freed by thread T37 (replica.default) here:
    #0 0x7faecabf524f in operator delete(void*, unsigned long) ../../../../src/libsanitizer/asan/asan_new_delete.cpp:172
    #1 0x557ccb08d7dc in dsn::blob::create_from_bytes(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&&)::{lambda(char*)#1}::operator()(char*) const (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x23c7dc)
    #2 0x557ccb0eac8a in std::_Sp_counted_deleter<char*, dsn::blob::create_from_bytes(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&&)::{lambda(char*)#1}, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose() /usr/include/c++/11/bits/shared_ptr_base.h:442
    #3 0x557ccb0a5e7e in std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release() /usr/include/c++/11/bits/shared_ptr_base.h:168
    #4 0x557ccb09da45 in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::~__shared_count() /usr/include/c++/11/bits/shared_ptr_base.h:705
    #5 0x557ccb08d649 in std::__shared_ptr<char, (__gnu_cxx::_Lock_policy)2>::~__shared_ptr() (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x23c649)
    #6 0x557ccb08d669 in std::shared_ptr<char>::~shared_ptr() (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x23c669)
    #7 0x557ccb08d781 in dsn::blob::~blob() (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x23c781)
    #8 0x557ccb0de6e7 in std::_Head_base<2ul, dsn::blob, false>::~_Head_base() /usr/include/c++/11/tuple:187
    #9 0x557ccb0de707 in std::_Tuple_impl<2ul, dsn::blob>::~_Tuple_impl() /usr/include/c++/11/tuple:416
    #10 0x557ccb0de727 in std::_Tuple_impl<1ul, dsn::task_code, dsn::blob>::~_Tuple_impl() /usr/include/c++/11/tuple:258
    #11 0x557ccb0de747 in std::_Tuple_impl<0ul, unsigned long, dsn::task_code, dsn::blob>::~_Tuple_impl() /usr/include/c++/11/tuple:258
    #12 0x557ccb0de767 in std::tuple<unsigned long, dsn::task_code, dsn::blob>::~tuple() /usr/include/c++/11/tuple:609
    #13 0x557ccb0de78b in void __gnu_cxx::new_allocator<std::_Rb_tree_node<std::tuple<unsigned long, dsn::task_code, dsn::blob> > >::destroy<std::tuple<unsigned long, dsn::task_code, dsn::blob> >(std::tuple<unsigned long, dsn::task_code, dsn::blob>*) /usr/include/c++/11/ext/new_allocator.h:168
    #14 0x557ccb0d5a2e in void std::allocator_traits<std::allocator<std::_Rb_tree_node<std::tuple<unsigned long, dsn::task_code, dsn::blob> > > >::destroy<std::tuple<unsigned long, dsn::task_code, dsn::blob> >(std::allocator<std::_Rb_tree_node<std::tuple<unsigned long, dsn::task_code, dsn::blob> > >&, std::tuple<unsigned long, dsn::task_code, dsn::blob>*) /usr/include/c++/11/bits/alloc_traits.h:535
    #15 0x557ccb0ca58e in std::_Rb_tree<std::tuple<unsigned long, dsn::task_code, dsn::blob>, std::tuple<unsigned long, dsn::task_code, dsn::blob>, std::_Identity<std::tuple<unsigned long, dsn::task_code, dsn::blob> >, dsn::replication::mutation_tuple_cmp, std::allocator<std::tuple<unsigned long, dsn::task_code, dsn::blob> > >::_M_destroy_node(std::_Rb_tree_node<std::tuple<unsigned long, dsn::task_code, dsn::blob> >*) /usr/include/c++/11/bits/stl_tree.h:623
    #16 0x557ccb0ba8de in std::_Rb_tree<std::tuple<unsigned long, dsn::task_code, dsn::blob>, std::tuple<unsigned long, dsn::task_code, dsn::blob>, std::_Identity<std::tuple<unsigned long, dsn::task_code, dsn::blob> >, dsn::replication::mutation_tuple_cmp, std::allocator<std::tuple<unsigned long, dsn::task_code, dsn::blob> > >::_M_drop_node(std::_Rb_tree_node<std::tuple<unsigned long, dsn::task_code, dsn::blob> >*) /usr/include/c++/11/bits/stl_tree.h:631
    #17 0x557ccb0acede in std::_Rb_tree<std::tuple<unsigned long, dsn::task_code, dsn::blob>, std::tuple<unsigned long, dsn::task_code, dsn::blob>, std::_Identity<std::tuple<unsigned long, dsn::task_code, dsn::blob> >, dsn::replication::mutation_tuple_cmp, std::allocator<std::tuple<unsigned long, dsn::task_code, dsn::blob> > >::_M_erase(std::_Rb_tree_node<std::tuple<unsigned long, dsn::task_code, dsn::blob> >*) /usr/include/c++/11/bits/stl_tree.h:1891
    #18 0x557ccb0a2529 in std::_Rb_tree<std::tuple<unsigned long, dsn::task_code, dsn::blob>, std::tuple<unsigned long, dsn::task_code, dsn::blob>, std::_Identity<std::tuple<unsigned long, dsn::task_code, dsn::blob> >, dsn::replication::mutation_tuple_cmp, std::allocator<std::tuple<unsigned long, dsn::task_code, dsn::blob> > >::~_Rb_tree() /usr/include/c++/11/bits/stl_tree.h:984
    #19 0x557ccb0989f5 in std::set<std::tuple<unsigned long, dsn::task_code, dsn::blob>, dsn::replication::mutation_tuple_cmp, std::allocator<std::tuple<unsigned long, dsn::task_code, dsn::blob> > >::~set() /usr/include/c++/11/bits/stl_set.h:281
    #20 0x557ccb1d4d3c in dsn::replication::mutation_batch_test::check_mutation_contents(std::set<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, dsn::replication::mutation_batch&) /root/apache/pegasus/src/replica/duplication/test/mutation_batch_test.cpp:72
    #21 0x557ccb1be242 in dsn::replication::mutation_batch_test_add_mutation_if_valid_Test::TestBody() /root/apache/pegasus/src/replica/duplication/test/mutation_batch_test.cpp:132
    #22 0x557ccb2d7dd6 in void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x486dd6)
    #23 0x557ccb2d07a0 in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x47f7a0)
    #24 0x557ccb2aae31 in testing::Test::Run() (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x459e31)
    #25 0x557ccb2ab94c in testing::TestInfo::Run() (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x45a94c)
    #26 0x557ccb2ac352 in testing::TestSuite::Run() (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x45b352)
    #27 0x557ccb2bc879 in testing::internal::UnitTestImpl::RunAllTests() (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x46b879)
    #28 0x557ccb2d8f19 in bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x487f19)
    #29 0x557ccb2d1ac6 in bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x480ac6)

previously allocated by thread T37 (replica.default) here:
    #0 0x7faecabf41e7 in operator new(unsigned long) ../../../../src/libsanitizer/asan/asan_new_delete.cpp:99
    #1 0x557ccb08d8b2 in dsn::blob::create_from_bytes(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&&) (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x23c8b2)
    #2 0x557ccb099cd6 in dsn::replication::replica_test_base::create_test_mutation(long, long, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /root/apache/pegasus/src/replica/test/replica_test_base.h:78
    #3 0x557ccb09aa20 in dsn::replication::duplication_test_base::create_test_mutation(long, long, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /root/apache/pegasus/src/replica/duplication/test/duplication_test_base.h:82
    #4 0x557ccb09ab3a in dsn::replication::duplication_test_base::create_test_mutation(long, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /root/apache/pegasus/src/replica/duplication/test/duplication_test_base.h:89
    #5 0x557ccb1be0b4 in dsn::replication::mutation_batch_test_add_mutation_if_valid_Test::TestBody() /root/apache/pegasus/src/replica/duplication/test/mutation_batch_test.cpp:130
    #6 0x557ccb2d7dd6 in void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x486dd6)
    #7 0x557ccb2d07a0 in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x47f7a0)
    #8 0x557ccb2aae31 in testing::Test::Run() (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x459e31)
    #9 0x557ccb2ab94c in testing::TestInfo::Run() (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x45a94c)
    #10 0x557ccb2ac352 in testing::TestSuite::Run() (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x45b352)
    #11 0x557ccb2bc879 in testing::internal::UnitTestImpl::RunAllTests() (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x46b879)
    #12 0x557ccb2d8f19 in bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x487f19)
    #13 0x557ccb2d1ac6 in bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x480ac6)
    #14 0x557ccb2bae7a in testing::UnitTest::Run() (/root/apache/pegasus/build/debug/src/replica/duplication/test/dsn_replica_dup_test+0x469e7a)
    #15 0x557ccb1b411b in RUN_ALL_TESTS() /root/apache/pegasus/thirdparty/output/include/gtest/gtest.h:2317
    #16 0x557ccb1b4c90 in gtest_app::start(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) /root/apache/pegasus/src/replica/duplication/test/main.cpp:39
    #17 0x7faec8372559 in dsn::service_node::start_app() /root/apache/pegasus/src/runtime/service_engine.cpp:87
    #18 0x7faec83ab598 in dsn::service_control_task::exec() /root/apache/pegasus/src/runtime/tool_api.cpp:66
    #19 0x7faec84ddcba in dsn::task::exec_internal() /root/apache/pegasus/src/runtime/task/task.cpp:173
    #20 0x7faec85837ab in dsn::task_worker::loop() /root/apache/pegasus/src/runtime/task/task_worker.cpp:245
    #21 0x7faec8583309 in dsn::task_worker::run_internal() /root/apache/pegasus/src/runtime/task/task_worker.cpp:225
    #22 0x7faec858e76f in void std::__invoke_impl<void, void (dsn::task_worker::*&)(), dsn::task_worker*&>(std::__invoke_memfun_deref, void (dsn::task_worker::*&)(), dsn::task_worker*&) /usr/include/c++/11/bits/invoke.h:74
    #23 0x7faec858e5ea in std::__invoke_result<void (dsn::task_worker::*&)(), dsn::task_worker*&>::type std::__invoke<void (dsn::task_worker::*&)(), dsn::task_worker*&>(void (dsn::task_worker::*&)(), dsn::task_worker*&) /usr/include/c++/11/bits/invoke.h:96
    #24 0x7faec858e516 in void std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>::__call<void, , 0ul>(std::tuple<>&&, std::_Index_tuple<0ul>) /usr/include/c++/11/functional:420
    #25 0x7faec858e3d5 in void std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>::operator()<, void>() /usr/include/c++/11/functional:503
    #26 0x7faec858e307 in void std::__invoke_impl<void, std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>>(std::__invoke_other, std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>&&) /usr/include/c++/11/bits/invoke.h:61
    #27 0x7faec858e2c2 in std::__invoke_result<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>>::type std::__invoke<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>>(std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>&&) /usr/include/c++/11/bits/invoke.h:96
    #28 0x7faec858e263 in void std::thread::_Invoker<std::tuple<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()> > >::_M_invoke<0ul>(std::_Index_tuple<0ul>) /usr/include/c++/11/bits/std_thread.h:259
    #29 0x7faec858e233 in std::thread::_Invoker<std::tuple<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()> > >::operator()() /usr/include/c++/11/bits/std_thread.h:266

Thread T37 (replica.default) created by T0 here:
    #0 0x7faecab96685 in __interceptor_pthread_create ../../../../src/libsanitizer/asan/asan_interceptors.cpp:216
    #1 0x7faec0cd7328 in std::thread::_M_start_thread(std::unique_ptr<std::thread::_State, std::default_delete<std::thread::_State> >, void (*)()) (/lib/x86_64-linux-gnu/libstdc++.so.6+0xdc328)
    #2 0x7faec858bb6f in std::_MakeUniq<std::thread>::__single_object std::make_unique<std::thread, std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()> >(std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>&&) /usr/include/c++/11/bits/unique_ptr.h:962
    #3 0x7faec857f3d8 in dsn::task_worker::start() /root/apache/pegasus/src/runtime/task/task_worker.cpp:92
    #4 0x7faec85018d0 in dsn::task_worker_pool::start() /root/apache/pegasus/src/runtime/task/task_engine.cpp:105
    #5 0x7faec8507b5e in dsn::task_engine::start() /root/apache/pegasus/src/runtime/task/task_engine.cpp:262
    #6 0x7faec83740da in dsn::service_node::start() /root/apache/pegasus/src/runtime/service_engine.cpp:135
    #7 0x7faec837720d in dsn::service_engine::start_node(dsn::service_app_spec&) /root/apache/pegasus/src/runtime/service_engine.cpp:239
    #8 0x7faec835822b in run /root/apache/pegasus/src/runtime/service_api_c.cpp:557
    #9 0x7faec834f4ae in dsn_run_config(char const*, bool) /root/apache/pegasus/src/runtime/service_api_c.cpp:226
    #10 0x557ccb1afdc4 in main /root/apache/pegasus/src/replica/duplication/test/main.cpp:54
    #11 0x7faec08f4d8f  (/lib/x86_64-linux-gnu/libc.so.6+0x29d8f)

SUMMARY: AddressSanitizer: heap-use-after-free ../../../../src/libsanitizer/sanitizer_common/sanitizer_common_interceptors.inc:827 in __interceptor_memcpy
Shadow bytes around the buggy address:
  0x0c068020e410: fd fd fd fa fa fa fa fa fa fa fa fa fd fd fd fa
  0x0c068020e420: fa fa fd fd fd fd fa fa fd fd fd fd fa fa fd fd
  0x0c068020e430: fd fa fa fa 00 00 00 fa fa fa fd fd fd fa fa fa
  0x0c068020e440: fd fd fd fa fa fa fd fd fd fa fa fa fd fd fd fd
  0x0c068020e450: fa fa fa fa fa fa fa fa fd fd fd fa fa fa fd fd
=>0x0c068020e460:[fd]fd fa fa fd fd fd fd fa fa fd fd fd fd fa fa
  0x0c068020e470: fd fd fd fa fa fa fd fd fd fa fa fa fd fd fd fd
  0x0c068020e480: fa fa fd fd fd fa fa fa fd fd fd fd fa fa fd fd
  0x0c068020e490: fd fa fa fa fd fd fd fd fa fa fd fd fd fd fa fa
  0x0c068020e4a0: fd fd fd fd fa fa fd fd fd fd fa fa fd fd fd fa
  0x0c068020e4b0: fa fa fd fd fd fa fa fa fd fd fd fa fa fa fd fd
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
  Shadow gap:              cc
==39111==ABORTING
./run.sh: line 45: 39111 Aborted                 (core dumped) ./dsn_replica_dup_test
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2089/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2089,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6IhbQp,incubator-pegasus,2290463785,2089,NA,empiredan,743379,Dan Wang,,NA,2024-08-15T03:09:52Z,2024-08-15T03:09:52Z,This issue has been fixed by https://github.com/apache/incubator-pegasus/pull/2088.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6IhbQp/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2092,https://api.github.com/repos/apache/incubator-pegasus/issues/2092,incubator-pegasus,2455080962,2092,Checking that a remote table with only one replica is ready for duplication after copying checkpoints failed for ERR_NOT_ENOUGH_MEMBER,empiredan,743379,Dan Wang,,CLOSED,2024-08-08T07:28:46Z,2024-08-15T03:10:31Z,"After copying checkpoints for full duplication, checking a remote table whose replication factor is 1 failed due to `ERR_NOT_ENOUGH_MEMBER`. The error log of meta server is as follows:

```
E2024-08-07 18:52:42.537 (1723027962537681375 5005)   meta.meta_state0.0103000000000109: meta_duplication_service.cpp:577:operator()(): query follower app[target_cluster.dup_test_3] replica configuration completed, result: duplication_status = DS_APP, query_err = ERR_NOT_ENOUGH_MEMBER, update_err = ERR_NO_NEED_OPERATE
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2092/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2092,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Ihbez,incubator-pegasus,2290464691,2092,NA,empiredan,743379,Dan Wang,,NA,2024-08-15T03:10:31Z,2024-08-15T03:10:31Z,This issue has been fixed by https://github.com/apache/incubator-pegasus/pull/2093.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Ihbez/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2098,https://api.github.com/repos/apache/incubator-pegasus/issues/2098,incubator-pegasus,2469794268,2098,Assertion always fail after the replica server is restarted while a duplication is still in the status of `DS_PREPARE` due to some reason,empiredan,743379,Dan Wang,,CLOSED,2024-08-16T08:36:08Z,2024-08-19T11:56:17Z,"Firstly, we create two pegasus clusters: cluster A (including only one node) is configured without authentication, while cluster B (including three nodes) is configured with authentication. Both are configured with duplication.

Create a table named `test1` with only one replica on cluster A, and write some records into it:

```
>>> create test1 -p 8 -r 1
test1 not ready yet, still waiting... (0/8)
test1 not ready yet, still waiting... (0/8)
test1 not ready yet, still waiting... (0/8)
test1 not ready yet, still waiting... (0/8)
test1 is ready now: (8/8)
test1 is ready now!
create app ""test1"" succeed
```

Then, create a full duplication with checkpoints on it:

```
>>> add_dup test1 target_cluster -s -a test_dup_1 -r 3
trying to add duplication [app_name: test1, remote_cluster_name: target_cluster, is_duplicating_checkpoint: true, remote_app_name: test_dup_1, remote_replica_count: 3]
adding duplication succeed [app_name: test1, remote_cluster_name: target_cluster, appid: 5, dupid: 1723795449, is_duplicating_checkpoint: true, remote_app_name: test_dup_1, remote_replica_count: 3]
```

Since this duplication would create a new table on remote cluster B which is enabled with authentication, the duplication would be blocked and kept in the status of `DS_PREPARE`. At this moment, once the only replica server of cluster A is restarted, it always exit abnormally after the restart due to an assertion:

```
D2024-08-16 16:08:56.33 (1723795736033205343 40ed) replica.default7.0201000000000031: replica_duplicator.cpp:76:replica_duplicator(): [5.0@10.1.136.75:8171] initialize replica_duplicator[DS_PREPARE] [dupid:1723795449, meta_confirmed_decree:0]
D2024-08-16 16:08:56.33 (1723795736033261509 40ed) replica.default7.0201000000000031: replica_duplicator.cpp:98:prepare_dup(): [5.0@10.1.136.75:8171] start to trigger checkpoint: min_checkpoint_decree=-1, last_committed_decree=3, last_applied_decree=3, last_flushed_decree=2, last_durable_decree=2, plog_max_decree_on_disk=3, plog_max_commit_on_disk=2
E2024-08-16 16:08:56.33 (1723795736033293038 40ed) replica.default7.0201000000000031: replica_chkpt.cpp:165:async_trigger_manual_emergency_checkpoint(): assertion expression: min_checkpoint_decree > 0
F2024-08-16 16:08:56.33 (1723795736033315722 40ed) replica.default7.0201000000000031: replica_chkpt.cpp:165:async_trigger_manual_emergency_checkpoint(): [5.0@10.1.136.75:8171] -1 vs 0 min_checkpoint_decree should be a number greater than 0 which means a new checkpoint must be created
```

According to the assertion, the reason is that after replica server was restarted, `min_checkpoint_decree` would be reset to the invalid decree, which is wrong for an existing duplication that is still in the status of `DS_PREPARE`, meaning the replica would continue to generate checkpoints for the full duplication.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2098/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2098,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6I4DzY,incubator-pegasus,2296397016,2098,NA,empiredan,743379,Dan Wang,,NA,2024-08-19T11:56:17Z,2024-08-19T11:56:17Z,This issue is fixed by https://github.com/apache/incubator-pegasus/pull/2097.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6I4DzY/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2100,https://api.github.com/repos/apache/incubator-pegasus/issues/2100,incubator-pegasus,2472390450,2100,Bulkload can not work during duplication,ninsmiracle,110282526,,,OPEN,2024-08-19T03:44:55Z,2024-08-19T03:45:30Z,"## Bug Report

Please answer these questions before submitting your issue. Thanks!
There are a lot of Pegasus user need to do bulkload, however if those apps is doing duplication, clusters may coredump all.

1. What did you do?
- use this app
- add dup mission
- doing bulkload

2. What version of Pegasus are you using?
2.4","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2100/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2102,https://api.github.com/repos/apache/incubator-pegasus/issues/2102,incubator-pegasus,2472608479,2102, Dynamic reload duplication config,ninsmiracle,110282526,,,OPEN,2024-08-19T07:01:12Z,2024-09-26T11:58:03Z,"## Feature Request
In PR #2000 , we can ignore cluster id with  paramater `dup_ignore_other_cluster_ids`.
However, I think this function can be more flexible.
There is no doubt that, cluster id is very useful in multiple clusters duplication situation, backup-cluster can use it to judge data's priority.
So we may want to dynamic relod duplication config when we doing duplication.

**Describe the feature you'd like:**
Here is a case:
At beging, we have two clusters like  cluster A dup data to cluster B. (A ----> B)
And now, demand changes:
C cluster also need to dup to cluster B. (A,C -----> B)
We don't need to restart the nodes on B, and can reload the config **ABOUT DUP**.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2102/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2102,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6NT0Z0,incubator-pegasus,2370782836,2102,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-09-24T09:39:50Z,2024-09-24T09:39:50Z,"@ninsmiracle 
How many configuration key-value pairs are needed to be reload in this case?

There are several ways to update configs, http and remote_command, it's not a good idea to introduce another method.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6NT0Z0/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2102,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Nc1PP,incubator-pegasus,2373145551,2102,NA,ninsmiracle,110282526,,,NA,2024-09-25T06:30:59Z,2024-09-25T06:30:59Z,"There are 2 key-value paris needed to be reload,such like this:
```
[[pegasus.clusters]]
   c3tst-cluster1 = 10.xxx.xxx.x:22601,10.xxx.xx.x:22601
   c4tst-cluster2 = 10.xxx.xx.x:37001,10.xxx.xx.x:37001
   # you can add cluster's meta list here
   xxxxx = xxxxxxxxxxxx
   
  [[duplication-group]]
  c3tst-cluster1 = 1 
  c4tst-cluster2 = 2 
 # you can add cluster_id here
  xxxx = 3      
```
We only need to recognize these two parameters in this issues.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Nc1PP/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2102,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Nc61Z,incubator-pegasus,2373168473,2102,NA,ninsmiracle,110282526,,,NA,2024-09-25T06:43:36Z,2024-09-25T06:43:36Z,"Regarding the second discussion, I think update the config via http is not a good idea.
Here is the method we used (If we apply this feat):
1.Push new config file (config.ini) with your own method to target cluster.
2.Open pegasus-shell and type `remote_command -t all dup-config-reload`. 
3.Begin duplication as normal.  Just like: `dup add -c c3tst-cluster3 -p`

In step1, we require the cluster maintenance personnel to push the config file to the nodes of the target cluster first. This step is to ensure that the latest config file can be read immediately when the cluster node is restarted after a downtime.

In step2, we can see that we use `remote_command` to upload the latest config file paramaters (those about duplication) to memory.

So that, in step 3, you can use dup command without restart target cluster's nodes.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Nc61Z/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2104,https://api.github.com/repos/apache/incubator-pegasus/issues/2104,incubator-pegasus,2476981007,2104,Reduce verberos logs,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,OPEN,2024-08-21T03:46:01Z,2024-08-21T03:46:01Z,"When the server maintenances a large number of replicas (say 2000), it produce too many verberos logs, the INFO level logs will be rotate very soon. Since most of them are not necessary, we can lower some log levels.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2104/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2105,https://api.github.com/repos/apache/incubator-pegasus/issues/2105,incubator-pegasus,2477027323,2105,Required write permission for module labeler on Github,empiredan,743379,Dan Wang,,CLOSED,2024-08-21T04:30:17Z,2024-08-21T09:40:38Z,"Workflow for module labeler failed with following error reported:

```workflow
Run actions/labeler@v4
  with:
    repo-token: ***
    configuration-path: .github/workflows/module_labeler_conf.yml
    sync-labels: true
    dot: false
The configuration file (path: .github/workflows/module_labeler_conf.yml) is found locally, reading from the file
Warning: The action requires write permission to add labels to pull requests. For more information please refer to the action documentation: https://github.com/actions/labeler#permissions
Error: Resource not accessible by integration
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2105/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2107,https://api.github.com/repos/apache/incubator-pegasus/issues/2107,incubator-pegasus,2482440280,2107,The backup-cluster may doing an incomplete learn with duplication,ninsmiracle,110282526,,,OPEN,2024-08-23T06:48:41Z,2024-08-23T06:53:16Z,"## Bug Report

  At present, the implementation of dup is that when the backup-cluster executes the dup rpc processing function, multiple requests in dup are written to rocksdb in multiple times. 
  Each time it is written to rocksdb, the decree of the dup mutation is written at the same time. If the backup-cluster is checkpointed at this time, the data of the decree may not be completely written to rocksdb. 
  If the learner of the backup-cluster uses this checkpoint to start learning, it will start to request plog from decree+1 after learning. As a result, some dup requests of the decree are not learned, and some data is lost.


```
int pegasus_write_service::duplicate(int64_t decree,
                                     const dsn::apps::duplicate_request &requests,
                                     dsn::apps::duplicate_response &resp)
{
    // If the `for` loop has not yet been completed, and there is a need to checkpoint.
    // The checkpoint may not include all data cause these request share the same decree.
    // In other word, this creates an inconsistency.
    for (const auto &request : requests.entries) {
    // ...
     }
}
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2107/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2107,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6JeRwh,incubator-pegasus,2306415649,2107,NA,ninsmiracle,110282526,,,NA,2024-08-23T06:53:15Z,2024-08-23T06:53:15Z,"But we can use `duplicate_log_batch_bytes = 0`  to deal with this problem.
So I'm not very sure should I fix this 'bug'.
If I should fix it, executing a dup request should write multiple requests and one decree as a write_batch?
@acelyc111  @empiredan ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6JeRwh/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2108,https://api.github.com/repos/apache/incubator-pegasus/issues/2108,incubator-pegasus,2496994064,2108,How to connected to target cluster via pegasus shell config ,ninsmiracle,110282526,,,OPEN,2024-08-30T11:37:47Z,2024-09-02T03:30:59Z,"## General Question
  I have read the documentation, but can not found the way that use `./run.sh shell --config my_config.ini` to connected to my target cluster. In other word, I don't know how to config it.
  I tried to use the config file named `my_cluster.cfg` which I used to deployed cluster. But it not work,with the error like this:
```
W2024-08-30 11:17:43.874 (1725016663874826365 42119) : overwrite default thread pool for task RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX_ACK from THREAD_POOL_META_SERVER to THREAD_POOL_DEFAULT
Pegasus Shell 2.4.5-without-slog
Type ""help"" for more information.
Type ""Ctrl-D"" or ""Ctrl-C"" to exit the shell.

cannot find factory '' with factory type provider
        the following 2 factories are registered:
                nativerun (type: provider)
                simulator (type: provider)
        Please specify the correct factory name in your tool_app or in configuration file
got signal id: 11
./run.sh: line 1724: 42119 Segmentation fault      ./pegasus_shell ${CONFIG} $CLUSTER_NAM
```
And I read the code , I think we should get the section named[meta] to get meta list , I just konw how to write the xxxx.ini file
![image](https://github.com/user-attachments/assets/ec091d61-92ba-46f4-9600-4f183e55f41f)

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2108/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2108,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6KVyQq,incubator-pegasus,2320966698,2108,NA,ninsmiracle,110282526,,,NA,2024-08-30T11:44:44Z,2024-08-30T11:44:44Z,"I also copy and change the config.ini file in `CONFIG=${ROOT}/config-shell.ini.$PID`
file like this:
```
[apps..default]
run = true
count = 1


[apps.mimic]
type = dsn.app.mimic
arguments =
pools = THREAD_POOL_DEFAULT,THREAD_POOL_META_SERVER
run = true
count = 1

[core]
tool = nativerun
pause_on_start = false

logging_start_level = LOG_LEVEL_DEBUG
logging_factory_name = dsn::tools::simple_logger
logging_flush_on_exit = false

enable_default_app_mimic = true

data_dir = ./pegasus_shell.data

[tools.simple_logger]
short_header = false
fast_flush = true
max_number_of_log_files_on_disk = 10
stderr_start_level = LOG_LEVEL_FATAL

[tools.simulator]
random_seed = 0

[network]
io_service_worker_count = 4

[threadpool..default]
worker_count = 4
partitioned = false
worker_priority = THREAD_xPRIORITY_NORMAL

[threadpool.THREAD_POOL_DEFAULT]
name = default
worker_count = 20

[threadpool.THREAD_POOL_META_SERVER]
name = meta_server

[task..default]
is_trace = false
is_profile = false
allow_inline = false
rpc_call_header_format = NET_HDR_DSN
rpc_call_channel = RPC_CHANNEL_TCP
rpc_timeout_milliseconds = 10000


[pegasus.clusters]
my_cluster = ip:port,ip2:port

[security]
  enable_auth = true
  krb5_keytab = /home/work/pegasus/xxxxx.keytab
  krb5_config = /etc/krb5.conf
  krb5_principal = xxxx@xxxx.realm
  sasl_plugin_path = /home/work/pegasus/thirdparty/output/lib/sasl2
  service_fqdn = my_cluster_master_meta_host_name
  service_name = my_cluster
```
It worked,
But pegasus-shell connected to onebox, I am so confused ， because I think I configed meta-list in `[pegasus.clusters]`.

During my operation, the output of the shell is as follows
```
[root@xxxxxx pegasus]# ./run.sh shell --config ker.ini 
W2024-08-30 11:41:19.453 (1725018079453708642 42130) : overwrite default thread pool for task RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX from THREAD_POOL_META_SERVER to THREAD_POOL_DEFAULT
W2024-08-30 11:41:19.453 (1725018079453808438 42130) : overwrite default thread pool for task RPC_CM_QUERY_PARTITION_CONFIG_BY_INDEX_ACK from THREAD_POOL_META_SERVER to THREAD_POOL_DEFAULT
Pegasus Shell 2.4.5-without-slog
Type ""help"" for more information.
Type ""Ctrl-D"" or ""Ctrl-C"" to exit the shell.

The config file is: ker.ini
The cluster name is: onebox
The cluster meta list is: 
>>> ls
list apps failed, error=ERR_TIMEOUT
>>>
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6KVyQq/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2108,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6KgYXX,incubator-pegasus,2323744215,2108,NA,ninsmiracle,110282526,,,NA,2024-09-02T03:30:59Z,2024-09-02T03:30:59Z,"When I change the file `src/shell/main.cpp` and  `run.sh` (I want to make pegasus shell connected to target cluster forcely).
Executing any command will time out.
![image](https://github.com/user-attachments/assets/e09f5921-5f7d-458b-9afe-c9d64cd57681)


If I'm not change the file , pegasus shell will be never connected to target cluster and always connected to `onebox` cluster. 
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6KgYXX/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2109,https://api.github.com/repos/apache/incubator-pegasus/issues/2109,incubator-pegasus,2504997781,2109,replica server with security enabled exited abnormally due to `blob::create_from_bytes` assertion,empiredan,743379,Dan Wang,,CLOSED,2024-09-04T10:55:42Z,2024-09-06T03:04:55Z,"Replica server was started with security enabled as following configurations:

```ini
[security]
enable_auth = true
krb5_keytab = ...
krb5_config = /etc/krb5.conf
krb5_principal = ...
sasl_plugin_path = /usr/lib64/sasl2
service_fqdn = ...
service_name = ...
mandatory_auth = true
enable_acl = true
super_users =
meta_acl_rpc_allow_list =
enable_ranger_acl = true
```

However, it failed immediately after started with following error logs and generate core dump file:

```bash
E2024-09-04 12:06:36.870 (1725422796870535314 3e18) replica.default2.0201000500000008: compaction_filter_rule.cpp:109:create_from_bytes(): assertion expression: (s) != nullptr
F2024-09-04 12:06:36.870 (1725422796870576704 3e18) replica.default2.0201000500000008: compaction_filter_rule.cpp:109:create_from_bytes(): null source pointer would lead to undefined behaviour
```

The backtrace of core dump file is:

```bash
#0  0x00007f6375ec7387 in raise () from /lib64/libc.so.6
#1  0x00007f6375ec8a78 in abort () from /lib64/libc.so.6
#2  0x00007f637ae8b716 in dsn_coredump () at /data/code/pegasus-2.4/src/rdsn/src/runtime/service_api_c.cpp:95
#3  0x00007f6379af0e39 in dsn::tools::(anonymous namespace)::process_fatal_log (log_level=LOG_LEVEL_FATAL) at /data/code/pegasus-2.4/src/rdsn/src/utils/simple_logger.cpp:91
#4  0x00007f6379af2293 in dsn::tools::simple_logger::dsn_log (this=0x2ad8e70, file=0xcb99e5 ""compaction_filter_rule.cpp"",
    function=0xcbc730 <dsn::blob::create_from_bytes(char const*, unsigned long)::__FUNCTION__> ""create_from_bytes"", line=109, log_level=LOG_LEVEL_FATAL,
    str=0x6e667e8 ""null source pointer would lead to undefined behaviour"") at /data/code/pegasus-2.4/src/rdsn/src/utils/simple_logger.cpp:334
#5  0x00007f6379abf2df in dsn_log (file=0xcb99e5 ""compaction_filter_rule.cpp"",
    function=0xcbc730 <dsn::blob::create_from_bytes(char const*, unsigned long)::__FUNCTION__> ""create_from_bytes"", line=109, log_level=LOG_LEVEL_FATAL,
    str=0x6e667e8 ""null source pointer would lead to undefined behaviour"") at /data/code/pegasus-2.4/src/rdsn/src/utils/logging.cpp:155
#6  0x0000000000728b7e in dsn::blob::create_from_bytes (s=0x0, len=0) at /data/code/pegasus-2.4/src/rdsn/include/dsn/utility/blob.h:109
#7  0x00007f637b02d876 in dsn::security::sasl_client_wrapper::step (this=0x6aaad80, input=..., output=...) at /data/code/pegasus-2.4/src/rdsn/src/runtime/security/sasl_client_wrapper.cpp:69
#8  0x00007f637afe2ee6 in dsn::security::client_negotiation::on_challenge (this=0x677e510, challenge=...) at /data/code/pegasus-2.4/src/rdsn/src/runtime/security/client_negotiation.cpp:154
#9  0x00007f637afe26b6 in dsn::security::client_negotiation::handle_response(dsn::error_code, dsn::security::negotiation_response const&&) (this=0x677e510, err=...,
    response=<unknown type in /data/pegasus/replica_server/lib/libdsn_replica_server.so, CU 0x59bb2b0, DIE 0x5a5bc61>)
    at /data/code/pegasus-2.4/src/rdsn/src/runtime/security/client_negotiation.cpp:80
#10 0x00007f637aee0916 in dsn::security::negotiation_manager::on_negotiation_response (err=..., rpc=...) at /data/code/pegasus-2.4/src/rdsn/src/runtime/security/negotiation_manager.cpp:170
#11 0x00007f637afe3212 in dsn::security::client_negotiation::<lambda(dsn::error_code)>::operator()(dsn::error_code) (__closure=0x6e96260, err=...)
    at /data/code/pegasus-2.4/src/rdsn/src/runtime/security/client_negotiation.cpp:191
#12 0x00007f637afe3520 in dsn::rpc_holder<dsn::security::negotiation_request, dsn::security::negotiation_response>::<lambda(dsn::error_code, dsn::message_ex*, dsn::message_ex*)>::operator()(dsn::error_code, dsn::message_ex *, dsn::message_ex *) (__closure=0x6e96260, err=..., req=0x67806e0, resp=0x6780370) at /data/code/pegasus-2.4/src/rdsn/include/dsn/cpp/rpc_holder.h:161
#13 0x00007f637afe38f7 in std::_Function_handler<void(dsn::error_code, dsn::message_ex*, dsn::message_ex*), dsn::rpc_holder<TRequest, TResponse>::call(const dsn::rpc_address&, dsn::task_tracker*, TCallback&&, int) [with TCallback = dsn::security::client_negotiation::send(dsn::security::negotiation_status::type, const dsn::blob&)::<lambda(dsn::error_code)>; TRequest = dsn::security::negotiation_request; TResponse = dsn::security::negotiation_response; dsn::task_ptr = dsn::ref_ptr<dsn::task>]::<lambda(dsn::error_code, dsn::message_ex*, dsn::message_ex*)> >::_M_invoke(const std::_Any_data &, <unknown type in /data/pegasus/replica_server/lib/libdsn_replica_server.so, CU 0x59bb2b0, DIE 0x5a5022b>, <unknown type in /data/pegasus/replica_server/lib/libdsn_replica_server.so, CU 0x59bb2b0, DIE 0x5a5023a>, <unknown type in /data/pegasus/replica_server/lib/libdsn_replica_server.so, CU 0x59bb2b0, DIE 0x5a5024a>) (__functor=...,
    __args#0=<unknown type in /data/pegasus/replica_server/lib/libdsn_replica_server.so, CU 0x59bb2b0, DIE 0x5a5022b>,
    __args#1=<unknown type in /data/pegasus/replica_server/lib/libdsn_replica_server.so, CU 0x59bb2b0, DIE 0x5a5023a>,
    __args#2=<unknown type in /data/pegasus/replica_server/lib/libdsn_replica_server.so, CU 0x59bb2b0, DIE 0x5a5024a>)
    at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/std_function.h:316
#14 0x0000000000be5003 in std::function<void (dsn::error_code, dsn::message_ex*, dsn::message_ex*)>::operator()(dsn::error_code, dsn::message_ex*, dsn::message_ex*) const (this=0x4c0b1e0,
    __args#0=..., __args#1=0x67806e0, __args#2=0x6780370) at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/std_function.h:706
#15 0x00007f637af2857b in dsn::rpc_response_task::exec (this=0x4c0b100) at /data/code/pegasus-2.4/src/rdsn/include/dsn/tool-api/task.h:478
#16 0x00007f637af29bf4 in dsn::task::exec_internal (this=0x4c0b100) at /data/code/pegasus-2.4/src/rdsn/src/runtime/task/task.cpp:176
#17 0x00007f637af5a7f3 in dsn::task_worker::loop (this=0x2ad96b0) at /data/code/pegasus-2.4/src/rdsn/src/runtime/task/task_worker.cpp:224
#18 0x00007f637af5a741 in dsn::task_worker::run_internal (this=0x2ad96b0) at /data/code/pegasus-2.4/src/rdsn/src/runtime/task/task_worker.cpp:204
#19 0x00007f637af5d78a in std::__invoke_impl<void, void (dsn::task_worker::*&)(), dsn::task_worker*&> (
    __f=@0x33965e8: (void (dsn::task_worker::*)(dsn::task_worker * const)) 0x7f637af5a53a <dsn::task_worker::run_internal()>, __t=@0x33965f8: 0x2ad96b0)
    at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/invoke.h:73
#20 0x00007f637af5d679 in std::__invoke<void (dsn::task_worker::*&)(), dsn::task_worker*&> (
    __fn=@0x33965e8: (void (dsn::task_worker::*)(dsn::task_worker * const)) 0x7f637af5a53a <dsn::task_worker::run_internal()>, __args#0=@0x33965f8: 0x2ad96b0)
    at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/invoke.h:95
#21 0x00007f637af5d4c6 in std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>::__call<void, , 0ul>(std::tuple<>&&, std::_Index_tuple<0ul>) (this=0x33965e8,
    __args=<unknown type in /data/pegasus/replica_server/lib/libdsn_replica_server.so, CU 0x51576e8, DIE 0x51b9cdc>)
    at /opt/rh/devtoolset-7/root/usr/include/c++/7/functional:467
#22 0x00007f637af5d252 in std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>::operator()<, void>() (this=0x33965e8) at /opt/rh/devtoolset-7/root/usr/include/c++/7/functional:551
#23 0x00007f637af5cf61 in std::__invoke_impl<void, std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>>(std::__invoke_other, std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>&&) (__f=<unknown type in /data/pegasus/replica_server/lib/libdsn_replica_server.so, CU 0x51576e8, DIE 0x51bde89>)
    at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/invoke.h:60
#24 0x00007f637af5cb5c in std::__invoke<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>>(std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()>&&) (
    __fn=<unknown type in /data/pegasus/replica_server/lib/libdsn_replica_server.so, CU 0x51576e8, DIE 0x51c096d>)
    at /opt/rh/devtoolset-7/root/usr/include/c++/7/bits/invoke.h:95
#25 0x00007f637af5d97e in std::thread::_Invoker<std::tuple<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()> > >::_M_invoke<0ul>(std::_Index_tuple<0ul>) (this=0x33965e8)
    at /opt/rh/devtoolset-7/root/usr/include/c++/7/thread:234
---Type <return> to continue, or q <return> to quit---f
#26 0x00007f637af5d94f in std::thread::_Invoker<std::tuple<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()> > >::operator()() (this=0x33965e8)
    at /opt/rh/devtoolset-7/root/usr/include/c++/7/thread:243
#27 0x00007f637af5d92e in std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::_Bind<void (dsn::task_worker::*(dsn::task_worker*))()> > > >::_M_run() (this=0x33965e0)
    at /opt/rh/devtoolset-7/root/usr/include/c++/7/thread:186
#28 0x00007f6379b4499f in execute_native_thread_routine () from /data/pegasus/replica_server/lib/libdsn_utils.so
#29 0x00007f6377ab7ea5 in start_thread () from /lib64/libpthread.so.0
#30 0x00007f6375f8fb0d in clone () from /lib64/libc.so.6
```

The reason is that `blob::create_from_bytes` is not allowed to accept `nullptr`:

```c++
 58 error_s sasl_client_wrapper::step(const blob &input, blob &output)
 59 {
 60     FAIL_POINT_INJECT_F(""sasl_client_wrapper_step"", [](dsn::string_view str) {
 61         error_code err = error_code::try_get(str.data(), ERR_UNKNOWN);
 62         return error_s::make(err);
 63     });
 64
 65     const char *msg = nullptr;
 66     unsigned msg_len = 0;
 67     int sasl_err = sasl_client_step(_conn, input.data(), input.length(), nullptr, &msg, &msg_len);
 68
 69     output = blob::create_from_bytes(msg, msg_len);
 70     return wrap_error(sasl_err);
 71 }
```

```c++
105     /// Create shared buffer from allocated raw bytes.
106     /// NOTE: this operation is not efficient since it involves a memory copy.
107     [[nodiscard]] static blob create_from_bytes(const char *s, size_t len)
108     {
109         dcheck_notnull(s, ""null source pointer would lead to undefined behaviour"");
110
111         std::shared_ptr<char> s_arr(new char[len], std::default_delete<char[]>());
112         memcpy(s_arr.get(), s, len);
113         return {std::move(s_arr), static_cast<unsigned int>(len)};
114     }
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2109/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2112,https://api.github.com/repos/apache/incubator-pegasus/issues/2112,incubator-pegasus,2513632695,2112,Java client cannot access the meta server when authentication is enabled,Samunroyu,36890229,,yujingweiop@gmail.com,OPEN,2024-09-09T10:59:44Z,2024-09-09T10:59:44Z,"## Bug Report
When all shards of the table become unavailable, and at the same time the meta server has performed a master switch, this situation will cause the Java client to be unable to access the meta server. The error message is ""Negotiation failed"".
Like 
![image](https://github.com/user-attachments/assets/2f859b11-49cc-4a2f-9bd4-8e2d71962159)


The java client cant recovery with retry policy, this situation is not expected.
 


1. What did you do?
Shutdown replica server, and shutdown the mechine of leader meta server. Will see the Negotiation failed happend.

2. What did you expect to see?
The java client can access table after replica server recovery  with retry policy.

3. What version of Pegasus are you using?
2.4.0 release version
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2112/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2114,https://api.github.com/repos/apache/incubator-pegasus/issues/2114,incubator-pegasus,2521242643,2114,Internal authentication of the cluster failed via Pegasus 2.4,ninsmiracle,110282526,,,OPEN,2024-09-12T03:19:15Z,2024-09-12T03:53:19Z,"## General Question

when I use Pegasus 2.4 access controller , I can use peagsus-shell access target cluster , but I found all the RPC will be failed in the internal of  the cluster.

1. I use `pegasus_prc/pegasus@COMPANY.HADOOP` as my server principal.

Here is my keytab Principal used in target cluster(To check my keytab file is consistent with principal):
```
[work@xxxxxxxx pegasus]$ klist -k pegasus_prc@COMPANY.HADOOP.keytab
Keytab name: FILE:pegasus_prc@COMPANY.HADOOP.keytab
KVNO Principal
---- --------------------------------------------------------------------------
   1 pegasus_prc/pegasus@COMPANY.HADOOP
   1 pegasus_prc/pegasus@COMPANY.HADOOP
```

2. Here is the config in target cluster's config.ini:
```
[[security]]
  enable_acl = false
  super_users = u_guoningshen
  service_name = pegasus_prc
  service_fqdn = pegasus
  sasl_plugin_path = /usr/lib64/sasl2
  krb5_keytab = /home/work/app/pegasus/pegasus_prc@COMPANY.HADOOP.keytab
  krb5_config = /home/work/app/pegasus/krb5.conf
  krb5_principal = pegasus_prc/pegasus@COMPANY.HADOOP
  mandatory_auth = false
  enable_auth = true
```

3. Here is my pegasus-shell ini file , and I use it to access target cluster
```
[apps..default]
run = true
count = 1

[apps.mimic]
type = dsn.app.mimic
arguments =
pools = THREAD_POOL_DEFAULT,THREAD_POOL_META_SERVER
run = true
count = 1

[core]
tool = nativerun
pause_on_start = false

logging_start_level = LOG_LEVEL_DEBUG
logging_factory_name = dsn::tools::simple_logger
logging_flush_on_exit = false

enable_default_app_mimic = true

data_dir = ./pegasus_shell.data

[tools.simple_logger]
short_header = false
fast_flush = true
max_number_of_log_files_on_disk = 10
stderr_start_level = LOG_LEVEL_FATAL

[tools.simulator]
random_seed = 0

[network]
io_service_worker_count = 4

[threadpool..default]
worker_count = 4
partitioned = false
worker_priority = THREAD_xPRIORITY_NORMAL

[threadpool.THREAD_POOL_DEFAULT]
name = default
worker_count = 20

[threadpool.THREAD_POOL_META_SERVER]
name = meta_server

[task..default]
is_trace = false
is_profile = false
allow_inline = false
rpc_call_header_format = NET_HDR_DSN
rpc_call_channel = RPC_CHANNEL_TCP
rpc_timeout_milliseconds = 10000


[pegasus.clusters]
c4tst-function2 = 10.xxx.xx.1:32601,10.xxx.xx.2:32601

[security]
enable_auth = true
krb5_keytab = /home/work/2.4.4_pegasus/pegasus/u_guoningshen.keytab
krb5_config = /etc/krb5.conf
krb5_principal = u_guoningshen@COMPANY.HADOOP
sasl_plugin_path = /home/work/2.4.4_pegasus/pegasus/thirdparty/output/lib/sasl2
service_fqdn = pegasus
service_name = pegasus_prc
```


4. What happen?
* Connected to cluster via pegasus-shell 
```
./run.sh shell -c ker.ini
```

* `u_guoningshen` is super user for cluster , so I have full permissions.
```
The cluster name is: c4tst-function2
The cluster meta list is: 10.xxx.xx.1:32601,10.xxx.xx.2:32601
>>> ls
[general_info]
app_id  status     app_name  app_type  partition_count  replica_count  is_stateful  create_time          drop_time  drop_expire  envs_count  
238     AVAILABLE  test      pegasus   4                3              true         2024-09-11_07:30:20  -          -            0           
239     AVAILABLE  gns       pegasus   4                3              true         2024-09-12_02:30:50  -          -            0           

[summary]
total_app_count  : 2

>>> drop gns
reserve_seconds = 0
drop app gns succeed

>>> ls
[general_info]
app_id  status     app_name  app_type  partition_count  replica_count  is_stateful  create_time          drop_time  drop_expire  envs_count  
238     AVAILABLE  test      pegasus   4                3              true         2024-09-11_07:30:20  -          -            0           

[summary]
total_app_count  : 1

>>> 
```

* But I can not create table, because all the rpc send from master meta to another nodes will `negotiation failed, with err = ERR_UNKNOWN, msg = ERR_UNKNOWN`
```
>>> create gns_test
create app gns_test succeed, waiting for app ready
gns_test not ready yet, still waiting... (0/4)
gns_test not ready yet, still waiting... (0/4)
gns_test not ready yet, still waiting... (0/4)
gns_test not ready yet, still waiting... (0/4)
...
```







","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2114/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2114,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6LyN3o,incubator-pegasus,2345197032,2114,NA,ninsmiracle,110282526,,,NA,2024-09-12T03:31:28Z,2024-09-12T03:31:28Z,"When I check the app status:
```
[replicas]
pidx  ballot  replica_count  primary  secondaries  
0     0       0/3            -        []           
1     0       0/3            -        []           
2     0       0/3            -        []           
3     0       0/3            -        [] 
```
We can see that even the primary is not successfully established. So I check the log of replica server for this replica:
```
D2024-09-11 15:32:08.284 (1726039928284328485 93977) replica.io-thrd.93977: server_negotiation.cpp:40:start(): SERVER_NEGOTIATION(CLIENT=10.xxx.xx.1:41689): start negotiation
D2024-09-11 15:32:08.284 (1726039928284356416 93977) replica.io-thrd.93977: network.cpp:696:on_server_session_accepted(): server session accepted, remote_client = 10.xxx.xx.1:41689, current_count = 1
D2024-09-11 15:32:08.284 (1726039928284364187 93977) replica.io-thrd.93977: network.cpp:701:on_server_session_accepted(): ip session inserted, remote_client = 10.xxx.xx.1:41689, current_count = 1
W2024-09-11 15:32:08.289 (1726039928289020684 93993) replica.default10.04006f170001004b: server_negotiation.cpp:137:do_challenge(): SERVER_NEGOTIATION(CLIENT=10.xxx.xx.1:41689): negotiation failed, with err = ERR_UNKNOWN, msg = ERR_UNKNOWN
D2024-09-11 15:32:08.289 (1726039928289039571 93993) replica.default10.04006f170001004b: network.cpp:738:on_server_session_disconnected(): session 10.xxx.xx1:41689 disconnected, the total client sessions count remains 0
D2024-09-11 15:32:08.289 (1726039928289046389 93993) replica.default10.04006f170001004b: network.cpp:744:on_server_session_disconnected(): client ip 10.xxx.xx.1:41689 has no more session to this server
E2024-09-11 15:32:08.289 (1726039928289069504 93975) replica.io-thrd.93975: asio_rpc_session.cpp:96:operator()(): asio read from 10.xxx.xx.1:41689 failed: Operation canceled
```

I think the key message is `server_negotiation.cpp:137:do_challenge(): SERVER_NEGOTIATION(CLIENT=10.xxx.xx.1:41689): negotiation failed, with err = ERR_UNKNOWN, msg = ERR_UNKNOWN`

In my opinion, it failed on `SASL_INITIATE` step of SASL process between meta server and replica server.
```
       client                              server
        | ---    SASL_LIST_MECHANISMS     --> |
        | <--  SASL_LIST_MECHANISMS_RESP  --- |
        | --     SASL_SELECT_MECHANISMS   --> |
        | <-- SASL_SELECT_MECHANISMS_RESP --- |
        |                                     |
        | ---       SASL_INITIATE         --> |
        |                                     |
        | <--       SASL_CHALLENGE        --- |
        | ---     SASL_CHALLENGE_RESP     --> |
        |                                     |
        |               .....                 |
        |                                      |
        | <--       SASL_CHALLENGE        --- |
        | ---     SASL_CHALLENGE_RESP     --> |
        |                                     |
        |                                     |
        | <--         SASL_SUCC           --- |
        |                                     |
        |                                     |
        | ---         RPC_CALL           ---> |
        | <--         RPC_RESP           ---- |
```

![image](https://github.com/user-attachments/assets/9294dbe6-b4c8-4123-9701-f36e6f022ac1)









","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6LyN3o/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2114,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6LyQT8,incubator-pegasus,2345207036,2114,NA,ninsmiracle,110282526,,,NA,2024-09-12T03:43:41Z,2024-09-12T03:43:41Z,"I also check the log in KDC:
```
Sep 11 15:26:24 [c4-hadoop-krb02.bj](http://c4-hadoop-krb02.bj/) krb5kdc[59974](info): AS_REQ (6 etypes {18 17 16 23 25 26}) [10.XXX.XX.1](http://10.XXX.XX.1/): NEEDED_PREAUTH: pegasus_prc/pegasus@COMPANY.HADOOP for krbtgt/COMPANY.HADOOP@COMPANY.HADOOP, Additional pre-authentication required
Sep 11 15:26:24 [c4-hadoop-krb02.bj](http://c4-hadoop-krb02.bj/) krb5kdc[59974](info): TGS_REQ (6 etypes {18 17 16 23 25 26}) [10.XXX.XX.1](http://10.XXX.XX.1/): ISSUE: authtime 1726039584, etypes {rep=18 tkt=18 ses=18}, pegasus_prc/pegasus@COMPANY.HADOOP for pegasus_prc/pegasus@COMPANY.HADOOP
Sep 11 15:26:38 [c4-hadoop-krb02.bj](http://c4-hadoop-krb02.bj/) krb5kdc[59974](info): TGS_REQ (4 etypes {18 17 16 23}) [10.132.5.3](http://10.132.5.3/): ISSUE: authtime 1726039598, etypes {rep=18 tkt=18 ses=18}, pegasus_prc/pegasus@COMPANY.HADOOP for pegasus_prc/pegasus@COMPANY.HADOOP
Sep 11 15:26:47 [c4-hadoop-krb02.bj](http://c4-hadoop-krb02.bj/) krb5kdc[59974](info): TGS_REQ (6 etypes {18 17 16 23 25 26}) [10.XXX.XX.1](http://10.XXX.XX.1/): ISSUE: authtime 1726039567, etypes {rep=18 tkt=18 ses=18}, pegasus_prc/pegasus@COMPANY.HADOOP for pegasus_prc/pegasus@COMPANY.HADOOP
Sep 11 15:26:47 [c4-hadoop-krb02.bj](http://c4-hadoop-krb02.bj/) krb5kdc[59974](info): TGS_REQ (6 etypes {18 17 16 23 25 26}) [10.XXX.XX.1](http://10.XXX.XX.1/): ISSUE: authtime 1726039567, etypes {rep=18 tkt=18 ses=18}, pegasus_prc/pegasus@COMPANY.HADOOP for pegasus_prc/pegasus@COMPANY.HADOOP
Sep 11 15:26:48 [c4-hadoop-krb02.bj](http://c4-hadoop-krb02.bj/) krb5kdc[59974](info): AS_REQ (6 etypes {18 17 16 23 25 26}) [10.XXX.XX.1](http://10.XXX.XX.1/): ISSUE: authtime 1726039608, etypes {rep=18 tkt=18 ses=18}, pegasus_prc/pegasus@COMPANY.HADOOP for krbtgt/COMPANY.HADOOP@COMPANY.HADOOP
Sep 11 15:27:48 [c4-hadoop-krb02.bj](http://c4-hadoop-krb02.bj/) krb5kdc[59974](info): TGS_REQ (6 etypes {18 17 16 23 25 26}) [10.XXX.XX.1](http://10.XXX.XX.1/): ISSUE: authtime 1726039608, etypes {rep=18 tkt=18 ses=18}, pegasus_prc/pegasus@COMPANY.HADOOP for pegasus_prc/pegasus@COMPANY.HADOOP
```

I can see `TGS_REQ` here, so I think client(here is meta server,use sasl client side to pass the permission check on replica servers) can get TGT from KDC.

And I check backup mate server, still have some error on it:
```
server_negotiation.cpp:137:do_challenge(): SERVER_NEGOTIATION(CLIENT=10.XXX.XX.1:55545): negotiation failed, with err = ERR_UNKNOWN, msg = ERR_UNKNOWN
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6LyQT8/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2114,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6LySUn,incubator-pegasus,2345215271,2114,NA,ninsmiracle,110282526,,,NA,2024-09-12T03:53:18Z,2024-09-12T03:53:18Z,"  I also tried the following. When I changed the principal used by the pegasus-shell to be consistent with the server, I found that the pegasus-shell could not connect to the cluster at all. When you enter the `ls` command, the pegasus-shell will return an `ERR_TIMEOUT`. I checked the information from internet and it shows that when sasl calls kerberos, both ends of sasl cannot be set to the same principal.

  So I was confused. If this is the case, different nodes in the cluster must use the same principal, because we cannot assign a principal to each node. In this way, internal verification will definitely fail.

  At the same time, I noticed that `mandatory_auth = false` was developed in the code, and it seems that the internal verification between nodes can be skipped from the code logic. But I don’t know why it didn’t work.
There is too little log information in various places, and the only error `ERR_UNKNOW` report is not very usefull, so my work is in trouble...","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6LySUn/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2128,https://api.github.com/repos/apache/incubator-pegasus/issues/2128,incubator-pegasus,2552168953,2128,zookeeper-bin cannot be found while uploading artifact,empiredan,743379,Dan Wang,,CLOSED,2024-09-27T07:04:26Z,2024-09-29T09:49:38Z,"3 building workflows on Github failed for the reason that  zookeeper-bin cannot be found as the following logging:

![Pasted Graphic 17](https://github.com/user-attachments/assets/6a67cd58-1a68-4521-b8a7-d0b8c630cd16)

```
Prepare all required actions
Getting action download info
Download action repository 'actions/upload-artifact@v4' (SHA:50769540e7f4bd5e[2](https://github.com/apache/incubator-pegasus/actions/runs/11064265374/job/30741757003?pr=2078#step:6:2)1e526ee35c689e35e0d6874)
Run ./.github/actions/upload_artifact
  env:
    ONEBOX_OPTS: disk_min_available_space_ratio=5
    TEST_OPTS: disk_min_available_space_ratio=5;throttle_test_medium_value_kb=10;throttle_test_large_value_kb=2[5](https://github.com/apache/incubator-pegasus/actions/runs/11064265374/job/30741757003?pr=2078#step:6:5)
    USE_JEMALLOC: OFF
    ARTIFACT_NAME: release
    BUILD_OPTIONS: -t release --test
Run rm -rf thirdparty
  rm -rf thirdparty
  # The following operations are tricky, these directories and files don't exist if not build with '--test'.
  # When build binaries for client tests, it's not needed to add '--test'.
  mkdir -p build/latest/bin
  mkdir -p build/latest/src/server/test
  touch build/latest/src/server/test/config.ini
  tar -zcvhf ${ARTIFACT_NAME}_builder.tar build/latest/output build/latest/bin build/latest/src/server/test/config.ini hadoop-bin zookeeper-bin
  shell: bash --noprofile --norc -e -o pipefail {0}
  env:
    ONEBOX_OPTS: disk_min_available_space_ratio=5
    TEST_OPTS: disk_min_available_space_ratio=5;throttle_test_medium_value_kb=[10](https://github.com/apache/incubator-pegasus/actions/runs/11064265374/job/30741757003?pr=2078#step:6:10);throttle_test_large_value_kb=25
    USE_JEMALLOC: OFF
    ARTIFACT_NAME: release
    BUILD_OPTIONS: -t release --test
build/latest/output/
build/latest/output/lib/
build/latest/output/lib/libdsn_replication_common.a
build/latest/output/lib/libdsn.block_service.local.a
build/latest/output/lib/libdsn_replica_server.so
build/latest/output/lib/libdsn_rpc.a
build/latest/output/lib/libdsn_dist_cmd.a
```

```
hadoop-bin/sbin/mr-jobhistory-daemon.sh
hadoop-bin/sbin/kms.sh
hadoop-bin/sbin/stop-yarn.sh
hadoop-bin/sbin/yarn-daemons.sh
hadoop-bin/sbin/start-all.sh
hadoop-bin/sbin/stop-all.sh
hadoop-bin/sbin/workers.sh
hadoop-bin/sbin/stop-dfs.sh
hadoop-bin/sbin/start-yarn.sh
hadoop-bin/LICENSE-binary
hadoop-bin/include/
hadoop-bin/include/SerialUtils.hh
hadoop-bin/include/hdfs.h
tar: zookeeper-bin: Cannot stat: No such file or directory
hadoop-bin/include/StringUtils.hh
hadoop-bin/include/TemplateFactory.hh
hadoop-bin/include/Pipes.hh
tar: Exiting with failure status due to previous errors
Error: Process completed with exit code 2.
```

The problem is that zookeeper-bin would be moved only when third-party is changed. Once third-party is not changed, zookeeper-bin would not be found.

![Pasted Graphic 16](https://github.com/user-attachments/assets/6d99ab12-661d-4f25-9649-dc6ef59bf5a8)

![Pasted Graphic 15](https://github.com/user-attachments/assets/a1094c28-3410-4f70-abcd-bc51d85a50fa)



","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2128/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2130,https://api.github.com/repos/apache/incubator-pegasus/issues/2130,incubator-pegasus,2559528124,2130,How can I obtain the pre-built version packages instead of building from the source code myself?,lsasta,27410399,,1247858701@qq.com,OPEN,2024-10-01T15:08:31Z,2024-11-06T08:21:29Z,"## General Question

Could you please tell me, is there a pre-built tar package for each version? It's quite troublesome to package and build based on the image, and accessing the external network often times out with slow downloads. Or where can I obtain the pre-built packages?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2130/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2130,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6OO6hQ,incubator-pegasus,2386274384,2130,NA,lsasta,27410399,,1247858701@qq.com,NA,2024-10-01T15:10:02Z,2024-10-01T15:10:02Z,"download timeout
<img width=""810"" alt=""image"" src=""https://github.com/user-attachments/assets/7c5be90d-e5e7-47fe-83ab-f19cb42f1543"">
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6OO6hQ/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2130,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6OO7Hu,incubator-pegasus,2386276846,2130,NA,lsasta,27410399,,1247858701@qq.com,NA,2024-10-01T15:11:00Z,2024-10-01T15:11:00Z,"`                       
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  33:03 min
[INFO] Finished at: 2024-10-01T22:03:30+08:00
[INFO] ------------------------------------------------------------------------
[ERROR] Plugin org.apache.maven.plugins:maven-javadoc-plugin:3.1.1 or one of its dependencies could not be resolved: Could not transfer artifact org.apache.maven.plugins:maven-javadoc-plugin:jar:3.1.1 from/to central (https://repo.maven.apache.org/maven2): GET request of: org/apache/maven/plugins/maven-javadoc-plugin/3.1.1/maven-javadoc-plugin-3.1.1.jar from central failed: Read timed out -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginResolutionException
make[2]: *** [CMakeFiles/zookeeper.dir/build.make:117: Stamp/zookeeper/zookeeper-patch] Error 1
make[1]: *** [CMakeFiles/Makefile2:390: CMakeFiles/zookeeper.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....`","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6OO7Hu/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2130,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6SkPo5,incubator-pegasus,2458974777,2130,NA,ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,NA,2024-11-06T08:21:28Z,2024-11-06T08:21:28Z,"You can follow dockerfile to build.  https://github.com/apache/incubator-pegasus/blob/master/docker/pegasus-build-env/ubuntu2004/Dockerfile

Try update maven settings to build.
```shell
sudo cp /usr/share/maven/conf/settings.xml /usr/share/maven/conf/settings.xmlold
sudo vim /usr/share/maven/conf/settings.xml
``` 
```c++
<?xml version=""1.0"" encoding=""UTF-8""?>
<settings xmlns=""http://maven.apache.org/SETTINGS/1.0.0"" 
    xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" 
    xsi:schemaLocation=""http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"">

    <mirrors>
<mirror>
<id>nexus-tencentyun</id>
<mirrorOf>*</mirrorOf>
<name>Nexus tencent下·yun</name>
<url>http://mirrors.cloud.tencent.com/nexus/repository/maven-public/</url>
</mirror>
    </mirrors>
</settings>
``` ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6SkPo5/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2133,https://api.github.com/repos/apache/incubator-pegasus/issues/2133,incubator-pegasus,2594482867,2133,`Negotiation failed` java client after pegasus servers with authentication enabled were restarted,empiredan,743379,Dan Wang,,CLOSED,2024-10-17T11:48:16Z,2024-10-24T06:24:37Z,"After all servers (including both meta/replica servers) in a pegasus cluster with authentication enabled were restarted, the java client that had connected to them before would never succeed in connecting to the meta server due to `Negotiation failed` with  `ERR_TIMEOUT` for port **33391** as follows:
```
I1012 17:22:33.122887 28573 MetaSession.java:197] query meta got error, rpc error(ERR_OK), meta error(ERR_FORWARD_TO_OTHERS), forward address(rpc_address(10.1.136.90:8170)), current leader
(rpc_address(10.1.136.41:8170)), remain retry count(1), need switch leader(true), need delay(false)
I1012 17:22:33.123011 28571 ReplicaSession.java:189] rpc_address(10.1.136.90:8170): the session is disconnected, needs to reconnect
I1012 17:22:33.123739 28522 ReplicaSession.java:204] rpc_address(10.1.136.90:8170): start to async connect to target, wait channel to active
I1012 17:22:33.123829 28522 ReplicaSession.java:424] Channel [id: 0xea9a05e5, L:/10.1.136.26:33391 - R:/10.1.136.90:8170] for session rpc_address(10.1.136.90:8170) is active

......

E1012 17:22:43.134124 28520 Negotiation.java:85] Negotiation failed
Java exception follows:
com.xiaominew.infra.pegasus.rpc.ReplicationException: ERR_TIMEOUT
        at com.xiaominew.infra.pegasus.security.Negotiation$RecvHandler.run(Negotiation.java:81)
        at com.xiaominew.infra.pegasus.rpc.async.ReplicaSession.tryNotifyFailureWithSeqID(ReplicaSession.java:321)
        at com.xiaominew.infra.pegasus.rpc.async.ReplicaSession$4.run(ReplicaSession.java:367)
        at io.netty.util.concurrent.PromiseTask$RunnableAdapter.call(PromiseTask.java:38)
        at io.netty.util.concurrent.ScheduledFutureTask.run(ScheduledFutureTask.java:120)
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:399)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:464)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
I1012 17:22:43.134315 28520 ReplicaSession.java:138] channel to rpc_address(10.1.136.90:8170) closed
W1012 17:22:43.134387 28522 ReplicaSession.java:418] Channel [id: 0xea9a05e5, L:/10.1.136.26:33391 ! R:/10.1.136.90:8170] for session rpc_address(10.1.136.90:8170) is inactive
I1012 17:22:43.134445 28522 ReplicaSession.java:274] rpc_address(10.1.136.90:8170): mark the session to be disconnected from state=CONNECTED
```

corresponding logs of primary meta server is as follows:
```
D2024-10-12 17:22:33.123 (1728724953123732943 6bad)   meta.io-thrd.27565: server_negotiation.cpp:40:start(): SERVER_NEGOTIATION(CLIENT=10.1.136.26:33391): start negotiation
D2024-10-12 17:22:33.123 (1728724953123744766 6bad)   meta.io-thrd.27565: network.cpp:702:on_server_session_accepted(): server session accepted, remote_client = 10.1.136.26:33391, current_count = 6
D2024-10-12 17:22:33.123 (1728724953123748123 6bad)   meta.io-thrd.27565: network.cpp:707:on_server_session_accepted(): ip session increased, remote_client = 10.1.136.26:33391, current_count = 4
D2024-10-12 17:22:33.123 (1728724953123964597 6bad)   meta.io-thrd.27565: network.cpp:744:on_server_session_disconnected(): session 10.1.136.26:33391 disconnected, the total client sessions count remains 5
D2024-10-12 17:22:33.123 (1728724953123970218 6bad)   meta.io-thrd.27565: network.cpp:754:on_server_session_disconnected(): client ip 10.1.136.26:33391 has still 3 of sessions to this server
W2024-10-12 17:22:33.123 (1728724953123972893 6bad)   meta.io-thrd.27565: network.cpp:397:on_failure(): disconnect to remote {}, the socket will be lazily closed when the session destroyed
D2024-10-12 17:22:33.123 (1728724953123997959 6bbb)   meta.default7.01006bad00011326: negotiation_manager.cpp:376:get_negotiation(): negotiation was removed for msg: RPC_NEGOTIATION, 10.1.136.26:33391
```

Or `Negotiation failed` with  `ERR_SESSION_RESET` for port **33375** as follows:
```
I1012 17:22:23.114938 28573 MetaSession.java:197] query meta got error, rpc error(ERR_OK), meta error(ERR_FORWARD_TO_OTHERS), forward address(rpc_address(10.1.136.90:8170)), current leader
(rpc_address(10.1.136.41:8170)), remain retry count(4), need switch leader(true), need delay(false)
I1012 17:22:23.115047 28571 ReplicaSession.java:189] rpc_address(10.1.136.90:8170): the session is disconnected, needs to reconnect
I1012 17:22:23.115684 28572 ReplicaSession.java:204] rpc_address(10.1.136.90:8170): start to async connect to target, wait channel to active
I1012 17:22:23.115765 28572 ReplicaSession.java:424] Channel [id: 0xae9c99dd, L:/10.1.136.26:33375 - R:/10.1.136.90:8170] for session rpc_address(10.1.136.90:8170) is active

......

W1012 17:22:28.117709 28570 ReplicaSession.java:309] rpc_address(10.1.136.90:8170): actively close the session because it's not responding for 30 seconds
I1012 17:22:28.117861 28570 ReplicaSession.java:138] channel to rpc_address(10.1.136.90:8170) closed
W1012 17:22:28.117931 28572 ReplicaSession.java:418] Channel [id: 0xae9c99dd, L:/10.1.136.26:33375 ! R:/10.1.136.90:8170] for session rpc_address(10.1.136.90:8170) is inactive
I1012 17:22:28.118019 28570 MetaSession.java:197] query meta got error, rpc error(ERR_SESSION_RESET), meta error(ERR_UNKNOWN), forward address(null), current leader(rpc_address(10.1.136.90:8170)), remain retry count(3), need switch leader(true), need delay(false)
E1012 17:22:28.118207 28572 Negotiation.java:85] Negotiation failed
Java exception follows:
com.xiaominew.infra.pegasus.rpc.ReplicationException: ERR_SESSION_RESET
        at com.xiaominew.infra.pegasus.security.Negotiation$RecvHandler.run(Negotiation.java:81)
        at com.xiaominew.infra.pegasus.rpc.async.ReplicaSession.tryNotifyFailureWithSeqID(ReplicaSession.java:321)
        at com.xiaominew.infra.pegasus.rpc.async.ReplicaSession.markSessionDisconnect(ReplicaSession.java:266)
        at com.xiaominew.infra.pegasus.rpc.async.ReplicaSession$DefaultHandler.channelInactive(ReplicaSession.java:419)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:240)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:226)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)
        at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:379)
        at io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:344)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:240)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:226)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1299)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:240)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:226)
        at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:903)
        at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:768)
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:399)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:464)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
I1012 17:22:28.118362 28572 ReplicaSession.java:138] channel to rpc_address(10.1.136.90:8170) closed
I1012 17:22:28.118568 28572 ReplicaSession.java:274] rpc_address(10.1.136.90:8170): mark the session to be disconnected from state=CONNECTED
```

corresponding logs of primary meta server is as follows:
```
D2024-10-12 17:22:23.115 (1728724943115678962 6bad)   meta.io-thrd.27565: server_negotiation.cpp:40:start(): SERVER_NEGOTIATION(CLIENT=10.1.136.26:33375): start negotiation
D2024-10-12 17:22:23.115 (1728724943115698138 6bad)   meta.io-thrd.27565: network.cpp:702:on_server_session_accepted(): server session accepted, remote_client = 10.1.136.26:33375, current_count = 6
D2024-10-12 17:22:23.115 (1728724943115702576 6bad)   meta.io-thrd.27565: network.cpp:707:on_server_session_accepted(): ip session increased, remote_client = 10.1.136.26:33375, current_count = 4
D2024-10-12 17:22:23.115 (1728724943115923801 6bad)   meta.io-thrd.27565: network.cpp:744:on_server_session_disconnected(): session 10.1.136.26:33375 disconnected, the total client sessions count remains 5
D2024-10-12 17:22:23.115 (1728724943115929441 6bad)   meta.io-thrd.27565: network.cpp:754:on_server_session_disconnected(): client ip 10.1.136.26:33375 has still 3 of sessions to this server
W2024-10-12 17:22:23.115 (1728724943115959959 6bad)   meta.io-thrd.27565: network.cpp:397:on_failure(): disconnect to remote {}, the socket will be lazily closed when the session destroyed
D2024-10-12 17:22:23.115 (1728724943115988613 6bb7)   meta.default3.01006bad00011320: negotiation_manager.cpp:376:get_negotiation(): negotiation was removed for msg: RPC_NEGOTIATION, 10.1.136.26:33375
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2133/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2135,https://api.github.com/repos/apache/incubator-pegasus/issues/2135,incubator-pegasus,2596752672,2135,Workflows failed due to EOL of CentOS 7,empiredan,743379,Dan Wang,,CLOSED,2024-10-18T07:55:22Z,2025-01-19T07:41:26Z,"[BuildCompilationEnvDocker](https://github.com/apache/incubator-pegasus/actions/workflows/build-push-env-docker.yml) failed due to [EOL of CentOS 7](https://www.redhat.com/en/blog/centos-linux-has-reached-its-end-life-eol):

![image](https://github.com/user-attachments/assets/e3720c12-add6-431b-a8c9-ea443800f4d0)

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2135/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2135,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6QYf3V,incubator-pegasus,2422341077,2135,NA,shalk,2435781,shalk(xiao kun),,NA,2024-10-18T12:16:55Z,2024-10-18T12:16:55Z,"https://github.com/docker-library/official-images/pull/17094
remove the Dockerfile is a good chose","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6QYf3V/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2135,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6QkkOL,incubator-pegasus,2425504651,2135,NA,empiredan,743379,Dan Wang,,NA,2024-10-21T03:40:17Z,2024-10-21T03:40:17Z,"> [docker-library/official-images#17094](https://github.com/docker-library/official-images/pull/17094) remove the Dockerfile is a good chose

Thanks @shalk ! Actually I tried very hard to make [BuildCompilationEnvDocker](https://github.com/apache/incubator-pegasus/actions/workflows/build-push-env-docker.yml) work well, however it always failed since everything about centos 7 was not found:
![image](https://github.com/user-attachments/assets/2827fd76-c666-4f5a-8784-1c4c9efdcdeb)

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6QkkOL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2135,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6QlMtD,incubator-pegasus,2425670467,2135,NA,shalk,2435781,shalk(xiao kun),,NA,2024-10-21T06:07:08Z,2024-10-21T06:07:08Z,Let me have a try to fix it.,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6QlMtD/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2135,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Tecsb,incubator-pegasus,2474232603,2135,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-11-13T17:07:00Z,2024-11-13T17:07:00Z,"I've tried on my own develop enviroment, the centos:7.5.1804 image can be pulled and the packages can be installed sucessfully.
```
# laiyingchun @ laiyingchun in ~/dev/pegasus/docker/pegasus-build-env/centos7 on git:master o [0:59:49]
$ sudo docker build .
[+] Building 339.0s (6/9)                                                                                                                                                                                                                                        docker:default
 => [internal] load build definition from Dockerfile                                                                                                                                                                                                                       0.0s
 => => transferring dockerfile: 3.24kB                                                                                                                                                                                                                                     0.0s
 => [internal] load metadata for docker.io/library/centos:7.5.1804                                                                                                                                                                                                        18.6s
 => [internal] load .dockerignore                                                                                                                                                                                                                                          0.0s
 => => transferring context: 2B                                                                                                                                                                                                                                            0.0s
 => [1/6] FROM docker.io/library/centos:7.5.1804@sha256:7a45e4a1efbaafc1d9aa89925b6fdb33288a96d35ea0581412316e2f0ad3720a                                                                                                                                                   3.7s
 => => resolve docker.io/library/centos:7.5.1804@sha256:7a45e4a1efbaafc1d9aa89925b6fdb33288a96d35ea0581412316e2f0ad3720a                                                                                                                                                   0.0s
 => => sha256:65decb5f8c6d37cdd06332ef1116a92fdb52aa1b55fe6256bb3b843ee97d2279 529B / 529B                                                                                                                                                                                 0.0s
 => => sha256:cf49811e3cdb94cbdfd645f3888d7add06a315449cf2c7ca7b81c312f1e46c63 2.21kB / 2.21kB                                                                                                                                                                             0.0s
 => => sha256:5ad559c5ae16b8980924ceae7f7662d07740debd4467db19e69339926ec8f255 74.69MB / 74.69MB                                                                                                                                                                           2.5s
 => => sha256:7a45e4a1efbaafc1d9aa89925b6fdb33288a96d35ea0581412316e2f0ad3720a 319B / 319B                                                                                                                                                                                 0.0s
 => => extracting sha256:5ad559c5ae16b8980924ceae7f7662d07740debd4467db19e69339926ec8f255                                                                                                                                                                                  1.1s
 => [2/6] RUN sed -i s/mirror.centos.org/vault.centos.org/g /etc/yum.repos.d/*.repo &&     sed -i s/^#.*baseurl=http/baseurl=http/g /etc/yum.repos.d/*.repo &&     sed -i s/^mirrorlist=http/#mirrorlist=http/g /etc/yum.repos.d/*.repo &&     yum -y install centos-rel  97.0s
 => [3/6] RUN pip3 install --upgrade pip --no-cache-dir && pip3 install --no-cache-dir cmake -i https://pypi.tuna.tsinghua.edu.cn/simple                                                                                                                                   8.8s
 => [4/6] RUN wget --progress=dot:giga https://archive.apache.org/dist/maven/maven-3/3.8.3/binaries/apache-maven-3.8.3-bin.tar.gz -P /opt/maven     && cd /opt/maven     && tar -zxf apache-maven-3.8.3-bin.tar.gz     && rm apache-maven-3.8.3-bin.tar.gz               210.7s
```

Did I miss something?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Tecsb/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2135,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6TjgGW,incubator-pegasus,2475557270,2135,NA,shalk,2435781,shalk(xiao kun),,NA,2024-11-14T06:57:14Z,2024-11-14T06:57:14Z,"i know. The error message is contain aarch64. But centos 7.5.1804 not support aarch64  in sclo

https://vault.centos.org/7.5.1804/sclo/ .

I will remove  linux/arm64 from  github workflow","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6TjgGW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2135,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6VAEtW,incubator-pegasus,2499824470,2135,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-11-26T07:08:30Z,2024-11-26T07:08:30Z,"I found a new issue about CentOS7:
```
Run actions/checkout@v3
  with:
    repository: apache/incubator-pegasus
    token: ***
    ssh-strict: true
    persist-credentials: true
    clean: true
    sparse-checkout-cone-mode: true
    fetch-depth: 1
    fetch-tags: false
    lfs: false
    submodules: false
    set-safe-directory: true
  env:
    ONEBOX_OPTS: disk_min_available_space_ratio=5
    TEST_OPTS: disk_min_available_space_ratio=5;throttle_test_medium_value_kb=10;throttle_test_large_value_kb=[2](https://github.com/apache/incubator-pegasus/actions/runs/12022410210/job/33514582769?pr=2151#step:3:2)5
    USE_JEMALLOC: OFF
    BUILD_OPTIONS: -t debug --test --separate_servers
    PACK_OPTIONS: --separate_servers
    ACTIONS_RUNNER_FORCE_ACTIONS_NODE_VERSION: node16
    ACTIONS_ALLOW_USE_UNSECURE_NODE_VERSION: true
/usr/bin/docker exec  1[3](https://github.com/apache/incubator-pegasus/actions/runs/12022410210/job/33514582769?pr=2151#step:3:3)d0fa4f31cf181af078cd316753fde27974710f8cf1afcca97bade19da[4](https://github.com/apache/incubator-pegasus/actions/runs/12022410210/job/33514582769?pr=2151#step:3:4)26de sh -c ""cat /etc/*release | grep ^ID""
/__e/node20/bin/node: /lib64/libm.so.6: version `GLIBC_2.27' not found (required by /__e/node20/bin/node)
/__e/node20/bin/node: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by /__e/node20/bin/node)
/__e/node20/bin/node: /lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by /__e/node20/bin/node)
/__e/node20/bin/node: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /__e/node20/bin/node)
/__e/node20/bin/node: /lib64/libc.so.6: version `GLIBC_2.28' not found (required by /__e/node20/bin/node)
/__e/node20/bin/node: /lib64/libc.so.6: version `GLIBC_2.2[5](https://github.com/apache/incubator-pegasus/actions/runs/12022410210/job/33514582769?pr=2151#step:3:5)' not found (required by /__e/node20/bin/node)
```
https://github.com/apache/incubator-pegasus/actions/runs/12022410210/job/33514582769?pr=2151#step:3:1

It seems `actions/checkout@v3` requires higher `glibc`.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6VAEtW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2135,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6VAixL,incubator-pegasus,2499947595,2135,NA,shalk,2435781,shalk(xiao kun),,NA,2024-11-26T08:15:14Z,2024-11-26T08:15:14Z,"it look like https://github.com/apache/incubator-pegasus/pull/2072.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6VAixL/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2135,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6VbVjR,incubator-pegasus,2506971345,2135,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-11-29T02:20:58Z,2024-11-29T02:20:58Z,"Maybe it's time for us to consider almalinux or rockylinux.
https://github.com/apache/incubator-pegasus/pull/2137#issuecomment-2428150240

@empiredan @shalk ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6VbVjR/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2135,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6bBC8F,incubator-pegasus,2600742661,2135,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2025-01-19T07:41:23Z,2025-01-19T07:41:23Z,"CentOS in github action has been replaced by Rockylinux9, see https://github.com/apache/incubator-pegasus/pull/2179.

However, the Dockerfile is still kept now, you can refer to it when needed.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6bBC8F/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2136,https://api.github.com/repos/apache/incubator-pegasus/issues/2136,incubator-pegasus,2596753139,2136,Directory `hadoop-bin` failed to be moved while uploading artifact on Github,empiredan,743379,Dan Wang,,CLOSED,2024-10-18T07:55:32Z,2024-10-21T08:48:12Z,"While uploading artifact after pegasus was built an error occurred `mv: cannot move 'thirdparty/hadoop-bin' to './hadoop-bin': Directory not empty` as below:

![image](https://github.com/user-attachments/assets/afa4de2a-065f-40ac-8149-ccc519171d16)

The reason is that [config_hdfs.sh](https://github.com/apache/incubator-pegasus/blob/master/admin_tools/config_hdfs.sh) would also generate `hadoop-bin` while packaging server during building pegasus; therefore, as the next stage, uploading artifact would failed due to the existing `hadoop-bin` directory.

After this problem was fixed, to test the fix workflow [BuildCompilationEnvDocker](https://github.com/apache/incubator-pegasus/actions/workflows/build-push-env-docker.yml) was run on Github.  However it failed due to [EOL of CentOS 7](https://www.redhat.com/en/blog/centos-linux-has-reached-its-end-life-eol). This problem is traced by https://github.com/apache/incubator-pegasus/issues/2135.

Since [thirdparties-src image](https://github.com/apache/incubator-pegasus/blob/master/docker/thirdparties-src/Dockerfile) has been built only on CentOS 7 and many downstream workflows depend on it, we have to disable workflows based on CentOS 7 temporarily and build [thirdparties-src image](https://github.com/apache/incubator-pegasus/blob/master/docker/thirdparties-src/Dockerfile) on Ubuntu to ensure that workflows based on it could work well.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2136/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2146,https://api.github.com/repos/apache/incubator-pegasus/issues/2146,incubator-pegasus,2660578377,2146,Full duplication got stuck in `DS_PREPARE` due to some error after sending request to create the follower table on target cluster,empiredan,743379,Dan Wang,,CLOSED,2024-11-15T03:05:15Z,2024-11-15T04:10:44Z,"Firstly, I created a new duplication, and related logs of primary meta server of source cluster could be found as follows:

```
D2024-10-22 10:37:21.642 (1729564641642000641 0088)   meta.meta_state0.0102000500000411: meta_duplication_service.cpp:183:add_duplication(): add duplication for app(zgy_test_dup), remote cluster name is target_cluster, remote app name is wetry_db.zgy_test_dup, remote replica count is 3
D2024-10-22 10:37:21.644 (1729564641644158230 0088)   meta.meta_state0.01030000000000dd: meta_duplication_service.cpp:324:operator()(): [a27d1729564641] add duplication successfully [app_name: zgy_test_dup, remote_cluster_name: target_cluster, remote_app_name: wetry_db.zgy_test_dup]
D2024-10-22 10:37:21.644 (1729564641644182306 0088)   meta.meta_state0.01030000000000dd: duplication_info.cpp:172:persist_status(): [a27d1729564641] change duplication status from DS_INIT to DS_PREPARE successfully [app_id: 27]
W2024-10-22 10:37:21.827 (1729564641827424045 0088)   meta.meta_state0.01020003000003dd: meta_duplication_service.cpp:172:all_checkpoint_has_prepared(): replica checkpoint still running: 0/4
D2024-10-22 10:37:21.827 (1729564641827543128 0088)   meta.meta_state0.01020003000003dd: duplication_info.cpp:203:report_progress_if_time_up(): duplication report: {""create_ts"":""2024-10-22 10:37:21"",""dupid"":1729564641,""fail_mode"":""FAIL_SLOW"",""progress"":{""0"":-1,""1"":-1,""2"":-1,""3"":-1},""remote"":""target_cluster"",""remote_app_name"":""wetry_db.zgy_test_dup"",""remote_replica_count"":3,""status"":""DS_PREPARE""}
```

However, it seems that the duplication got stuck in `DS_PREPARE` and never forwarded to next status. To find the reason, some error logs primary meta server of source cluster could be found as follows:

```
E2024-10-22 10:38:06.829 (1729564686829455037 0088)   meta.meta_state0.01030000000000e6: meta_duplication_service.cpp:502:operator()(): create follower app[target_cluster.wetry_db.zgy_test_dup] to trigger duplicate checkpoint failed: duplication_status = DS_PREPARE, create_err = ERR_TIMEOUT, update_err = ERR_NO_NEED_OPERATE
......
E2024-10-22 10:38:11.833 (1729564691833345364 0088)   meta.meta_state0.01030000000000e8: meta_duplication_service.cpp:502:operator()(): create follower app[target_cluster.wetry_db.zgy_test_dup] to trigger duplicate checkpoint failed: duplication_status = DS_PREPARE, create_err = ERR_APP_EXIST, update_err = ERR_NO_NEED_OPERATE
......
E2024-10-22 13:23:02.220 (1729574582220478204 0088)   meta.meta_state0.01030000000008a2: meta_duplication_service.cpp:502:operator()(): create follower app[target_cluster.wetry_db.zgy_test_dup] to trigger duplicate checkpoint failed: duplication_status = DS_PREPARE, create_err = ERR_APP_EXIST, update_err = ERR_NO_NEED_OPERATE
```

Some error occurred while creating table on follower cluster, and the primary meta server of source cluster did not receive successful response. `ERR_APP_EXIST` means table has been created on the follower cluster, and the source cluster could not send the same request to create the table again.

On the follower cluster, the ID of the created table is `4672`, and the primary meta server of target cluster sent the proposal to proposal to the primary replica of each partition of the table:

```
D2024-10-22 13:22:56.98 (1729574576098497750 2263d3)   meta.meta_state0.01010000000009b9: server_state.cpp:1381:send_proposal(): send proposal replication::config_type::CT_ASSIGN_PRIMARY for gpid(4672.0), ballot = 0, target = 10.2.131.174:8171, node = 10.2.131.174:8171
D2024-10-22 13:22:56.98 (1729574576098581429 2263d3)   meta.meta_state0.01010000000009b9: server_state.cpp:1381:send_proposal(): send proposal replication::config_type::CT_ASSIGN_PRIMARY for gpid(4672.1), ballot = 0, target = 10.2.131.173:8171, node = 10.2.131.173:8171
D2024-10-22 13:22:56.98 (1729574576098612769 2263d3)   meta.meta_state0.01010000000009b9: server_state.cpp:1381:send_proposal(): send proposal replication::config_type::CT_ASSIGN_PRIMARY for gpid(4672.2), ballot = 0, target = 10.2.131.172:8171, node = 10.2.131.172:8171
D2024-10-22 13:22:56.98 (1729574576098655070 2263d3)   meta.meta_state0.01010000000009b9: server_state.cpp:1381:send_proposal(): send proposal replication::config_type::CT_ASSIGN_PRIMARY for gpid(4672.3), ballot = 0, target = 10.2.131.170:8171, node = 10.2.131.170:8171
```

The replica server in charge of the primary replica also received the proposal. However, it failed to connect to the replica server of source cluster to get the checkpoint data(actually there is some security policy on the source cluster which would ignore the packaged from other clusters):
 
```
D2024-10-22 13:22:56.98 (1729574576098611769 2d5346) replica.replica9.020053150cc06bae: replica_stub.cpp:1028:on_config_proposal(): 4672.1@10.2.131.173:8171: received config proposal replication::config_type::CT_ASSIGN_PRIMARY for [10.2.131.173:8171](https://10.2.131.173:8171/)
D2024-10-22 13:22:56.99 (1729574576099024706 2d5371) replica.rep_long2.02040009060b292b: fs_manager.cpp:273:allocate_dir(): 10.2.131.173:8171: put pid(4672.1) to dir(data0), which has 0 replicas of current app, 10762 replicas totally
D2024-10-22 13:22:56.99 (1729574576099271963 2d5371) replica.rep_long2.02040009060b292b: replica_follower.cpp:83:duplicate_checkpoint(): [4672.1@10.2.131.173:8171] start duplicate master[source_cluster.zgy_test_dup] checkpoint
D2024-10-22 13:22:56.99 (1729574576099314314 2d5323) replica.default6.020600020000babc: replica_follower.cpp:109:async_duplicate_checkpoint_from_master_replica(): [4672.1@10.2.131.173:8171] query master[source_cluster.zgy_test_dup] replica configuration
......
E2024-10-22 13:23:01.99 (1729574581099441055 2d5324) replica.default7.020100060717fc1e: replica_follower.cpp:138:update_master_replica_config(): [4672.1@10.2.131.173:8171] query master[source_cluster.zgy_test_dup] config failed: ERR_TIMEOUT
E2024-10-22 13:23:01.99 (1729574581099538649 2d5371) replica.rep_long2.02040009060b292b: replica_stub.cpp:2320:new_replica(): 4672.1@10.2.131.173:8171: try to duplicate replica checkpoint failed, error(ERR_TRY_AGAIN) and please check previous detail error log
D2024-10-22 13:23:01.99 (1729574581099608162 2d5371) replica.rep_long2.02040009060b292b: replica.cpp:546:close(): 4672.1@10.2.131.173:8171: replica closed, time_used = 0ms
D2024-10-22 13:23:01.99 (1729574581099614989 2d5371) replica.rep_long2.02040009060b292b: replica.cpp:546:close(): 4672.1@10.2.131.173:8171: replica closed, time_used = 0ms
D2024-10-22 13:23:01.99 (1729574581099665868 2d5371) replica.rep_long2.02040009060b292b: fs_manager.cpp:295:remove_replica(): 10.2.131.173:8171: remove gpid(4672.1) from dir(data0)
D2024-10-22 13:23:01.99 (1729574581099673613 2d5371) replica.rep_long2.02040009060b292b: replica_stub.cpp:2259:open_replica(): 4672.1@10.2.131.173:8171: open replica failed, erase from opening replicas
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2146/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2147,https://api.github.com/repos/apache/incubator-pegasus/issues/2147,incubator-pegasus,2670461809,2147,make: *** [all] Error 2,yokie121,13161820,,,OPEN,2024-11-19T01:39:22Z,2024-11-26T07:07:05Z,"## General Question

incubator-pegasus-2.5
centos7 
# gcc --version
gcc (GCC) 13.3.0

# gcc --version
gcc (GCC) 13.3.0

源码编译报错如下，请求帮助
[ 80%] Performing install step for 'boost'
[ 80%] Completed 'boost'
[ 80%] Built target boost
make: *** [all] Error 2","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2147/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2147,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6ULGNk,incubator-pegasus,2485936996,2147,NA,ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,NA,2024-11-19T14:53:11Z,2024-11-19T14:53:11Z,"First, follow this [link](https://github.com/apache/incubator-pegasus/blob/master/docker/pegasus-build-env/ubuntu2004/Dockerfile) to build the environment. If the build is successful, proceed to build incubator-pegasus-2.5 using CentOS 7.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6ULGNk/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2147,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6VAEOp,incubator-pegasus,2499822505,2147,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-11-26T07:07:04Z,2024-11-26T07:07:04Z,@yokie121 Has this issue been resolved?,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6VAEOp/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2149,https://api.github.com/repos/apache/incubator-pegasus/issues/2149,incubator-pegasus,2678158442,2149,Meta server with authentication enabled failed and could not be started normally after dropping table,empiredan,743379,Dan Wang,,OPEN,2024-11-21T06:08:31Z,2025-01-20T03:05:20Z,"There were a pegasus cluster with 3 meta servers and 5 replica servers. The authentication was enabled. And a script is written to drop a great number of tables.

While the script was being executed, the meta server failed with nothing but `got signal id: 11` and following `dmesg`:

```
[Tue Nov 12 15:32:39 2024]  meta.meta_stat[681978]: segfault at 40 ip 00007faa351ea839 sp 00007faa0d48abc0 error 4 in libdsn_utils.so[7faa35124000+115000]
[Tue Nov 12 15:32:39 2024] Code: 23 f9 ff 0f 1f 40 00 66 2e 0f 1f 84 00 00 00 00 00 55 48 89 e5 41 57 41 56 41 55 41 54 48 8d 45 cf 53 4c 8d 67 08 48 83 ec 28 <4c> 8b 7f 18 48 89 45 b8 48 8d 45 ce 4d 39 e7 48 89 45 b0 74 6c 48
```

In the logs of failed meta server (namely primary meta server), lots of errors are also found: 

```
E2024-11-12 15:32:45.711 (1731396765711870108 a67f5)   meta.meta_server4.01010000000009fa: ranger_resource_policy_manager.cpp:641:sync_policies_to_app_envs(): ERR_INVALID_PARAMETERS: set_app_envs failed.
E2024-11-12 15:32:45.713 (1731396765713890084 a67f5)   meta.meta_server4.01010000000009fa: ranger_resource_policy_manager.cpp:304:update_policies_from_ranger_service(): ERR_INVALID_PARAMETERS: Sync policies to app envs failed.
E2024-11-12 15:32:52.225 (1731396772225348205 a67f6)   meta.meta_server5.01010000000009fa: ranger_resource_policy_manager.cpp:641:sync_policies_to_app_envs(): ERR_INVALID_PARAMETERS: set_app_envs failed.
E2024-11-12 15:32:52.226 (1731396772226529887 a67f6)   meta.meta_server5.01010000000009fa: ranger_resource_policy_manager.cpp:304:update_policies_from_ranger_service(): ERR_INVALID_PARAMETERS: Sync policies to app envs failed.
E2024-11-12 15:32:58.919 (1731396778919427343 a67f3)   meta.meta_server2.01010000000009fa: ranger_resource_policy_manager.cpp:641:sync_policies_to_app_envs(): ERR_INVALID_PARAMETERS: set_app_envs failed.
E2024-11-12 15:32:58.921 (1731396778921276545 a67f3)   meta.meta_server2.01010000000009fa: ranger_resource_policy_manager.cpp:304:update_policies_from_ranger_service(): ERR_INVALID_PARAMETERS: Sync policies to app envs failed.
E2024-11-12 15:33:06.374 (1731396786374687523 a67f6)   meta.meta_server5.01010000000009fa: ranger_resource_policy_manager.cpp:641:sync_policies_to_app_envs(): ERR_INVALID_PARAMETERS: set_app_envs failed.
E2024-11-12 15:33:06.376 (1731396786376019669 a67f6)   meta.meta_server5.01010000000009fa: ranger_resource_policy_manager.cpp:304:update_policies_from_ranger_service(): ERR_INVALID_PARAMETERS: Sync policies to app envs failed.
E2024-11-12 15:33:14.775 (1731396794775332362 a67f2)   meta.meta_server1.01010000000009fa: ranger_resource_policy_manager.cpp:641:sync_policies_to_app_envs(): ERR_INVALID_PARAMETERS: set_app_envs failed.
E2024-11-12 15:33:14.777 (1731396794777299007 a67f2)   meta.meta_server1.01010000000009fa: ranger_resource_policy_manager.cpp:304:update_policies_from_ranger_service(): ERR_INVALID_PARAMETERS: Sync policies to app envs failed.
E2024-11-12 15:33:30.679 (1731396810679840313 a67f3)   meta.meta_server2.01010000000009fa: ranger_resource_policy_manager.cpp:641:sync_policies_to_app_envs(): ERR_INVALID_PARAMETERS: set_app_envs failed.
E2024-11-12 15:33:30.681 (1731396810681638580 a67f3)   meta.meta_server2.01010000000009fa: ranger_resource_policy_manager.cpp:304:update_policies_from_ranger_service(): ERR_INVALID_PARAMETERS: Sync policies to app envs failed.
E2024-11-12 15:33:37.501 (1731396817501816052 a67f7)   meta.meta_server6.01010000000009fa: ranger_resource_policy_manager.cpp:641:sync_policies_to_app_envs(): ERR_INVALID_PARAMETERS: set_app_envs failed.
E2024-11-12 15:33:37.503 (1731396817503027320 a67f7)   meta.meta_server6.01010000000009fa: ranger_resource_policy_manager.cpp:304:update_policies_from_ranger_service(): ERR_INVALID_PARAMETERS: Sync policies to app envs failed.
E2024-11-12 15:33:44.338 (1731396824338693868 a67f4)   meta.meta_server3.01010000000009fa: ranger_resource_policy_manager.cpp:641:sync_policies_to_app_envs(): ERR_INVALID_PARAMETERS: set_app_envs failed.
E2024-11-12 15:33:44.339 (1731396824339976731 a67f4)   meta.meta_server3.01010000000009fa: ranger_resource_policy_manager.cpp:304:update_policies_from_ranger_service(): ERR_INVALID_PARAMETERS: Sync policies to app envs failed.
```

After that, other standby meta servers also failed while they tried to take over. See following logs:
 
```
E2024-11-12 15:34:33.624 (1731396873624300621 19c265)   meta.meta_server0.010200030000042e: server_state.cpp:689:operator()(): assertion expression: false
F2024-11-12 15:34:33.624 (1731396873624310529 19c265)   meta.meta_server0.010200030000042e: server_state.cpp:689:operator()(): invalid status(app_status::AS_DROPPING) for app(abc(1)) in remote storage
```

`AS_DROPPING` was found persistent on the remote meta storage (namely ZooKeeper) as the status of the table. 

```
{""status"":""app_status::AS_DROPPING"",""app_type"":""pegasus"",""app_name"":""abc"",""app_id"":1,""partition_count"":8, ...}
```

However, this state is just an intermediate state, which should not be found on `ZooKeeper`.

Then, all meta server were never be started normally: they exited immediately after they were started.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2149/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2149,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Ubyfz,incubator-pegasus,2490312691,2149,NA,empiredan,743379,Dan Wang,,NA,2024-11-21T07:58:35Z,2024-11-21T07:58:35Z,"There are two problems that should be solved:

1. why the primary meta server failed with `segfault` while dropping tables ?
2. why all meta servers were never be restarted normally after the primary meta server failed ?

To illustrate the reasons for both problems more clearly, I'll put here some mechanisms about updating meta data. A pegasus cluster would flush security policies to remote meta storage periodically (by `update_ranger_policy_interval_sec`) in the form of environment variables. We do this by `server_state::set_app_envs()`. However, after updating the meta data on the remote storage (namely ZooKeeper), the table is not checked that if it still exists while updating environment variables of local memory. See the following code:

```C++
void server_state::set_app_envs(const app_env_rpc &env_rpc)
{

...

    do_update_app_info(app_path, ainfo, [this, app_name, keys, values, env_rpc](error_code ec) {
        CHECK_EQ_MSG(ec, ERR_OK, ""update app info to remote storage failed"");

        zauto_write_lock l(_lock);
        std::shared_ptr<app_state> app = get_app(app_name);
        std::string old_envs = dsn::utils::kv_map_to_string(app->envs, ',', '=');
        for (int idx = 0; idx < keys.size(); idx++) {
            app->envs[keys[idx]] = values[idx];
        }
        std::string new_envs = dsn::utils::kv_map_to_string(app->envs, ',', '=');
        LOG_INFO(""app envs changed: old_envs = {}, new_envs = {}"", old_envs, new_envs);
    });
}
```

In `std::string old_envs = dsn::utils::kv_map_to_string(app->envs, ',', '=');`, since `app` is `nullptr`, `app->envs` would point an invalid address, leading to `segfault` in `libdsn_utils.so` where `dsn::utils::kv_map_to_string` is.

Therefore, the reason for the 1st problem is very clear: the callback for updating meta data on remove storage is called immediately after the table is removed, and an invalid address is accessed due to null pointer.

Then, the meta server would load meta data from remote storage after it is restart. However,  the intermediate status `AS_DROPPING` is also flushed to remote storage with security policies since all meta data for a table is a unitary `json` object: the whole `json` would be set to remote storage once any property is updated. However `AS_DROPPING` is invalid, and cannot pass the assertion which would make meta server fail again and again, which is the reason of the 2nd problem. See following code: 

```C++
server_state::sync_apps_from_remote_storage()
{

...

                    std::shared_ptr<app_state> app = app_state::create(info);
                    {
                        zauto_write_lock l(_lock);
                        _all_apps.emplace(app->app_id, app);
                        if (app->status == app_status::AS_AVAILABLE) {
                            app->status = app_status::AS_CREATING;
                            _exist_apps.emplace(app->app_name, app);
                            _table_metric_entities.create_entity(app->app_id, app->partition_count);
                        } else if (app->status == app_status::AS_DROPPED) {
                            app->status = app_status::AS_DROPPING;
                        } else {
                            CHECK(false,
                                  ""invalid status({}) for app({}) in remote storage"",
                                  enum_to_string(app->status),
                                  app->get_logname());
                        }
                    }

...

}
```
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Ubyfz/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2149,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6ZCotu,incubator-pegasus,2567605102,2149,NA,empiredan,743379,Dan Wang,,NA,2025-01-02T11:06:58Z,2025-01-02T11:06:58Z,"Immediately after table is dropped, actually meta server would failed not only while setting environments, but also some other operations, such as deleting environments. All of the fixes would be listed here:
- [x] https://github.com/apache/incubator-pegasus/pull/2148
- [x] https://github.com/apache/incubator-pegasus/pull/2170
- [x] https://github.com/apache/incubator-pegasus/pull/2181","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6ZCotu/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2152,https://api.github.com/repos/apache/incubator-pegasus/issues/2152,incubator-pegasus,2692888297,2152,"Bug(java client): When using batch, a null pointer exception may occur",lengyuexuexuan,46274877,,,OPEN,2024-11-26T02:57:33Z,2024-11-26T02:57:33Z,"## Bug Report


1. What did you do?
![小米办公20241126-105239](https://github.com/user-attachments/assets/525500aa-6e72-4de5-b254-0a267982aac7)
in function ""commitWaitAllComplete"", a null pointer execption occur.

2. What did you see instead?
https://github.com/apache/incubator-pegasus/blob/f8de6dabd2bf96a8eafa6c23136ba1776540556d/java-client/src/main/java/org/apache/pegasus/client/FutureGroup.java#L115-L118
Add a judge which judge the fu.cause() is null before using  it.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2152/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2155,https://api.github.com/repos/apache/incubator-pegasus/issues/2155,incubator-pegasus,2697286029,2155,The image of compilation environment failed to be built in that thrift 0.11.0 tarball was removed from Github,empiredan,743379,Dan Wang,,CLOSED,2024-11-27T06:33:10Z,2024-11-28T09:05:36Z,"[The image of compilation environment](https://github.com/apache/incubator-pegasus/actions/workflows/build-push-env-docker.yml) for [master branch](https://github.com/apache/incubator-pegasus) failed to be built for the reason that [thrift 0.11.0 tarball](https://github.com/apache/thrift/releases?page=2) was removed from Github:
 
```
#10 [6/7] RUN wget --progress=dot:giga https://github.com/apache/thrift/archive/refs/tags/0.11.0.tar.gz -P /opt/thrift &&     cd /opt/thrift && tar xzf 0.11.0.tar.gz && cd thrift-0.11.0 && ./bootstrap.sh &&     ./configure --enable-libs=no &&     make -j$(($(nproc)/2+1)) && make install && cd - &&     rm -rf thrift-0.11.0 0.11.0.tar.gz
#10 0.101 --2024-11-27 04:53:45--  https://github.com/apache/thrift/archive/refs/tags/0.11.0.tar.gz
#10 0.102 Resolving github.com (github.com)... 140.82.113.4
#10 0.104 Connecting to github.com (github.com)|140.82.113.4|:443... connected.
#10 0.118 HTTP request sent, awaiting response... 302 Found
#10 0.186 Location: https://codeload.github.com/apache/thrift/tar.gz/0.11.0/tags/0.11.0 [following]
#10 0.186 --2024-11-27 04:53:45--  https://codeload.github.com/apache/thrift/tar.gz/0.11.0/tags/0.11.0
#10 0.186 Resolving codeload.github.com (codeload.github.com)... 140.82.112.9
#10 0.188 Connecting to codeload.github.com (codeload.github.com)|140.82.112.9|:443... connected.
#10 0.201 HTTP request sent, awaiting response... 404 Not Found
#10 0.213 2024-11-27 04:53:45 ERROR 404: Not Found.
#10 0.213 
#10 ERROR: process ""/bin/sh -c wget --progress=dot:giga https://github.com/apache/thrift/archive/refs/tags/0.11.0.tar.gz -P /opt/thrift &&     cd /opt/thrift && tar xzf 0.11.0.tar.gz && cd thrift-0.11.0 && ./bootstrap.sh &&     ./configure --enable-libs=no &&     make -j$(($(nproc)/2+1)) && make install && cd - &&     rm -rf thrift-0.11.0 0.11.0.tar.gz"" did not complete successfully: exit code: 8
------
 > [6/7] RUN wget --progress=dot:giga https://github.com/apache/thrift/archive/refs/tags/0.11.0.tar.gz -P /opt/thrift &&     cd /opt/thrift && tar xzf 0.11.0.tar.gz && cd thrift-0.11.0 && ./bootstrap.sh &&     ./configure --enable-libs=no &&     make -j$(($(nproc)/2+1)) && make install && cd - &&     rm -rf thrift-0.11.0 0.11.0.tar.gz:
0.101 --2024-11-27 04:53:45--  https://github.com/apache/thrift/archive/refs/tags/0.11.0.tar.gz
404 Not Found
0.213 2024-11-27 04:53:45 ERROR 404: Not Found.
0.213 
------

 1 warning found (use docker --debug to expand):
 - UndefinedVar: Usage of undefined variable '$LD_LIBRARY_PATH' (line 73)
Dockerfile:64
--------------------
  63 |     
  64 | >>> RUN wget --progress=dot:giga https://github.com/apache/thrift/archive/refs/tags/0.11.0.tar.gz -P /opt/thrift && \
  65 | >>>     cd /opt/thrift && tar xzf 0.11.0.tar.gz && cd thrift-0.11.0 && ./bootstrap.sh && \
  66 | >>>     ./configure --enable-libs=no && \
  67 | >>>     make -j$(($(nproc)/2+1)) && make install && cd - && \
  68 | >>>     rm -rf thrift-0.11.0 0.11.0.tar.gz
  69 |     
--------------------
ERROR: failed to solve: process ""/bin/sh -c wget --progress=dot:giga https://github.com/apache/thrift/archive/refs/tags/0.11.0.tar.gz -P /opt/thrift &&     cd /opt/thrift && tar xzf 0.11.0.tar.gz && cd thrift-0.11.0 && ./bootstrap.sh &&     ./configure --enable-libs=no &&     make -j$(($(nproc)/2+1)) && make install && cd - &&     rm -rf thrift-0.11.0 0.11.0.tar.gz"" did not complete successfully: exit code: 8
Reference
  builder-3dc5e488-9652-4c7e-8ae4-c5673e9020b4/builder-3dc5e488-9652-4c7e-8ae4-c5673e9020b40/x8333hvf69025j8fg2oi5g041
Check build summary support
  Build summary supported!
Error: buildx failed with: ERROR: failed to solve: process ""/bin/sh -c wget --progress=dot:giga https://github.com/apache/thrift/archive/refs/tags/0.11.0.tar.gz -P /opt/thrift &&     cd /opt/thrift && tar xzf 0.11.0.tar.gz && cd thrift-0.11.0 && ./bootstrap.sh &&     ./configure --enable-libs=no &&     make -j$(($(nproc)/2+1)) && make install && cd - &&     rm -rf thrift-0.11.0 0.11.0.tar.gz"" did not complete successfully: exit code: 8
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2155/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2156,https://api.github.com/repos/apache/incubator-pegasus/issues/2156,incubator-pegasus,2698418451,2156,Bug(Bulkload) Sometime ingest will hang when encountering write throttling.,ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,CLOSED,2024-11-27T12:44:01Z,2024-12-13T03:29:25Z,"## Bug Report
Sometime ingest will hang when encountering write throttling.   

![image](https://github.com/user-attachments/assets/45c1aac3-be36-405b-8d0e-a98287d41dd2)
![image](https://github.com/user-attachments/assets/48086bd3-37f8-4d0d-ab61-d5d22abe79b7)


1. What did you do?
When i find ingest hang, i can use `cancel_bulk_load -f` and `clear_bulk_load` to fix this problem.

4. What version of Pegasus are you using?
2.4.7-without-slog, all version has this problem.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2156/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2160,https://api.github.com/repos/apache/incubator-pegasus/issues/2160,incubator-pegasus,2710799707,2160,refactor: support close admin-cli client,ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,CLOSED,2024-12-02T07:08:40Z,2025-01-08T09:29:07Z,"## Feature Request
**Is your feature request related to a problem? Please describe:**
I found that the current admin CLI client does not provide an interface to close replica server TCP connections.  And the `QueryAllNodesDiskInfo`  function is difficult to use.  `sendQueryDiskInfoRequest` function is private.

**Describe the feature you'd like:**
1. Support close admin-cli client.
2. Refactor `QueryAllNodesDiskInfo` function.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2160/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2168,https://api.github.com/repos/apache/incubator-pegasus/issues/2168,incubator-pegasus,2735755019,2168,Validation for command failed if there is no extra positional argument,empiredan,743379,Dan Wang,,CLOSED,2024-12-12T12:23:18Z,2024-12-13T06:21:02Z,"Validation for command failed if there is no extra positional argument:

```
>>> dups
ERROR: ERR_INVALID_PARAMETERS: there shouldn't be any positional arguments
USAGE: 	dups                    [-a|--app_name_pattern str] [-m|--match_type str]
	                        [-p|--list_partitions] [-g|--progress_gap num] [-u|--show_unfinishd]
	                        [-o|--output file_name] [-j|--json]
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2168/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2171,https://api.github.com/repos/apache/incubator-pegasus/issues/2171,incubator-pegasus,2746586920,2171,remove zookeeper from Overall architecture?,jcy1001,24226340,,,OPEN,2024-12-18T02:30:29Z,2024-12-29T07:55:48Z,"## Feature Request

remove zookeeper from Overall architecture?  ","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2171/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2171,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Y3VyF,incubator-pegasus,2564643973,2171,NA,acelyc111,10775040,Yingchun Lai,laiyingchun@apache.org,NA,2024-12-29T07:55:47Z,2024-12-29T07:55:47Z,"@jcy1001 It's a good idea, we have ever considered about it, but there are some some components depends on the zookeeper implementation. Are you interesting to this work?","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6Y3VyF/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2173,https://api.github.com/repos/apache/incubator-pegasus/issues/2173,incubator-pegasus,2763124518,2173,feat: support reset dead partition,ruojieranyishen,93246280,Pengfan Lu,lu_peng_fan@163.com,OPEN,2024-12-30T12:15:46Z,2025-01-03T06:19:50Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
When data disk damage causes all three replicas to be unavailable, most of Pegasus's functions—such as backup, bulk load, and duplication—will not be usable. Therefore, a fallback strategy is needed to address this situation. 

**Describe the feature you'd like:**
<!-- A clear and concise description of what you want to happen. -->
By resetting the dead paritions to abandon some data, you can trick the meta server into allowing functions like backup to proceed normally.


**Mainly two parts:**
1. Reset the partition configuration on the meta server and zookeeper.
2. Reserve the dead partition data on the target replica server.","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2173/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2183,https://api.github.com/repos/apache/incubator-pegasus/issues/2183,incubator-pegasus,2798088979,2183,feat: update load_from_private_log of duplication task priority from LOW to COMMON,ninsmiracle,110282526,,,OPEN,2025-01-20T03:29:13Z,2025-01-20T06:46:14Z,"## Feature Request

**Is your feature request related to a problem? Please describe:**
When I test the performance bottleneck of duplication, I feel strange about the task priority of  `load_from_private_log stage`.
`load_from_private_log` is the main time-consuming stage that generates IO, and is handled by the THREAD_POOL_REPLICATION_LONG thread. The priority of the dup-related tasks is also defined as LOW.
We can see all the RPC heandled by THREAD_POOL_REPLICATION_LONG:
```
// THREAD_POOL_REPLICATION_LONG
#define CURRENT_THREAD_POOL THREAD_POOL_REPLICATION_LONG
MAKE_EVENT_CODE(LPC_LEARN_REMOTE_DELTA_FILES, TASK_PRIORITY_COMMON)
MAKE_EVENT_CODE_AIO(LPC_REPLICATION_COPY_REMOTE_FILES, TASK_PRIORITY_COMMON)
MAKE_EVENT_CODE(LPC_GARBAGE_COLLECT_LOGS_AND_REPLICAS, TASK_PRIORITY_COMMON)
MAKE_EVENT_CODE(LPC_OPEN_REPLICA, TASK_PRIORITY_COMMON)
MAKE_EVENT_CODE(LPC_CLOSE_REPLICA, TASK_PRIORITY_COMMON)
MAKE_EVENT_CODE(LPC_CHECKPOINT_REPLICA, TASK_PRIORITY_COMMON)
MAKE_EVENT_CODE(LPC_CATCHUP_WITH_PRIVATE_LOGS, TASK_PRIORITY_COMMON)
MAKE_EVENT_CODE(LPC_DISK_STAT, TASK_PRIORITY_COMMON)
MAKE_EVENT_CODE(LPC_BACKGROUND_COLD_BACKUP, TASK_PRIORITY_COMMON)
MAKE_EVENT_CODE(LPC_PARTITION_SPLIT_ASYNC_LEARN, TASK_PRIORITY_COMMON)
MAKE_EVENT_CODE(LPC_REPLICATION_LONG_LOW, TASK_PRIORITY_LOW)
MAKE_EVENT_CODE(LPC_REPLICATION_LONG_COMMON, TASK_PRIORITY_COMMON)
MAKE_EVENT_CODE(LPC_REPLICATION_LONG_HIGH, TASK_PRIORITY_HIGH)
#undef CURRENT_THREAD_POOL
```

  In fact, the Pegasus users always requires real-time dup. If the delay of the dup process of a piece of data is too high, when the master cluster down, the backup cluster will be unable to completely replace all the data of the master cluster to provide services.
  And I do not think `LPC_DISK_STAT`, `LPC_GARBAGE_COLLECT_LOGS_AND_REPLICAS` task is importanter than duplicate.



**Describe the feature you'd like:**
I'd like to raise the priority of `load_from_private_log` when doing duplication.
","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2183/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2188,https://api.github.com/repos/apache/incubator-pegasus/issues/2188,incubator-pegasus,2803472386,2188,Workflow for building java client failed in that `package org.apache.pegasus.utils` does not exist,empiredan,743379,Dan Wang,,CLOSED,2025-01-22T06:06:29Z,2025-01-22T07:09:52Z,"Workflow for building java client failed while errors occurred as follows:

```
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/duplication_list_request.java:[25,33] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/duplication_list_request.java:[109,28] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/duplication_list_request.java:[166,33] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/duplication_list_request.java:[174,72] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/configuration_list_apps_request.java:[31,33] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/configuration_list_apps_request.java:[219,33] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/configuration_list_apps_request.java:[227,79] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/configuration_list_apps_request.java:[115,116] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/configuration_list_apps_request.java:[269,47] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/configuration_list_apps_request.java:[518,58] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/configuration_list_apps_request.java:[610,52] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/duplication_list_request.java:[99,116] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/duplication_list_request.java:[208,47] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/duplication_list_request.java:[409,58] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/duplication_list_request.java:[482,52] error: package org.apache.pegasus.utils does not exist
[INFO] 15 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  15.552 s
[INFO] Finished at: 2025-01-21T23:43:53+08:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.10.1:compile (default-compile) on project pegasus-client: Compilation failure: Compilation failure: 
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/duplication_list_request.java:[25,33] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/duplication_list_request.java:[109,28] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/duplication_list_request.java:[166,33] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/duplication_list_request.java:[174,72] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/configuration_list_apps_request.java:[31,33] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/configuration_list_apps_request.java:[219,33] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/configuration_list_apps_request.java:[227,79] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/configuration_list_apps_request.java:[115,116] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/configuration_list_apps_request.java:[269,47] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/configuration_list_apps_request.java:[518,58] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/configuration_list_apps_request.java:[610,52] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/duplication_list_request.java:[99,116] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/duplication_list_request.java:[208,47] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/duplication_list_request.java:[409,58] error: package org.apache.pegasus.utils does not exist
[ERROR] /__w/incubator-pegasus/incubator-pegasus/java-client/src/main/java/org/apache/pegasus/replication/duplication_list_request.java:[482,52] error: package org.apache.pegasus.utils does not exist
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
Error: Process completed with exit code 1.
```","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2188/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2189,https://api.github.com/repos/apache/incubator-pegasus/issues/2189,incubator-pegasus,2825155896,2189,Remove google-analytics.html File,niallkp,8334575,Niall Pemberton,,CLOSED,2025-02-01T11:26:36Z,2025-02-05T15:30:45Z,"Hi Pegasus Team!

The pegasus website is currently included in a report to the _**Privacy Committee**_ for using _**Google Analytics**_. From what I can see it looks like this is not the case, but is probably because of the following file in your website repo:

- https://github.com/apache/incubator-pegasus-website/blob/master/_includes/google-analytics.html

It would be really helpful if you could remove that file so that pegasus doesn't incorrectly get flagged.

Thank You

Niall","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2189/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2189,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6csgxi,incubator-pegasus,2628914274,2189,NA,niallkp,8334575,Niall Pemberton,,NA,2025-02-01T11:29:00Z,2025-02-01T11:29:00Z,See https://github.com/apache/incubator-pegasus-website/pull/91,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6csgxi/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
comment,https://api.github.com/repos/apache/incubator-pegasus/issues/2189,https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6dMUTW,incubator-pegasus,2637251798,2189,NA,niallkp,8334575,Niall Pemberton,,NA,2025-02-05T15:30:44Z,2025-02-05T15:30:44Z,Thank you @acelyc111  - much appreciated,"{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/comments/IC_kwDOAnx6zM6dMUTW/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
issue,https://api.github.com/repos/apache/incubator-pegasus/issues/2190,https://api.github.com/repos/apache/incubator-pegasus/issues/2190,incubator-pegasus,2834686330,2190,The wait_manual_compact() function in the pegasus_manual_compact.sh script cannot correctly detect the execution progress of the manual compact,limowang,64081036,Guangshuo Wang,,OPEN,2025-02-06T06:43:00Z,2025-02-06T06:43:00Z,"## Bug Report

1. What did you do?
When using the command sh pegasus_manual_compact.sh -c {meta_server_list} -a {table_name} --bottommost_level_compaction force, the wait_manual_compact() function keeps writing the log [35s] 0 finished, 8 not finished (0 in queue, 0 in running), estimate remaining unknown seconds. table [**] manual compaction is running now. even though the manual compaction task has already finished. This causes the wait_manual_compact() function's output to be used to determine the task progress when encapsulating the manual_compact tool, leading to a timeout failure.

2. What did you expect to see?
The function wait_manual_compact() can correctly parse the output of the shell command remote_command -t replica-server replica.query-compact ${app_id}.

3. What did you see instead?
Cannot correctly parse.

","{""url"": ""https://api.github.com/repos/apache/incubator-pegasus/issues/2190/reactions"", ""total_count"": 0, ""+1"": 0, ""-1"": 0, ""laugh"": 0, ""hooray"": 0, ""confused"": 0, ""heart"": 0, ""rocket"": 0, ""eyes"": 0}"
