{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset, drop 0 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def clean_csv_files(folder_path):\n",
    "  cleaned_dataframes = {}\n",
    "\n",
    "  # List of columns to drop\n",
    "  columns_to_drop = [\n",
    "      \"status\", \"start_date\", \"end_date\", \"window_start_date\", \"window_end_date\",\n",
    "      \"emails\", \"devs\", \"emails_thread_starter\", \"emails_thread_starter_word_count\",\n",
    "      \"emails_thread_starter_characters\", \"emails_threads\", \"emails_threads_word_count\",\n",
    "      \"emails_threads_characters\", \"emails_no_replies\", \"emails_no_replies_word_count\",\n",
    "      \"emails_no_replies_characters\", \"emails_jira\", \"most_complex_unit_loc\",\n",
    "      \"most_complex_unit_mcabe_index\", \"total_number_of_files\", \"number_of_files_main\",\n",
    "      \"lines_of_code_main\", \"number_of_files_test\", \"lines_of_code_test\",\n",
    "      \"test_vs_main_lines_of_code_percentage\", \"number_of_files_generated\",\n",
    "      \"lines_of_code_generated\", \"number_of_files_build_and_deployment\",\n",
    "      \"lines_of_code_build_and_deployment\", \"negligible_risk_file_size_count\",\n",
    "      \"low_risk_file_size_count\", \"medium_risk_file_size_count\", \"high_risk_file_size_count\",\n",
    "      \"very_high_risk_file_size_count\", \"negligible_risk_file_size_loc\", \"low_risk_file_size_loc\",\n",
    "      \"medium_risk_file_size_loc\", \"high_risk_file_size_loc\", \"very_high_risk_file_size_loc\",\n",
    "      \"number_of_units\", \"lines_of_code_in_units\", \"lines_of_code_outside_units\",\n",
    "      \"unit_size_negligible_risk_loc\", \"unit_size_negligible_risk_count\", \"unit_size_low_risk_loc\",\n",
    "      \"unit_size_low_risk_count\", \"unit_size_medium_risk_loc\", \"unit_size_medium_risk_count\",\n",
    "      \"unit_size_high_risk_loc\", \"unit_size_high_risk_count\", \"unit_size_very_high_risk_loc\",\n",
    "      \"unit_size_very_high_risk_count\", \"conditional_complexity_negligible_risk_loc\",\n",
    "      \"conditional_complexity_negligible_risk_count\", \"conditional_complexity_low_risk_loc\",\n",
    "      \"conditional_complexity_low_risk_count\", \"conditional_complexity_medium_risk_loc\",\n",
    "      \"conditional_complexity_medium_risk_count\", \"conditional_complexity_high_risk_loc\",\n",
    "      \"conditional_complexity_high_risk_count\", \"conditional_complexity_very_high_risk_loc\",\n",
    "      \"conditional_complexity_very_high_risk_count\", \"conditional_complexity_high_plus_risk_count\",\n",
    "      \"conditional_complexity_high_plus_risk_loc\", \"number_of_contributors\",\n",
    "      \"duplication_number_of_duplicates\", \"duplication_number_of_files_with_duplicates\",\n",
    "      \"duplication_number_of_duplicated_lines\", \"duplication_percentage\", \"unit_duplicates_count\", \"releases\"\n",
    "  ]\n",
    "\n",
    "  for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "      file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "      # Load CSV file\n",
    "      df = pd.read_csv(file_path)\n",
    "\n",
    "      # Drop specified columns\n",
    "      df = df.drop(\n",
    "          columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "      key = os.path.splitext(filename)[0]\n",
    "      cleaned_dataframes[key] = df\n",
    "\n",
    "  return cleaned_dataframes\n",
    "\n",
    "folder_path = \"./scraper-output\"\n",
    "cleaned_data = clean_csv_files(folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "for key, df in cleaned_data.items():\n",
    "    # Replace NaN values in numerical columns with 0\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "    # Replace NaN and blank/empty values in 'programming_lang' column with the mode\n",
    "    if 'programming_lang' in df.columns:\n",
    "        # Calculate mode value\n",
    "        mode_value = df['programming_lang'].mode()[0] if not df['programming_lang'].mode().empty else 'Unknown'\n",
    "        \n",
    "        # Replace NaN values with the mode\n",
    "        df['programming_lang'] = df['programming_lang'].fillna(mode_value)\n",
    "        \n",
    "        # Replace blank or whitespace-only values with the mode\n",
    "        df['programming_lang'] = df['programming_lang'].replace(r'^\\s*$', mode_value, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute PCA to rank relevance of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Ranked Features by Average Importance:\n",
      "commits: 0.3743\n",
      "authors: 0.1941\n",
      "committers: 0.1070\n",
      "minor_contributors: 0.0689\n",
      "major_contributors: 0.0564\n",
      "directories: 0.0438\n",
      "top_level_dirs: 0.0367\n",
      "active_days: 0.0298\n",
      "files_modified: 0.0242\n",
      "files_added: 0.0195\n",
      "files_deleted: 0.0156\n",
      "files_renamed: 0.0124\n",
      "added_lines: 0.0094\n",
      "deleted_lines: 0.0072\n",
      "new_contributors: 0.0054\n",
      "avg_files_modified_commit: 0.0040\n",
      "code: 0.0028\n",
      "blanks: 0.0020\n",
      "files: 0.0014\n",
      "comments: 0.0009\n",
      "lines: 0.0005\n",
      "stars: 0.0003\n",
      "forks: 0.0002\n",
      "open_prs: 0.0002\n",
      "closed_prs: 0.0001\n",
      "merged_prs: 0.0000\n",
      "stale_prs: 0.0000\n",
      "deploys: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def perform_pca_on_each(cleaned_data):\n",
    "  feature_importance_list = []\n",
    "\n",
    "  for key, df in cleaned_data.items():\n",
    "    # Exclude 'project', 'measurement_month', and 'programming_lang' columns\n",
    "    features = df.drop(\n",
    "        columns=['project', 'measurement_month', 'programming_lang'], errors='ignore')\n",
    "\n",
    "    # Handle missing values - fill or drop NaNs\n",
    "    features = features.fillna(0)\n",
    "\n",
    "    # Drop columns with zero variance\n",
    "    features = features.loc[:, features.var() > 0]\n",
    "\n",
    "    # Check if there are any numeric features left\n",
    "    numeric_features = features.select_dtypes(include=[np.number])\n",
    "    if numeric_features.empty:\n",
    "      print(\n",
    "          f\"Warning: No numeric features left for PCA in {key}. Skipping PCA.\")\n",
    "      continue\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(numeric_features)\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(scaled_features)\n",
    "\n",
    "    # Collect feature importance\n",
    "    feature_importance = dict(\n",
    "        zip(numeric_features.columns, pca.explained_variance_ratio_))\n",
    "    feature_importance_list.append(feature_importance)\n",
    "\n",
    "  # Compute average importance across all DataFrames\n",
    "  avg_feature_importance = {}\n",
    "  for feature_dict in feature_importance_list:\n",
    "    for feature, importance in feature_dict.items():\n",
    "      if feature not in avg_feature_importance:\n",
    "        avg_feature_importance[feature] = []\n",
    "      avg_feature_importance[feature].append(importance)\n",
    "\n",
    "  # Compute final average\n",
    "  avg_feature_importance = {feature: sum(\n",
    "      values) / len(values) for feature, values in avg_feature_importance.items()}\n",
    "\n",
    "  # Rank features by average importance\n",
    "  ranked_features = sorted(avg_feature_importance.items(),\n",
    "                           key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  # Display ranked features\n",
    "  print(\"Final Ranked Features by Average Importance:\")\n",
    "  for feature, importance in ranked_features:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "\n",
    "# Perform PCA on each DataFrame and compute overall importance\n",
    "perform_pca_on_each(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 154 entries, 0 to 153\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   project  154 non-null    object\n",
      " 1   status   154 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.5+ KB\n"
     ]
    }
   ],
   "source": [
    "status_data = pd.read_csv(\"./project-status.csv\")\n",
    "status_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interaction analysis on most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter out projects with fewer than 10 data points\n",
    "cleaned_data = {project: df for project,\n",
    "                df in cleaned_data.items() if len(df) >= 10}\n",
    "\n",
    "# Filter status_data to only include projects present in cleaned_data\n",
    "status_data_filtered = status_data[status_data['project'].isin(\n",
    "    cleaned_data.keys())]\n",
    "\n",
    "\n",
    "# Merge project status into each dataframe\n",
    "def merge_status(cleaned_data, status_data):\n",
    "  status_dict = status_data.set_index('project')['status'].to_dict()\n",
    "  for project, df in cleaned_data.items():\n",
    "    df['status'] = status_dict.get(project, 'Unknown')\n",
    "  return cleaned_data\n",
    "\n",
    "# Function to extract 1/10th segments and compute averages\n",
    "def extract_average_feature(df, feature, num_bins=10):\n",
    "  df = df.sort_values(by='measurement_month')  # Ensure time is sorted\n",
    "  bin_size = max(1, len(df) // num_bins)  # Determine bin size\n",
    "  averages = [df[feature].iloc[i *\n",
    "                               bin_size: (i + 1) * bin_size].mean() for i in range(num_bins)]\n",
    "  return averages\n",
    "\n",
    "# Function to plot feature trends\n",
    "def plot_feature(cleaned_data, status_data, feature, num_bins=10):\n",
    "  cleaned_data = merge_status(cleaned_data, status_data)\n",
    "\n",
    "  # Collect averaged data for each status\n",
    "  grouped_data = {status: [[] for _ in range(\n",
    "      num_bins)] for status in status_data['status'].unique()}\n",
    "  for project, df in cleaned_data.items():\n",
    "    if feature in df.columns:\n",
    "      status = df['status'].iloc[0]\n",
    "      averages = extract_average_feature(df, feature, num_bins)\n",
    "      for i, avg in enumerate(averages):\n",
    "        grouped_data[status][i].append(avg)\n",
    "  # Compute overall average per bin for each status group\n",
    "  vals[feature] = {}\n",
    "  for status, bins in grouped_data.items():\n",
    "    avg_series = [\n",
    "        np.mean(bin_values) if bin_values else 0 for bin_values in bins]\n",
    "    vals[feature][status] = avg_series\n",
    "\n",
    "vals = {}\n",
    "# Function to plot all features\n",
    "def plot_all_features(cleaned_data, status_data, features, num_bins=10):\n",
    "  for feature in features:\n",
    "    plot_feature(cleaned_data, status_data, feature, num_bins)\n",
    "\n",
    "plot_all_features(cleaned_data, status_data, [\n",
    "    'commits', 'authors', 'committers', 'minor_contributors', 'major_contributors',\n",
    "    'directories', 'top_level_dirs', 'active_days', 'files_modified', 'files_added',\n",
    "    'files_deleted', 'files_renamed', 'added_lines', 'deleted_lines', 'new_contributors',\n",
    "    'avg_files_modified_commit'\n",
    "])\n",
    "cols = [\n",
    "    'commits', 'authors', 'committers', 'minor_contributors', 'major_contributors',\n",
    "    'directories', 'top_level_dirs', 'active_days', 'files_modified', 'files_added',\n",
    "    'files_deleted', 'files_renamed', 'added_lines', 'deleted_lines', 'new_contributors',\n",
    "    'avg_files_modified_commit']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Graduated': [np.float64(3.6469620540321324), np.float64(4.834731567723854), np.float64(5.7134331558558475), np.float64(6.789886546881157), np.float64(7.83981690976823), np.float64(8.287940487997902), np.float64(9.38176788002519), np.float64(9.205150983126684), np.float64(7.821308593205243), np.float64(7.032237664218397)], 'Retired': [np.float64(2.4073766498488065), np.float64(2.9339578059550218), np.float64(2.6154146665428524), np.float64(2.1636765491713015), np.float64(2.24237218292048), np.float64(2.0806747808841406), np.float64(1.8183319551631825), np.float64(1.773077465477337), np.float64(1.4839097504432692), np.float64(1.1302161710520033)]}\n"
     ]
    }
   ],
   "source": [
    "print(vals['authors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea31ce5a299a431889fbc1563fa67ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Y-axis:1', index=1, options=('commits', 'authors', 'committers', 'minor_c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799fdb9c6bbb49a787f5d0f88b67ef3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "# Dropdown widgets\n",
    "y_axis_1 = widgets.Dropdown(options=cols, value='authors', description='Y-axis:1')\n",
    "y_axis_2 = widgets.Dropdown(options=cols, value='commits', description='Y-axis:2')\n",
    "print(y_axis_1.value)\n",
    "# Function to update the plot\n",
    "def update_plot(x_col, y_col):\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 6))\n",
    "    print(y_axis_1.value)\n",
    "    ax1.plot(range(10), vals[y_axis_1.value]['Graduated'], color=\"red\", marker='o', label=y_axis_1.value+'-Graduated')\n",
    "    ax1.plot(range(10), vals[y_axis_1.value]['Retired'], color=\"blue\", marker='o', label=y_axis_1.value+'-Retired')\n",
    "    ax1.set_xlabel('Normalized Time (Bins)')\n",
    "    ax1.set_ylabel(y_axis_1.value, color='black')\n",
    "    ax1.tick_params(axis='y', labelcolor='black')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(range(10), vals[y_axis_2.value]['Graduated'], color=\"green\", marker='o', label=y_axis_2.value+'-Graduated')\n",
    "    ax2.plot(range(10), vals[y_axis_2.value]['Retired'], color=\"purple\", marker='o', label=y_axis_2.value+'-Retired')\n",
    "    ax2.tick_params(axis='y', labelcolor='black')    \n",
    "    ax2.set_ylabel(y_axis_2.value, color='black')\n",
    "    fig.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widget\n",
    "ui = widgets.VBox([y_axis_1, y_axis_2])\n",
    "out = widgets.interactive_output(update_plot, {'x_col': y_axis_1, 'y_col': y_axis_2})\n",
    "\n",
    "# Display widgets and plot\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38efa1785c974a29a514ac0bc82ccd8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Y-axis:1', index=1, options=('commits', 'authors', 'committers', 'minor_c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5141bdf23f74a529a52e1502708b34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dropdown widgets\n",
    "import pandas as pd\n",
    "y_axis_1 = widgets.Dropdown(options=cols, value='authors', description='Y-axis:1')\n",
    "y_axis_2 = widgets.Dropdown(options=cols, value='commits', description='Y-axis:2')\n",
    "print(y_axis_1.value)\n",
    "# Function to update the plot\n",
    "def update_plot(x_col, y_col):\n",
    "    df = pd.DataFrame({\n",
    "        y_axis_1.value+'-Graduated': vals[y_axis_1.value]['Graduated'],\n",
    "        y_axis_1.value+'-Retired': vals[y_axis_1.value]['Retired'],\n",
    "        y_axis_2.value+'-Graduated': vals[y_axis_2.value]['Graduated'],\n",
    "        y_axis_2.value+'-Retired': vals[y_axis_2.value]['Retired'],\n",
    "    })\n",
    "    corr = df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    cax = ax.imshow(corr, cmap='coolwarm', interpolation='nearest')\n",
    "    \n",
    "    # Add colorbar\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    # Set axis labels\n",
    "    ax.set_xticks(np.arange(len(corr.columns)))\n",
    "    ax.set_yticks(np.arange(len(corr.columns)))\n",
    "    ax.set_xticklabels(corr.columns, rotation=45)\n",
    "    ax.set_yticklabels(corr.columns)\n",
    "    \n",
    "    # Display values in cells\n",
    "    for i in range(len(corr.columns)):\n",
    "        for j in range(len(corr.columns)):\n",
    "            ax.text(j, i, f\"{corr.iloc[i, j]:.2f}\", ha='center', va='center', color='black')\n",
    "    \n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widget\n",
    "ui = widgets.VBox([y_axis_1, y_axis_2])\n",
    "out = widgets.interactive_output(update_plot, {'x_col': y_axis_1, 'y_col': y_axis_2})\n",
    "\n",
    "# Display widgets and plot\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6c3053c163497d8c0d526e077a2108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='X-axis:', options=('commits', 'authors', 'committers', 'minor_contributor…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003f472fe37d4b9bb8fe227a4aa392a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dropdown widgets\n",
    "y_axis = widgets.Dropdown(options=cols, value='authors', description='Y-axis:')\n",
    "x_axis = widgets.Dropdown(options=cols, value='commits', description='X-axis:')\n",
    "\n",
    "# Function to update the scatter plot\n",
    "def update_plot(x_col, y_col):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Extract numerical values for x and y axes\n",
    "    x_values = vals[x_col] if isinstance(vals[x_col], list) else list(vals[x_col].values())\n",
    "    y_values = vals[y_col] if isinstance(vals[y_col], list) else list(vals[y_col].values())\n",
    "    \n",
    "    # Scatter plot with selected x and y dimensions\n",
    "    ax.scatter(x_values, y_values, color=\"blue\", label=y_axis.value, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel(x_axis.value)\n",
    "    ax.set_ylabel(y_axis.value, color='black')\n",
    "    ax.tick_params(axis='y', labelcolor='black')\n",
    "    \n",
    "    fig.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widget\n",
    "ui = widgets.VBox([x_axis, y_axis])\n",
    "out = widgets.interactive_output(update_plot, {'x_col': x_axis, 'y_col': y_axis})\n",
    "\n",
    "# Display widgets and plot\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
