{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def clean_csv_files(folder_path):\n",
    "  cleaned_dataframes = {}\n",
    "\n",
    "  # List of additional columns to drop\n",
    "  columns_to_drop = [\n",
    "      \"status\", \"start_date\", \"end_date\", \"window_start_date\", \"window_end_date\",\n",
    "      \"emails\", \"devs\", \"emails_thread_starter\", \"emails_thread_starter_word_count\",\n",
    "      \"emails_thread_starter_characters\", \"emails_threads\", \"emails_threads_word_count\",\n",
    "      \"emails_threads_characters\", \"emails_no_replies\", \"emails_no_replies_word_count\",\n",
    "      \"emails_no_replies_characters\", \"emails_jira\", \"most_complex_unit_loc\",\n",
    "      \"most_complex_unit_mcabe_index\", \"total_number_of_files\", \"number_of_files_main\",\n",
    "      \"lines_of_code_main\", \"number_of_files_test\", \"lines_of_code_test\",\n",
    "      \"test_vs_main_lines_of_code_percentage\", \"number_of_files_generated\",\n",
    "      \"lines_of_code_generated\", \"number_of_files_build_and_deployment\",\n",
    "      \"lines_of_code_build_and_deployment\", \"negligible_risk_file_size_count\",\n",
    "      \"low_risk_file_size_count\", \"medium_risk_file_size_count\", \"high_risk_file_size_count\",\n",
    "      \"very_high_risk_file_size_count\", \"negligible_risk_file_size_loc\", \"low_risk_file_size_loc\",\n",
    "      \"medium_risk_file_size_loc\", \"high_risk_file_size_loc\", \"very_high_risk_file_size_loc\",\n",
    "      \"number_of_units\", \"lines_of_code_in_units\", \"lines_of_code_outside_units\",\n",
    "      \"unit_size_negligible_risk_loc\", \"unit_size_negligible_risk_count\", \"unit_size_low_risk_loc\",\n",
    "      \"unit_size_low_risk_count\", \"unit_size_medium_risk_loc\", \"unit_size_medium_risk_count\",\n",
    "      \"unit_size_high_risk_loc\", \"unit_size_high_risk_count\", \"unit_size_very_high_risk_loc\",\n",
    "      \"unit_size_very_high_risk_count\", \"conditional_complexity_negligible_risk_loc\",\n",
    "      \"conditional_complexity_negligible_risk_count\", \"conditional_complexity_low_risk_loc\",\n",
    "      \"conditional_complexity_low_risk_count\", \"conditional_complexity_medium_risk_loc\",\n",
    "      \"conditional_complexity_medium_risk_count\", \"conditional_complexity_high_risk_loc\",\n",
    "      \"conditional_complexity_high_risk_count\", \"conditional_complexity_very_high_risk_loc\",\n",
    "      \"conditional_complexity_very_high_risk_count\", \"conditional_complexity_high_plus_risk_count\",\n",
    "      \"conditional_complexity_high_plus_risk_loc\", \"number_of_contributors\",\n",
    "      \"duplication_number_of_duplicates\", \"duplication_number_of_files_with_duplicates\",\n",
    "      \"duplication_number_of_duplicated_lines\", \"duplication_percentage\", \"unit_duplicates_count\", \"releases\"\n",
    "  ]\n",
    "\n",
    "  for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "      file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "      # Load CSV file\n",
    "      df = pd.read_csv(file_path)\n",
    "\n",
    "      # Drop specified columns\n",
    "      df = df.drop(\n",
    "          columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "      key = os.path.splitext(filename)[0]\n",
    "      cleaned_dataframes[key] = df\n",
    "\n",
    "  return cleaned_dataframes\n",
    "\n",
    "folder_path = \"scraper-output\"\n",
    "cleaned_data = clean_csv_files(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Ranked Features by Average Importance:\n",
      "commits: 0.3890\n",
      "authors: 0.1939\n",
      "committers: 0.1037\n",
      "minor_contributors: 0.0650\n",
      "major_contributors: 0.0525\n",
      "directories: 0.0422\n",
      "top_level_dirs: 0.0351\n",
      "active_days: 0.0281\n",
      "files_modified: 0.0225\n",
      "files_added: 0.0185\n",
      "files_deleted: 0.0149\n",
      "files_renamed: 0.0120\n",
      "added_lines: 0.0092\n",
      "deleted_lines: 0.0070\n",
      "new_contributors: 0.0054\n",
      "avg_files_modified_commit: 0.0039\n",
      "code: 0.0028\n",
      "blanks: 0.0019\n",
      "files: 0.0014\n",
      "comments: 0.0009\n",
      "lines: 0.0006\n",
      "stars: 0.0003\n",
      "forks: 0.0002\n",
      "open_prs: 0.0002\n",
      "closed_prs: 0.0001\n",
      "merged_prs: 0.0000\n",
      "stale_prs: 0.0000\n",
      "deploys: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def perform_pca_on_each(cleaned_data):\n",
    "  feature_importance_list = []\n",
    "\n",
    "  for key, df in cleaned_data.items():\n",
    "    # Exclude 'project', 'measurement_month', and 'programming_lang' columns\n",
    "    features = df.drop(\n",
    "        columns=['project', 'measurement_month', 'programming_lang'], errors='ignore')\n",
    "\n",
    "    # Handle missing values - fill or drop NaNs\n",
    "    features = features.fillna(0)\n",
    "\n",
    "    # Drop columns with zero variance\n",
    "    features = features.loc[:, features.var() > 0]\n",
    "\n",
    "    # Check if there are any numeric features left\n",
    "    numeric_features = features.select_dtypes(include=[np.number])\n",
    "    if numeric_features.empty:\n",
    "      print(\n",
    "          f\"Warning: No numeric features left for PCA in {key}. Skipping PCA.\")\n",
    "      continue\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(numeric_features)\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(scaled_features)\n",
    "\n",
    "    # Collect feature importance\n",
    "    feature_importance = dict(\n",
    "        zip(numeric_features.columns, pca.explained_variance_ratio_))\n",
    "    feature_importance_list.append(feature_importance)\n",
    "\n",
    "  # Compute average importance across all DataFrames\n",
    "  avg_feature_importance = {}\n",
    "  for feature_dict in feature_importance_list:\n",
    "    for feature, importance in feature_dict.items():\n",
    "      if feature not in avg_feature_importance:\n",
    "        avg_feature_importance[feature] = []\n",
    "      avg_feature_importance[feature].append(importance)\n",
    "\n",
    "  # Compute final average\n",
    "  avg_feature_importance = {feature: sum(\n",
    "      values) / len(values) for feature, values in avg_feature_importance.items()}\n",
    "\n",
    "  # Rank features by average importance\n",
    "  ranked_features = sorted(avg_feature_importance.items(),\n",
    "                           key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  # Display ranked features\n",
    "  print(\"Final Ranked Features by Average Importance:\")\n",
    "  for feature, importance in ranked_features:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "\n",
    "# Perform PCA on each DataFrame and compute overall importance\n",
    "perform_pca_on_each(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_data = pd.read_csv(\"project-status.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
