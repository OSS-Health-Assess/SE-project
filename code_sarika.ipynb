{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset, drop 0 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'scraper-output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m cleaned_dataframes\n\u001b[0;32m     56\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscraper-output\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 57\u001b[0m cleaned_data \u001b[38;5;241m=\u001b[39m \u001b[43mclean_csv_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 40\u001b[0m, in \u001b[0;36mclean_csv_files\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# List of columns to drop\u001b[39;00m\n\u001b[0;32m     11\u001b[0m columns_to_drop \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow_start_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow_end_date\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memails\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memails_thread_starter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memails_thread_starter_word_count\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduplication_number_of_duplicated_lines\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduplication_percentage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munit_duplicates_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreleases\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m ]\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     41\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     42\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, filename)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'scraper-output'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def clean_csv_files(folder_path):\n",
    "  cleaned_dataframes = {}\n",
    "\n",
    "  # List of columns to drop\n",
    "  columns_to_drop = [\n",
    "      \"status\", \"start_date\", \"end_date\", \"window_start_date\", \"window_end_date\",\n",
    "      \"emails\", \"devs\", \"emails_thread_starter\", \"emails_thread_starter_word_count\",\n",
    "      \"emails_thread_starter_characters\", \"emails_threads\", \"emails_threads_word_count\",\n",
    "      \"emails_threads_characters\", \"emails_no_replies\", \"emails_no_replies_word_count\",\n",
    "      \"emails_no_replies_characters\", \"emails_jira\", \"most_complex_unit_loc\",\n",
    "      \"most_complex_unit_mcabe_index\", \"total_number_of_files\", \"number_of_files_main\",\n",
    "      \"lines_of_code_main\", \"number_of_files_test\", \"lines_of_code_test\",\n",
    "      \"test_vs_main_lines_of_code_percentage\", \"number_of_files_generated\",\n",
    "      \"lines_of_code_generated\", \"number_of_files_build_and_deployment\",\n",
    "      \"lines_of_code_build_and_deployment\", \"negligible_risk_file_size_count\",\n",
    "      \"low_risk_file_size_count\", \"medium_risk_file_size_count\", \"high_risk_file_size_count\",\n",
    "      \"very_high_risk_file_size_count\", \"negligible_risk_file_size_loc\", \"low_risk_file_size_loc\",\n",
    "      \"medium_risk_file_size_loc\", \"high_risk_file_size_loc\", \"very_high_risk_file_size_loc\",\n",
    "      \"number_of_units\", \"lines_of_code_in_units\", \"lines_of_code_outside_units\",\n",
    "      \"unit_size_negligible_risk_loc\", \"unit_size_negligible_risk_count\", \"unit_size_low_risk_loc\",\n",
    "      \"unit_size_low_risk_count\", \"unit_size_medium_risk_loc\", \"unit_size_medium_risk_count\",\n",
    "      \"unit_size_high_risk_loc\", \"unit_size_high_risk_count\", \"unit_size_very_high_risk_loc\",\n",
    "      \"unit_size_very_high_risk_count\", \"conditional_complexity_negligible_risk_loc\",\n",
    "      \"conditional_complexity_negligible_risk_count\", \"conditional_complexity_low_risk_loc\",\n",
    "      \"conditional_complexity_low_risk_count\", \"conditional_complexity_medium_risk_loc\",\n",
    "      \"conditional_complexity_medium_risk_count\", \"conditional_complexity_high_risk_loc\",\n",
    "      \"conditional_complexity_high_risk_count\", \"conditional_complexity_very_high_risk_loc\",\n",
    "      \"conditional_complexity_very_high_risk_count\", \"conditional_complexity_high_plus_risk_count\",\n",
    "      \"conditional_complexity_high_plus_risk_loc\", \"number_of_contributors\",\n",
    "      \"duplication_number_of_duplicates\", \"duplication_number_of_files_with_duplicates\",\n",
    "      \"duplication_number_of_duplicated_lines\", \"duplication_percentage\", \"unit_duplicates_count\", \"releases\"\n",
    "  ]\n",
    "\n",
    "  for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "      file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "      # Load CSV file\n",
    "      df = pd.read_csv(file_path)\n",
    "\n",
    "      # Drop specified columns\n",
    "      df = df.drop(\n",
    "          columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "      key = os.path.splitext(filename)[0]\n",
    "      cleaned_dataframes[key] = df\n",
    "\n",
    "  return cleaned_dataframes\n",
    "\n",
    "folder_path = \"scraper-output\"\n",
    "cleaned_data = clean_csv_files(folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute PCA to rank relevance of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Ranked Features by Average Importance:\n",
      "commits: 0.3743\n",
      "authors: 0.1941\n",
      "committers: 0.1070\n",
      "minor_contributors: 0.0689\n",
      "major_contributors: 0.0564\n",
      "directories: 0.0438\n",
      "top_level_dirs: 0.0367\n",
      "active_days: 0.0298\n",
      "files_modified: 0.0242\n",
      "files_added: 0.0195\n",
      "files_deleted: 0.0156\n",
      "files_renamed: 0.0124\n",
      "added_lines: 0.0094\n",
      "deleted_lines: 0.0072\n",
      "new_contributors: 0.0054\n",
      "avg_files_modified_commit: 0.0040\n",
      "code: 0.0028\n",
      "blanks: 0.0020\n",
      "files: 0.0014\n",
      "comments: 0.0009\n",
      "lines: 0.0005\n",
      "stars: 0.0003\n",
      "forks: 0.0002\n",
      "open_prs: 0.0002\n",
      "closed_prs: 0.0001\n",
      "merged_prs: 0.0000\n",
      "stale_prs: 0.0000\n",
      "deploys: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def perform_pca_on_each(cleaned_data):\n",
    "  feature_importance_list = []\n",
    "\n",
    "  for key, df in cleaned_data.items():\n",
    "    # Exclude 'project', 'measurement_month', and 'programming_lang' columns\n",
    "    features = df.drop(\n",
    "        columns=['project', 'measurement_month', 'programming_lang'], errors='ignore')\n",
    "\n",
    "    # Handle missing values - fill or drop NaNs\n",
    "    features = features.fillna(0)\n",
    "\n",
    "    # Drop columns with zero variance\n",
    "    features = features.loc[:, features.var() > 0]\n",
    "\n",
    "    # Check if there are any numeric features left\n",
    "    numeric_features = features.select_dtypes(include=[np.number])\n",
    "    if numeric_features.empty:\n",
    "      print(\n",
    "          f\"Warning: No numeric features left for PCA in {key}. Skipping PCA.\")\n",
    "      continue\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(numeric_features)\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(scaled_features)\n",
    "\n",
    "    # Collect feature importance\n",
    "    feature_importance = dict(\n",
    "        zip(numeric_features.columns, pca.explained_variance_ratio_))\n",
    "    feature_importance_list.append(feature_importance)\n",
    "\n",
    "  # Compute average importance across all DataFrames\n",
    "  avg_feature_importance = {}\n",
    "  for feature_dict in feature_importance_list:\n",
    "    for feature, importance in feature_dict.items():\n",
    "      if feature not in avg_feature_importance:\n",
    "        avg_feature_importance[feature] = []\n",
    "      avg_feature_importance[feature].append(importance)\n",
    "\n",
    "  # Compute final average\n",
    "  avg_feature_importance = {feature: sum(\n",
    "      values) / len(values) for feature, values in avg_feature_importance.items()}\n",
    "\n",
    "  # Rank features by average importance\n",
    "  ranked_features = sorted(avg_feature_importance.items(),\n",
    "                           key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  # Display ranked features\n",
    "  print(\"Final Ranked Features by Average Importance:\")\n",
    "  for feature, importance in ranked_features:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "\n",
    "# Perform PCA on each DataFrame and compute overall importance\n",
    "perform_pca_on_each(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 154 entries, 0 to 153\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   project  154 non-null    object\n",
      " 1   status   154 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.5+ KB\n"
     ]
    }
   ],
   "source": [
    "status_data = pd.read_csv(\"project-status.csv\")\n",
    "status_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series analysis on most important features based on Programming Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter out projects with fewer than 10 data points\n",
    "cleaned_data = {project: df for project, df in cleaned_data.items() if len(df) >= 10}\n",
    "\n",
    "# Function to merge programming language information into each dataframe by computing the mode\n",
    "def merge_programming_lang(cleaned_data):\n",
    "    for project, df in cleaned_data.items():\n",
    "        if 'programming_lang' in df.columns:\n",
    "            mode_val = df['programming_lang'].mode()\n",
    "            if not mode_val.empty:\n",
    "                # Set the mode value as the programming language for the entire dataframe\n",
    "                df['programming_lang'] = mode_val[0]\n",
    "            else:\n",
    "                df['programming_lang'] = 'Unknown'\n",
    "        else:\n",
    "            df['programming_lang'] = 'Unknown'\n",
    "    return cleaned_data\n",
    "\n",
    "# Function to extract 1/10th segments and compute averages\n",
    "def extract_average_feature(df, feature, num_bins=10):\n",
    "    df = df.sort_values(by='measurement_month')  # Ensure time is sorted\n",
    "    bin_size = max(1, len(df) // num_bins)         # Determine bin size\n",
    "    averages = [df[feature].iloc[i * bin_size: (i + 1) * bin_size].mean() for i in range(num_bins)]\n",
    "    return averages\n",
    "\n",
    "# Function to plot feature trends grouped by programming language\n",
    "def plot_feature(cleaned_data, feature, num_bins=10):\n",
    "    # Merge programming language information into each project's dataframe\n",
    "    cleaned_data = merge_programming_lang(cleaned_data)\n",
    "    \n",
    "    # Get the unique programming languages across all projects\n",
    "    languages = {df['programming_lang'].iloc[0] for project, df in cleaned_data.items()}\n",
    "    \n",
    "    # Prepare a dictionary to hold binned values for each programming language\n",
    "    grouped_data = {lang: [[] for _ in range(num_bins)] for lang in languages}\n",
    "    \n",
    "    # Process each project and collect the average values per bin by programming language\n",
    "    for project, df in cleaned_data.items():\n",
    "        if feature in df.columns:\n",
    "            lang = df['programming_lang'].iloc[0]  # Each project’s programming language (mode)\n",
    "            averages = extract_average_feature(df, feature, num_bins)\n",
    "            for i, avg in enumerate(averages):\n",
    "                grouped_data[lang][i].append(avg)\n",
    "    \n",
    "    # Plot the overall average per bin for each programming language\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for lang, bins in grouped_data.items():\n",
    "        avg_series = [np.mean(bin_values) if bin_values else 0 for bin_values in bins]\n",
    "        plt.plot(range(num_bins), avg_series, marker='o', label=lang)\n",
    "    \n",
    "    plt.xlabel('Normalized Time (Bins)')\n",
    "    plt.ylabel(feature)\n",
    "    plt.title(f'Time Series Analysis of {feature} grouped by Programming Language')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot all features\n",
    "def plot_all_features(cleaned_data, features, num_bins=10):\n",
    "    for feature in features:\n",
    "        plot_feature(cleaned_data, feature, num_bins)\n",
    "\n",
    "# Example usage:\n",
    "plot_all_features(cleaned_data, [\n",
    "    'commits', 'authors', 'committers', 'minor_contributors', 'major_contributors', 'new_contributors'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series analysis on most important features based on Project Status and Programming Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter out projects with fewer than 10 data points\n",
    "cleaned_data = {project: df for project, df in cleaned_data.items() if len(df) >= 10}\n",
    "\n",
    "# Filter status_data to only include projects present in cleaned_data\n",
    "status_data_filtered = status_data[status_data['project'].isin(cleaned_data.keys())]\n",
    "\n",
    "# Merge project status into each dataframe\n",
    "def merge_status(cleaned_data, status_data):\n",
    "    status_dict = status_data.set_index('project')['status'].to_dict()\n",
    "    for project, df in cleaned_data.items():\n",
    "        df['status'] = status_dict.get(project, 'Unknown')\n",
    "    return cleaned_data\n",
    "\n",
    "# Merge programming language into each dataframe by computing the mode\n",
    "def merge_language(cleaned_data):\n",
    "    for project, df in cleaned_data.items():\n",
    "        if 'programming_lang' in df.columns:\n",
    "            mode_val = df['programming_lang'].mode()\n",
    "            if not mode_val.empty:\n",
    "                df['programming_lang'] = mode_val[0]\n",
    "            else:\n",
    "                df['programming_lang'] = 'Unknown'\n",
    "        else:\n",
    "            df['programming_lang'] = 'Unknown'\n",
    "    return cleaned_data\n",
    "\n",
    "# Function to extract 1/10th segments and compute averages for a given feature\n",
    "def extract_average_feature(df, feature, num_bins=10):\n",
    "    df = df.sort_values(by='measurement_month')  # Ensure time is sorted\n",
    "    bin_size = max(1, len(df) // num_bins)         # Determine bin size\n",
    "    averages = [df[feature].iloc[i * bin_size: (i + 1) * bin_size].mean() for i in range(num_bins)]\n",
    "    return averages\n",
    "\n",
    "# Function to plot feature trends grouped by status and further categorized by programming language\n",
    "def plot_feature(cleaned_data, status_data, feature, num_bins=10):\n",
    "    # Merge status and programming language info into each dataframe\n",
    "    cleaned_data = merge_status(cleaned_data, status_data)\n",
    "    cleaned_data = merge_language(cleaned_data)\n",
    "    \n",
    "    # Get unique statuses from the cleaned data\n",
    "    statuses = sorted({df['status'].iloc[0] for df in cleaned_data.values()})\n",
    "    \n",
    "    # Create a figure with a separate subplot for each status\n",
    "    num_statuses = len(statuses)\n",
    "    plt.figure(figsize=(10, 6 * num_statuses))\n",
    "    \n",
    "    for i, status in enumerate(statuses, 1):\n",
    "        # Filter projects by current status\n",
    "        projects_for_status = {project: df for project, df in cleaned_data.items() \n",
    "                               if df['status'].iloc[0] == status}\n",
    "        \n",
    "        # Get unique programming languages in this status group\n",
    "        languages = sorted({df['programming_lang'].iloc[0] for df in projects_for_status.values()})\n",
    "        \n",
    "        # Create nested dictionary: for each language, store a list (one per bin) of average values\n",
    "        grouped_data = {lang: [[] for _ in range(num_bins)] for lang in languages}\n",
    "        \n",
    "        # Process each project in the current status group\n",
    "        for project, df in projects_for_status.items():\n",
    "            if feature in df.columns:\n",
    "                lang = df['programming_lang'].iloc[0]\n",
    "                averages = extract_average_feature(df, feature, num_bins)\n",
    "                for j, avg in enumerate(averages):\n",
    "                    grouped_data[lang][j].append(avg)\n",
    "        \n",
    "        # Plot overall average per bin for each programming language within the current status group\n",
    "        ax = plt.subplot(num_statuses, 1, i)\n",
    "        for lang, bins in grouped_data.items():\n",
    "            avg_series = [np.mean(bin_vals) if bin_vals else 0 for bin_vals in bins]\n",
    "            plt.plot(range(num_bins), avg_series, marker='o', label=lang)\n",
    "        plt.xlabel('Normalized Time (Bins)')\n",
    "        plt.ylabel(feature)\n",
    "        plt.title(f'{feature} Trend for Status: {status}')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot all features\n",
    "def plot_all_features(cleaned_data, status_data, features, num_bins=10):\n",
    "    for feature in features:\n",
    "        plot_feature(cleaned_data, status_data, feature, num_bins)\n",
    "\n",
    "# Example usage:\n",
    "plot_all_features(cleaned_data, status_data, [\n",
    "    'commits', 'authors', 'committers', 'minor_contributors', 'major_contributors','new_contributors'\n",
    "])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
