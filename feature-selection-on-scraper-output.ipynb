{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset, drop 0 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def clean_csv_files(folder_path):\n",
    "  cleaned_dataframes = {}\n",
    "\n",
    "  # List of columns to drop\n",
    "  columns_to_drop = [\n",
    "      \"status\", \"start_date\", \"end_date\", \"window_start_date\", \"window_end_date\",\n",
    "      \"emails\", \"devs\", \"emails_thread_starter\", \"emails_thread_starter_word_count\",\n",
    "      \"emails_thread_starter_characters\", \"emails_threads\", \"emails_threads_word_count\",\n",
    "      \"emails_threads_characters\", \"emails_no_replies\", \"emails_no_replies_word_count\",\n",
    "      \"emails_no_replies_characters\", \"emails_jira\", \"most_complex_unit_loc\",\n",
    "      \"most_complex_unit_mcabe_index\", \"total_number_of_files\", \"number_of_files_main\",\n",
    "      \"lines_of_code_main\", \"number_of_files_test\", \"lines_of_code_test\",\n",
    "      \"test_vs_main_lines_of_code_percentage\", \"number_of_files_generated\",\n",
    "      \"lines_of_code_generated\", \"number_of_files_build_and_deployment\",\n",
    "      \"lines_of_code_build_and_deployment\", \"negligible_risk_file_size_count\",\n",
    "      \"low_risk_file_size_count\", \"medium_risk_file_size_count\", \"high_risk_file_size_count\",\n",
    "      \"very_high_risk_file_size_count\", \"negligible_risk_file_size_loc\", \"low_risk_file_size_loc\",\n",
    "      \"medium_risk_file_size_loc\", \"high_risk_file_size_loc\", \"very_high_risk_file_size_loc\",\n",
    "      \"number_of_units\", \"lines_of_code_in_units\", \"lines_of_code_outside_units\",\n",
    "      \"unit_size_negligible_risk_loc\", \"unit_size_negligible_risk_count\", \"unit_size_low_risk_loc\",\n",
    "      \"unit_size_low_risk_count\", \"unit_size_medium_risk_loc\", \"unit_size_medium_risk_count\",\n",
    "      \"unit_size_high_risk_loc\", \"unit_size_high_risk_count\", \"unit_size_very_high_risk_loc\",\n",
    "      \"unit_size_very_high_risk_count\", \"conditional_complexity_negligible_risk_loc\",\n",
    "      \"conditional_complexity_negligible_risk_count\", \"conditional_complexity_low_risk_loc\",\n",
    "      \"conditional_complexity_low_risk_count\", \"conditional_complexity_medium_risk_loc\",\n",
    "      \"conditional_complexity_medium_risk_count\", \"conditional_complexity_high_risk_loc\",\n",
    "      \"conditional_complexity_high_risk_count\", \"conditional_complexity_very_high_risk_loc\",\n",
    "      \"conditional_complexity_very_high_risk_count\", \"conditional_complexity_high_plus_risk_count\",\n",
    "      \"conditional_complexity_high_plus_risk_loc\", \"number_of_contributors\",\n",
    "      \"duplication_number_of_duplicates\", \"duplication_number_of_files_with_duplicates\",\n",
    "      \"duplication_number_of_duplicated_lines\", \"duplication_percentage\", \"unit_duplicates_count\", \"releases\"\n",
    "  ]\n",
    "\n",
    "  for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "      file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "      # Load CSV file\n",
    "      df = pd.read_csv(file_path)\n",
    "\n",
    "      # Drop specified columns\n",
    "      df = df.drop(\n",
    "          columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "      key = os.path.splitext(filename)[0]\n",
    "      cleaned_dataframes[key] = df\n",
    "\n",
    "  return cleaned_dataframes\n",
    "\n",
    "folder_path = \"scraper-output\"\n",
    "cleaned_data = clean_csv_files(folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "for key, df in cleaned_data.items():\n",
    "    # Replace NaN values in numerical columns with 0\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "    # Replace NaN and blank/empty values in 'programming_lang' column with the mode\n",
    "    if 'programming_lang' in df.columns:\n",
    "        # Calculate mode value\n",
    "        mode_value = df['programming_lang'].mode()[0] if not df['programming_lang'].mode().empty else 'Unknown'\n",
    "        \n",
    "        # Replace NaN values with the mode\n",
    "        df['programming_lang'] = df['programming_lang'].fillna(mode_value)\n",
    "        \n",
    "        # Replace blank or whitespace-only values with the mode\n",
    "        df['programming_lang'] = df['programming_lang'].replace(r'^\\s*$', mode_value, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute PCA to rank relevance of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Ranked Features by Average Importance:\n",
      "commits: 0.3743\n",
      "authors: 0.1941\n",
      "committers: 0.1070\n",
      "minor_contributors: 0.0689\n",
      "major_contributors: 0.0564\n",
      "directories: 0.0438\n",
      "top_level_dirs: 0.0367\n",
      "active_days: 0.0298\n",
      "files_modified: 0.0242\n",
      "files_added: 0.0195\n",
      "files_deleted: 0.0156\n",
      "files_renamed: 0.0124\n",
      "added_lines: 0.0094\n",
      "deleted_lines: 0.0072\n",
      "new_contributors: 0.0054\n",
      "avg_files_modified_commit: 0.0040\n",
      "code: 0.0028\n",
      "blanks: 0.0020\n",
      "files: 0.0014\n",
      "comments: 0.0009\n",
      "lines: 0.0005\n",
      "stars: 0.0003\n",
      "forks: 0.0002\n",
      "open_prs: 0.0002\n",
      "closed_prs: 0.0001\n",
      "merged_prs: 0.0000\n",
      "stale_prs: 0.0000\n",
      "deploys: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def perform_pca_on_each(cleaned_data):\n",
    "  feature_importance_list = []\n",
    "\n",
    "  for key, df in cleaned_data.items():\n",
    "    # Exclude 'project', 'measurement_month', and 'programming_lang' columns\n",
    "    features = df.drop(\n",
    "        columns=['project', 'measurement_month', 'programming_lang'], errors='ignore')\n",
    "\n",
    "    # Handle missing values - fill or drop NaNs\n",
    "    features = features.fillna(0)\n",
    "\n",
    "    # Drop columns with zero variance\n",
    "    features = features.loc[:, features.var() > 0]\n",
    "\n",
    "    # Check if there are any numeric features left\n",
    "    numeric_features = features.select_dtypes(include=[np.number])\n",
    "    if numeric_features.empty:\n",
    "      print(\n",
    "          f\"Warning: No numeric features left for PCA in {key}. Skipping PCA.\")\n",
    "      continue\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(numeric_features)\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(scaled_features)\n",
    "\n",
    "    # Collect feature importance\n",
    "    feature_importance = dict(\n",
    "        zip(numeric_features.columns, pca.explained_variance_ratio_))\n",
    "    feature_importance_list.append(feature_importance)\n",
    "\n",
    "  # Compute average importance across all DataFrames\n",
    "  avg_feature_importance = {}\n",
    "  for feature_dict in feature_importance_list:\n",
    "    for feature, importance in feature_dict.items():\n",
    "      if feature not in avg_feature_importance:\n",
    "        avg_feature_importance[feature] = []\n",
    "      avg_feature_importance[feature].append(importance)\n",
    "\n",
    "  # Compute final average\n",
    "  avg_feature_importance = {feature: sum(\n",
    "      values) / len(values) for feature, values in avg_feature_importance.items()}\n",
    "\n",
    "  # Rank features by average importance\n",
    "  ranked_features = sorted(avg_feature_importance.items(),\n",
    "                           key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  # Display ranked features\n",
    "  print(\"Final Ranked Features by Average Importance:\")\n",
    "  for feature, importance in ranked_features:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "\n",
    "# Perform PCA on each DataFrame and compute overall importance\n",
    "perform_pca_on_each(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Features for Kernel: Linear\n",
      "commits: 9.4270\n",
      "authors: 4.9040\n",
      "committers: 2.7017\n",
      "minor_contributors: 1.7820\n",
      "major_contributors: 1.4192\n",
      "\n",
      "Top 5 Features for Kernel: Poly\n",
      "commits: 47.8842\n",
      "authors: 9.1173\n",
      "committers: 4.0361\n",
      "directories: 2.1914\n",
      "minor_contributors: 2.0498\n",
      "\n",
      "Top 5 Features for Kernel: Rbf\n",
      "commits: 0.1688\n",
      "authors: 0.0838\n",
      "committers: 0.0533\n",
      "active_days: 0.0395\n",
      "minor_contributors: 0.0370\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def perform_kernel_pca_on_each(cleaned_data):\n",
    "  # List to hold feature importance for each kernel\n",
    "  feature_importance_dict = {\n",
    "      'linear': [],\n",
    "      'poly': [],\n",
    "      'rbf': [],\n",
    "  }\n",
    "\n",
    "  # You can add more kernels if needed\n",
    "  kernels = ['linear', 'poly', 'rbf']\n",
    "\n",
    "  for key, df in cleaned_data.items():\n",
    "    # Exclude 'project', 'measurement_month', and 'programming_lang' columns\n",
    "    features = df.drop(\n",
    "        columns=['project', 'measurement_month', 'programming_lang'], errors='ignore')\n",
    "\n",
    "    # Handle missing values - fill or drop NaNs\n",
    "    features = features.fillna(0)\n",
    "\n",
    "    # Drop columns with zero variance\n",
    "    features = features.loc[:, features.var() > 0]\n",
    "\n",
    "    # Check if there are any numeric features left\n",
    "    numeric_features = features.select_dtypes(include=[np.number])\n",
    "    if numeric_features.empty:\n",
    "      print(\n",
    "          f\"Warning: No numeric features left for Kernel PCA in {key}. Skipping Kernel PCA.\")\n",
    "      continue\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(numeric_features)\n",
    "\n",
    "    # Perform Kernel PCA for each kernel and collect feature importance\n",
    "    for kernel in kernels:\n",
    "      kpca = KernelPCA(kernel=kernel, n_components=min(\n",
    "          5, len(numeric_features.columns)))\n",
    "      kpca.fit(scaled_features)\n",
    "\n",
    "      # Collect feature importance based on explained variance ratio\n",
    "      explained_variance_ratio = np.var(\n",
    "          kpca.transform(scaled_features), axis=0)\n",
    "      feature_importance = dict(\n",
    "          zip(numeric_features.columns, explained_variance_ratio))\n",
    "\n",
    "      feature_importance_dict[kernel].append(feature_importance)\n",
    "\n",
    "  # For each kernel, compute average feature importance across all dataframes\n",
    "  avg_feature_importance_dict = {}\n",
    "  for kernel in kernels:\n",
    "    avg_feature_importance = {}\n",
    "\n",
    "    # Aggregate feature importance for each kernel\n",
    "    for feature_dict in feature_importance_dict[kernel]:\n",
    "      for feature, importance in feature_dict.items():\n",
    "        if feature not in avg_feature_importance:\n",
    "          avg_feature_importance[feature] = []\n",
    "        avg_feature_importance[feature].append(importance)\n",
    "\n",
    "    # Compute the average importance for each feature\n",
    "    avg_feature_importance = {feature: sum(\n",
    "        values) / len(values) for feature, values in avg_feature_importance.items()}\n",
    "\n",
    "    # Sort features by average importance\n",
    "    ranked_features = sorted(avg_feature_importance.items(),\n",
    "                             key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    avg_feature_importance_dict[kernel] = ranked_features\n",
    "\n",
    "  # Display top 5 ranked features for each kernel\n",
    "  for kernel, ranked_features in avg_feature_importance_dict.items():\n",
    "    print(f\"\\nTop 5 Features for Kernel: {kernel.capitalize()}\")\n",
    "    for feature, importance in ranked_features[:5]:\n",
    "      print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "\n",
    "# Perform Kernel PCA on each DataFrame and compute overall importance\n",
    "perform_kernel_pca_on_each(cleaned_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
